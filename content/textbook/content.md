# Computer Graphics Textbook


### 1.1 Applications of Computer Graphics   3

Supercomputers now allow researchers in many areas to solve previously in-
tractable problems. The ﬁeld of scientiﬁc visualization provides graphical tools that
help these researchers to interpret the vast quantity of data that they generate. In ﬁelds
such as ﬂuid ﬂow, molecular biology, and mathematics, images generated by conver-
sion of data to geometric entities that can be displayed have yielded new insights into
complex processes. For example, Color Plate 19 shows ﬂuid dynamics in the mantle
of the earth. The system used a mathematical model to generate the data. We present
various visualization techniques as examples throughout the rest of the text.

#### 1.1.2 Design

Professions such as engineering and architecture are concerned with design. Starting
with a set of speciﬁcations, engineers and architects seek a cost-effective and esthetic
solution that satisﬁes the speciﬁcations. Design is an iterative process. Rarely in the
real world is a problem speciﬁed such that there is a unique optimal solution. Design
problems are either overdetermined, such that they possess no solution that satisﬁes
all the criteria, much less an optimal solution, or underdetermined, such that they
have multiple solutions that satisfy the design criteria. Thus, the designer works in an
iterative manner. She generates a possible design, tests it, and then uses the results as
the basis for exploring other solutions.
The power of the paradigm of humans interacting with images on the screen
of a CRT was recognized by Ivan Sutherland over 40 years ago. Today, the use of

```cpp
interactive graphical tools in computer-aided design (CAD) pervades ﬁelds such as
```

architecture and the design of mechanical parts and of very-large-scale integrated
(VLSI) circuits. In many such applications, the graphics are used in a number of
distinct ways. For example, in a VLSI design, the graphics provide an interactive

```cpp
interface between the user and the design package, usually by means of such tools
```

as menus and icons. In addition, after the user produces a possible design, other
tools analyze the design and display the analysis graphically. Color Plates 9 and 10
show two views of the same architectural design. Both images were generated with the
same CAD system. They demonstrate the importance of having the tools available to
generate different images of the same objects at different stages of the design process.

#### 1.1.3 Simulation and Animation

Once graphics systems evolved to be capable of generating sophisticated images in
real time, engineers and researchers began to use them as simulators. One of the most
important uses has been in the training of pilots. Graphical ﬂight simulators have
proved both to increase safety and to reduce training expenses. The use of special
VLSI chips has led to a generation of arcade games as sophisticated as ﬂight simula-
tors. Games and educational software for home computers are almost as impressive.
The success of ﬂight simulators led to the use of computer graphics for anima-
tion in the television, motion-picture, and advertising industries. Entire animated
movies can now be made by computer at a cost less than that of movies made with
traditional hand-animation techniques. The use of computer graphics with hand an-
imation allows the creation of technical and artistic effects that are not possible with
either alone. Whereas computer animations have a distinct look, we can also generate
4   Chapter 1   Graphics Systems and Models
photorealistic images by computer. Images that we see on television, in movies, and
in magazines often are so realistic that we cannot distinguish computer-generated
or computer-altered images from photographs. In Chapter 5 we discuss many of the
lighting effects used to produce computer animations. Color Plates 23 and 16 show
realistic lighting effects that were created by artists and computer scientists using an-
imation software. Although these images were created for commercial animations,

```cpp
interactive software to create such effects is widely available, Color Plate 14 shows
```

some of the steps used to create an animation. The images in Color Plates 15 and 16
also are realistic renderings.
The ﬁeld of virtual reality (VR) has opened up many new horizons. A human
viewer can be equipped with a display headset that allows her to see separate images
with her right eye and her left eye so that she has the effect of stereoscopic vision. In
addition, her body location and position, possibly including her head and ﬁnger po-
sitions, are tracked by the computer. She may have other interactive devices available,
including force-sensing gloves and sound. She can then act as part of a computer-
generated scene, limited only by the image-generation ability of the computer. For
example, a surgical intern might be trained to do an operation in this way, or an as-
tronaut might be trained to work in a weightless environment. Color Plate 22 shows
one frame of a VR simulation of a simulated patient used for remote training of med-
ical personnel.
Simulation and virtual reality have come together in many exciting ways in the
ﬁlm industry. Recently, stereo (3D) movies have become both proﬁtable and highly
acclaimed by audiences. Special effects created using computer graphics are part of
virtually all movies, as are more mundane uses of computer graphics such as removal
of artifacts from scenes. Simulations of physics are used to create visual effects ranging
from ﬂuid ﬂow to crowd dynamics.

#### 1.1.4 User Interfaces

Our interaction with computers has become dominated by a visual paradigm that in-
cludes windows, icons, menus, and a pointing device, such as a mouse. From a user’s
perspective, windowing systems such as the X Window System, Microsoft Windows,
and the Macintosh Operating System differ only in details. More recently, millions of
people have become users of the Internet. Their access is through graphical network
browsers, such as Firefox, Chrome, Safari, and Internet Explorer, that use these same

```cpp
interface tools. We have become so accustomed to this style of interface that we often
```

forget that what we are doing is working with computer graphics.
Although we are familiar with the style of graphical user interface used on most
workstations,1 advances in computer graphics have made possible other forms of in-
1. Although personal computers and workstations evolved by somewhat different paths, at present,
there is virtually no fundamental difference between them. Hence, we shall use the terms personal
computer and workstation synonymously.
                                                                                 1.2 A Graphics System   5
terfaces. Color Plate 13 shows the interface used with a high-level modeling package.
It demonstrates the variety both of the tools available in such packages and of the

```cpp
interactive devices the user can employ in modeling geometric objects.
```


### 1.2     A GRAPHICS SYSTEM

A computer graphics system is a computer system; as such, it must have all the
components of a general-purpose computer system. Let us start with the high-level
view of a graphics system, as shown in the block diagram in Figure 1.1. There are six
major elements in our system:
1. Input devices
2. Central Processing Unit
3. Graphics Processing Unit
4. Memory
5. Frame buffer
6. Output devices
This model is general enough to include workstations and personal computers,

```cpp
interactive game systems, mobile phones, GPS systems, and sophisticated image-
```

generation systems. Although most of the components are present in a standard
computer, it is the way each element is specialized for computer graphics that char-
acterizes this diagram as a portrait of a graphics system.

#### 1.2.1 Pixels and the Frame Buffer

Virtually all modern graphics systems are raster based. The image we see on the out-
put device is an array—the raster—of picture elements, or pixels, produced by the
graphics system. As we can see from Figure 1.2, each pixel corresponds to a location,
processor      processor          buffer

*FIGURE 1.1 A graphics system.*

6   Chapter 1   Graphics Systems and Models

*FIGURE 1.2 Pixels. (a) Image of Yeti the cat. (b) Detail of area around*

one eye showing individual pixels.
or small area, in the image. Collectively, the pixels are stored in a part of mem-
ory called the frame buffer. The frame buffer can be viewed as the core element
of a graphics system. Its resolution—the number of pixels in the frame buffer—
determines the detail that you can see in the image. The depth, or precision, of the
frame buffer, deﬁned as the number of bits that are used for each pixel, determines
properties such as how many colors can be represented on a given system. For exam-
ple, a 1-bit-deep frame buffer allows only two colors, whereas an 8-bit-deep frame
buffer allows 28 (256) colors. In full-color systems, there are 24 (or more) bits per
pixel. Such systems can display sufﬁcient colors to represent most images realistically.
They are also called true-color systems, or RGB-color systems, because individual
groups of bits in each pixel are assigned to each of the three primary colors—red,
green, and blue—used in most displays. High dynamic range (HDR) systems use 12
or more bits for each color component. Until recently, frame buffers stored colors in

```cpp
integer formats. Recent frame buffers use ﬂoating point and thus support HDR colors
```

more easily.
In a very simple system, the frame buffer holds only the colored pixels that are
displayed on the screen. In most systems, the frame buffer holds far more informa-
tion, such as depth information needed for creating images from three-dimensional
data. In these systems, the frame buffer comprises multiple buffers, one or more of
which are color buffers that hold the colored pixels that are displayed. For now, we
can use the terms frame buffer and color buffer synonymously without confusion.

#### 1.2.2 The CPU and the GPU

In a simple system, there may be only one processor, the central processing unit
(CPU) of the system, which must do both the normal processing and the graphi-
cal processing. The main graphical function of the processor is to take speciﬁcations
                                                                                    1.2 A Graphics System   7
of graphical primitives (such as lines, circles, and polygons) generated by application
programs and to assign values to the pixels in the frame buffer that best represent
these entities. For example, a triangle is speciﬁed by its three vertices, but to display
its outline by the three line segments connecting the vertices, the graphics system
must generate a set of pixels that appear as line segments to the viewer. The conver-
sion of geometric entities to pixel colors and locations in the frame buffer is known
as rasterization, or scan conversion. In early graphics systems, the frame buffer was
part of the standard memory that could be directly addressed by the CPU. Today, vir-
tually all graphics systems are characterized by special-purpose graphics processing
units (GPUs), custom-tailored to carry out speciﬁc graphics functions. The GPU can
be either on the mother board of the system or on a graphics card. The frame buffer
is accessed through the graphics processing unit and usually is on the same circuit
board as the GPU.
GPUs have evolved to where they are as complex or even more complex than
CPUs. They are characterized by both special-purpose modules geared toward graph-
ical operations and a high degree of parallelism—recent GPUs contain over 100 pro-
cessing units, each of which is user programmable. GPUs are so powerful that they
can often be used as mini supercomputers for general purpose computing. We will
discuss GPU architectures in more detail in Section 1.7.

#### 1.2.3 Output Devices

Until recently, the dominant type of display (or monitor) was the cathode-ray tube
(CRT). A simpliﬁed picture of a CRT is shown in Figure 1.3. When electrons strike the
phosphor coating on the tube, light is emitted. The direction of the beam is controlled
by two pairs of deﬂection plates. The output of the computer is converted, by digital-
to-analog converters, to voltages across the x and y deﬂection plates. Light appears
on the surface of the CRT when a sufﬁciently intense beam of electrons is directed at
the phosphor.
Electron gun                           x deflect         Phosphor

*FIGURE 1.3 The cathode-ray tube (CRT).*

8   Chapter 1   Graphics Systems and Models
If the voltages steering the beam change at a constant rate, the beam will trace
a straight line, visible to a viewer. Such a device is known as the random-scan,
calligraphic, or vector CRT, because the beam can be moved directly from any
position to any other position. If intensity of the beam is turned off, the beam can
be moved to a new position without changing any visible display. This conﬁguration
was the basis of early graphics systems that predated the present raster technology.
A typical CRT will emit light for only a short time—usually, a few milliseconds—
after the phosphor is excited by the electron beam. For a human to see a steady,
ﬂicker-free image on most CRT displays, the same path must be retraced, or re-
freshed, by the beam at a sufﬁciently high rate, the refresh rate. In older systems,
the refresh rate is determined by the frequency of the power system, 60 cycles per sec-
ond or 60 Hertz (Hz) in the United States and 50 Hz in much of the rest of the world.
Modern displays are no longer coupled to these low frequencies and operate at rates
up to about 85 Hz.
In a raster system, the graphics system takes pixels from the frame buffer and
displays them as points on the surface of the display in one of two fundamental
ways. In a noninterlaced system, the pixels are displayed row by row, or scan line
by scan line, at the refresh rate. In an interlaced display, odd rows and even rows
are refreshed alternately. Interlaced displays are used in commercial television. In an

```cpp
interlaced display operating at 60 Hz, the screen is redrawn in its entirety only 30
```

times per second, although the visual system is tricked into thinking the refresh rate
is 60 Hz rather than 30 Hz. Viewers located near the screen, however, can tell the
difference between the interlaced and noninterlaced displays. Noninterlaced displays
are becoming more widespread, even though these displays process pixels at twice the
rate of the interlaced display.
Color CRTs have three different colored phosphors (red, green, and blue), ar-
ranged in small groups. One common style arranges the phosphors in triangular
groups called triads, each triad consisting of three phosphors, one of each primary.
Most color CRTs have three electron beams, corresponding to the three types of phos-
phors. In the shadow-mask CRT (Figure 1.4), a metal screen with small holes—the
shadow mask—ensures that an electron beam excites only phosphors of the proper
color.
Although CRTs are still common display devices, they are rapidly being replaced
by ﬂat-screen technologies. Flat-panel monitors are inherently raster based. Although
there are multiple technologies available, including light-emitting diodes (LEDs),
liquid-crystal displays (LCDs), and plasma panels, all use a two-dimensional grid
to address individual light-emitting elements. Figure 1.5 shows a generic ﬂat-panel
monitor. The two outside plates each contain parallel grids of wires that are oriented
perpendicular to each other. By sending electrical signals to the proper wire in each
grid, the electrical ﬁeld at a location, determined by the intersection of two wires, can
be made strong enough to control the corresponding element in the middle plate.
The middle plate in an LED panel contains light-emitting diodes that can be turned
on and off by the electrical signals sent to the grid. In an LCD display, the electrical
ﬁeld controls the polarization of the liquid crystals in the middle panel, thus turning
on and off the light passing through the panel. A plasma panel uses the voltages on
                                                                                 1.2 A Graphics System   9
Blue gun                                 Triad
Green gun                                              Blue

*FIGURE 1.4 Shadow-mask CRT.*


*FIGURE 1.5 Generic flat-panel display.*

the grids to energize gases embedded between the glass panels holding the grids. The
energized gas becomes a glowing plasma.
Most projection systems are also raster devices. These systems use a variety of
technologies, including CRTs and digital light projection (DLP). From a user perspec-
tive, they act as standard monitors with similar resolutions and precisions. Hard-copy
devices, such as printers and plotters, are also raster based but cannot be refreshed.

#### 1.2.4 Input Devices

Most graphics systems provide a keyboard and at least one other input device. The
most common input devices are the mouse, the joystick, and the data tablet. Each
provides positional information to the system, and each usually is equipped with one
or more buttons to provide signals to the processor. Often called pointing devices,
these devices allow a user to indicate a particular location on the display.
Modern systems, such as game consoles, provide a much richer set of input
devices, with new devices appearing almost weekly. In addition, there are devices
which provide three- (and more) dimensional input. Consequently, we want to pro-
vide a ﬂexible model for incorporating the input from such devices into our graphics
programs.
10   Chapter 1   Graphics Systems and Models
We can think about input devices in two distinct ways. The obvious one is to look
at them as physical devices, such as a keyboard or a mouse, and to discuss how they
work. Certainly, we need to know something about the physical properties of our in-
put devices, so such a discussion is necessary if we are to obtain a full understanding
of input. However, from the perspective of an application programmer, we should not
need to know the details of a particular physical device to write an application pro-
gram. Rather, we prefer to treat input devices as logical devices whose properties are
speciﬁed in terms of what they do from the perspective of the application program. A
logical device is characterized by its high-level interface with the user program rather
than by its physical characteristics. Logical devices are familiar to all writers of high-
level programs. For example, data input and output in C are done through functions
such as printf, scanf, getchar, and putchar, whose arguments use the standard
C data types, and through input (cin) and output (cout) streams in C++. When we
output a string using printf, the physical device on which the output appears could
be a printer, a terminal, or a disk ﬁle. This output could even be the input to another
program. The details of the format required by the destination device are of minor
concern to the writer of the application program.
In computer graphics, the use of logical devices is slightly more complex because
the forms that input can take are more varied than the strings of bits or characters
to which we are usually restricted in nongraphical applications. For example, we can
use the mouse—a physical device—either to select a location on the screen of our
CRT or to indicate which item in a menu we wish to select. In the ﬁrst case, an x, y
pair (in some coordinate system) is returned to the user program; in the second, the
application program may receive an integer as the identiﬁer of an entry in the menu.
The separation of physical from logical devices allows us to use the same physical
devices in multiple markedly different logical ways. It also allows the same program
to work, without modiﬁcation, if the mouse is replaced by another physical device,
such as a data tablet or trackball.

#### 1.2.5 Physical Input Devices

From the physical perspective, each input device has properties that make it more
suitable for certain tasks than for others. We take the view used in most of the work-
station literature that there are two primary types of physical devices: pointing devices
and keyboard devices. The pointing device allows the user to indicate a position on
the screen and almost always incorporates one or more buttons to allow the user to
send signals or interrupts to the computer. The keyboard device is almost always a
physical keyboard but can be generalized to include any device that returns character
codes. We use the American Standard Code for Information Interchange (ASCII) in
our examples. ASCII assigns a single unsigned byte to each character. Nothing we do
restricts us to this particular choice, other than that ASCII is the prevailing code used.
Note, however, that other codes, especially those used for Internet applications, use
multiple bytes for each character, thus allowing for a much richer set of supported

```cpp
characters.
```

                                                                                     1.2 A Graphics System             11
The mouse (Figure 1.6) and trackball (Figure 1.7) are similar in use and often
in construction as well. A typical mechanical mouse when turned over looks like a
trackball. In both devices, the motion of the ball is converted to signals sent back to
the computer by pairs of encoders inside the device that are turned by the motion of
the ball. The encoders measure motion in two orthogonal directions.
There are many variants of these devices. Some use optical detectors rather than
mechanical detectors to measure motion. Small trackballs are popular with portable
computers because they can be incorporated directly into the keyboard. There are
also various pressure-sensitive devices used in keyboards that perform similar func-           FIGURE 1.6 Mouse.
tions to the mouse and trackball but that do not move; their encoders measure the
pressure exerted on a small knob that often is located between two keys in the middle
of the keyboard.
We can view the output of the mouse or trackball as two independent values
provided by the device. These values can be considered as positions and converted—
either within the graphics system or by the user program—to a two-dimensional
location in a convenient coordinate system. If it is conﬁgured in this manner, we can
use the device to position a marker (cursor) automatically on the display; however,
we rarely use these devices in this direct manner.                                             FIGURE 1.7 Trackball.
It is not necessary that the output of the mouse or trackball encoders be inter-
preted as a position. Instead, either the device driver or a user program can interpret
the information from the encoder as two independent velocities. The computer can
then integrate these values to obtain a two-dimensional position. Thus, as a mouse
moves across a surface, the integrals of the velocities yield x, y values that can be con-
verted to indicate the position for a cursor on the screen, as shown in Figure 1.8.
By interpreting the distance traveled by the ball as a velocity, we can use the device
as a variable-sensitivity input device. Small deviations from rest cause slow or small
changes; large deviations cause rapid large changes. With either device, if the ball does
not rotate, then there is no change in the integrals and a cursor tracking the posi-
tion of the mouse will not move. In this mode, these devices are relative-positioning
devices because changes in the position of the ball yield a position in the user pro-
gram; the absolute location of the ball (or the mouse) is not used by the application
program.
Relative positioning, as provided by a mouse or trackball, is not always desirable.
In particular, these devices are not suitable for an operation such as tracing a diagram.
If, while the user is attempting to follow a curve on the screen with a mouse, she
lifts and moves the mouse, the absolute position on the curve being traced is lost.



*FIGURE 1.8 Cursor positioning.*

12           Chapter 1   Graphics Systems and Models
Data tablets provide absolute positioning. A typical data tablet (Figure 1.9) has rows
and columns of wires embedded under its surface. The position of the stylus is
determined through electromagnetic interactions between signals traveling through
the wires and sensors in the stylus. Touch-sensitive transparent screens that can be
placed over the face of a CRT have many of the same properties as the data tablet.

*FIGURE 1.9 Data tablet.       Small, rectangular, pressure-sensitive touchpads are embedded in the keyboards of*

many portable computers. These touchpads can be conﬁgured as either relative- or
absolute-positioning devices.
One other device, the joystick (Figure 1.10), is particularly worthy of mention.
The motion of the stick in two orthogonal directions is encoded, interpreted as two
velocities, and integrated to identify a screen location. The integration implies that if
the stick is left in its resting position, there is no change in the cursor position and that
the farther the stick is moved from its resting position, the faster the screen location
changes. Thus, the joystick is a variable-sensitivity device. The other advantage of
the joystick is that the device can be constructed with mechanical elements, such as

*FIGURE 1.10 Joystick.*

springs and dampers, that give resistance to a user who is pushing the stick. Such a
mechanical feel, which is not possible with the other devices, makes the joystick well
suited for applications such as ﬂight simulators and games.
For three-dimensional graphics, we might prefer to use three-dimensional in-
put devices. Although various such devices are available, none have yet won the
widespread acceptance of the popular two-dimensional input devices. A spaceball
looks like a joystick with a ball on the end of the stick (Figure 1.11); however, the
stick does not move. Rather, pressure sensors in the ball measure the forces applied
by the user. The spaceball can measure not only the three direct forces (up–down,
front–back, left–right) but also three independent twists. The device measures six in-

*FIGURE 1.11 Spaceball.        dependent values and thus has six degrees of freedom. Such an input device could be*

used, for example, both to position and to orient a camera.
The Nintendo Wiimote provides three-dimensional position and orientation of
a hand-held device by sending infrared light to the device, which then sends back
what it measures wirelessly to the host computer.
Other three-dimensional devices, such as laser-based structured-lighting systems
and laser-ranging systems, measure three-dimensional positions. Numerous tracking
systems used in virtual reality applications sense the position of the user. Virtual
reality and robotics applications often need more degrees of freedom than the 2 to
6 provided by the devices that we have described. Devices such as data gloves can
sense motion of various parts of the human body, thus providing many additional
input signals.

#### 1.2.6 Logical Devices

We can now return to looking at input from inside the application program—that is,
from the logical point of view. Two major characteristics describe the logical behavior
of an input device: (1) the measurements that the device returns to the user program
and (2) the time when the device returns those measurements.
                                                                                    1.2 A Graphics System   13
The logical string device is the same as the use of character input through scanf
or cin. A physical keyboard will return a string of characters to an application pro-
gram; the same string might be provided from a ﬁle or the user may see a keyboard
displayed on the output and use the pointing device to generate the string of charac-
ters. Logically, all three methods are examples of a string device, and application code
for using such input can be the same regardless of which physical device is used.
The physical pointing device can be used in a variety of logical ways. As a locator
it can provide a position to the application in either a device-independent coordinate
system, such as world coordinates, as in OpenGL, or in screen coordinates, which the
application can then transform to another coordinate system. A logical pick device
returns the identiﬁer of an object on the display to the application program. It is
usually implemented with the same physical device as a locator but has a separate
software interface to the user program.
A widget is a graphical interactive device, provided by either the window system
or a toolkit. Typical widgets include menus, scrollbars, and graphical buttons. Most
widgets are implemented as special types of windows. Widgets can be used to provide
additional types of logical devices. For example, a menu provides one of a number of
choices as may a row of graphical buttons. A logical valuator provides analog input
to the user program, usually through a widget such as a slidebar, although the same
logical input could be provided by a user typing numbers into a physical keyboard.

#### 1.2.7 Input Modes

Besides the variety of types of input that characterize computer graphics, how the
input is provided to the application is more varied than with simple C and C++
programs that use only a keyboard. The manner by which physical and logical input
devices provide input to an application program can be described in terms of two
entities: a measure process and a device trigger. The measure of a device is what the
device returns to the user program. The trigger of a device is a physical input on
the device with which the user can signal the computer. For example, the measure of
a keyboard contains a string, and the trigger can be the Return or Enter key. For a
locator, the measure includes the position, and the associated trigger can be a button
on the pointing device.
We can obtain the measure of a device in three distinct modes. Each mode is
deﬁned by the relationship between the measure process and the trigger. Once the
measure process is started, the measure is taken and placed in a buffer, even though
the contents of the buffer may not yet be available to the program. For example,
the position of a mouse is tracked continuously by the underlying window system,
regardless of whether the application program needs mouse input.
In request mode, the measure of the device is not returned to the program until
the device is triggered. This input mode is standard in nongraphical applications. For
example, if a typical C program requires character input, we use a function such as
scanf. When the program needs the input, it halts when it encounters the scanf
statement and waits while we type characters at our terminal. We can backspace
14   Chapter 1   Graphics Systems and Models
to correct our typing, and we can take as long as we like. The data are placed in a
keyboard buffer, whose contents are returned to our program only after a particular
key, such as the Enter key (the trigger), is depressed. For a logical device, such as a
locator, we can move our pointing device to the desired location and then trigger
the device with its button; the trigger will cause the location to be returned to the
application program.
Sample-mode input is immediate. As soon as the function call in the application
program is encountered, the measure is returned. In sample mode, the user must have
positioned the pointing device or entered data using the keyboard before the function
call, because the measure is extracted immediately from the buffer.
One characteristic of both request- and sample-mode input in APIs that support
them is that the user must identify which device is to provide the input. Consequently,
we ignore any other information that becomes available from any input device other
than the one speciﬁed. Both request and sample modes are useful for situations where
the program guides the user, but they are not useful in applications where the user
controls the ﬂow of the program. For example, a ﬂight simulator or computer game
might have multiple input devices—such as a joystick, dials, buttons, and switches—
most of which can be used at any time. Writing programs to control the simulator
with only sample- and request-mode input is nearly impossible, because we do not
know what devices the pilot will use at any point in the simulation. More generally,
sample- and request-mode input are not sufﬁcient for handling the variety of possible
human–computer interactions that arise in a modern computing environment.
Our third mode, event mode, can handle these other interactions. Suppose that
we are in an environment with multiple input devices, each with its own trigger
and each running a measure process. Each time that a device is triggered, an event
is generated. The device measure, including the identiﬁer for the device, is placed
in an event queue. This process of placing events in the event queue is completely
independent of what the application program does with these events. One way that
the application program can work with events is shown in Figure 1.12. The user
program can examine the front event in the queue or, if the queue is empty, can wait
for an event to occur. If there is an event in the queue, the program can look at the
event’s type and then decide what to do.
Another approach is to associate a function called a callback with a speciﬁc type
of event. From the perspective of the window system, the operating system queries or
polls the event queue regularly and executes the callbacks corresponding to events in
the queue. We take this approach because it is the one currently used with the major
window systems and has proved effective in client–server environments.
Trigger                Measure                  Event
process                process                  queue
Trigger                Measure                  Event

*FIGURE 1.12 Event-mode model.*

                                                                      1.3 Images: Physical and Synthetic   15

### 1.3    IMAGES: PHYSICAL AND SYNTHETIC

For many years, the pedagogical approach to teaching computer graphics started with
how to construct raster images of simple two-dimensional geometric entities (for
example, points, line segments, and polygons) in the frame buffer. Next, most text-
books discussed how to deﬁne two- and three-dimensional mathematical objects in
the computer and image them with the set of two-dimensional rasterized primitives.
This approach worked well for creating simple images of simple objects. In mod-
ern systems, however, we want to exploit the capabilities of the software and hardware
to create realistic images of computer-generated three-dimensional objects—a task
that involves many aspects of image formation, such as lighting, shading, and prop-
erties of materials. Because such functionality is supported directly by most present
computer graphics systems, we prefer to set the stage for creating these images here,
rather than to expand a limited model later.
Computer-generated images are synthetic or artiﬁcial, in the sense that the ob-
jects being imaged do not exist physically. In this chapter, we argue that the preferred
method to form computer-generated images is similar to traditional imaging meth-
ods, such as cameras and the human visual system. Hence, before we discuss the
mechanics of writing programs to generate images, we discuss the way images are
formed by optical systems. We construct a model of the image-formation process that
we can then use to understand and develop computer-generated imaging systems.
In this chapter, we make minimal use of mathematics. We want to establish a par-
adigm for creating images and to present a computer architecture for implementing
that paradigm. Details are presented in subsequent chapters, where we shall derive
the relevant equations.

#### 1.3.1 Objects and Viewers

We live in a world of three-dimensional objects. The development of many branches
of mathematics, including geometry and trigonometry, was in response to the de-
sire to systematize conceptually simple ideas, such as the measurement of size of
objects and distance between objects. Often, we seek to represent our understand-
ing of such spatial relationships with pictures or images, such as maps, paintings,
and photographs. Likewise, the development of many physical devices—including
cameras, microscopes, and telescopes—was tied to the desire to visualize spatial re-
lationships among objects. Hence, there always has been a fundamental link between
the physics and the mathematics of image formation—one that we can exploit in our
development of computer image formation.
Two basic entities must be part of any image-formation process, be it mathe-
matical or physical: object and viewer. The object exists in space independent of any
image-formation process and of any viewer. In computer graphics, where we deal
with synthetic objects, we form objects by specifying the positions in space of various
geometric primitives, such as points, lines, and polygons. In most graphics systems,
a set of locations in space, or of vertices, is sufﬁcient to deﬁne, or approximate, most
16         Chapter 1   Graphics Systems and Models
(a)                      (b)                    (c)

*FIGURE 1.13 Image seen by three different viewers. (a) A’s view. (b) B’s*

view. (c) C’s view.
objects. For example, a line can be speciﬁed by two vertices; a polygon can be spec-
iﬁed by an ordered list of vertices; and a sphere can be speciﬁed by two vertices that
specify its center and any point on its circumference. One of the main functions of
a CAD system is to provide an interface that makes it easy for a user to build a syn-
thetic model of the world. In Chapter 2, we show how OpenGL allows us to build
simple objects; in Chapter 8, we learn to deﬁne objects in a manner that incorporates
relationships among objects.
Every imaging system must provide a means of forming images from objects.
To form an image, we must have someone or something that is viewing our objects,
be it a human, a camera, or a digitizer. It is the viewer that forms the image of our
objects. In the human visual system, the image is formed on the back of the eye. In a
camera, the image is formed in the ﬁlm plane. It is easy to confuse images and objects.
We usually see an object from our single perspective and forget that other viewers,
located in other places, will see the same object differently. Figure 1.13(a) shows two
viewers observing the same building. This image is what is seen by an observer A
who is far enough away from the building to see both the building and the two other
viewers, B and C. From A’s perspective, B and C appear as objects, just as the building
does. Figures 1.13(b) and (c) show the images seen by B and C, respectively. All three
images contain the same building, but the image of the building is different in all
three.

*FIGURE 1.14 Camera system.        Figure 1.14 shows a camera system viewing a building. Here we can observe that*

both the object and the viewer exist in a three-dimensional world. However, the im-
age that they deﬁne—what we ﬁnd on the projection plane—is two-dimensional. The
process by which the speciﬁcation of the object is combined with the speciﬁcation of
the viewer to produce a two-dimensional image is the essence of image formation,
and we shall study it in detail.

#### 1.3.2 Light and Images

The preceding description of image formation is far from complete. For example, we
have yet to mention light. If there were no light sources, the objects would be dark,
                                                                                  1.3 Images: Physical and Synthetic   17

*FIGURE 1.15 A camera system with an object and a light source.*

and there would be nothing visible in our image. Nor have we indicated how color
enters the picture or what the effects of the surface properties of the objects are.
Taking a more physical approach, we can start with the arrangement in Fig-
ure 1.15, which shows a simple physical imaging system. Again, we see a physical
object and a viewer (the camera); now, however, there is a light source in the scene.
Light from the source strikes various surfaces of the object, and a portion of the re-
ﬂected light enters the camera through the lens. The details of the interaction between
light and the surfaces of the object determine how much light enters the camera.
Light is a form of electromagnetic radiation. Taking the classical view, we look
at electromagnetic energy travels as waves2 that can be characterized by either their
wavelengths or their frequencies.3 The electromagnetic spectrum (Figure 1.16) in-
cludes radio waves, infrared (heat), and a portion that causes a response in our visual
systems. This visible spectrum, which has wavelengths in the range of 350 to 780
nanometers (nm), is called (visible) light. A given light source has a color determined
by the energy that it emits at various wavelengths. Wavelengths in the middle of the
range, around 520 nm, are seen as green; those near 450 nm are seen as blue; and
those near 650 nm are seen as red. Just as with a rainbow, light at wavelengths be-
tween red and green, we see as yellow, and wavelengths shorter than blue generate
violet light.
Light sources can emit light either as a set of discrete frequencies or continuously.
A laser, for example, emits light at a single frequency, whereas an incandescent lamp
emits energy over a range of frequencies. Fortunately, in computer graphics, except
for recognizing that distinct frequencies are visible as distinct colors, we rarely need
to deal with the physical properties of light.
2. In Chaper 11, we will introduce photon mapping that is based on light being emitted in discrete
packets.
3. The relationship between frequency (f ) and wavelength (λ) is f λ = c, where c is the speed of light.
18   Chapter 1   Graphics Systems and Models
X rays                         Light                          Radio
 (nm)
350           (nm)          780

*FIGURE 1.16 The electromagnetic spectrum.*

Instead, we can follow a more traditional path that is correct when we are operat-
ing with sufﬁciently high light levels and at a scale where the wave nature of light is not
a signiﬁcant factor. Geometric optics models light sources as emitters of light energy,
each of which have a ﬁxed intensity. Modeled geometrically, light travels in straight
lines, from the sources to those objects with which it interacts. An ideal point source
emits energy from a single location at one or more frequencies equally in all direc-
tions. More complex sources, such as a light bulb, can be characterized as emitting
light over an area and by emitting more light in one direction than another. A partic-
ular source is characterized by the intensity of light that it emits at each frequency and
by that light’s directionality. We consider only point sources for now. More complex
sources often can be approximated by a number of carefully placed point sources.
Modeling of light sources is discussed in Chapter 5.

#### 1.3.3 Imaging Models

There are multiple approaches to how we can form images from a set of objects,
the light-reﬂecting properties of these objects, and the properties of the light sources
in the scene. In this section, we introduce two physical approaches. Although these
approaches are not suitable for the real-time graphics that we ultimately want, they
will give us some insight into how we can build a useful imaging architecture. We
return to these approaches in Chapter 11.
We can start building an imaging model by following light from a source. Con-
sider the scene in Figure 1.17; it is illuminated by a single point source. We include
the viewer in the ﬁgure because we are interested in the light that reaches her eye.
The viewer can also be a camera, as shown in Figure 1.18. A ray is a semi-inﬁnite line
that emanates from a point and travels to inﬁnity in a particular direction. Because
light travels in straight lines, we can think in terms of rays of light emanating in all
directions from our point source. A portion of these inﬁnite rays contributes to the
image on the ﬁlm plane of our camera. For example, if the source is visible from the
camera, some of the rays go directly from the source through the lens of the camera
and strike the ﬁlm plane. Most rays, however, go off to inﬁnity, neither entering the
camera directly nor striking any of the objects. These rays contribute nothing to the
                                                                          1.3 Images: Physical and Synthetic   19

*FIGURE 1.17 Scene with a single point light source.*


*FIGURE 1.18 Ray interactions. Ray A enters camera directly. Ray B*

goes off to infinity. Ray C is reflected by a mirror. Ray D goes through a
transparent sphere.
image, although they may be seen by some other viewer. The remaining rays strike
and illuminate objects. These rays can interact with the objects’ surfaces in a variety
of ways. For example, if the surface is a mirror, a reﬂected ray might—depending on
the orientation of the surface—enter the lens of the camera and contribute to the im-
age. Other surfaces scatter light in all directions. If the surface is transparent, the light
ray from the source can pass through it and may interact with other objects, enter the
camera, or travel to inﬁnity without striking another surface. Figure 1.18 shows some
of the possibilities.
20   Chapter 1   Graphics Systems and Models
Ray tracing and photon mapping are image-formation techniques that are based
on these ideas and that can form the basis for producing computer-generated images.
We can use the ray-tracing idea to simulate physical effects as complex as we wish, as
long as we are willing to carry out the requisite computing. Although tracing rays can
provide a close approximation to the physical world, it is usually not well suited for
real-time computation.
Other physical approaches to image formation are based on conservation of
energy. The most important in computer graphics is radiosity. This method works
best for surfaces that scatter the incoming light equally in all directions. Even in this
case, radiosity requires more computation than can be done in real time. We defer
discussion of these techniques until Chapter 11.

### 1.4      IMAGING SYSTEMS

We now introduce two imaging systems: the pinhole camera and the human visual
system. The pinhole camera is a simple example of an imaging system that will enable
us to understand the functioning of cameras and of other optical imagers. We emu-
late it to build a model of image formation. The human visual system is extremely
complex but still obeys the physical principles of other optical imaging systems. We

```cpp
introduce it not only as an example of an imaging system but also because under-
```

standing its properties will help us to exploit the capabilities of computer-graphics
systems.

#### 1.4.1 The Pinhole Camera

The pinhole camera in Figure 1.19 provides an example of image formation that we
can understand with a simple geometric model. A pinhole camera is a box with a
small hole in the center of one side of the box; the ﬁlm is placed inside the box on
the side opposite the pinhole. Suppose that we orient our camera along the z-axis,
with the pinhole at the origin of our coordinate system. We assume that the hole is
(x, y, z)
(xp, yp, zp )

*FIGURE 1.19 Pinhole camera.*

                                                                                                 1.4 Imaging Systems   21
(y, z )
(yp , d )

*FIGURE 1.20 Side view of pinhole camera.*

so small that only a single ray of light, emanating from a point, can enter it. The ﬁlm
plane is located a distance d from the pinhole. A side view (Figure 1.20) allows us to
calculate where the image of the point (x, y, z) is on the ﬁlm plane z = −d. Using the
fact that the two triangles in Figure 1.20 are similar, we ﬁnd that the y coordinate of
the image is at yp, where
yp = −       .
z/d
A similar calculation, using a top view, yields
xp = −       .
z/d
The point (xp , yp , −d) is called the projection of the point (x, y, z). In our idealized
model, the color on the ﬁlm plane at this point will be the color of the point (x, y, z).
The ﬁeld, or angle, of view of our camera is the angle made by the largest object that
our camera can image on its ﬁlm plane. We can calculate the ﬁeld of view with the
aid of Figure 1.21.4 If h is the height of the camera, the angle of view θ is
θ = 2 tan−1         .
2d
The ideal pinhole camera has an inﬁnite depth of ﬁeld: Every point within its ﬁeld
of view is in focus. Every point in its ﬁeld of view projects to a point on the back of
the camera. The pinhole camera has two disadvantages. First, because the pinhole is
so small—it admits only a single ray from a point source—almost no light enters the
camera. Second, the camera cannot be adjusted to have a different angle of view.
4. If we consider the problem in three, rather than two, dimensions, then the diagonal length of the
ﬁlm will substitute for h.
22              Chapter 1         Graphics Systems and Models

h                                         z

*FIGURE 1.21 Angle of view.*

The jump to more sophisticated cameras and to other imaging systems that have
lenses is a small one. By replacing the pinhole with a lens, we solve the two problems
of the pinhole camera. First, the lens gathers more light than can pass through the
pinhole. The larger the aperture of the lens, the more light the lens can collect.
Second, by picking a lens with the proper focal length—a selection equivalent to
choosing d for the pinhole camera—we can achieve any desired angle of view (up to
180 degrees). Lenses, however, do not have an inﬁnite depth of ﬁeld: Not all distances
from the lens are in focus.
For our purposes, in this chapter we can work with a pinhole camera whose focal
Retina                        length is the distance d from the front of the camera to the ﬁlm plane. Like the pinhole
camera, computer graphics produces images in which all objects are in focus.
and cones                 1.4.2 The Human Visual System
Our extremely complex visual system has all the components of a physical imaging
Optic nerve                             system, such as a camera or a microscope. The major components of the visual

*FIGURE 1.22 The human vi-               system are shown in Figure 1.22. Light enters the eye through the lens and cornea,*

sual system.                            a transparent structure that protects the eye. The iris opens and closes to adjust the
amount of light entering the eye. The lens forms an image on a two-dimensional

```cpp
structure called the retina at the back of the eye. The rods and cones (so named
```

because of their appearance when magniﬁed) are light sensors and are located on
the retina. They are excited by electromagnetic energy in the range of 350 to 780 nm.
The rods are low-level-light sensors that account for our night vision and are not
color sensitive; the cones are responsible for our color vision. The sizes of the rods
and cones, coupled with the optical properties of the lens and cornea, determine the
resolution of our visual systems, or our visual acuity. Resolution is a measure of what
size objects we can see. More technically, it is a measure of how close we can place two
points and still recognize that there are two distinct points.
The sensors in the human eye do not react uniformly to light energy at different
wavelengths. There are three types of cones and a single type of rod. Whereas intensity
is a physical measure of light energy, brightness is a measure of how intense we
                                                                                  1.5 The Synthetic-Camera Model   23
perceive the light emitted from an object to be. The human visual system does not
have the same response to a monochromatic (single-frequency) red light as to a
monochromatic green light. If these two lights were to emit the same energy, they
would appear to us to have different brightness, because of the unequal response
of the cones to red and green light. We are most sensitive to green light, and least
sensitive to red and blue.
Brightness is an overall measure of how we react to the intensity of light. Human
color-vision capabilities are due to the different sensitivities of the three types of
cones. The major consequence of having three types of cones is that instead of having
to work with all visible wavelengths individually, we can use three standard primaries
to approximate any color that we can perceive. Consequently, most image-production
systems, including ﬁlm and video, work with just three basic, or primary, colors. We
discuss color in depth in Chapter 2.
The initial processing of light in the human visual system is based on the same
principles used by most optical systems. However, the human visual system has a
back end much more complex than that of a camera or telescope. The optic nerves
are connected to the rods and cones in an extremely complex arrangement that has
many of the characteristics of a sophisticated signal processor. The ﬁnal processing
is done in a part of the brain called the visual cortex, where high-level functions,
such as object recognition, are carried out. We shall omit any discussion of high-level
processing; instead, we can think simply in terms of an image that is conveyed from
the rods and cones to the brain.

### 1.5     THE SYNTHETIC-CAMERA MODEL

Our models of optical imaging systems lead directly to the conceptual foundation
for modern three-dimensional computer graphics. We look at creating a computer-
generated image as being similar to forming an image using an optical system. This
paradigm has become known as the synthetic-camera model. Consider the imaging
system shown in Figure 1.23. We again see objects and a viewer. In this case, the viewer
is a bellows camera.5 The image is formed on the ﬁlm plane at the back of the camera.
So that we can emulate this process to create artiﬁcial images, we need to identify a
few basic principles.
First, the speciﬁcation of the objects is independent of the speciﬁcation of the
viewer. Hence, we should expect that, within a graphics library, there will be separate
functions for specifying the objects and the viewer.
Second, we can compute the image using simple geometric calculations, just as
we did with the pinhole camera. Consider the side view of the camera and a simple
object in Figure 1.24. The view in part (a) of the ﬁgure is similar to that of the
5. In a bellows camera, the front plane of the camera, where the lens is located, and the back of the
camera, the ﬁlm plane, are connected by ﬂexible sides. Thus, we can move the back of the camera
independently of the front of the camera, introducing additional ﬂexibility in the image-formation
process. We use this ﬂexibility in Chapter 4.
24   Chapter 1   Graphics Systems and Models

*FIGURE 1.23 Imaging system.*

y                                             y
Camera                                    ( y, z )                       (y, z)
Projector                               (yp , d )
(yp, –d )
(a)                                                (b)

*FIGURE 1.24 Equivalent views of image formation. (a) Image formed on*

the back of the camera. (b) Image plane moved in front of the camera.
pinhole camera. Note that the image of the object is ﬂipped relative to the object.
Whereas with a real camera we would simply ﬂip the ﬁlm to regain the original
orientation of the object, with our synthetic camera we can avoid the ﬂipping by a
simple trick. We draw another plane in front of the lens (Figure 1.24(b)) and work in
three dimensions, as shown in Figure 1.25. We ﬁnd the image of a point on the object
on the virtual image plane by drawing a line, called a projector, from the point to
the center of the lens, or the center of projection (COP). Note that all projectors
are rays emanating from the center of projection. In our synthetic camera, the virtual
image plane that we have moved in front of the lens is called the projection plane. The
image of the point is located where the projector passes through the projection plane.
In Chapter 4, we discuss this process in detail and derive the relevant mathematical
formulas.
We must also consider the limited size of the image. As we saw, not all objects
can be imaged onto the pinhole camera’s ﬁlm plane. The angle of view expresses this
limitation. In the synthetic camera, we can move this limitation to the front by plac-
ing a clipping rectangle, or clipping window, in the projection plane (Figure 1.26).
This rectangle acts as a window, through which a viewer, located at the center of pro-
                                                                        1.6 The Programmer’s Interface   25

*FIGURE 1.25 Imaging with the synthetic camera.*

(a)                                 (b)

*FIGURE 1.26 Clipping. (a) Window in initial position. (b) Window shifted.*

jection, sees the world. Given the location of the center of projection, the location
and orientation of the projection plane, and the size of the clipping rectangle, we can
determine which objects will appear in the image.

### 1.6   THE PROGRAMMER’S INTERFACE

There are numerous ways that a user can interact with a graphics system. With
completely self-contained packages, such as those used in the CAD community, a
user develops images through interactions with the display using input devices, such
as a mouse and a keyboard. In a typical application, such as the painting program in
Figure 1.27, the user sees menus and icons that represent possible actions. By clicking
26   Chapter 1   Graphics Systems and Models

*FIGURE 1.27 Interface for a painting program.*

library             Drivers                         Mouse
(API)

*FIGURE 1.28 Application programmer’s model of graphics system.*

on these items, the user guides the software and produces images without having to
write programs.
Of course, someone has to develop the code for these applications, and many
of us, despite the sophistication of commercial products, still have to write our own
graphics application programs (and even enjoy doing so).
The interface between an application program and a graphics system can be
speciﬁed through a set of functions that resides in a graphics library. These speci-
ﬁcations are called the application programming interface (API). The application
programmer’s model of the system is shown in Figure 1.28. The application program-
mer sees only the API and is thus shielded from the details of both the hardware and
the software implementation of the graphics library. The software drivers are respon-
sible for interpreting the output of the API and converting these data to a form that
is understood by the particular hardware. From the perspective of the writer of an
application program, the functions available through the API should match the con-
ceptual model that the user wishes to employ to specify images.
                                                                          1.6 The Programmer’s Interface             27

#### 1.6.1 The Pen-Plotter Model

Historically, most early graphics systems were two-dimensional systems. The concep-
tual model that they used is now referred to as the pen-plotter model, referencing the
output device that was available on these systems. A pen plotter (Figure 1.29) pro-
duces images by moving a pen held by a gantry, a structure that can move the pen in          FIGURE 1.29 Pen plotter.
two orthogonal directions across the paper. The plotter can raise and lower the pen as
required to create the desired image. Pen plotters are still in use; they are well suited
for drawing large diagrams, such as blueprints. Various APIs—such as LOGO and
PostScript—have their origins in this model. Although they differ from one another,
they have a common view of the process of creating an image as being similar to the
process of drawing on a pad of paper. The user works on a two-dimensional surface
of some size. She moves a pen around on this surface, leaving an image on the paper.
We can describe such a graphics system with two drawing functions:
moveto(x,y);
lineto(x,y);
Execution of the moveto function moves the pen to the location (x, y) on the paper
without leaving a mark. The lineto function moves the pen to (x, y) and draws a
(a)
line from the old to the new location of the pen. Once we add a few initialization
and termination procedures, as well as the ability to change pens to alter the drawing
color or line thickness, we have a simple—but complete—graphics system. Here is a
fragment of a simple program in such a system:
moveto(0, 0);
lineto(1, 0);
lineto(1, 1);
lineto(0, 1);
lineto(0, 0);
(b)
This fragment would generate the output in Figure 1.30(a). If we added the code             FIGURE 1.30 Output of pen-
plotter program for (a) a
moveto(0, 1);                                                                               square, and (b) a projection
lineto(0.5, 1.866);                                                                         of a cube.
lineto(1.5, 1.866);
lineto(1.5, 0.866);
lineto(1, 0);
moveto(1, 1);
lineto(1.5, 1.866);
we would have the image of a cube formed by an oblique projection, as is shown in
Figure 1.30(b).
For certain applications, such as page layout in the printing industry, systems
built on this model work well. For example, the PostScript page-description language,
a sophisticated extension of these ideas, is a standard for controlling typesetters and
printers.
28   Chapter 1   Graphics Systems and Models
An alternate raster-based, but still limiting, two-dimensional model relies on
writing pixels directly into a frame buffer. Such a system could be based on a single
write_pixel(x, y, color);
where x,y is the location of the pixel in the frame buffer and color gives the color
to be written there. Such models are well suited to writing the algorithms for rasteri-
zation and processing of digital images.
We are much more interested, however, in the three-dimensional world. The
pen-plotter model does not extend well to three-dimensional graphics systems. For
example, if we wish to use the pen-plotter model to produce the image of a three-
dimensional object on our two-dimensional pad, either by hand or by computer, then
we have to ﬁgure out where on the page to place two-dimensional points correspond-
ing to points on our three-dimensional object. These two-dimensional points are,
as we saw in Section 1.5, the projections of points in three-dimensional space. The
mathematical process of determining projections is an application of trigonometry.
We develop the mathematics of projection in Chapter 4; understanding projection
is crucial to understanding three-dimensional graphics. We prefer, however, to use
an API that allows users to work directly in the domain of their problems and to use
computers to carry out the details of the projection process automatically, without the
users having to make any trigonometric calculations within the application program.
That approach should be a boon to users who have difﬁculty learning to draw various
projections on a drafting board or sketching objects in perspective. More important,
users can rely on hardware and software implementations of projections within the
implementation of the API that are far more efﬁcient than any possible implementa-
tion of projections within their programs would be.

#### 1.6.2 Three-Dimensional APIs

The synthetic-camera model is the basis for a number of popular APIs, including
OpenGL and Direct3D. If we are to follow the synthetic-camera model, we need
functions in the API to specify the following:
A viewer
Objects are usually deﬁned by sets of vertices. For simple geometric objects—
such as line segments, rectangles, and polygons—there is a simple relationship be-
tween a list of vertices, or positions in space, and the object. For more complex
objects, there may be multiple ways of deﬁning the object from a set of vertices. A cir-
cle, for example, can be deﬁned by three points on its circumference, or by its center
and one point on the circumference.
                                                                        1.6 The Programmer’s Interface                  29
Most APIs provide similar sets of primitive objects for the user. These primi-
tives are usually those that can be displayed rapidly on the hardware. The usual sets
include points, line segments, polygons, and sometimes text. OpenGL programs de-
ﬁne primitives through lists of vertices. The following code fragment speciﬁes three
vertices:

```cpp
float vertices[3][3];
```

vertices[0][0] = 0.0;       /* vertex A */
vertices[1][0] = 0.0;                                                                                   y
vertices[2][0] = 0.0;
vertices[0][1] = 0.0;       /* vertex B */
vertices[1][1] = 1.0;                                                                                       B
vertices[2][1] = 0.0;
vertices[0][2] = 0.0;       /* vertex C */
vertices[1][2] = 0.0;
vertices[2][2] = 1.0;
In OpenGL, we could either send this array to the GPU each time that we want
it to be displayed or store it on the GPU for later display. Note that these three
vertices only give three locations in a three-dimensional space and do not specify
the geometric entity that they deﬁne. The locations could describe a triangle, as         z
in Figure 1.31, or we could use them to specify two line segments using the ﬁrst              FIGURE 1.31 A triangle.
two locations to specify the ﬁrst segment and the second and third locations to
specify the second segment. We could also use the three points to display three pixels
at locations in the frame buffer corresponding to the three vertices. We make this
choice on our application by setting a parameter corresponding to the geometric
entity we would like these locations to specify. For example, in OpenGL we would
use GL_TRIANGLES, GL_LINE_STRIP, or GL_POINTS for the three possibilities we
just described. Although we are not yet ready to describe all the details of how we
accomplish this task, we can note that regardless of which geometric entity we wish
our vertices to specify, we are specifying the geometry and leaving it to the graphics        COP
system to determine which pixels to color in the frame buffer.
Some APIs let the user work directly in the frame buffer by providing functions                                h
that read and write pixels. Additionally, some APIs provide curves and surfaces as
primitives; often, however, these types are approximated by a series of simpler prim-
itives within the application program. OpenGL provides access to the frame buffer.
We can deﬁne a viewer or camera in a variety of ways. Available APIs differ both
in how much ﬂexibility they provide in camera selection and in how many different
methods they allow. If we look at the camera in Figure 1.32, we can identify four types   FIGURE 1.32 Camera
of necessary speciﬁcations:                                                               specification.
1. Position The camera location usually is given by the position of the center
of the lens, which is the center of projection (COP).
30   Chapter 1   Graphics Systems and Models

*FIGURE 1.33 Two-point perspective of a cube.*

2. Orientation Once we have positioned the camera, we can place a camera
coordinate system with its origin at the center of projection. We can then
rotate the camera independently around the three axes of this system.
3. Focal length The focal length of the lens determines the size of the image
on the ﬁlm plane or, equivalently, the portion of the world the camera sees.
4. Film plane The back of the camera has a height and a width. On the bellows
camera, and in some APIs, the orientation of the back of the camera can be
adjusted independently of the orientation of the lens.
These speciﬁcations can be satisﬁed in various ways. One way to develop the
speciﬁcations for the camera location and orientation uses a series of coordinate-
system transformations. These transformations convert object positions represented
in a coordinate system that speciﬁes object vertices to object positions in a coordinate
system centered at the COP. This approach is useful, both for doing implementation
and for getting the full set of views that a ﬂexible camera can provide. We use this
approach extensively, starting in Chapter 4.
Having many parameters to adjust, however, can also make it difﬁcult to get a
desired image. Part of the problem lies with the synthetic-camera model. Classical
viewing techniques, such as are used in architecture, stress the relationship between
the object and the viewer, rather than the independence that the synthetic-camera
model emphasizes. Thus, the classical two-point perspective of a cube in Figure 1.33
is a two-point perspective because of a particular relationship between the viewer and
the planes of the cube (see Exercise 1.7). Although the OpenGL API allows us to set
transformations with complete freedom, it also provides helpful extra functions. For
example, consider the two function calls
LookAt(cop, at, up);
Perspective(field_of_view, aspect_ratio, near, far);
The ﬁrst function call points the camera from the center of projection toward a
desired point (the at point), with a speciﬁed up direction for the camera. The second
selects a lens for a perspective view (the ﬁeld of view) and how much of the world that
the camera should image (the aspect ratio and the near and far distances). However,
none of the APIs built on the synthetic-camera model provide functions for directly
specifying a desired relationship between the camera and an object.
Light sources are deﬁned by their location, strength, color, and directionality.
APIs provide a set of functions to specify these parameters for each source. Material
                                                                        1.6 The Programmer’s Interface   31
properties are characteristics, or attributes, of the objects, and such properties are
speciﬁed through a series of function calls at the time that each object is deﬁned.
Both light sources and material properties depend on the models of light–material

```cpp
interactions supported by the API. We discuss such models in Chapter 5.
```


#### 1.6.3 A Sequence of Images

In Chapter 2, we begin our detailed discussion of the OpenGL API that we will use
throughout this book. The images deﬁned by your OpenGL programs will be formed
automatically by the hardware and software implementation of the image-formation
process.
Here we look at a sequence of images that shows what we can create using the
OpenGL API. We present these images as an increasingly more complex series of
renderings of the same objects. The sequence not only loosely follows the order in
which we present related topics but also reﬂects how graphics systems have developed
over the past 30 years.
Color Plate 1 shows an image of an artist’s creation of a sunlike object. Color
Plate 2 shows the object rendered using only line segments. Although the object con-
sists of many parts, and although the programmer may have used sophisticated data

```cpp
structures to model each part and the relationships among the parts, the rendered
```

object shows only the outlines of the parts. This type of image is known as a wire-
frame image because we can see only the edges of surfaces: Such an image would be
produced if the objects were constructed with stiff wires that formed a frame with no
solid material between the edges. Before raster-graphics systems became available,
wireframe images were the only type of computer-generated images that we could
produce.
In Color Plate 3, the same object has been rendered with ﬂat polygons. Certain
surfaces are not visible, because there is a solid surface between them and the viewer;
these surfaces have been removed by a hidden-surface-removal (HSR) algorithm.
Most raster systems can ﬁll the interior of polygons with a solid color in approxi-
mately the same time that they can render a wireframe image. Although the objects
are three-dimensional, each surface is displayed in a single color, and the image fails
to show the three-dimensional shapes of the objects. Early raster systems could pro-
duce images of this form.
In Chapters 2 and 3, we show you how to generate images composed of simple
geometric objects—points, line segments, and polygons. In Chapters 3 and 4, you
will learn how to transform objects in three dimensions and how to obtain a desired
three-dimensional view of a model, with hidden surfaces removed.
Color Plate 4 illustrates smooth shading of the polygons that approximate the
object; it shows that the object is three-dimensional and gives the appearance of a
smooth surface. We develop shading models that are supported by OpenGL in Chap-
ter 5. These shading models are also supported in the hardware of most recent work-
stations; generating the shaded image on one of these systems takes approximately
the same amount of time as does generating a wireframe image.
32   Chapter 1   Graphics Systems and Models
Color Plate 5 shows a more sophisticated wireframe model constructed using
NURBS surfaces, which we introduce in Chapter 10. Such surfaces give the applica-
tion programmer great ﬂexibility in the design process but are ultimately rendered
using line segments and polygons.
In Color Plates 6 and 7, we add surface texture to our object; texture is one
of the effects that we discuss in Chapter 6. All recent graphics processors support
texture mapping in hardware, so rendering of a texture-mapped image requires little
additional time. In Color Plate 6, we use a technique called bump mapping that gives
the appearance of a rough surface even though we render the same ﬂat polygons as
in the other examples. Color Plate 7 shows an environment map applied to the surface
of the object, which gives the surface the appearance of a mirror. These techniques
will be discussed in detail in Chapter 7.
Color Plate 8 shows a small area of the rendering of the object using an environ-
ment map. The image on the left shows the jagged artifacts known as aliasing errors
that are due to the discrete nature of the frame buffer. The image on the right has been
rendered using a smoothing or antialiasing method that we shall study in Chapters 5
and 6.
Not only do these images show what is possible with available hardware and a
good API, but they are also simple to generate, as we shall see in subsequent chapters.
In addition, just as the images show incremental changes in the renderings, the
programs are incrementally different from one another.

#### 1.6.4 The Modeling–Rendering Paradigm

In many situations—especially in CAD applications and in the development of com-
plex images, such as for movies—we can separate the modeling of the scene from
the production of the image, or the rendering of the scene. Hence, we can look at
image formation as the two-step process shown in Figure 1.34. Although the tasks
are the same as those we have been discussing, this block diagram suggests that we
might implement the modeler and the renderer with different software and hard-
ware. For example, consider the production of a single frame in an animation. We
ﬁrst want to design and position our objects. This step is highly interactive, and we
do not need to work with detailed images of the objects. Consequently, we prefer to
carry out this step on an interactive workstation with good graphics hardware. Once
we have designed the scene, we want to render it, adding light sources, material prop-
erties, and a variety of other detailed effects, to form a production-quality image.
This step requires a tremendous amount of computation, so we might prefer to use a

*FIGURE 1.34 The modeling–rendering pipeline.*

                                                                                1.7 Graphics Architectures   33
render farm, a cluster of computers conﬁgured for numerical computing. Not only is
the optimal hardware different in the modeling and rendering steps, but the software
that we use also may be different.
The interface between the modeler and renderer can be as simple as a ﬁle pro-
duced by the modeler that describes the objects and that contains additional infor-
mation important only to the renderer, such as light sources, viewer location, and
material properties. Pixar’s RenderMan Interface follows this approach and uses a
ﬁle format that allows modelers to pass models to the renderer in text format. One
of the other advantages of this approach is that it allows us to develop modelers
that, although they use the same renderer, are custom-tailored to particular applica-
tions. Likewise, different renderers can take as input the same interface ﬁle. It is even
possible, at least in principle, to dispense with the modeler completely and to use a
standard text editor to generate an interface ﬁle. For any but the simplest scenes, how-
ever, users cannot edit lists of information for a renderer. Rather, they use interactive
modeling software. Because we must have at least a simple image of our objects to

```cpp
interact with a modeler, most modelers use the synthetic-camera model to produce
```

these images in real time.
This paradigm has become popular as a method for generating computer games
and images over the Internet. Models, including the geometric objects, lights, cam-
eras, and material properties, are placed in a data structure called a scene graph that
is passed to a renderer or game engine. We shall examine scene graphs in Chapter 8.

### 1.7     GRAPHICS ARCHITECTURES

On one side of the API is the application program. On the other is some combination
of hardware and software that implements the functionality of the API. Researchers
have taken various approaches to developing architectures to support graphics APIs.
Early graphics systems used general-purpose computers with the standard von
Neumann architecture. Such computers are characterized by a single processing unit
that processes a single instruction at a time. A simple model of these early graphics
systems is shown in Figure 1.35. The display in these systems was based on a calli-
graphic CRT display that included the necessary circuitry to generate a line segment
connecting two points. The job of the host computer was to run the application pro-
gram and to compute the endpoints of the line segments in the image (in units of the
display). This information had to be sent to the display at a rate high enough to avoid

*FIGURE 1.35 Early graphics system.*

34   Chapter 1   Graphics Systems and Models

*FIGURE 1.36 Display-processor architecture.*

ﬂicker on the display. In the early days of computer graphics, computers were so slow
that refreshing even simple images, containing a few hundred line segments, would
burden an expensive computer.

#### 1.7.1 Display Processors

The earliest attempts to build special-purpose graphics systems were concerned pri-
marily with relieving the general-purpose computer from the task of refreshing the
display continuously. These display processors had conventional architectures (Fig-
ure 1.36) but included instructions to display primitives on the CRT. The main ad-
vantage of the display processor was that the instructions to generate the image could
be assembled once in the host and sent to the display processor, where they were
stored in the display processor’s own memory as a display list, or display ﬁle. The
display processor would then execute repetitively the program in the display list, at
a rate sufﬁcient to avoid ﬂicker, independently of the host, thus freeing the host for
other tasks. This architecture has become closely associated with the client–server ar-
chitectures that are used in most systems.

#### 1.7.2 Pipeline Architectures

The major advances in graphics architectures parallel closely the advances in work-
stations. In both cases, the ability to create special-purpose VLSI chips was the key
enabling technology development. In addition, the availability of inexpensive solid-
state memory led to the universality of raster displays. For computer-graphics appli-
cations, the most important use of custom VLSI circuits has been in creating pipeline
architectures.
The concept of pipelining is illustrated in Figure 1.37 for a simple arithmetic
calculation. In our pipeline, there is an adder and a multiplier. If we use this con-
ﬁguration to compute a + (b ∗ c), the calculation takes one multiplication and one
addition—the same amount of work required if we use a single processor to carry
out both operations. However, suppose that we have to carry out the same computa-
tion with many values of a, b, and c. Now, the multiplier can pass on the results of its
calculation to the adder and can start its next multiplication while the adder carries
out the second step of the calculation on the ﬁrst set of data. Hence, whereas it takes
                                                                                1.7 Graphics Architectures   35
c           *                   +

*FIGURE 1.37 Arithmetic pipeline.*

Vertices           Vertex           Clipper and                               Fragment      Pixels
processor      primitive assembler                          processor

*FIGURE 1.38 Geometric pipeline.*

the same amount of time to calculate the results for any one set of data, when we are
working on two sets of data at one time, our total time for calculation is shortened
markedly. Here the rate at which data ﬂows through the system, the throughput of
the system, has been doubled. Note that as we add more boxes to a pipeline, it takes
more time for a single datum to pass through the system. This time is called the la-
tency of the system; we must balance it against increased throughput in evaluating
the performance of a pipeline.
We can construct pipelines for more complex arithmetic calculations that will
afford even greater increases in throughput. Of course, there is no point in building a
pipeline unless we will do the same operation on many data sets. But that is just what
we do in computer graphics, where large sets of vertices and pixels must be processed
in the same manner.

#### 1.7.3 The Graphics Pipeline

We start with a set of objects. Each object comprises a set of graphical primitives. Each
primitive comprises a set of vertices. We can think of the collection of primitive types
and vertices as deﬁning the geometry of the scene. In a complex scene, there may be
thousands—even millions—of vertices that deﬁne the objects. We must process all
these vertices in a similar manner to form an image in the frame buffer. If we think in
terms of processing the geometry of our objects to obtain an image, we can employ
the block diagram in Figure 1.38, which shows the four major steps in the imaging
process:
1. Vertex processing
2. Clipping and primitive assembly
3. Rasterization
4. Fragment processing
In subsequent chapters, we discuss the details of these steps. Here we are content to
overview these steps and show that they can be pipelined.
36   Chapter 1   Graphics Systems and Models

#### 1.7.4 Vertex Processing

In the ﬁrst block of our pipeline, each vertex is processed independently. The two
major functions of this block are to carry out coordinate transformations and to
compute a color for each vertex.
Many of the steps in the imaging process can be viewed as transformations be-
tween representations of objects in different coordinate systems. For example, in our
discussion of the synthetic camera, we observed that a major part of viewing is to
convert to a representation of objects from the system in which they were deﬁned to
a representation in terms of the coordinate system of the camera. A further example
of a transformation arises when we ﬁnally put our images onto the output device.
The internal representation of objects—whether in the camera coordinate system or
perhaps in a system used by the graphics software—eventually must be represented
in terms of the coordinate system of the display. We can represent each change of
coordinate systems by a matrix. We can represent successive changes in coordinate
systems by multiplying, or concatenating, the individual matrices into a single ma-
trix. In Chapter 3, we examine these operations in detail. Because multiplying one
matrix by another matrix yields a third matrix, a sequence of transformations is an
obvious candidate for a pipeline architecture. In addition, because the matrices that
we use in computer graphics will always be small (4 × 4), we have the opportunity to
use parallelism within the transformation blocks in the pipeline.
Eventually, after multiple stages of transformation, the geometry is transformed
by a projection transformation. We shall see in Chapter 4 that we can implement this
step using 4 × 4 matrices, and thus projection ﬁts in the pipeline. In general, we want
to keep three-dimensional information as long as possible, as objects pass through
the pipeline. Consequently, the projection transformation is somewhat more general
than the projections in Section 1.5. In addition to retaining three-dimensional infor-
mation, there is a variety of projections that we can implement. We shall see these
projections in Chapter 4.
The assignment of vertex colors can be as simple as the program specifying a
color or as complex as the computation of a color from a physically realistic lighting
model that incorporates the surface properties of the object and the characteristic
light sources in the scene. We shall discuss lighting models in Chapter 5.

#### 1.7.5 Clipping and Primitive Assembly

The second fundamental block in the implementation of the standard graphics
pipeline is for clipping and primitive assembly. We must do clipping because of the
limitation that no imaging system can see the whole world at once. The human retina
has a limited size corresponding to an approximately 90-degree ﬁeld of view. Cameras
have ﬁlm of limited size, and we can adjust their ﬁelds of view by selecting different
lenses.
We obtain the equivalent property in the synthetic camera by considering a clip-
ping volume, such as the pyramid in front of the lens in Figure 1.25. The projections
of objects in this volume appear in the image. Those that are outside do not and
                                                                             1.8 Programmable Pipelines   37
are said to be clipped out. Objects that straddle the edges of the clipping volume are
partly visible in the image. Efﬁcient clipping algorithms are developed in Chapter 6.
Clipping must be done on a primitive-by-primitive basis rather than on a vertex-
by-vertex basis. Thus, within this stage of the pipeline, we must assemble sets of
vertices into primitives, such as line segments and polygons, before clipping can take
place. Consequently, the output of this stage is a set of primitives whose projections
can appear in the image.

#### 1.7.6 Rasterization

The primitives that emerge from the clipper are still represented in terms of their
vertices and must be converted to pixels in the frame buffer. For example, if three
vertices specify a triangle with a solid color, the rasterizer must determine which
pixels in the frame buffer are inside the polygon. We discuss this rasterization (or
scan-conversion) process in Chapter 6 for line segments and polygons. The output of
the rasterizer is a set of fragments for each primitive. A fragment can be thought of
as a potential pixel that carries with it information, including its color and location,
that is used to update the corresponding pixel in the frame buffer. Fragments can
also carry along depth information that allows later stages to determine if a particular
fragment lies behind other previously rasterized fragments for a given pixel.

#### 1.7.7 Fragment Processing

The ﬁnal block in our pipeline takes in the fragments generated by the rasterizer and
updates the pixels in the frame buffer. If the application generated three-dimensional
data, some fragments may not be visible because the surfaces that they deﬁne are
behind other surfaces. The color of a fragment may be altered by texture mapping or
bump mapping, as in Color Plates 6 and 7. The color of the pixel that corresponds to
a fragment can also be read from the frame buffer and blended with the fragment’s
color to create translucent effects. These effects will be covered in Chapter 7.

### 1.8    PROGRAMMABLE PIPELINES

Graphics architectures have gone through multiple design cycles in which the impor-
tance of special-purpose hardware relative to standard CPUs has gone back and forth.
However, the importance of the pipeline architecture has remained regardless of this
cycle. None of the other approaches—ray tracing, radiosity, photon mapping—can
achieve real-time behavior, that is, the ability to render complex dynamic scenes
so that the viewer sees the display without defects. However, the term real-time is
becoming increasingly difﬁcult to deﬁne as graphics hardware improves. Although
some approaches such as ray tracing can come close to real time, none can achieve the
performance of pipeline architectures with simple application programs and simple
GPU programs. Hence, the commodity graphics market is dominated by graphics
38   Chapter 1   Graphics Systems and Models
cards that have pipelines built into the graphics processing unit. All of these com-
modity cards implement the pipeline that we have just described, albeit with more
options, many of which we shall discuss in later chapters.
For many years, these pipeline architectures have had a ﬁxed functionality. Al-
though the application program could set many parameters, the basic operations
available within the pipeline were ﬁxed. Recently, there has been a major advance
in pipeline architectures. Both the vertex processor and the fragment processor are
now programmable by the application program. One of the most exciting aspects of
this advance is that many of the techniques that formerly could not be done in real
time because they were not part of the ﬁxed-function pipeline can now be done in
real time. Bump mapping, which we illustrated in Color Plate 6, is but one example
of an algorithm that is now programmable but formerly could only be done off-line.
Vertex programs can alter the location or color of each vertex as it ﬂows through
the pipeline. Thus, we can implement a variety of light–material models or create new
kinds of projections. Fragment programs allow us to use textures in new ways and to
implement other parts of the pipeline, such as lighting, on a per-fragment basis rather
than per vertex.
Programmability is now available at every level, including hand-held devices
such as cell phones. WebGL is being built into Web browsers. At the high end, the
speed and parallelism in programmable GPUs make them suitable for carrying out
high-performance computing that does not involve graphics. The latest versions of
OpenGL have responded to these advances ﬁrst by adding programmability to the
standard as an option that an application programmer could use as an alternative to
the ﬁxed-function pipeline and later through versions that require the application
to provide both a vertex shader and a fragment shader. We will follow these new
standards throughout. Although it will take a little more code for our ﬁrst programs
because we will not use a ﬁxed-function pipeline, the rewards will be signiﬁcant as
our code will be efﬁcient and easily extendable.

### 1.9   PERFORMANCE CHARACTERISTICS

There are two fundamentally different types of processing in our architecture. At
the front end, there is geometric processing, based on processing vertices through
the various transformations, vertex shading, clipping, and primitive assembly. This
processing is ideally suited for pipelining, and it usually involves ﬂoating-point cal-
culations. The geometry engine developed by Silicon Graphics, Inc. (SGI) was a VLSI
implementation for many of these operations in a special-purpose chip that became
the basis for a series of fast graphics workstations. Later, ﬂoating-point accelerator
chips put 4 × 4 matrix-transformation units on the chip, reducing a matrix multi-
plication to a single instruction. Nowadays, graphics workstations and commodity
graphics cards use graphics processing units (GPUs) that perform most of the graph-
ics operations at the chip level. Pipeline architectures are the dominant type of high-
performance system.
                                                                                     Summary and Notes   39
Beginning with rasterization and including many features that we discuss later,
processing involves a direct manipulation of bits in the frame buffer. This back-end
processing is fundamentally different from front-end processing, and we implement
it most effectively using architectures that have the ability to move blocks of bits
quickly. The overall performance of a system is characterized by how fast we can
move geometric entities through the pipeline and by how many pixels per second
we can alter in the frame buffer. Consequently, the fastest graphics workstations are

```cpp
characterized by geometric pipelines at the front ends and parallel bit processors at
```

the back ends. Until about 10 years ago, there was a clear distinction between front-
and back-end processing and there were different components and boards dedicated
to each. Now commodity graphics cards use GPUs that contain the entire pipeline
within a single chip. The latest cards implement the entire pipeline using ﬂoating-
point arithmetic and have ﬂoating-point frame buffers. These GPUs are so powerful
that even the highest level systems—systems that incorporate multiple pipelines—use
these processors.
Pipeline architectures dominate the graphics ﬁeld, especially where real-time
performance is of importance. Our presentation has made a case for using such
an architecture to implement the hardware in a system. Commodity graphics cards
incorporate the pipeline within their GPUs. Cards that cost less than $100 can render
millions of shaded texture-mapped polygons per second. However, we can also make
as strong a case for pipelining being the basis of a complete software implementation
of an API. The power of the synthetic-camera paradigm is that the latter works well
in both cases.
However, where realism is important, other types of renderers can perform bet-
ter at the expense of requiring more computation time. Pixar’s RenderMan interface
was created to interface to their off-line renderer. Physically based techniques, such
as ray tracing and radiosity, can create photorealistic images with great ﬁdelity, but
usually not in real time.
In this chapter, we have set the stage for our top-down development of computer
graphics. We presented the overall picture so that you can proceed to writing graphics
application programs in the next chapter without feeling that you are working in a
vacuum.
We have stressed that computer graphics is a method of image formation that
should be related to classical methods of image formation—in particular, to image
formation in optical systems, such as in cameras. In addition to explaining the pin-
hole camera, we have introduced the human visual system; both are examples of
imaging systems.
We described multiple image-formation paradigms, each of which has applica-
bility in computer graphics. The synthetic-camera model has two important conse-
quences for computer graphics. First, it stresses the independence of the objects and
the viewer—a distinction that leads to a good way of organizing the functions that
40   Chapter 1   Graphics Systems and Models
will be in a graphics library. Second, it leads to the notion of a pipeline architecture,
in which each of the various stages in the pipeline performs distinct operations on
geometric entities and then passes on the transformed objects to the next stage.
We also introduced the idea of tracing rays of light to obtain an image. This para-
digm is especially useful in understanding the interaction between light and materials
that is essential to physical image formation. Because ray tracing and other physically
based strategies cannot render scenes in real time, we defer further discussion of them
until Chapter 11.
The modeling–rendering paradigm is becoming increasingly important. A stan-
dard graphics workstation can generate millions of line segments or polygons per
second at a resolution exceeding 2048 × 1546 pixels. Such a workstation can shade
the polygons using a simple shading model and can display only visible surfaces at this
rate. However, realistic images may require a resolution of up to 4000 × 6000 pixels
to match the resolution of ﬁlm and may use light and material effects that cannot be
implemented in real time. Even as the power of available hardware and software con-
tinues to grow, modeling and rendering have such different goals that we can expect
the distinction between a modeling and a rendering to survive.
Our next step will be to explore the application side of graphics programming.
We use the OpenGL API, which is powerful, is supported on most platforms, and
has a distinct architecture that will allow us to use it to understand how computer
graphics works, from an application program to a ﬁnal image on a display.
There are many excellent graphics textbooks. The book by Newman and Sproull
[New73] was the ﬁrst to take the modern point of view of using the synthetic-camera
model. The various versions of Foley et al. [Fol90, Fol94] have been the standard
references for over a decade. Other good texts include Hearn and Baker [Hea11], Hill
[Hil07], and Shirley [Shi02].
Good general references include Computer Graphics, the quarterly journal of
SIGGRAPH (the Association for Computing Machinery’s Special Interest Group on
Graphics), IEEE Computer Graphics and Applications, and Visual Computer. The pro-
ceedings of the annual SIGGRAPH conference include the latest techniques. These
proceedings formerly were published as the summer issue of Computer Graphics.
Now, they are published as an issue of the ACM Transactions on Graphics and are
available on DVD. Of particular interest to newcomers to the ﬁeld are the state-of-
the-art animations available from SIGGRAPH and the notes from tutorial courses
taught at that conference, both of which are now available on DVD or in ACM’s dig-
ital library.
Sutherland’s doctoral dissertation, published as Sketchpad: A Man–Machine
Graphical Communication System [Sut63] was probably the seminal paper in the de-
velopment of interactive computer graphics. Sutherland was the ﬁrst person to realize
the power of the new paradigm in which humans interacted with images on a CRT
display. Videotape copies of ﬁlm of his original work are still available.
                                                                                           Exercises   41
Tufte’s books [Tuf83, Tuf90, Tuf97] show the importance of good visual design
and contain considerable historical information on the development of graphics. The
article by Carlbom and Paciorek [Car78] gives a good discussion of some of the
relationships between classical viewing, as used in ﬁelds such as architecture, and
viewing by computer.
Many books describe the human visual system. Pratt [Pra78] gives a good short
discussion for working with raster displays. Also see Glassner [Gla95], Wyszecki and
Stiles [Wys82], and Hall [Hal89].

### 1.1   The pipeline approach to image generation is nonphysical. What are the main

advantages and disadvantages of such a nonphysical approach?

### 1.2   In computer graphics, objects such as spheres are usually approximated by

simpler objects constructed from ﬂat polygons (polyhedra). Using lines of lon-
gitude and latitude, deﬁne a set of simple polygons that approximate a sphere
centered at the origin. Can you use only quadrilaterals or only triangles?

### 1.3   A different method of approximating a sphere starts with a regular tetrahe-

dron, which is constructed from four triangles. Find its vertices, assuming that
it is centered at the origin and has one vertex on the y-axis. Derive an algo-
rithm for obtaining increasingly closer approximations to a unit sphere, based
on subdividing the faces of the tetrahedron.

### 1.4   Consider the clipping of a line segment in two dimensions against a rectan-

gular clipping window. Show that you require only the endpoints of the line
segment to determine whether the line segment is not clipped, is partially vis-
ible, or is clipped out completely.

### 1.5   For a line segment, show that clipping against the top of the clipping rectangle

can be done independently of the clipping against the other sides. Use this
result to show that a clipper can be implemented as a pipeline of four simpler
clippers.

### 1.6   Extend Exercises 1.4 and 1.5 to clipping against a three-dimensional right

parallelepiped.

### 1.7   Consider the perspective views of the cube shown in Figure 1.39. The one on

the left is called a one-point perspective because parallel lines in one direction
of the cube—along the sides of the top—converge to a vanishing point in the
image. In contrast, the image on the right is a two-point perspective. Charac-
terize the particular relationship between the viewer, or a simple camera, and
the cube that determines why one is a two-point perspective and the other a
one-point perspective.

### 1.8   The memory in a frame buffer must be fast enough to allow the display to

be refreshed at a rate sufﬁciently high to avoid ﬂicker. A typical workstation
display can have a resolution of 1280 × 1024 pixels. If it is refreshed 72 times
42   Chapter 1   Graphics Systems and Models

*FIGURE 1.39 Perspective views of a cube.*

per second, how fast must the memory be? That is, how much time can we take
to read one pixel from memory? What is this number for a 480 × 640 display
that operates at 60 Hz but is interlaced?

### 1.9 Movies are generally produced on 35 mm ﬁlm that has a resolution of approx-

imately 2000 × 3000 pixels. What implication does this resolution have for
producing animated images for television as compared with ﬁlm?

### 1.10 Consider the design of a two-dimensional graphical API for a speciﬁc appli-

cation, such as for VLSI design. List all the primitives and attributes that you
would include in your system.

### 1.11 It is possible to design a color CRT that uses a single electron gun and does not

have a shadow mask. The single beam is turned on and off at the appropriate
times to excite the desired phosphors. Why might such a CRT be more difﬁcult
to design, as compared to the shadow-mask CRT?

### 1.12 In a typical shadow-mask CRT, if we want to have a smooth display, the width

of a pixel must be about three times the width of a triad. Assume that a monitor
displays 1280 × 1024 pixels, has a CRT diameter of 50 cm, and has a CRT depth
of 25 cm. Estimate the spacing between holes in the shadow mask.

### 1.13 An interesting exercise that should help you understand how rapidly graphics

performance has improved is to go to the Web sites of some of the GPU
manufacturers, such as NVIDIA, ATI, and Intel, and look at the speciﬁcations
for their products. Often the specs for older cards and GPUs are still there. How
rapidly has geometric performance improved? What about pixel processing?
How has the cost per rendered triangle decreased?
                                                                    CHA P TE R            2
O      ur approach to computer graphics is programming oriented. Consequently, we
want you to get started programming graphics as soon as possible. To this end,
we will introduce a minimal application programming interface (API). This API will
be sufﬁcient to allow you to program many interesting two- and three-dimensional
problems and to familiarize you with the basic graphics concepts.
We regard two-dimensional graphics as a special case of three-dimensional
graphics. This perspective allows us to get started, even though we will touch on
three-dimensional concepts lightly in this chapter. Our two-dimensional code will
execute without modiﬁcation on a three-dimensional system.
Our development will use a simple but informative problem: the Sierpinski gas-
ket. It shows how we can generate an interesting and, to many people, unexpectedly
sophisticated image using only a handful of graphics functions. We use OpenGL as
our API, but our discussion of the underlying concepts is broad enough to encompass
most modern systems. The functionality that we introduce in this chapter is sufﬁcient
to allow you to write basic two- and three-dimensional programs that do not require
user interaction.

### 2.1   THE SIERPINSKI GASKET

We will use as a sample problem the drawing of the Sierpinski gasket—an interesting
shape that has a long history and is of interest in areas such as fractal geometry. The
Sierpinski gasket is an object that can be deﬁned recursively and randomly; in the
limit, however, it has properties that are not at all random. We start with a two-
dimensional version, but as we will see in Section 2.10, the three-dimensional version
is almost identical.
Suppose that we start with three points in space. As long as the points are not
collinear, they are the vertices of a unique triangle and also deﬁne a unique plane.
We assume that this plane is the plane z = 0 and that these points, as speciﬁed in
44             Chapter 2    Graphics Programming
some convenient coordinate system,1 are (x1, y1, 0), (x2 , y2 , 0), and (x3 , y3 , 0). The

```cpp
construction proceeds as follows:
```

1. Pick an initial point p = (x, y, 0) at random inside the triangle.
2. Select one of the three vertices at random.
3. Find the point q halfway between p and the randomly selected vertex.
v2
4. Display q by putting some sort of marker, such as a small circle, at the corre-
sponding location on the display.
p0
5. Replace p with q.
p2                 6. Return to step 2.
p1
v1                    v3
Thus, each time that we generate a new point, we display it on the output device. This

*FIGURE 2.1 Generation of          process is illustrated in Figure 2.1, where p0 is the initial point, and p1 and p2 are the*

the Sierpinski gasket.            ﬁrst two points generated by our algorithm.
Before we develop the program, you might try to determine what the resulting
image will be. Try to construct it on paper; you might be surprised by your results.
A possible form for our graphics program might be this:
main( )

```cpp
{
```

initialize_the_system();
p = find_initial_point();
for(some_number_of_points)

```cpp
{
```

q = generate_a_point(p);
display_the_point(q);
p = q;

```cpp
}
```

cleanup();

```cpp
}
```

This form can be converted into a real program fairly easily. However, even at
this level of abstraction, we can see two other alternatives. Consider the pseudocode
main( )

```cpp
{
```

initialize_the_system();
p = find_initial_point();
1. In Chapter 3, we expand the concept of a coordinate system to the more general formulation of a
frame.
                                                                              2.1 The Sierpinski Gasket   45
for(some_number_of_points)

```cpp
{
```

q = generate_a_point(p);
store_the_point(q);
p = q;

```cpp
}
```

display_all_points();
cleanup();

```cpp
}
```

In this algorithm, we compute all the points ﬁrst and put them into an array or
some other data structure. We then display all the points through a single function
call. This approach avoids the overhead of sending small amounts of data to the
graphics processor for each point we generate at the cost of having to store all the
data. The strategy used in the ﬁrst algorithm is known as immediate mode graphics
and, until recently, was the standard method for displaying graphics, especially where

```cpp
interactive performance was needed. One consequence of immediate mode is that
```

there is no memory of the geometric data. With our ﬁrst example, if we want to
display the points again, we would have to go through the entire creation and display
process a second time.
In our second algorithm, because the data are stored in a data structure, we can
redisplay the data, perhaps with some changes such as altering the color or changing
the size of a displayed point, by resending the array without regenerating the points.
The method of operation is known as retained mode graphics and goes back to some
of the earliest special purpose graphics display hardware. The architecture of modern
graphics systems that employ a GPU leads to a third version of our program.
Our second approach has one major ﬂaw. Suppose that, as we might in an
animation, we wish to redisplay the same objects. The geometry of the objects is
unchanged, but the objects may be moving. Displaying all the points involves sending
the data from the CPU to the GPU each time we wish to display the objects in a new
position. For large amounts of data, this data transfer is the major bottleneck in the
display process. Consider the following alternative scheme:
main( )

```cpp
{
```

initialize_the_system();
p = find_initial_point();
for(some_number_of_points)

```cpp
{
```

q = generate_a_point(p);
store_the_point(q);
p = q;

```cpp
}
```

46   Chapter 2   Graphics Programming
send_all_points_to_GPU();
display_data_on_GPU();
cleanup();

```cpp
}
```

As before, we place data in an array, but now we have broken the display process into
two parts: storing the data on the GPU and displaying the data that has been stored.
If we only have to display our data once, there is no advantage over our previous
method, but if we want to animate the display, our data are already on the GPU and
redisplay does not require any additional data transfer, only a simple function call
that alters the location of some spatial data describing the objects that have moved.
Although our ﬁnal OpenGL program will have a slightly different organization,
it will follow this third strategy. We develop the full program in stages. First, we
concentrate on the core: generating and displaying points. We must answer two
questions:
How do we represent points in space?
Should we use a two-dimensional, three-dimensional, or other representa-
tion?
Once we answer these questions, we will be able to place our geometry on the GPU in
a form that can be rendered. Then, we will be able to address how we view our objects
using the power of programmable shaders.

### 2.2    PROGRAMMING TWO-DIMENSIONAL APPLICATIONS

For two-dimensional applications, such as the Sierpinski gasket, although we could
use a pen-plotter API, such an approach would limit us. Instead, we choose to start
with a three-dimensional world; we regard two-dimensional systems, such as the one
on which we will produce our image, as special cases. Mathematically, we view the
two-dimensional plane, or a simple two-dimensional curved surface, as a subspace of
a three-dimensional space. Hence, statements—both practical and abstract—about
the larger three-dimensional world hold for the simpler two-dimensional world.
We can represent a point in the plane z = 0 as p = (x, y, 0) in the three-
dimensional world, or as p = (x, y) in the two-dimensional plane. OpenGL, like most
three-dimensional graphics systems, allows us to use either representation, with the
underlying internal representation being the same, regardless of which form the user
chooses. We can implement representations of points in a number of ways, but the
simplest is to think of a three-dimensional point as being represented by a triplet
p = (x, y, z) or a column matrix
⎡ ⎤
p= y⎦,
⎣
                                                       2.2 Programming Two-Dimensional Applications   47
whose components give the location of the point. For the moment, we can leave aside
the question of the coordinate system in which p is represented.
We use the terms vertex and point in a somewhat different manner in OpenGL.
A vertex is a position in space; we use two-, three-, and four-dimensional spaces in
computer graphics. We use vertices to specify the atomic geometric primitives that
are recognized by our graphics system. The simplest geometric primitive is a point
in space, which is usually speciﬁed by a single vertex. Two vertices can specify a line
segment, a second primitive object; three vertices can specify either a triangle or a
circle; four vertices can specify a quadrilateral; and so on. Two vertices can also specify
either a circle or a rectangle. Likewise, three vertices can also specify three points
or two connected line segments, and four vertices can specify a variety of objects
including two triangles.
The heart of our Sierpinski gasket program is generating the points. In order to
go from our third algorithm to a working OpenGL program, we need to introduce a
little more detail on OpenGL. We want to start with as simple a program as possible.
One simpliﬁcation is to delay a discussion of coordinate systems and transformations
among them by putting all the data we want to display inside a cube centered at the
origin whose diagonal goes from (−1, −1, −1) and (1, 1, 1). This system known
as clip coordinates is the one that our vertex shader uses to send information to
the rasterizer. Objects outside this cube will be eliminated, or clipped, and cannot
appear on the display. Later, we will learn to specify geometry in our application
program in coordinates better suited for our application—object coordinates—and
use transformations to convert the data to a representation in clip coordinates.
We could write the program using a simple array of two elements to hold the
x- and y-values of each point. We will have far clearer code if we ﬁrst deﬁne a two-
dimensional point type and operations for this type. We have created such classes
and operators and put them in a ﬁle vec.h. The types in vec.h and the other
types deﬁned later in the three- and four-dimensional classes match the types in the
OpenGL Shading Language and so should make all our coding examples clearer than
if we had used ordinary arrays. In addition to deﬁning these new types, vec.h and its
companion ﬁle mat2.h also deﬁne overloaded operators and constructors for these
types that match GLSL. Hence, code such as
vec2 a = vec2(1.0, 2.0);
vec2 b = vec2(3.0, 4.0);
vec2 c = a + b;
can appear either in a shader or in the application. We can input and output points
using the usual stream operators cin and cout. We can access individual elements
using either the usual membership operator, e.g., p.x or p.y, or by indexing as we
would an array (p[0] and p[1]).
One small addition will make our applications even clearer. Rather than using
the GLSL vec2, we typedef a point2
typedef vec2 point2;
48   Chapter 2   Graphics Programming
Within vec.h, the type vec2 is speciﬁed as a struct with two elements of type
GLfloat. In OpenGL, we often use basic OpenGL types, such as GLfloat and
GLint, rather than the corresponding C types float and int. These types are
deﬁned in the OpenGL header ﬁles and usually in the obvious way—for example,
typedef float GLfloat;
However, use of the OpenGL types allows additional ﬂexibility for implementations
where, for example, we might want to change ﬂoats to doubles without altering
existing application programs.
The following code generates 5000 points starting with the vertices of a triangle
that lie in the plane z = 0:

```cpp
#include "vec.h" // include point types and operations
#include <stdlib.h> //includes random number generator
```

typedef vec2 point2; //defines a point2 type identical to a vec2

```cpp
void init()
{
const int NumPoints = 5000;
```

point2 points[NumPoints];

```cpp
// A triangle in the plane z= 0
```

point2 vertices[3]={point2(-1.0,-1.0), point2(0.0,1.0),
point2(1.0,-1.0)};

```cpp
// An arbitrary initial point inside the triangle
```

points[0] = point2(0.25, 0.50);

```cpp
// compute and store NumPoints-1 new points
```

for(int k = 1; k < NumPoints; k++)

```cpp
{
int j = rand() % 3; // pick a vertex at random
// Compute the point halfway between selected
// vertex and previous point
```

points[k] = (points[k-1]+vertices[j])/2.0;

```cpp
}
}
```

Note that because every point we generate must lie inside the triangle determined by
these vertices, we know that none of the generated points will be clipped out.
                                                          2.2 Programming Two-Dimensional Applications   49
The function rand() is a standard random-number generator that produces a
new random integer each time it is called. We use the modulus operator to reduce
these random integers to the three integers 0, 1, and 2. For a small number of itera-
tions, the particular characteristics of the random-number generator are not crucial,
and any other random-number generator should work at least as well as rand.
We intend to generate the points only once and then place them on the GPU.
Hence, we make their creation part of an initialization function init.
We speciﬁed our points in two dimensions. We could have also speciﬁed them in
three dimensions by adding a z-coordinate, which is always zero through the three-
dimensional types in mat.h and vec.h. The changes to the code would be minimal.
We would have the code lines

```cpp
#include "vec.h" // three-dimensional type
```

typedef vec3 point3;
point3 points [NumPoints];
point3 vertices[3] = {point3(-1.0,-1.0, 0.0), point3(0.0,1.0, 0.0),
point3(1.0,-1.0, 0.0)};
as part of initialization. Although we still do not have a complete program, Figure 2.2
shows the output that we expect to see.
Note that because any three noncollinear points specify a unique plane, had we
started with three points (x1, y1, z1), (x2 , y2 , z2), and (x3 , y3 , z3) along with an initial
point in the same plane, then the gasket would be generated in the plane speciﬁed by
the original three vertices.
We have now written the core of the program. Although we have some data,
we have not placed these data on the GPU nor have we asked the GPU to display
anything. We have not even introduced a single OpenGL function. Before we can
display anything, we still have to address issues such as the following:
1. In what colors are we drawing?
2. Where on the display does our image appear?
3. How large will the image be?
4. How do we create an area of the display—a window—for our image?
5. How much of our inﬁnite drawing surface will appear on the display?
6. How long will the image remain on the display?
The answers to all these questions are important, although initially they may appear
to be peripheral to our major concerns. As we will see, the basic code that we de-
velop to answer these questions and to control the placement and appearance of our
50   Chapter 2   Graphics Programming

*FIGURE 2.2 The Sierpinski gasket as generated with 5000 random*

points.
renderings will not change substantially across programs. Hence, the effort that we
expend now will be repaid later.

### 2.3   THE OPENGL APPLICATION PROGRAMMING INTERFACE

We have the heart of a simple graphics program; now, we want to gain control
over how our objects appear on the display. We also want to control the ﬂow of
the program, and we have to interact with the window system. Before completing
our program, we describe the OpenGL Application Programming Interface (API) in
more detail. Because vertices are represented in the same manner internally, whether
they are speciﬁed as two- or three-dimensional entities, everything that we do here
will be equally valid in three dimensions. Of course, we can do much more in three
dimensions, but we are only getting started. In this chapter, we concentrate on how
to specify primitives to be displayed.
OpenGL’s structure is similar to that of most modern APIs, such as DirectX.
Hence, any effort that you put into learning OpenGL will carry over to other soft-
ware systems. Although OpenGL is easy to learn, compared with other APIs, it is
nevertheless powerful. It supports the simple two- and three-dimensional programs
that we will develop in Chapters 2 through 5; it also supports the advanced rendering
techniques that we study in Chapters 7 through 11.
Our prime goal is to study computer graphics; we are using an API to help us
attain that goal. Consequently, we do not present all OpenGL functions, and we
omit many details. However, our sample programs will be complete. More detailed
                                                2.3 The OpenGL Application Programming Interface   51
Application                    Graphics                     Input/Output
program                        system                         devices

*FIGURE 2.3 Graphics system as a black box.*

information on OpenGL and on other APIs is given in the Suggested Readings section
at the end of the chapter.

#### 2.3.1 Graphics Functions

Our basic model of a graphics package is a black box, a term that engineers use to
denote a system whose properties are described only by its inputs and outputs; we
may know nothing about its internal workings. We can think of the graphics system
as a box whose inputs are function calls from an application program; measurements
from input devices, such as the mouse and keyboard; and possibly other input, such
as messages from the operating system. The outputs are primarily the graphics sent
to our output devices. For now, we can take the simpliﬁed view of inputs as function
calls and outputs as primitives displayed on our monitor, as shown in Figure 2.3.
A graphics system performs multiple tasks to produce output and handle user
input. An API for interfacing with this system can contain hundreds of individual
functions. It will be helpful to divide these functions into seven major groups:
1. Primitive functions
2. Attribute functions
3. Viewing functions
4. Transformation functions
5. Input functions
6. Control functions
7. Query functions
Although we will focus on OpenGL as the particular system that we use, all graphics
APIs support similar functionality. What differs among APIs is where these functions
are supported. OpenGL is designed around a pipeline architecture, and modern ver-
sions are based on using programmable shaders. Consequently, OpenGL and other
APIs such as DirectX that support a similar architecture will have much in common,
whereas OpenGL and an API for a ray tracer will have less overlap. Nevertheless, re-
gardless of the underlying architecture and API, we still have to address all the seven
tasks.
The primitive functions deﬁne the low-level objects or atomic entities that our
system can display. Depending on the API, the primitives can include points, line
segments, polygons, pixels, text, and various types of curves and surfaces. OpenGL
supports a very limited set of primitives directly, only points, line segments, and
52   Chapter 2   Graphics Programming
triangles. Support for other primitives comes from the application approximating
them with the supported primitives. For the most important objects such as reg-
ular polyhedra, quadrics, and Bezier curves and surfaces that are not directly sup-
ported by OpenGL, there are libraries that provide the necessary code. Support for
expanded sets of primitives is usually done with great efﬁciency through program-
mable shaders.
If primitives are the what of an API—the primitive objects that can be
displayed—then attributes are the how. That is, the attributes govern the way that a
primitive appears on the display. Attribute functions allow us to perform operations
ranging from choosing the color with which we display a line segment, to picking a
pattern with which to ﬁll the inside of a polygon, to selecting a typeface for the titles
on a graph. In OpenGL, we can set colors by passing the information from the appli-
cation to the shader or by having a shader compute a color, for example, through a
lighting model that uses data specifying light sources and properties of the surfaces
in our model.
Our synthetic camera must be described if we are to create an image. As we saw in

## Chapter 1, we must describe the camera’s position and orientation in our world and

must select the equivalent of a lens. This process will not only ﬁx the view but also
allow us to clip out objects that are too close or too far away. The viewing functions
allow us to specify various views, although APIs differ in the degree of ﬂexibility they
provide in choosing a view. OpenGL does not provide any viewing functions but
relies on the use of transformations in the shaders to provide the desired view.
One of the characteristics of a good API is that it provides the user with a set of
transformation functions that allows her to carry out transformations of objects,
such as rotation, translation, and scaling. Our developments of viewing in Chap-
ter 4 and of modeling in Chapter 8 will make heavy use of matrix transformations.
In OpenGL, we carry out transformations by forming transformations in our appli-
cations and then applying them either in the application or in the shaders.
For interactive applications, an API must provide a set of input functions to
allow us to deal with the diverse forms of input that characterize modern graphics
systems. We need functions to deal with devices such as keyboards, mice, and data
tablets. Later in this chapter, we will introduce functions for working with different
input modes and with a variety of input devices.
In any real application, we also have to worry about handling the complexities of
working in a multiprocessing, multiwindow environment—usually an environment
where we are connected to a network and there are other users. The control functions
enable us to communicate with the window system, to initialize our programs, and
to deal with any errors that take place during the execution of our programs.
If we are to write device-independent programs, we should expect the imple-
mentation of the API to take care of differences between devices, such as how many
colors are supported or the size of the display. However, there are applications where
we need to know some properties of the particular implementation. For example, we
would probably choose to do things differently if we knew in advance that we were
working with a display that could support only two colors rather than millions of
colors. More generally, within our applications we can often use other information
                                                 2.3 The OpenGL Application Programming Interface   53
within the API, including camera parameters or values in the frame buffer. A good
API provides this information through a set of query functions.

#### 2.3.2 The Graphics Pipeline and State Machines

If we put together some of these perspectives on graphics APIs, we can obtain another
view, one closer to the way OpenGL, in particular, is actually organized and imple-
mented. We can think of the entire graphics system as a state machine, a black box
that contains a ﬁnite-state machine. This state machine has inputs that come from the
application program. These inputs may change the state of the machine or can cause
the machine to produce a visible output. From the perspective of the API, graphics
functions are of two types: those that specify primitives that ﬂow through a pipeline
inside the state machine and those that either change the state inside the machine
or return state information. In OpenGL, there are very few functions that can cause
any output. Most set the state, either by enabling various OpenGL features—hidden-
surface removal, texture—or set parameters used for rendering.
Until recently, OpenGL deﬁned many state variables and contained separate
functions for setting the values of individual variables. The latest versions have elim-
inated most of these variables and functions. Instead, the application program can
deﬁne its own state variables and use them or send their values to the shaders.
One important consequence of the state machine view is that most parame-
ters are persistent; their values remain unchanged until we explicitly change them
through functions that alter the state. For example, once we set a color, that color
remains the current color until it is changed through a color-altering function. An-
other consequence of this view is that attributes that we may conceptualize as bound
to objects—a red line or a blue circle—are in fact part of the state, and a line will be
drawn in red only if the current color state calls for drawing in red. Although within
our applications it is usually harmless, and often preferable, to think of attributes as
bound to primitives, there can be annoying side effects if we neglect to make state
changes when needed or lose track of the current state.

#### 2.3.3 The OpenGL Interface

OpenGL functions are in a single library named GL (or OpenGL in Windows). Func-
tion names begin with the letters gl. Shaders are written in the OpenGL Shading
Language (GLSL), which has a separate speciﬁcation from OpenGL, although the
functions to interface the shaders with the application are part of the OpenGL API.
To interface with the window system and to get input from external devices into
our programs, we need at least one more library. For each major window system there
is a system-speciﬁc library that provides the “glue” between the window system and
OpenGL. For the X Window System, this library is called GLX, for Windows, it is wgl,
and for the Macintosh, it is agl. Rather than using a different library for each system,
we use two readily available libraries, the OpenGL Extension Wrangler (GLEW) and
the OpenGL Utility Toolkit (GLUT). GLEW removes operating system dependencies.
GLUT provides the minimum functionality that should be expected in any modern
54   Chapter 2   Graphics Programming
GL                  GLX                Xlib, Xt

*FIGURE 2.4 Library organization.*

windowing system.2 We introduce a few of its functions in this chapter and describe
more of them in Chapter 3.
Figure 2.4 shows the organization of the libraries for an X Window System en-
vironment. For this window system, GLUT will use GLX and the X libraries. The
application program, however, can use only GLUT functions and thus can be recom-
piled with the GLUT library for other window systems.
OpenGL makes heavy use of deﬁned constants to increase code readability and
avoid the use of magic numbers. Thus, strings such as GL_FILL and GL_POINTS are
deﬁned in header (.h) ﬁles. In most implementations, one of the include lines

```cpp
#include <GL/glut.h>
```


```cpp
#include <GLUT/glut.h>
```

is sufﬁcient to read in glut.h and gl.h.
Although OpenGL is not object oriented, it supports a variety of data types
through multiple forms for many functions. For example, we will use various forms
of the function glUniform to transfer data to shaders. If we transfer a ﬂoating-point
number such as a time value, we would use glUniform1f. We could use glUni-
form3iv to transfer an integer position in three dimensions through a pointer to a
three-dimensional array of ints. Later, we will use the form glUniformMatrix4fv
to transfer a 4 × 4 matrix of floats. We will refer to such functions using the
glSomeFunction*();
where the * can be interpreted as either two or three characters of the form nt or ntv,
where n signiﬁes the number of dimensions (2, 3, 4, or matrix); t denotes the data
type, such as integer (i), ﬂoat (f), or double (d); and v, if present, indicates that the
variables are speciﬁed through a pointer to an array, rather than through an argument
2. A more up-to-date version of GLUT is provided by freeglut, which is available on the Web.
                                                  2.3 The OpenGL Application Programming Interface   55
list. We will use whatever form is best suited for our discussion, leaving the details
of the various other forms to the OpenGL Programming Guide [Shr10]. Regardless
of which form an application programmer chooses, the underlying representation is
the same, just as the plane on which we are constructing the gasket can be looked
at as either a two-dimensional space or the subspace of a three-dimensional space
corresponding to the plane z = 0. In Chapter 3, we will see that the underlying
representation is four-dimensional; however, we do not need to worry about that
fact yet. In general, the application programmer chooses the form to use that is best
suited for her application.

#### 2.3.4 Coordinate Systems

At this point, if we look back at our Sierpinski gasket code, you may be puzzled about
how to interpret the values of x, y, and z in our speciﬁcation of vertices. In what
units are they? Are they in feet, meters, microns? Where is the origin? In each case,
the simple answer is that it is up to you.
Originally, graphics systems required the user to specify all information, such as
vertex locations, directly in units of the display device. If that were true for high-level
application programs, we would have to talk about points in terms of screen locations
in pixels or centimeters from a corner of the display. There are obvious problems with
this method, not the least of which is the absurdity of using distances on the computer
screen to describe phenomena where the natural unit might be light years (such as
in displaying astronomical data) or microns (for integrated-circuit design). One of
the major advances in graphics software systems occurred when the graphics systems
allowed users to work in any coordinate system that they desired. The advent of
device-independent graphics freed application programmers from worrying about
the details of input and output devices. The user’s coordinate system became known
as the world coordinate system, or the application or object coordinate system.
Within the slight limitations of ﬂoating-point arithmetic on our computers, we can
use any numbers that ﬁt our application.
We will refer to the units that the application program uses to specify vertex posi-
tions as vertex coordinates. In most applications, vertex coordinates will be the same
as object or world coordinates, but depending on what we choose to do or not do in
our shaders, vertex coordinates can be one of the other internal coordinate systems
used in the pipeline. We will discuss these other coordinate systems in Chapters 3
and 4.
Units on the display were ﬁrst called physical-device coordinates or just device
coordinates. For raster devices, such as most CRT and ﬂat panel displays, we use
the term window coordinates or screen coordinates.Window coordinates are always
expressed in some integer type, because the center of any pixel in the frame buffer
must be located on a ﬁxed grid or, equivalently, because pixels are inherently discrete
and we specify their locations using integers.
At some point, the values in vertex coordinates must be mapped to window
coordinates, as shown in Figure 2.5. The graphics system, rather than the user, is
responsible for this task, and the mapping is performed automatically as part of the
56   Chapter 2   Graphics Programming
(xmax, ymax )
(rmax, smax)
(r, s)
(x, y)
(rmin, smin)
(xmin, ymin )
World coordinates           Screen coordinates

*FIGURE 2.5 Mapping from vertex coordinates to screen coordinates.*

rendering process. As we will see in the next few sections, to deﬁne this mapping the
user needs to specify only a few parameters—such as the area of the world that she
would like to see and the size of the display. However, between the application and the
frame buffer are the two shaders and rasterizer, and, as we shall see when we discuss
viewing, there are three other intermediate coordinate systems of importance.

### 2.4      PRIMITIVES AND ATTRIBUTES

Within the graphics community, there has been an ongoing debate about which
primitives should be supported in an API. The debate is an old one and has never
been fully resolved. On the minimalist side, the contention is that an API should
contain a small set of primitives that all hardware can be expected to support. In
addition, the primitives should be orthogonal, each giving a capability unobtainable
from the others. Minimal systems typically support lines, polygons, and some form of
text (strings of characters), all of which can be generated efﬁciently in hardware. On
the other end are systems that can also support a variety of primitives, such as circles,
curves, surfaces, and solids. The argument here is that users need more complex
primitives to build sophisticated applications easily. However, because few hardware
systems can be expected to support the large set of primitives that is the union of all
the desires of the user community, a program developed with such a system probably
would not be portable, because few implementations could be expected to support
the entire set of primitives.
As graphics hardware has improved and real-time performance has become mea-
sured in the tens of millions of polygons per second, the balance has tilted toward
supporting a minimum set of primitives. One reason is that GPUs achieve their speed
largely because they are optimized for points, lines, and triangles. We will develop
code later that will approximate various curves and surfaces with primitives that are
supported on GPUs.
We can separate primitives into two classes: geometric primitives and image,
or raster, primitives. Geometric primitives are speciﬁed in the problem domain and
include points, line segments, polygons, curves, and surfaces. These primitives pass
through a geometric pipeline, as shown in Figure 2.6, where they are subject to a series
                                                                              2.4 Primitives and Attributes   57
Transform           Project          Clip
application                         Pixel operations                           buffer

*FIGURE 2.6 Simplified OpenGL pipeline.*

of geometric operations that determine whether a primitive is visible, where on the
display it appears if it is visible, and the rasterization of the primitive into pixels in
the frame buffer. Because geometric primitives exist in a two- or three-dimensional
space, they can be manipulated by operations such as rotation and translation. In
addition, they can be used as building blocks for other geometric objects using these
same operations. Raster primitives, such as arrays of pixels, lack geometric properties
and cannot be manipulated in space in the same way as geometric primitives. They
pass through a separate parallel pipeline on their way to the frame buffer. We will
defer our discussion of raster primitives until Chapter 7.
The basic OpenGL geometric primitives are speciﬁed by sets of vertices. An
application starts by computing vertex data—positions and other attributes—and
putting the results into arrays that are sent to the GPU for display. When we want
to display some geometry, we execute functions whose parameters specify how the
vertices are to be interpreted. For example, we can display the vertices we computed
for the Sierpinski gasket, starting with the ﬁrst vertex, as points through the function
glDrawArrays(GL_POINTS, 0, NumPoints);
after they have been placed on the GPU.
All OpenGL geometric primitives are variants of points, line segments, and tri-
angular polygons. A point can be displayed as a single pixel or a small group of pixels.
Finite sections of lines between two vertices, called line segments—in contrast to
lines that are inﬁnite in extent—are of great importance in geometry and computer
graphics. You can use line segments to deﬁne approximations to curves, or you can
use a sequence of line segments to connect data values for a graph. You can also use
line segments to display the edges of closed objects, such as polygons, that have in-
teriors. Consequently, it is often helpful to think in terms of both vertices and line
segments.
If we wish to display points or line segments, we have a few choices in OpenGL
(Figure 2.7). The primitives and their type speciﬁcations include the following:
Points (GL_POINTS) Each vertex is displayed at a size of at least one pixel.
58           Chapter 2       Graphics Programming
p2                      p2                      p2                      p2
p1          p3          p1          p3           p1         p3           p1          p3
p0             p4       p0             p4       p0             p4       p0             p4
p7          p5          p7          p5          p7          p5          p7           p5
p6                      p6                      p6                      p6

*FIGURE 2.7 Point and line-segment types.*

Line segments (GL_LINES) The line-segment type causes successive pairs of ver-
tices to be interpreted as the endpoints of individual segments. Note that successive
segments usually are disconnected because the vertices are processed on a pairwise
basis.
Polylines (GL_LINE_STRIP, GL_LINE_LOOP) If successive vertices (and line seg-
ments) are to be connected, we can use the line strip, or polyline form. Many curves
can be approximated via a suitable polyline. If we wish the polyline to be closed, we

*FIGURE 2.8 Filled objects.        can locate the ﬁnal vertex in the same place as the ﬁrst, or we can use the GL_LINE_*

LOOP type, which will draw a line segment from the ﬁnal vertex to the ﬁrst, thus
creating a closed path.

#### 2.4.1 Polygon Basics

Line segments and polylines can model the edges of objects, but closed objects have

```cpp
interiors (Figure 2.8). Usually we reserve the name polygon for an object that has
```

a border that can be described by a line loop but also has a well-deﬁned interior.3
Polygons play a special role in computer graphics because we can display them rapidly

*FIGURE 2.9 Methods of dis-         and use them to approximate arbitrary surfaces. The performance of graphics systems*

playing a polygon.                 is characterized by the number of polygons per second that can be rendered.4 We can
render a polygon in a variety of ways: We can render only its edges, we can render its

```cpp
interior with a solid color or a pattern, and we can render or not render the edges, as
```

shown in Figure 2.9. Although the outer edges of a polygon are deﬁned easily by an
ordered list of vertices, if the interior is not well deﬁned, then the list of vertices may
not be rendered at all or rendered in an undesirable manner. Three properties will
ensure that a polygon will be displayed correctly: It must be simple, convex, and ﬂat.
(a)                       In two dimensions, as long as no two edges of a polygon cross each other, we have
a simple polygon. As we can see in Figure 2.10, simple two-dimensional polygons
have well-deﬁned interiors. Although the locations of the vertices determine whether
or not a polygon is simple, the cost of testing is sufﬁciently high (see Exercise 2.12)
that most graphics systems require that the application program does any necessary
(b)
3. The term ﬁll area is sometimes used instead of polygon.

*FIGURE 2.10 Polygons.              4. Measuring polygon rendering speeds involves both the number of vertices and the number of*

(a) Simple. (b) Nonsimple.         pixels inside.
                                                                              2.4 Primitives and Attributes        59
testing. We can ask what a graphics system will do if it is given a nonsimple polygon
to display and whether there is a way to deﬁne an interior for a nonsimple polygon.                  p1
We will examine these questions further in Chapter 6.
From the perspective of implementing a practical algorithm to ﬁll the interior of
a polygon, simplicity alone is often not enough. Some APIs guarantee a consistent ﬁll                         p2
from implementation to implementation only if the polygon is convex. An object is
convex if all points on the line segment between any two points inside the object, or
on its boundary, are inside the object. Thus, in Figure 2.11, p1 and p2 are arbitrary        FIGURE 2.11 Convexity.
points inside a polygon and the entire line segment connecting them is inside the
polygon. Although so far we have been dealing with only two-dimensional objects,
this deﬁnition makes reference neither to the type of object nor to the number of di-
mensions. Convex objects include triangles, tetrahedra, rectangles, circles, spheres,
and parallelepipeds (Figure 2.12). There are various tests for convexity (see Exer-
cise 2.19). However, like simplicity testing, convexity testing is expensive and usually
left to the application program.
In three dimensions, polygons present a few more difﬁculties because, unlike all
two-dimensional objects, all the vertices that specify the polygon need not lie in the
same plane. One property that most graphics systems exploit, and that is the basis of
OpenGL polygons, is that any three vertices that are not collinear determine both a
triangle and the plane in which that triangle lies. Hence, if we always use triangles, we
are safe—we can be sure that these objects will be rendered correctly. Often, we are
almost forced to use triangles because typical rendering algorithms are guaranteed to
be correct only if the vertices form a ﬂat convex polygon. In addition, hardware and
software often support a triangle type that is rendered faster than is a polygon with
three vertices.

#### 2.4.2 Polygons in OpenGL

Returning to the OpenGL types, the only OpenGL polygons (Figure 2.13) that
OpenGL supports are triangles. Triangles can be displayed in three ways: as points
corresponding to the vertices, as edges, or with the interiors ﬁlled. In OpenGL, we
use the function glPolygonMode to tell the renderer to generate only the edges or
just points for the vertices, instead of ﬁll (the default). However, if we want to draw a

*FIGURE 2.12 Convex objects.*

60          Chapter 2   Graphics Programming
p2                          p2
p1          p3               p1         p3
p0            p4             p0           p4
p7          p5               p7         p5
p6                          p6

*FIGURE 2.13 Triangle types.*

p1    p3        p5    p7                       p1
p2
p3
p4
p0     p2    p4        p6           p0

*FIGURE 2.14 Triangle strip and triangle fan.*

polygon that is ﬁlled and to display its edges, then we have to render it twice, once in
each mode, or to draw a ﬁlled polygon and a line loop with the same vertices.
Here are the types:
Triangles (GL_TRIANGLES) The edges are the same as they would be if we used line
loops. Each successive group of three vertices speciﬁes a new triangle.
Strips and Fans (GL_TRIANGLE_STRIP, GL_TRIANGLE_FAN) These objects are
based on groups of triangles that share vertices and edges. In the triangle strip, for
example, each additional vertex is combined with the previous two vertices to deﬁne
a new triangle (Figure 2.14). A triangle fan is based on one ﬁxed point. The next two
points determine the ﬁrst triangle, and subsequent triangles are formed from one
new point, the previous point, and the ﬁrst (ﬁxed) point.

#### 2.4.3 Approximating a Sphere

Fans and strips allow us to approximate many curved surfaces simply. For example,
one way to construct an approximation to a sphere is to use a set of polygons de-

*FIGURE 2.15 Sphere approx-     ﬁned by lines of longitude and latitude, as shown in Figure 2.15. We can do so very*

imation with quadrilaterals.
efﬁciently using triangle strips. Consider a unit sphere. We can describe it by the fol-
lowing three equations:
x(θ , φ) = sin θ cos φ,
y(θ , φ) = cos θ cos φ,
z(θ , φ) = sin φ.
                                                                           2.4 Primitives and Attributes   61
If we ﬁx θ and draw curves as we change φ, we get circles of constant longitude.
Likewise, if we ﬁx φ and vary θ, we obtain circles of constant latitude. By generating
points at ﬁxed increments of θ and φ, we can specify quadrilaterals, as shown in
Figure 2.15. However, because OpenGL supports triangles, not quadrilaterals, we
generate the data for two triangles for each quadrilateral. Remembering that we must
convert degrees to radians for the standard trigonometric functions, the code for
the quadrilaterals corresponding to increments of 20 degrees in θ and to 20 degrees
in φ is

```cpp
const float DegreesToRadians = M_PI / 180.0; // M_PI = 3.14159...
```

point3 quad_data[342];        // 8 rows of 18 quads

```cpp
int k = 0;
```

for(float phi = -80.0; phi <= 80.0; phi += 20.0)

```cpp
{
float phir   = phi*DegreesToRadians;
float phir20 = (phi + 20.0)*DegreesToRadians;
```

for(float theta = -180.0; theta <= 180.0; theta += 20.0)

```cpp
{
float thetar = theta*DegreesToRadians;
```

quad_data[k] = point3(sin(thetar)*cos(phir),
cos(thetar)*cos(phir), sin(phir));
k++;
quad_data[k] = point3(sin(thetar)*cos(phir20),
cos(thetar)*cos(phir20), sin(phir20));
k++;

```cpp
}
}
```

Later we can render these data using glDrawArrays(GL_LINE_LOOP,...) or
some other drawing function. However, we have a problem at the poles, where we can
no longer use strips because all lines of longitude converge there. We can, however,
use two triangle fans, one at each pole as follows:

```cpp
const float DegreesToRadians = M_PI / 180.0; // M_PI = 3.14159...
int k = 0;
```

point3 strip_data[40];
strip_data[k] = point3(0.0, 0.0, 1.0);
k++;

```cpp
float sin80 = sin(80.0*DegreesToRadians);
float cos80 = cos(80.0*DegreesToRadians);
```

62   Chapter 2   Graphics Programming
for(float theta = -180.0; theta <= 180.0; theta += 20.0)

```cpp
{
float thetar = theta*DegreesToRadians;
```

strip_data[k] = point3(sin(thetar)*cos80,
cos(thetar)*cos80, sin80);
k++;

```cpp
}
```

strip_data[k] = point3(0.0, 0.0, -1.0);
k++;
for(float theta = -180.0; theta <= 180.0; theta += 20.0)

```cpp
{
float thetar = theta;
```

strip_data[k] = point3(sin(thetar)*cos80,
cos(thetar)*cos80, sin80);
k++;

```cpp
}
```

These data could be rendered with glDrawArrays(GL_TRIANGLE_FAN, ....) or
another drawing function. Note that because triangle fans are polygons, if we want
to get the line segment display in Figure 2.15, we would ﬁrst have to set the polygon
mode to lines instead of ﬁll.

#### 2.4.4 Triangulation

We have been using the terms polygon and triangle somewhat interchangeably. If we
are interested in objects with interiors, general polygons are problematic. A set of
vertices may not all lie in the same plane or specify a polygon that is neither simple
nor convex. Such problems do not arise with triangles. As long as the three vertices of
a triangle are not collinear, its interior is well deﬁned and the triangle is simple, ﬂat,
and convex. Consequently, triangles are easy to render, and for these reasons triangles
are the only ﬁllable geometric entity that OpenGL recognizes. In practice, we need to
deal with more general polygons. The usual strategy is to start with a list of vertices
and generate a set of triangles consistent with the polygon deﬁned by the list, a process
known as triangulation.
Figure 2.16 shows a convex polygon and two different triangulations. Although
every set of vertices can be triangulated, not all triangulations are equivalent. Con-
sider the quadrilateral in Figure 2.17. If we triangulate it as in Figure 2.17(b), we create
two long thin triangles rather than two triangles closer to being equilateral as in Fig-
ure 2.17(c). As we shall see when we discuss lighting in Chapter 5, long thin triangles
can lead to visual artifacts when rendered. There are some simple algorithms that
work for planar convex polygons. We can start with the ﬁrst three vertices and form
a triangle. We can then remove the second vertex from the list of vertices and repeat
the process until we have only three vertices left, which form the ﬁnal triangle. This
process is illustrated in Figure 2.18, but it does not guarantee a good set of triangles
nor can it handle concave polygons. In Chapter 6, we will discuss the triangulation of
                                                                               2.4 Primitives and Attributes   63
(a)                        (b)                  (c)
FIGURE 2.16 (a) Two-dimensional polygon. (b) A triangulation. (c)
Another triangulation.
(a)                  (b)                  (c)
FIGURE 2.17 (a) Quadrilateral. (b) A triangulation. (c) Another triangu-
lation.
v2
v1                                    v3
v0                  v0

*FIGURE 2.18 Recursive triangulation of a convex polygon.*

simple but nonconvex polygons as part of rasterization. This technique will allows us
to render more general polygons than triangles.
We will delay a discussion of more general triangulation algorithms until we dis-
cuss curves and surfaces in Chapter 10. One reason for this delay is that there are a
number of related processes that arise when we consider modeling surfaces. For ex-
ample, laser-scanning technology allows us to gather millions of unstructured three-
dimensional vertices. We then have to form a surface from these vertices, usually in
the form of a mesh of triangles. The Delaunay triangulation algorithm ﬁnds a best
triangulation in the sense that if we consider the circle determined by any triangle, no
other vertex lies in this circle. Triangulation is a special case of the more general prob-
lem of tessellation, which divides a polygon into a polygonal mesh, not all of which
need be triangles. General tessellation algorithms are complex, especially when the
initial polygon may contain holes.
64         Chapter 2      Graphics Programming

#### 2.4.5 Text

Graphical output in applications such as data analysis and display requires annota-
tion, such as labels on graphs. Although in nongraphical programs textual output is
the norm, text in computer graphics is problematic. In nongraphical applications, we
Computer                      are usually content with a simple set of characters, always displayed in the same man-
ner. In computer graphics, however, we often wish to display text in a multitude of
fashions by controlling type styles, sizes, colors, and other parameters. We also want
Graphics                      to have available a choice of fonts. Fonts are families of typefaces of a particular style,
such as Times, Computer Modern, or Helvetica.

*FIGURE 2.19 Stroke text              There are two forms of text: stroke and raster. Stroke text (Figure 2.19) is con-*

(PostScript font).              structed as are other geometric objects. We use vertices to specify line segments or
curves that outline each character. If the characters are deﬁned by closed boundaries,
we can ﬁll them. The advantage of stroke text is that it can be deﬁned to have all the
detail of any other object, and because it is deﬁned in the same way as other graph-
ical objects are, it can be manipulated by our standard transformations and viewed
like any other graphical primitive. Using transformations, we can make a stroke char-
acter bigger or rotate it, retaining its detail and appearance. Consequently, we need
to deﬁne a character only once, and we can use transformations to generate it at the
desired size and orientation.
Deﬁning a full 128- or 256-character stroke font, however, can be complex, and
the font can take up signiﬁcant memory and processing time. The standard PostScript
fonts are deﬁned by polynomial curves, and they illustrate all the advantages and dis-
advantages of stroke text. The various PostScript fonts can be used for both high- and
low-resolution applications. Often, developers mitigate the problem of slow render-
ing of such stroke characters by putting considerable processing power in the printer.
Raster text (Figure 2.20) is simple and fast. Characters are deﬁned as rectangles
of bits called bit blocks. Each block deﬁnes a single character by the pattern of 0 and
1 bits in the block. A raster character can be placed in the frame buffer rapidly by a
bit-block-transfer (bitblt) operation, which moves the block of bits using a single
function call. We will discuss bitblt in Chapter 7.
You can increase the size of raster characters by replicating, or duplicating,
pixels, a process that gives larger characters a blocky appearance (Figure 2.21). Other
transformations of raster characters, such as rotation, may not make sense, because

*FIGURE 2.20 Raster text.*

                                                                              2.4 Primitives and Attributes   65

*FIGURE 2.21 Raster-character replication.*

the transformation may move the bits deﬁning the character to locations that do not
correspond to the location of pixels in the frame buffer.
Because stroke and bitmap characters can be created from other primitives,
OpenGL does not have a text primitive. However, the GLUT library provides a few
predeﬁned bitmap and stroke character sets that are deﬁned in software and are
portable.

#### 2.4.6 Curved Objects

The primitives in our basic set have all been deﬁned through vertices. With the
exception of the point type, all consist of line segments or use line segments to deﬁne
the boundary of a region that can be ﬁlled with a solid color or a pattern. We can take
two approaches to creating a richer set of objects.
First, we can use the primitives that we have to approximate curves and surfaces.
For example, if we want a circle, we can use a regular polygon of n sides. Likewise,
we have approximated a sphere with triangles and quadrilaterals. More generally, we
approximate a curved surface by a mesh of convex polygons—a tessellation—which
can occur either at the rendering stage or within the user program.
The other approach, which we will explore in Chapter 10, is to start with the
mathematical deﬁnitions of curved objects and then build graphics functions to im-
plement those objects. Objects such as quadric surfaces and parametric polynomial
curves and surfaces are well understood mathematically, and we can specify them
through sets of vertices. For example, we can specify a sphere by its center and a point
on its surface, or we can specify a cubic polynomial curve using data at four points.

#### 2.4.7 Attributes

Although we can describe a geometric object through a set of vertices, a given object
can be displayed in many different ways. Properties that describe how an object
should be rendered are called attributes. Available attributes depend on the type of
object. For example, a line could be black or green. It could be solid or dashed. A
polygon could be ﬁlled with a single color or with a pattern. We could display it as
ﬁlled or only by its edges. Several of these attributes are shown in Figure 2.22 for lines
and polygons.
66   Chapter 2   Graphics Programming
(a)                                    (b)

*FIGURE 2.22 Attributes for (a) lines and (b) polygons.*

Attributes may be associated with, or bound to, geometric objects, such as the
color of a cube. Often we will ﬁnd it better to model an object such as the cube by its
individual faces and to specify attributes for the faces. Hence, a cube would be green
because its six faces are green. Each face could then be described by two triangles so
ultimately a green cube would be rendered as 12 green triangles.
If we go one step further, we see that each of the triangles is speciﬁed through
three vertices. In a pipeline architecture, each vertex is processed independently
through a vertex shader. Hence, we can associate properties with each vertex. For
example, if we assign a different color to each vertex of a polygon, the rasterizer can

```cpp
interpolate these vertex colors to obtain different colors for each fragment. These ver-
```

tex attributes may also be dependent on the application. For example, in a simulation
of heat distribution of some object, the application might determine a temperature
for each vertex deﬁning the object. In Chapter 3, we will include vertex attribute data
in the array with our vertex locations that is sent to the GPU.
In systems that use immediate-mode graphics and a pipeline architecture, some
attributes are part of the state of the graphics systems. Hence, there would be a
current color that would be used to render all primitives until changed by some state-
set_current_color(color);
Likewise, there would be attribute-setting functions for a variety of attributes.5
Each geometric type has a set of attributes. For example, a point has a color
attribute and a size attribute. Line segments can have color, thickness, and pattern
(solid, dashed, or dotted). Filled primitives, such as polygons, have more attributes
because we must use multiple parameters to specify how the ﬁll should be done. We
can ﬁll with a solid color or a pattern. We can decide not to ﬁll the polygon and to
display only its edges. If we ﬁll the polygon, we might also display the edges in a color
different from that of the interior.
5. Earlier versions of OpenGL contained state-setting functions such as glColor, glLineWidth,
and glStipple. These deprecated attributes can be implemented in your shaders.
                                                                                                       2.5 Color         67

*FIGURE 2.23 Stroke-text attributes.*

In systems that support stroke text as a primitive, there is a variety of attributes.
Some of these attributes are demonstrated in Figure 2.23; they include the direction
of the text string, the path followed by successive characters in the string, the height
and width of the characters, the font, and the style (bold, italic, underlined).
Although the notion of current state works well for interactive applications, it
is inconsistent with our physical intuition. A box is green or red. It either contains a
pattern on its surfaces or it doesn’t. Object-oriented graphics takes a fundamentally
different approach in which attributes are part of a geometric object. In Chapter 8,
we will discuss scene graphs, which are fundamental to systems such as Open Scene
Graph, and we will see that they provide another higher-level object-oriented ap-
proach to computer graphics.

### 2.5    COLOR

Color is one of the most interesting aspects of both human perception and computer
graphics. We can use the model of the human visual system from Chapter 1 to obtain
C( )
a simple but useful color model. Full exploitation of the capabilities of the human
visual system using computer graphics requires a far deeper understanding of the
human anatomy, physiology, and psychophysics. We will present a more sophisticated
development in Chapter 6.                                                                             350          780
A visible color can be characterized by a function C(λ) that occupies wavelengths                       
from about 350 to 780 nm, as shown in Figure 2.24. The value for a given wavelength          FIGURE 2.24 A color distri-
λ in the visible spectrum gives the intensity of that wavelength in the color.               bution.
Although this characterization is accurate in terms of a physical color whose
properties we can measure, it does not take into account how we perceive color. As
noted in Chapter 1, the human visual system has three types of cones responsible for
68   Chapter 2   Graphics Programming
Blue                              Yellow
Red                Cyan        Cyan                    Red
White                               Black
Yellow                              Blue
Green                             Magenta
(a)                                (b)

*FIGURE 2.25 Color formation. (a) Additive color.*

(b) Subtractive color.
color vision. Hence, our brains do not receive the entire distribution C(λ) for a given
color but rather three values—the tristimulus values—that are the responses of the
three types of cones to the color. This reduction of a color to three values leads to the
basic tenet of three-color theory: If two colors produce the same tristimulus values,
then they are visually indistinguishable.
A consequence of this tenet is that, in principle, a display needs only three
primary colors to produce the three tristimulus values needed for a human observer.
We vary the intensity of each primary to produce a color, as we saw for the CRT in

## Chapter 1. The CRT is one example of the use of additive color, where the primary

colors add together to give the perceived color. Other examples that use additive color
include projectors and slide (positive) ﬁlm. In such systems, the primaries are usually
red, green, and blue. With additive color, primaries add light to an initially black
display, yielding the desired color.
For processes such as commercial printing and painting, a subtractive color
model is more appropriate. Here we start with a white surface, such as a sheet of
paper. Colored pigments remove color components from light that is striking the
surface. If we assume that white light hits the surface, a particular point will be red
if all components of the incoming light are absorbed by the surface except for wave-
lengths in the red part of the spectrum, which are reﬂected. In subtractive systems, the
primaries are usually the complementary colors: cyan, magenta, and yellow (CMY;
Figure 2.25). We will not explore subtractive color here. You need to know only that
an RGB additive system has a dual with a CMY subtractive system (see Exercise 2.8).
We can view a color as a point in a color solid, as shown in Figure 2.26 and
in Color Plate 21. We draw the solid using a coordinate system corresponding to
the three primaries. The distance along a coordinate axis represents the amount of
the corresponding primary in the color. If we normalize the maximum value of each
primary to be 1, then we can represent any color that we can produce with this set of
primaries as a point in a unit cube. The vertices of the cube correspond to black (no
primaries on); red, green, and blue (one primary fully on); the pairs of primaries,
cyan (green and blue fully on), magenta (red and blue fully on), and yellow (red
and green fully on); and white (all primaries fully on). The principal diagonal of
                                                                                          2.5 Color   69

*FIGURE 2.26 Color solid.*

the cube connects the origin (black) with white. All colors along this line have equal
tristimulus values and appear as shades of gray.
There are many matters that we are not exploring fully here and will return to
in Chapter 6. Most concern the differences among various sets of primaries or the
limitations conferred by the physical constraints of real devices. In particular, the
set of colors produced by one device—its color gamut—is not the same as for other
devices, nor will it match the human’s color gamut. In addition, the tristimulus values
used on one device will not produce the same visible color as the same tristimulus
values used on another device.

#### 2.5.1 RGB Color

Now we can look at how color is handled in a graphics system from the programmer’s
perspective—that is, through the API. There are two different approaches. We will
stress the RGB-color model because an understanding of it will be crucial for our
later discussion of shading. Historically, the indexed-color model (Section 2.5.2) was
easier to support in hardware because of its lower memory requirements and the
limited colors available on displays, but in modern systems RGB color has become
the norm.
In a three-primary-color, additive-color RGB system, there are conceptually sep-
arate buffers for red, green, and blue images. Each pixel has separate red, green, and
blue components that correspond to locations in memory (Figure 2.27). In a typical
system, there might be a 1280 × 1024 array of pixels, and each pixel might consist
of 24 bits (3 bytes): 1 byte for each of red, green, and blue. With present commodity
graphics cards having up to 12GB of memory, there is no longer a problem of storing
and displaying the contents of the frame buffer at video rates.
As programmers, we would like to be able to specify any color that can be
stored in the frame buffer. For our 24-bit example, there are 224 possible colors,
sometimes referred to as 16M colors, where M denotes 10242. Other systems may
70   Chapter 2   Graphics Programming

*FIGURE 2.27 RGB color.*

have as many as 12 (or more) bits per color or as few as 4 bits per color. Because
our API should be independent of the particulars of the hardware, we would like to
specify a color independently of the number of bits in the frame buffer and to let the
drivers and hardware match our speciﬁcation as closely as possible to the available
display. A natural technique is to use the color cube and to specify color components
as numbers between 0.0 and 1.0, where 1.0 denotes the maximum (or saturated
value) of the corresponding primary and 0.0 denotes a zero value of that primary.
In applications in which we want to assign a color to each vertex, we can put
colors into a separate data structure, such as
typedef vec3 color3;
color3 colors[3] = {color3(1.0, 0.0, 0.0), color3(0.0, 1.0, 0.0),
color3(0.0, 0.0. 1.0)};
which holds the colors red, green, and blue, or we could create a single array that
contains both vertex locations and vertex colors. These data can be sent to the shaders,
where colors will be applied to pixels in the frame buffer.
Later, we shall be interested in a four-color (RGBA) system. The fourth color (A,
or alpha) also is stored in the frame buffer, as are the RGB values; it can be set with
four-dimensional versions of the color functions. In Chapter 7, we will see various
uses for alpha, such as combining images. Here we need to specify the alpha value as
part of the initialization of an OpenGL program. If blending is enabled (Chapter 7),
then the alpha value will be treated by OpenGL as either an opacity or transparency
value. Transparency and opacity are complements of each other. An opaque object
passes no light through it; a transparent object passes all light. Opacity values can
range from fully transparent (A=0.0) to fully opaque (A=1.0).
One of the ﬁrst tasks that we must do in a program is to clear an area of the
screen—a drawing window—in which to display our output. We also must clear
this window whenever we want to draw a new frame. By using the four-dimensional
(RGBA) color system, the graphics and operating systems can interact to create effects
where the drawing window interacts with other windows that may be beneath it by
manipulating the opacity assigned to the window when it is cleared. The function call
                                                                                          2.5 Color   71
glClearColor(1.0, 1.0, 1.0, 1.0);
speciﬁes an RGB-color clearing color that is white, because the ﬁrst three components
are set to 1.0, and is opaque, because the alpha component is 1.0. We can then use
the function glClear to make the window on the screen solid and white. Note
that by default blending is not enabled. Consequently, the alpha value can be set in
glClearColor to a value other than 1.0 and the default window will still be opaque.

#### 2.5.2 Indexed Color

Early graphics systems had frame buffers that were limited in depth. For example,
we might have had a frame buffer with a spatial resolution of 1280 × 1024, but each
pixel was only 8 bits deep. We could divide each pixel’s 8 bits into smaller groups of
bits and assign red, green, and blue to each. Although this technique was adequate in
a few applications, it usually did not give us enough ﬂexibility with color assignment.
Indexed color provided a solution that allowed applications to display a wide range of
colors as long as the application did not need more colors than could be referenced
by a pixel. Although indexed color is no longer part of recent versions of OpenGL,
this technique can be created within an application.
We follow an analogy with an artist who paints in oils. The oil painter can
produce an almost inﬁnite number of colors by mixing together a limited number of
pigments from tubes. We say that the painter has a potentially large color palette. At
any one time, however, perhaps due to a limited number of brushes, the painter uses
only a few colors. In this fashion, she can create an image that, although it contains
a small number of colors, expresses her choices because she can select the few colors
from a large palette.
Returning to the computer model, we can argue that if we can choose for each
application a limited number of colors from a large selection (our palette), we should
be able to create good-quality images most of the time.
We can select colors by interpreting our limited-depth pixels as indices into a
table of colors rather than as color values. Suppose that our frame buffer has k bits
per pixel. Each pixel value or index is an integer between 0 and 2k − 1. Suppose that
we can display each color component with a precision of m bits; that is, we can choose
from 2m reds, 2m greens, and 2m blues. Hence, we can produce any of 23m colors on the
display, but the frame buffer can specify only 2k of them. We handle the speciﬁcation
through a user-deﬁned color-lookup table that is of size 2k × 3m (Figure 2.28). The
user program ﬁlls the 2k entries (rows) of the table with the desired colors, using m
bits for each of red, green, and blue. Once the user has constructed the table, she can
specify a color by its index, which points to the appropriate entry in the color-lookup
table (Figure 2.29). For k = m = 8, a common conﬁguration, she can choose 256 out
of 16 M colors. The 256 entries in the table constitute the user’s color palette.
In systems that support color-index mode, the present color is selected by a
function that selects a particular color out of the table. Setting and changing the
entries in the color-lookup table involves interacting with the window system. One
difﬁculty arises if the window system and underlying hardware support only a limited
72   Chapter 2   Graphics Programming
Input         Red     Green          Blue
0            0        0             0
1          2m  1     0             0
·            0      2 1            0
·             ·       ·             ·
·             ·       ·             ·
2 1             ·       ·             ·

*FIGURE 2.28 Color-lookup table.*

Color-         Red
Color-         Green
Color-         Blue
Frame buffer                 lookup table

*FIGURE 2.29 Indexed color.*

number of colors because the window system may have only a single color table that
must be used for all its windows, or it might have to juggle multiple tables, one for
each window on the screen.
Historically, color-index mode was important because it required less memory
for the frame buffer and fewer other hardware components. However, cost is no
longer an issue, and color-index mode presents a few problems. When we work with
dynamic images that must be shaded, usually we need more colors than are provided
by color-index mode. In addition, the interaction with the window system is also
more complex than with RGB color. Consequently, for the most part, we will assume
that we are using RGB color.
The major exception is when we consider a technique called pseudocoloring,
where we start with a monochromatic image. For example, we might have scalar
values of a physical entity such as temperature that we wish to display in color. We
can create a mapping of these values to red, green, and blue that are identical to the
color-lookup tables used for indexed color.

#### 2.5.3 Setting of Color Attributes

For our simple example program, we use RGB color. We have three attributes to set.
The ﬁrst is the clear color, which is set to white by the following function call:
                                                                                                        2.6 Viewing   73
glClearColor(1.0, 1.0, 1.0, 1.0);
Note this function uses RGBA color.
The color we use to render points is set in the shaders. We can set an RGB color
typedef vec3 color3;
color3 point_color = color3(1.0, 0.0, 0.0);
typedef vec4 color4;
color4 point_color = color4(1.0, 0.0, 0.0, 1.0);
and send this color to the vertex shader. We could also set the color totally in the
shader. We will see a few options later in this chapter. We can set the size of our
rendered points to be 2 pixels wide by using the following OpenGL function:
glPointSize(2.0);
Note that attributes, such as the point size6 and line width, are speciﬁed in terms of
the pixel size. Hence, if two displays have different-sized pixels (due to their particular
screen dimensions and resolutions), then the rendered images may appear slightly
different. Certain graphics APIs, in an attempt to ensure that identical displays will be
produced on all systems with the same user program, specify all attributes in a device-
independent manner. Unfortunately, ensuring that two systems produce the same
display has proved to be a difﬁcult implementation problem. OpenGL has chosen
a more practical balance between desired behavior and realistic constraints.

### 2.6     VIEWING

We can now put a variety of graphical information into our world, and we can
describe how we would like these objects to appear, but we do not yet have a method
for specifying exactly which of these objects should appear on the screen. Just as what
we record in a photograph depends on where we point the camera and what lens we
use, we have to make similar viewing decisions in our program.
A fundamental concept that emerges from the synthetic-camera model that we

```cpp
introduced in Chapter 1 is that the speciﬁcation of the objects in our scene is com-
```

pletely independent of our speciﬁcation of the camera. Once we have speciﬁed both
the scene and the camera, we can compose an image. The camera forms an image
by exposing the ﬁlm, whereas the computer system forms an image by carrying out
6. Note that point size is one of the few state variables that can be set using an OpenGL function in
the latest versions.
74   Chapter 2   Graphics Programming
a sequence of operations in its pipeline. The application program needs to worry
only about the speciﬁcation of the parameters for the objects and the camera, just
as the casual photographer is concerned about the resulting picture, not about how
the shutter works or the details of the photochemical interaction of ﬁlm with light.
There are default viewing conditions in computer image formation that are sim-
ilar to the settings on a basic camera with a ﬁxed lens. However, a camera that has a
ﬁxed lens and sits in a ﬁxed location forces us to distort our world to take a picture.
We can create pictures of elephants only if we place the animals sufﬁciently far from
the camera, or we can photograph ants only if we put the insects relatively close to the
lens. We prefer to have the ﬂexibility to change the lens to make it easier to form an
image of a collection of objects. The same is true when we use our graphics system.

#### 2.6.1 The Orthographic View

The simplest and OpenGL’s default view is the orthographic projection. We discuss
this projection and others in detail in Chapter 4, but we introduce the orthographic
projection here so that you can get started writing three-dimensional programs.
Mathematically, the orthographic projection is what we would get if the camera in
our synthetic-camera model had an inﬁnitely long telephoto lens and we could then
place the camera inﬁnitely far from our objects. We can approximate this effect, as
shown in Figure 2.30, by leaving the image plane ﬁxed and moving the camera far
from this plane. In the limit, all the projectors become parallel, and the center of
projection is replaced by a direction of projection.
Rather than worrying about cameras an inﬁnite distance away, suppose that we
start with projectors that are parallel to the positive z-axis and the projection plane at
z = 0, as shown in Figure 2.31. Note that not only are the projectors perpendicular or
orthogonal to the projection plane, but also we can slide the projection plane along
the z-axis without changing where the projectors intersect this plane.
For orthographic viewing, we can think of there being a special orthographic
camera that resides in the projection plane, something that is not possible for other
views. Perhaps more accurately stated, there is a reference point in the projection
plane from which we can make measurements of a view volume and a direction of
projection. In OpenGL, the reference point starts off at the origin and the camera
points in the negative z-direction, as shown in Figure 2.32. The orthographic pro-
jection takes a point (x, y, z) and projects it into the point (x, y, 0), as shown in
Figure 2.33. Note that if we are working in two dimensions with all vertices in the
plane z = 0, a point and its projection are the same; however, we can employ the
machinery of a three-dimensional graphics system to produce our image.
In OpenGL, an orthographic projection with a right-parallelepiped viewing vol-
ume is the default. The volume is the cube deﬁned by the planes
x = ±1,
y = ±1,
z = ±1.
                                                                                           2.6 Viewing   75

*FIGURE 2.30 Creating an orthographic view by moving the camera*

away from the projection plane.
The orthographic projection “sees” only those objects in the volume speciﬁed by
this viewing volume. Unlike a real camera, the orthographic projection can include
objects behind the camera. Thus, because the plane z = 0 is located between −1 and
1, the two-dimensional plane intersects the viewing volume.
In Chapters 3 and 4, we will learn to use transformations to create other views.
For now, we will scale and position our objects so those that we wish to view are inside
the default volume.
76   Chapter 2   Graphics Programming
z=0

*FIGURE 2.31 Orthographic projectors with projection plane z = 0.*

y        (right, top, far)
(left, bottom, near)

*FIGURE 2.32 The default camera and an orthographic view volume.*

z0
(x, y, 0)
(x, y, z)

*FIGURE 2.33 Orthographic projection.*

                                                                                           2.6 Viewing   77
Plane z  0

*FIGURE 2.34 Viewing volume.*

(b)
(a)

*FIGURE 2.35 Two-dimensional viewing. (a) Objects before clipping.*

(b) Image after clipping.

#### 2.6.2 Two-Dimensional Viewing

Remember that, in our view, two-dimensional graphics is a special case of
three-dimensional graphics. Our viewing area is in the plane z = 0 within a three-
dimensional viewing volume, as shown in Figure 2.34.
We could also consider two-dimensional viewing directly by taking a rectangular
area of our two-dimensional world and transferring its contents to the display, as
shown in Figure 2.35. The area of the world that we image is known as the viewing
rectangle, or clipping rectangle. Objects inside the rectangle are in the image; objects
outside are clipped out and are not displayed. Objects that straddle the edges of the
rectangle are partially visible in the image. The size of the window on the display
and where this window is placed on the display are independent decisions that we
examine in Section 2.7.
78   Chapter 2   Graphics Programming

### 2.7    CONTROL FUNCTIONS

We are almost done with our ﬁrst program, but we must discuss the minimal inter-
actions with the window and operating systems. If we look at the details for a speciﬁc
environment, such as the X Window System on a Linux platform or Microsoft Win-
dows on a PC, we see that the programmer’s interface between the graphics system
and the operating and window systems can be complex. Exploitation of the pos-
sibilities open to the application programmer requires knowledge speciﬁc to these
systems. In addition, the details can be different for two different environments, and
discussing these differences will do little to enhance our understanding of computer
graphics.
Rather than deal with these issues in detail, we look at a minimal set of operations
that must take place from the perspective of the graphics application program. Earlier
we discussed the OpenGL Utility Toolkit (GLUT); it is a library of functions that
provides a simple interface between the systems. Details speciﬁc to the underlying
windowing or operating system are inside the implementation, rather than being
part of its API. Operationally, we add another library to our standard library search
path. GLUT will help us to understand the interactions that characterize modern

```cpp
interactive graphics systems, including a wide range of APIs, operating systems, and
```

window systems. The application programs that we produce using GLUT should run
under multiple window systems.

#### 2.7.1 Interaction with the Window System

The term window is used in a number of different ways in the graphics and worksta-
tion literature. We use window, or screen window, to denote a rectangular area of
our display. We are concerned only with raster displays. A window has a height and
width, and because the window displays the contents of the frame buffer, positions
in the window are measured in window or screen coordinates,7 where the units are
pixels.
In a modern environment, we can display many windows on the monitor. Each
can have a different purpose, ranging from editing a ﬁle to monitoring our system.
We use the term window system to refer to the multiwindow environment provided by
systems such as the X Window System and Microsoft Windows. The window in which
the graphics output appears is one of the windows managed by the window system.
Hence, to the window system, the graphics window is a particular type of window—
one in which graphics can be displayed or rendered. References to positions in this
window are relative to one corner of the window. We have to be careful about which
corner is the origin. In science and engineering, the lower-left corner is the origin
and has window coordinates (0, 0). However, virtually all raster systems display
7. In OpenGL, window coordinates are three-dimensional, whereas screen coordinates are two-
dimensional. Both systems use units measured in pixels for x and y, but window coordinates retain
depth information.
                                                                                      2.7 Control Functions   79
their screens in the same way as commercial television systems do—from top to
bottom, left to right. From this perspective, the top-left corner should be the origin.
Our OpenGL functions assume that the origin is bottom left, whereas information
returned from the windowing system, such as the mouse position, often has the
origin at the top left and thus requires us to convert the position from one coordinate
system to the other.
Although our display may have a resolution of, say, 1280 × 1024 pixels, the
window that we use can have any size. Thus, the frame buffer should have a resolution
at least equal to the display size. Conceptually, if we use a window of 300 × 400 pixels,
we can think of it as corresponding to a 300 × 400 frame buffer, even though it uses
only a part of the real frame buffer.
Before we can open a window, there must be interaction between the windowing
system and OpenGL. In GLUT, this interaction is initiated by the following function
call:
glutInit(int *argc, char **argv);
The two arguments allow the user to pass command-line arguments, as in the stan-
dard C main function, and are usually the same as in main. We can now open an
glutCreateWindow(char *title);
where the title at the top of the window is given by the string title.
The window that we create has a default size, a position on the screen, and

```cpp
characteristics such as the use of RGB color. We can also use GLUT functions before
```

window creation to specify these parameters. For example, the code
glutInitDisplayMode(GLUT_RGB | GLUT_DEPTH | GLUT_DOUBLE);
glutInitWindowSize(640, 480);
glutInitWindowPosition(0, 0);
speciﬁes a 640 width × 480 height window in the top-left corner of the display. We
specify RGB rather than indexed (GLUT_INDEX) color, a depth buffer for hidden-
surface removal, and double rather than single (GLUT_SINGLE) buffering. The de-
faults, which are all we need for now, are RGB color, no hidden-surface removal, and
single buffering. Thus, we do not need to request these options explicitly, but speci-
fying them makes the code clearer. Note that parameters are logically or’ed together
in the argument to glutInitDisplayMode.

#### 2.7.2 Aspect Ratio and Viewports

The aspect ratio of a rectangle is the ratio of the rectangle’s width to its height.
The independence of the object, viewing, and workstation window speciﬁcations can
cause undesirable side effects if the aspect ratio of the viewing rectangle, speciﬁed
by camera parameters, is not the same as the aspect ratio of the window speciﬁed
80   Chapter 2   Graphics Programming
(a)                               (b)

*FIGURE 2.36 Aspect-ratio mismatch. (a) Viewing rectangle. (b) Display*

window.
by glutInitWindowSize. If they differ, as depicted in Figure 2.36, objects are
distorted on the screen. This distortion is a consequence of our default mode of
operation, in which the entire clipping rectangle is mapped to the display window.
The only way that we can map the entire contents of the clipping rectangle to the
entire display window is to distort the contents of the former to ﬁt inside the latter. We
can avoid this distortion if we ensure that the clipping rectangle and display window
have the same aspect ratio.
Another, more ﬂexible, method is to use the concept of a viewport. A viewport
is a rectangular area of the display window. By default, it is the entire window, but it
can be set to any smaller size in pixels via the function

```cpp
void glViewport(GLint x, GLint y, GLsizei w, GLsizei h);
```

where (x,y) is the lower-left corner of the viewport (measured relative to the lower-
left corner of the window) and w and h give the width and height, respectively.
The types are all integers that allow us to specify positions and distances in pixels.
Primitives are displayed in the viewport, as shown in Figure 2.37. For a given window,
we can adjust the height and width of the viewport to match the aspect ratio of the
clipping rectangle, thus preventing any object distortion in the image.
The viewport is part of the state. If we change the viewport between rendering
objects or rerender the same objects with the viewport changed, we achieve the effect
of multiple viewports with different images in different parts of the window. We will
see further uses of the viewport in Chapter 3, where we consider interactive changes
in the size and shape of the window.

#### 2.7.3 The main, display, and init Functions

In principle, we should be able to combine the simple initialization code with our
code from Section 2.1 to form a complete OpenGL program that generates the Sier-
pinski gasket. Unfortunately, life in a modern system is not that simple. There are
                                                                                     2.7 Control Functions   81

*FIGURE 2.37 A mapping to the viewport.*

two problems: One is generic to all graphics systems; the second has more to do with
problems of interacting with the underlying windowing system.
Our basic mechanism for display will be to form a data structure that contains
all the geometry and attributes we need to specify a primitive and how we would like
it displayed. We then send this structure to the shaders that will process our data and
display the results. Once the application has sent the data to the shaders, it is free
to do other tasks. In an interactive application, we would continue to generate more
primitives.
However, for an application such as our sample program, we draw a few primi-
tives and are ﬁnished. As the application ends, the application window will disappear
from the display before we have had a chance to see our output. A simple solution for
this problem might be to insert a delay, for example, via a standard function such as
sleep(enough_time) to give us enough time to view our output. For any but the
most trivial applications, however, we need a more sophisticated mechanism.
The mechanism employed by most graphics and window systems is to use event
processing, which gives us interactive control in our programs. Events are changes
that are detected by the operating system and include such actions as a user pressing
a key on the keyboard, the user clicking a mouse button or moving the mouse, or the
user iconifying a window on the display. Events are varied, and usually only a subset
of them is important to graphics applications. An event may generate data that are
stored with the occurrence of the event. For example, if a key is pressed, the code for
the key will be stored.
When events occur they are placed in queue, the event queue, that can be ex-
amined by an application program or by the operating system. A given event can be
ignored or cause an action to take place. For example, an application that does not use
the keyboard will ignore all pressing and releasing of keys, whereas an application that
uses the keyboard might use keyboard events to control the ﬂow of the application.
With GLUT, we can execute the function
glutMainLoop( );
82   Chapter 2   Graphics Programming
to begin an event-processing loop. If there are no events to process, the program will
sit in a wait state, with our graphics on the screen, until we terminate the program
through some external means—say, by hitting a special key or a combination of keys,
such as Control-C—that terminates the execution of the program.
If there are events in the queue, our program responds to them through func-
tions called callbacks. A callback function is associated with a speciﬁc type of event.
Hence, a typical interactive application would use a mouse callback and perhaps a
keyboard callback. For our simple example, we need only a single callback called the
display callback. A display callback is generated when the application program or the
operating system determines that the graphics in a window need to be redrawn. One
of these times is when during initialization, the application creates a window on the
display. Hence, virtually every program must have a display callback function that is
executed when the callback occurs.
The display callback function is named through the GLUT function

```cpp
void glutDisplayFunc(void (*func)(void));
```

and registered with the window system. Here the function named func will be called
whenever the windowing system determines that the OpenGL window needs to be
redisplayed. Because one of these times is when the window is ﬁrst opened, if we
put all our graphics into this function (for our noninteractive example), func will
be executed once and our gasket will be drawn. Although it may appear that our
use of the display function is merely a convenience for organizing our program, the
display function is required by GLUT. A display callback is also invoked, for example,
when the window is moved from one location on the screen to another and when
a window in front of the OpenGL window is destroyed, making visible the whole
OpenGL window.
Following is a main function that works for most noninteractive applications:

```cpp
#include <glew.h>
#include <GL/glut.h>
int main(int argc, char **argv)
{
```

glutInit(&argc, argv);
glutInitDisplayMode (GLUT_SINGLE | GLUT_RGB);
glutInitWindowSize(500, 500);
glutInitWindowPosition(0, 0);
glutCreateWindow("simple OpenGL example");
glewInit();
glutDisplayFunc(display);
init();
glutMainLoop();

```cpp
}
```

                                                                                2.8 The Gasket Program   83
We use an initialization function init() to set the OpenGL state variables dealing
with viewing and attributes—parameters that we prefer to set once, independently
of the display function. The standard include (.h) ﬁle for GLUT is loaded before
the beginning of the function deﬁnitions. In most implementations, the compiler

```cpp
#include <GL/glut.h>
```

will add in the header ﬁles for the GLUT library and the OpenGL library (gl.h).
The macro deﬁnitions for our standard values, such as GL_LINES and GL_RGB, are
in these ﬁles. If we are using the GLEW library, we usually need only add the include
ﬁle and execute glewInit.

#### 2.7.4 Program Structure

Every program we write will have a similar structure to our gasket program. We will
always use the GLUT toolkit. The main function will then consist of calls to GLUT
functions to set up our window(s) and to make sure that the local environment sup-
ports the required display properties. The main will also name the required callbacks
and callback functions. Every program must have a display callback, and most will
have other callbacks to set up interaction. The init function will set up user options,
usually through OpenGL functions in the GL library. Although these options could
be set in main, it is clearer to keep GLUT functions separate from OpenGL functions.
In the majority of programs, the graphics output will be generated in the display
callback.
Every application, no matter how simple, must provide both a vertex shader
and a fragment shader. Setting up the shaders requires a number of steps, including
reading the shader code from ﬁles, compiling the code, and linking the shaders with
the application. These steps are almost identical for most applications. Hence, we will
put this code into a function initShaders. These operations require a handful of
OpenGL functions that have little to do with graphics. Consequently, we place the
details of these functions in Appendix A.

### 2.8   THE GASKET PROGRAM

We can now complete our gasket program. We have already created the points and
put them in an array. Now we have to get these data to our GPU and render them.
We start by creating a vertex-array object that will allow us to bundle data associated
with a vertex array. Use of multiple vertex-array objects will make it easy to switch
among different vertex arrays. We use glGenVertexArray to ﬁnd an unused name
for the buffer. The ﬁrst time the function glBindVertexArray is executed for a
given name, the object is created. Subsequent calls to this function make the named
object active. For this example, we need only a single vertex array buffer that we set
up as follows:
84   Chapter 2   Graphics Programming
GLuint abuffer;
glGenVertexArrays(1, &abuffer);
glBindVertexArray(abuffer);
Next, we create a buffer object on the GPU and place our data in that object. We need
three functions that we can call after we have generated our points:
GLuint buffer;
glGenBuffers(1, &buffer);
glBindBuffer(GL_ARRAY_BUFFER, buffer);
glBufferData(GL_ARRAY_BUFFER, sizeof(points),
points, GL_STATIC_DRAW);
First, we use glGenBuffers to give us an unused identiﬁer for our buffer object
that is put into the variable buffer. The function glBindBuffer creates the buffer
with the identiﬁer from glGenBuffers. The type GL_ARRAY_BUFFER indicates that
the data in the buffer will be vertex attribute data rather than some one of the other
storage types that we will encounter later. Finally, with glBufferData, we allocate
sufﬁcient memory on the GPU for our data and provide a pointer to the array holding
the data. Once data is in GPU memory, we might, as in this example, simply display
it once. But in more realistic applications we might alter the data, redisplay it many
times, and even read data back from the GPU to the CPU. Modern GPUs can alter
how they store data to increase efﬁciency depending on the type of application. The
ﬁnal parameter in glBufferData gives a hint of how the application plans to use
the data. In our case, we are sending it once and displaying it so the choice of GL_
STATIC_DRAW is appropriate. The code to compute the points and create the buffer
object can be part of initialization.

#### 2.8.1 Rendering the Points

When we want to display our points, we can use the function
glDrawArrays(GL_POINTS, 0, N);
which causes N data to be rendered starting with the ﬁrst point. The value of the ﬁrst
parameter, GL_POINTS, tells the GPU we want the data to be used to display distinct
points rather than other primitives such as lines or polygons that could be described
by the same data. Thus, a simple display callback is

```cpp
void mydisplay(void)
{
```

glClear(GL_COLOR_BUFFER_BIT);
glDrawArrays(GL_POINTS, 0, N);
glFlush();

```cpp
}
```

                                                                                 2.8 The Gasket Program   85
We clear the frame buffer and then render the point data that is on the GPU. The
glFlush ensures that all the data are rendered as soon as possible. If you leave it out,
the program should work correctly, but you notice a delay in a busy or networked
environment.
But this is just the beginning of the story. The rendering process must be carried
out by the pipeline of the vertex shader, the rasterizer, and the fragment shader in
order to get the proper pixels displayed in the frame buffer. Because our example uses
only points, we need only develop very simple shaders and put together the whole
application. Even though our shaders will be almost trivial, we must provide both
a vertex shader and fragment shader to have a complete application. There are no
default shaders.

#### 2.8.2 The Vertex Shader

The only information that we put in our buffer object is the location of each point.
When we execute glDrawArrays, each of the NumPoints vertices generates an
execution of a vertex shader that we must provide. If we leave the color determination
to the fragment shader, all the vertex shader must do is pass the vertex’s location to
the rasterizer. Although we will see many more tasks that can be done in a vertex
shader, the absolute minimum it must do is send a vertex location to the rasterizer.
We write our shader using the OpenGL Shading Language (GLSL), which is a
C-like language with which we can write both vertex and fragment shaders. We will
discuss GLSL in more detail later when we want to write more sophisticated shaders,
but here is the code for a simple pass-through vertex shader:
in vec4 vPosition;

```cpp
void main()
{
```

gl_Position = vPosition;

```cpp
}
```

Each shader is a complete program with main as its entry point. GLSL expands
the C data types to include matrix and vector types. The type vec4 is equivalent to a
C++ class for a four-element array of floats. We have provided similar types for the
application side in vec.h and will introduce more in Chapter 3. The input vertex’s
location is given by the four-dimensional vector vPosition whose speciﬁcation
includes the keyword in to signify that its value is input to the shader when the shader
is initiated. There is one special state variable in our shader: gl_Position, which is
the position that will be passed to the rasterizer and must be output by every vertex
shader. Because gl_Position is known to OpenGL, we need not declare it in the
shader.
In general, a vertex shader will transform the representation of a vertex location
from whatever coordinate system in which it is speciﬁed to a representation in clip
coordinates for the rasterizer. However, because we speciﬁed the values in our appli-
cation in clip coordinates, our shader does not have to make any changes to the values
input to the shader and merely passes them through via gl_Position.
86   Chapter 2   Graphics Programming
We still have to establish a connection between the array points in the applica-
tion and the input array vPosition in the shader. We will do this after we compile
and link our shaders. First, we look at the fragment shader.

#### 2.8.3 The Fragment Shader

Each invocation of the vertex shader outputs a vertex that then goes through primitive
assembly and clipping before reaching the rasterizer. The rasterizer outputs fragments
for each primitive inside the clipping volume. Each fragment invokes an execution of
the fragment shader. At a minimum, each execution of the fragment shader must
output a color for the fragment unless the fragment is to be discarded. Here is a
minimum GLSL fragment shader:

```cpp
void main()
{
```

gl_FragColor = vec4(1.0, 0.0, 0.0, 1.0);

```cpp
}
```

All this shader does is assign a four-dimensional RGBA color to each fragment
through the built-in variable gl_FragColor. The A component of the color is its
opacity. We want our points to be opaque and not translucent, so we use A = 1.0.
Setting R to 1.0 and the other two components to 0.0 colors each fragment red.

#### 2.8.4 Combining the Parts

We now have the pieces but need to put them together. In particular, we have to
compile the shaders, connect variables in the application with their counterparts in
the shaders, and link everything together. We start with the bare minimum. Shaders
must be compiled and linked. Most of the time we will do these operations as part of
initialization so we can put the necessary code in a function initShader that will
remain almost unchanged from application to application.

#### 2.8.5 The initShader Function

A typical application contains three distinct parts: the application program, which
comprises a main function and other functions such as init, a vertex shader, and a
fragment shader. The ﬁrst part is a set of C (or C++) functions, whereas the shaders
are written in GLSL. To obtain a module that we can execute, we have to connect
these entities, a process that involves reading source code from ﬁles, compiling the
individual parts, and linking everything together. We can control this process through
our application using a set of OpenGL functions that we will discuss in detail in

## Chapter 3. Here it will be sufﬁcient to describe the steps brieﬂy.

Our ﬁrst step is to create a container called a program object to hold our shaders
and two shader objects, one for each type of shader. The program object has an

```cpp
integer identiﬁer we can use to refer to it in the application. After we create these
```

objects, we can attach the shaders to the program object. Generally, the shader source
code will be in standard text ﬁles. We read them into strings that can be attached
to the program and compiled. If the compilation is successful, the application and
                                                                                2.8 The Gasket Program   87
shaders can be linked together. Assuming we have the vertex shader source in a ﬁle
vshader.glsl and the fragment shader in a ﬁle fshader.glsl, we can execute
the above steps by a function call of the form
GLuint program;
program = InitShader("vsource.glsl", "fsource.glsl");
in the main function of the application.
When we link the program object and the shaders, the names of shader variables
are bound to indices in tables that are created in the linking process. The function
glGetAttribLocation returns the index of an attribute variable, such as the vertex
location attribute vPosition in our vertex shader. From the perspective of the
application program, the client, we have to do two things. We have to enable the
vertex attributes that are in the shaders (glEnableVertexAttribArray), and we
must describe the form of the data in the vertex array (glVertexAttribPointer),
GLuint loc;
loc = glGetAttribLocation(program, "vPosition");
glEnableVertexAttribArray(loc);
glVertexAttribPointer(loc, 2, GL_FLOAT, GL_FALSE, 0,
BUFFER_OFFSET(0));
In glVertexAttribPointer, the second and third parameters specify that the
array points is a two-dimensional array of floats. The fourth parameter says that
we do not want the data normalized to be the range (0.0, 1.0), whereas the ﬁfth states
that the values in the array are contiguous. We will deal with noncontiguous data in
later examples. The last parameter is the address in the buffer where the data begin.
In this example, we have only a single data array points so the zero value works. A
more robust strategy is to specify a buffer offset and use it as follows:

```cpp
#define BUFFER_OFFSET(bytes) ((GLvoid*) (bytes))
```

glVertexAttribPointer(loc, 2, GL_FLOAT, GL_FALSE, 0,
BUFFER_OFFSET(0));
Note that the data in points in the application consists of only x and y values,
whereas the array vPosition in the vertex shader is four dimensional. This dif-
ference does not create a problem, because we have described the data correctly in
our function parameters. The underlying reason for the differences is a fundamental
aspect of how our graphics systems work. We want our application programs to be as
close to the problem as possible. Some of our applications will be two dimensional;
most will be three dimensional, and some may even be four dimensional.
88         Chapter 2   Graphics Programming
A complete listing of this program, the initShader function and a function
for reading shader source code, as well as other example programs that we generate
in subsequent chapters, are given in Appendix A.

### 2.9    POLYGONS AND RECURSION

The output from our gasket program (Figure 2.2) shows considerable structure. If
we were to run the program with more iterations, then much of the randomness in
the image would disappear. Examining this structure, we see that regardless of how
many points we generate, there are no points in the middle. If we draw line segments
connecting the midpoints of the sides of the original triangle, dividing the original

*FIGURE 2.38 Bisecting the    triangle into four triangles, the middle one contains no points (Figure 2.38).*

sides of a triangle.              Looking at the other three triangles, we see that we can apply the same obser-
vation to each of them; that is, we can subdivide each of these triangles into four
triangles by connecting the midpoints of the sides, and each middle triangle will con-
tain no points.
This structure suggests a second method for generating the Sierpinski gasket—
one that uses polygons instead of points and does not require the use of a random-
number generator. One advantage of using polygons is that we can ﬁll solid areas on
our display. Our strategy is to start with a single triangle, subdivide it into four smaller
triangles by bisecting the sides, and then to remove the middle triangle from further
consideration. We repeat this procedure on the remaining triangles until the size of
the triangles that we are removing is small enough—about the size of one pixel—that
we can draw the remaining triangles.
We can implement the process that we just described through a recursive pro-
gram. We start its development with a simple function that adds the locations of the
three vertices that specify a triangle to an array points:

```cpp
#include "vec.h"
```

typedef vec2 point2;

```cpp
void triangle(point2 a, point2 b, point2 c)
```

/* specify one triangle */

```cpp
{
static int i = 0;
```

points[i] = a;
i++;
points[i] = b;
i++;
points[i] = c;
i++;

```cpp
}
```

                                                                              2.9 Polygons and Recursion   89
Hence, each time that triangle is called it adds three two-dimensional vertices to
the data array.
Suppose that the vertices of our original triangle are given by the following array:
point2 v[3];
Then the midpoints of the sides are given by the array mid[3], which can be com-
puted using the following code:
point2 mid[3];
mid[0] = (v[0] + v[1])/2.0;
mid[1] = (v[0] + v[2])/2.0;
mid[2] = (v[1] + v[2])/2.0;
With these six locations, we can use triangle to place the data for the three triangles
formed by (v[0], mid[0], mid[1]), (v[2], mid[1], mid[2]), and (v[1],
mid[2], mid[0]) in points. However, we do not simply want to draw these trian-
gles; we want to subdivide them. Hence, we make the process recursive. We specify a
divide_triangle(point2 a, point2 b, point2 c, int k);
that will draw the triangles only if k is zero. Otherwise, it will subdivide the triangle
speciﬁed by a, b, and c and decrease k. Here is the code:

```cpp
void divide_triangle(point2 a, point2 b, point2 c, int k)
{
```

if(k > 0)

```cpp
{
// compute midpoints of sides
```

point2 ab = (a + b)/2.0;
point2 ac = (a + c)/2.0;
point2 bc = (b + c)/2.0;

```cpp
// subdivide all but inner triangle
```

divide_triangle(a, ab, ac, k-1);
divide_triangle(c, ac, bc, k-1);
divide_triangle(b, bc, ab, k-1);

```cpp
}
```

else triangle(a,b,c); /* draw triangle at end of recursion */

```cpp
}
```

90   Chapter 2   Graphics Programming

*FIGURE 2.39 Triangles after five subdivisions.*

The display function is now almost trivial. It uses a global8 value of n determined
by the main program to ﬁx the number of subdivision steps we would like, and it calls
divide_triangle once with the single function call
divide_triangle(v[0], v[1], v[2], Ndivisions);
where Ndivisions is the number of times we want to subdivide the original trian-
gle. If we do not account for vertices shared by two vertices and treat each triangle
independently, then each subdivision triples the number of vertices, giving us
Nvertices = 3Ndivisions+1.
We set up the buffer object exactly as we did previously, and we can then render all

```cpp
void display( void )
{
```

glClear(GL_COLOR_BUFFER_BIT);
glDrawArrays(GL_TRIANGLES, 0, Nvertices);
glFlush();

```cpp
}
```

The rest of the program is almost identical to our previous gasket program. Output
for ﬁve subdivision steps is shown in Figure 2.39. The complete program is given in
Appendix A.
8. Note that often we have no convenient way to pass variables to GLUT callbacks other than through
global parameters. Although we prefer not to pass values in such a manner, because the form of these
functions is ﬁxed, we have no good alternative.
                                                                     2.10 The Three-Dimensional Gasket            91

### 2.10    THE THREE-DIMENSIONAL GASKET

We have argued that two-dimensional graphics is a special case of three-dimensional
graphics, but we have not yet seen a complete three-dimensional program. Next,
we convert our two-dimensional Sierpinski gasket program to a program that will
generate a three-dimensional gasket; that is, one that is not restricted to a plane.
We can follow either of the two approaches that we used for the two-dimensional
gasket. Both extensions start in a similar manner, replacing the initial triangle with a
tetrahedron (Figure 2.40).                                                                  FIGURE 2.40 Tetrahedron.

#### 2.10.1 Use of Three-Dimensional Points

Because every tetrahedron is convex, the midpoint of a line segment between a vertex
and any point inside a tetrahedron is also inside the tetrahedron. Hence, we can
follow the same procedure as before, but this time, instead of the three vertices
required to specify a triangle, we need four initial vertices to specify the tetrahedron.
Note that as long as no three vertices are collinear, we can choose the four vertices of
the tetrahedron at random without affecting the character of the result.
The required changes are primarily in the function display. We declare and
initialize an array to hold the vertices as follows:

```cpp
// vertices of an arbitrary tetrahedron
```

point3 vertices[4] = { point3(-1.0, -1.0, -1.0),
point3( 1.0, -1.0, -1.0),
point3( 0.0, 1.0, -1.0),
point3( 0.0, 0.0, 1.0) };

```cpp
// arbitrary initial location inside tetrahedron
```

point3 p = point3(0.0, 0.0, 0.0);
We now use the array
point3 points[NumPoints];
to store the vertex data. We compute a new location as before but add a midpoint
computation for the z component:

```cpp
// computes and plots a single new location
```

point3 p;

```cpp
int rand();
int j = rand() % 4; // pick a vertex at random
// compute point halfway between a vertex and the old location
```

p = (p + vertices[j])/2.0;
92   Chapter 2   Graphics Programming
We create vertex-array and buffer objects exactly as with the two-dimensional version
and can use the same display function.
One problem with the three-dimensional gasket that we did not have with the
two-dimensional gasket occurs because points are not restricted to a single plane;
thus, it may be difﬁcult to envision the three-dimensional structure from the two-
dimensional image displayed, especially if we render each point in the same color.
To get around this problem, we can add a more sophisticated color-setting
process to our shaders, one that makes the color of each point depend on that point’s
location. We can map the color cube to the default view volume by noting that both
are cubes but that whereas x, y, and z range from −1 to 1, each color component
must be between 0 and 1. If we use the mapping
1+ x
r=        ,
1+ y
g=        ,
1+ z
b=        ,
every point in the viewing volume maps to a distinct color. In the vertex shader, we
can set the color using the components of vPosition, so our shader becomes
in vec4 vPosition;
out vec4 color;

```cpp
void main()
{
```

color = vec4((1.0 + vPosition.xyz)/2.0, 1.0);
gl_Position = vPosition;

```cpp
}
```

This color is output to the rasterizer so the fragment shader can use it as input to set
the color of a fragment, so the fragment shader becomes
in vec4 color;

```cpp
void main()
{
```

gl_FragColor = color;

```cpp
}
```

Figure 2.41 shows that if we generate enough points, the resulting ﬁgure will look like
the initial tetrahedron with increasingly smaller tetrahedrons removed.

#### 2.10.2 Use of Polygons in Three Dimensions

There is a more interesting approach to the three-dimensional Sierpinski gasket that
uses both polygons and subdivision of a tetrahedron into smaller tetrahedrons. Sup-
pose that we start with a tetrahedron and ﬁnd the midpoints of its six edges and
                                                                  2.10 The Three-Dimensional Gasket               93

*FIGURE 2.41 Three-dimensional Sierpinski*

gasket.
connect these midpoints as shown in Figure 2.42. There are now four smaller tetra-
hedrons, one for each of the original vertices, and another area in the middle that we
will discard.
Following our second approach to a single triangle, we will use recursive sub-
division to subdivide the four tetrahedrons that we keep. Because the faces of a
tetrahedron are the four triangles determined by its four vertices, at the end of the    FIGURE 2.42 Subdivided
subdivisions, we can render each of the ﬁnal tetrahedrons by drawing four triangles.     tetrahedron.
Most of our code is almost the same as in two dimensions. Our triangle routine
now uses points in three dimensions rather than in two dimensions:

```cpp
#include "vec.h"
```

typedef vec3 point3;

```cpp
void triangle(point3 a, point3 b, point3 c)
```

/* specify one triangle */

```cpp
{
static int i = 0;
```

points[i] = a;
i++;
points[i] = b;
i++;
points[i]= c;
i++;

```cpp
}
```

94   Chapter 2   Graphics Programming
We draw each tetrahedron, coloring each face with a different color by using the
following function:

```cpp
void tetra(point3 a, point3 b, point3 c, point3 d)
{
```

triangle(a, b, c);
triangle(a, c, d);
triangle(a, d, b);
triangle(b, d, c);

```cpp
}
```

We subdivide a tetrahedron in a manner similar to subdividing a triangle.
Our code for divide_triangle does the same:

```cpp
void divide_tetra(point3 a, point3 b, point3 c, point3 d, int m)
{
```

if(m > 0)

```cpp
{
```

point3 mid[6];

```cpp
// compute six midpoints
```

mid[0] = (a + b)/2.0;
mid[1] = (a + c)/2.0;
mid[2] = (a + d)/2.0;
mid[3] = (b + c)/2.0;
mid[4] = (c + d)/2.0;
mid[5] = (b + d)/2.0;

```cpp
// create 4 tetrahedrons by subdivision
```

divide_tetra(a, mid[0], mid[1], mid[2], m-1);
divide_tetra(mid[0], b, mid[3], mid[5], m-1);
divide_tetra(mid[1], mid[3], c, mid[4], m-1);
divide_tetra(mid[2], mid[5], mid[5], d, m-1);

```cpp
}
```

else tetra(a,b,c,d); /* draw tetrahedron at end of recursion */

```cpp
}
```

We can now start with four vertices and do n subdivisions as follows:
divide_tetra(v[0], v[1], v[2], v[3], n);
There are two more problems that we must address before we have a useful three-
dimensional program. The ﬁrst is how to deal with color. If we use just a single color
as in our ﬁrst example, we won’t be able to see any of the three-dimensional structure.
Alternately, we could use the approach of our last example of letting the color of
                                                                        2.10 The Three-Dimensional Gasket   95
each fragment be determined by where the point is located in three dimensions. But
we would prefer to use a small number of colors and color the face of each triangle
with one of these colors. We can set this scheme by choosing some base colors in the
application, such as
typedef vec3 color3;
color3 base_colors[4] = {color3(1.0, 0.0, 0.0), color3(0.0, 1.0, 0.0),
color3(0.0, 0.0, 1.0), color3(0.0, 0.0, 0.0)};
and then assigning colors to each point as it is generated. We set a color index as we

```cpp
int colorindex;
void tetra(point3 a, point3 b, point3 c, point3 d)
{
```

colorindex = 0;
triangle(a,b,c);
colorindex = 1;
triangle(a,c,d);
colorindex = 2;
triangle(a,d,b);
colorindex = 3;
triangle(b,d,c);

```cpp
}
```

and then form a color array with a color for each point:
color3 colors[NumVertices];

```cpp
int i = 0;    // number of vertices
void triangle(point3 a, point3 b, point3 c)
```

/* specify one triangle */

```cpp
{
```

colors[i] = base_colors[colorindex];
points[i] = a;
i++;
colors[i] = base_colors[colorindex];
points[i] = b;
i++;
colors[i] = base_colors[colorindex];
points[i] = c;
i++;

```cpp
}
```

We send these colors to the GPU along with their associated vertices in a buffer
object. Inside of the buffer object, we’ll place the vertex data at the start of the buffer’s
96   Chapter 2   Graphics Programming
memory and then follow it with the color data. To do this, however, we’ll need to
ﬁrst allocate a buffer large enough to contain all of the data, and then load data into
the buffer in two operations using the OpenGL function glBufferSubData. The
function glBufferSubData allows us to update parts of an existing buffer object
with new data. The ﬁrst parameter speciﬁes which array in the buffer we want to
update. The second parameter speciﬁes which byte in the buffer to start writing
data at, and the third parameter speciﬁes how many bytes to read from the memory
pointer, which is passed in using the fourth parameter. Consider the code:
GLuint buffer;
glGenBuffers(1, &buffer);
glBindBuffer(GL_ARRAY_BUFFER, buffer);

```cpp
// Allocate a buffer of uninitialized data of the correct size
```

glBufferData(GL_ARRAY_BUFFER, sizeof(points) + sizeof(colors),
NULL, GL_STATIC_DRAW);

```cpp
// Load the separate arrays of data
```

glBufferSubData(GL_ARRAY_BUFFER, 0, sizeof(points), points );
glBufferSubData(GL_ARRAY_BUFFER, sizeof(points),
sizeof(colors), colors );
In the above example, the ﬁrst call updates the bytes in the range
[0, sizeof(points)-1]. Since we need to write the data for the colors immedi-
ately after in the buffer, we start at the byte immediately following the vertex data,
which is just the length (in bytes) of the points array, which is sizeof(points).
We use that size as the starting offset in the second glBufferSubData call.
If in the shader the color is named vColor, the second vertex array can be set up
in the shader initialization:
loc2 = glGetAttribLocation(program, "vColor");
glEnableVertexAttribArray(loc2);
glVertexAttribPointer(loc2, 3, GL_FLOAT, GL_FALSE, 0,
BUFFER_OFFSET(sizeof(points)));
In the vertex shader, we use vColor to set a color to be sent to the fragment
shader.

#### 2.10.3 Hidden-Surface Removal

If you execute the code in the previous section, you might be confused by the results.
The program draws triangles in the order that they are speciﬁed in the program.
This order is determined by the recursion in our program and not by the geometric
relationships among the triangles. Each triangle is drawn (ﬁlled) in a solid color and
is drawn over those triangles that have already been rendered to the display.
                                                                  2.10 The Three-Dimensional Gasket               97
Contrast this order to the way that we would see the triangles if we were to

```cpp
construct the three-dimensional Sierpinski gasket out of small solid tetrahedra. We
```

would see only those faces of tetrahedra that were in front of all other faces as seen                    A
by a viewer. Figure 2.43 shows a simpliﬁed version of this hidden-surface problem.
From the viewer’s position, quadrilateral A is seen clearly, but triangle B is blocked
from view, and triangle C is only partially visible. Without going into the details                           C
of any speciﬁc algorithm, you should be able to convince yourself that given the
position of the viewer and the triangles, we should be able to draw the triangles such
that the correct image is obtained. Algorithms for ordering objects so that they are     FIGURE 2.43 The hidden-
drawn correctly are called visible-surface algorithms or hidden-surface–removal          surface problem.
algorithms, depending on how we look at the problem. We discuss such algorithms
in detail in Chapters 3 and 6.
For now, we can simply use a particular hidden-surface–removal algorithm,
called the z-buffer algorithm, that is supported by OpenGL. This algorithm can be
turned on (enabled) and off (disabled) easily. In our main program, we must request
the auxiliary storage, a z (or depth) buffer, by modifying the initialization of the
display mode to the following:
glutInitDisplayMode(GLUT_SINGLE | GLUT_RGB | GLUT_DEPTH);
Note that the z-buffer is one of the buffers that make up the frame buffer. We enable
glEnable(GL_DEPTH_TEST);
either in main or in an initialization function such as init. Because the algorithm
stores information in the depth buffer, we must clear this buffer whenever we wish to
redraw the display; thus, we modify the clear procedure in the display function:
glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
The display callback is as follows:

```cpp
void display()
{
```

glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
glDrawArrays(GL_TRIANGLES, 0, NumVertices);
glFlush();

```cpp
}
```

The results are shown in Figure 2.44 for a recursion of four steps. The complete
program is given in Appendix A.
98   Chapter 2   Graphics Programming

*FIGURE 2.44 Three-dimensional gasket after four recursion steps.*


### 2.11    ADDING INTERACTION

In this section, we develop event-driven input through a set of simple examples that
use the callback mechanism that we introduced in Section 2.7. We examine various
events that are recognized by the window system, and, for those of interest to our
application, we write callback functions that govern how the application program
responds to these events.

#### 2.11.1 Using the Pointing Device

We start by altering the main function in the gasket program. In the original ver-
sion, we used functions in the GLUT library to put a window on the screen and
then entered the event loop by executing the function glutMainLoop. We entered
the loop but could do nothing else because there were no callbacks other than the
display callback. We could not even terminate the program, except through an ex-
ternal system-dependent mechanism, such as pressing control-c. Our ﬁrst example
will remedy this omission by using the pointing device to terminate a program. We
accomplish this task by having the program execute a standard termination function
called exit when a particular mouse button is depressed.
We discuss only those events recognized by GLUT. Standard window systems
such as the X Window System or Microsoft Windows recognize many more events,
which differ among systems. However, the GLUT library recognizes a small set of
events that is common to most window systems and is sufﬁcient for developing
basic interactive graphics programs. Because GLUT has been implemented for the
major window systems, we can use our simple applications on multiple systems by
recompiling the application.
                                                                                2.11 Adding Interaction   99
Two types of events are associated with the pointing device, which is conven-
tionally assumed to be a mouse but could be a trackpad or a data tablet. A move
event is generated when the mouse is moved with one of the buttons depressed. If
the mouse is moved without a button being held down, this event is called a passive
move event. After a move event, the position of the mouse is made available to the
application program. A mouse event occurs when one of the mouse buttons is ei-
ther depressed or released. When a button is depressed, the action generates a mouse
down event. When it is released, a mouse up event is generated. The information re-
turned includes the button that generated the event, the state of the button after the
event (up or down), and the position of the cursor tracking the mouse in window
coordinates (with the origin in the upper-left corner of the window). We register the
mouse callback function, usually in the main function, by means of the GLUT func-
glutMouseFunc(myMouse);
The mouse callback must have the form

```cpp
void myMouse(int button, int state, int x, int y);
```

and is provided by the application programmer. Within the callback function, we
deﬁne the actions that we want to take place if the speciﬁed event occurs. There
may be multiple actions deﬁned in the mouse callback function corresponding to the
many possible button and state combinations. For our simple example, we want the
depression of the left mouse button to terminate the program. The required callback
is the single-line function

```cpp
void myMouse(int button, int state, int x, int y)
{
```

if(button == GLUT_LEFT_BUTTON && state == GLUT_DOWN)
exit(0);

```cpp
}
```

If any other mouse event—such as a depression of one of the other buttons—occurs,
no response action will occur, because no action corresponding to these events has
been deﬁned in the callback function.
We will now develop an example that incorporates many of the aspects of CAD
programs and adds some interactivity. Along the way, we will introduce some addi-
tional callbacks. We start by developing a simple program that will display a single
triangle whose vertices are entered interactively using the pointing device. We will
use the same shaders so most of the code will be similar to our previous examples.
We specify a global array to hold the three two-dimensional vertices
point2 points[3];
100   Chapter 2   Graphics Programming
We can then use the mouse callback to capture the data each time the left mouse
button is depressed. Consider the code

```cpp
int w, h;
int count = 0;
void mouse(int button, int state, int x, int y)
{
```

if(button == GLUT_RIGHT_BUTTON && state == GLUT_DOWN)

```cpp
{
```

exit(0);

```cpp
}
```

if(button == GLUT_LEFT_BUTTON && state == GLUT_DOWN)

```cpp
{
```

points[count].x = (float) x / (w/2) - 1.0;
points[count].y = (float) (h-y) / (h/2) - 1.0;
count++;

```cpp
}
```

if(count == 3)

```cpp
{
```

glutPostRedisplay();
count = 0;

```cpp
}
}
```

The right mouse button is used to end the program. The left mouse button is used to
provide the vertex data for our triangle. We use the globals h and w to hold the height
and width of the OpenGL window. Hence, in our main we might see the code
w = 512;
h = 512;
glutInitWindowSize(w, h);
in our main function, which would give us the same 512 × 512 window we used
previously. The basic idea is that each time the left mouse button is depressed, we put
the scaled location of the mouse into points and then move on to the next vertex.
Scaling is necessary because the mouse callback returns the position of the mouse in
screen coordinates measured from the top-left corner of the window. Thus, for our
w × h window, the top-left corner has coordinates (0, 0) whereas the bottom right
corner has coordinates (w-1, h-1). This number of increasing y values from top to
bottom is common in window systems and has its origins in television systems that
display images top to bottom. The window we use in our application program has its
origin in the center, the bottom-left corner is at (−1.0, −1.0) and the top-right corner
has coordinates (1.0, 1.0). Because any primitives outside this region are clipped out,
                                                                                      2.11 Adding Interaction   101
we want to scale the values returned by the mouse callback to this region and make
sure to ﬂip the y values so that our triangles appear upright. The two lines
points[count].x = (float) x / (w/2) - 1.0;
points[count].y = (float) (h-y) / (h/2) - 1.0;
carry out this transformation of coordinates.
Once we have the data for three vertices, we can draw the triangle. We cause the
drawing of the triangle through the display callback. However, instead of invoking
the display callback directly through an execution of display, we instead use
glutPostRedisplay();
What this function does is set an internal ﬂag indicating that the display needs to be
redrawn. Each time the system goes through the event loop, multiple events may oc-
cur whose callbacks require refreshing the display. Rather than each of these callbacks
explicitly executing the display function, each uses glutPostRedisplay to set the
display ﬂag. Thus, at the end of the event loop, if the ﬂag is set, the display callback
is invoked and the ﬂag unset. This method prevents the display from being redrawn
multiple times in a single pass through the event loop.9 Returning to our example,
we see that each successive three depressions of the left mouse button speciﬁes a new
triangle that replaces the previous triangle on the display.
Although we have a program that has some interactivity, introducing a few more
callbacks will lead to a much more interesting program that can be expanded to a
painting or CAD program.

#### 2.11.2 Window Events

Most window systems allow a user to resize the window interactively, usually by using
the mouse to drag a corner of the window to a new location. This event is called a
reshape event and is an example of a window event. Other window events include
iconifying a window and exposing a window that was covered by another window.
Each can have a callback that speciﬁed which actions to take if the event occurs.
Unlike most other callbacks, there is a default reshape callback that simply changes
the viewport to the new window size, an action that might not be what the user
desires. If the window size changes, we have to consider the three questions:
1. Do we redraw all the objects that were in the window before it was resized?
2. What do we do if the aspect ratio of the new window is different from that of
the old window?
3. Do we change the sizes or attributes of new primitives if the size of the new
window is different from that of the old?
9. Some interactive applications may need to execute the display callback directly.
102   Chapter 2   Graphics Programming
There is no single answer to any of these questions. If we are displaying the image
of a real-world scene, our reshape function probably should make sure that no shape
distortions occur. But this choice may mean that part of the resized window is unused
or that part of the scene cannot be displayed in the window. If we want to redraw
the objects that were in the window before it was resized, we need a mechanism for
storing and recalling them. Often we do this recall by encapsulating all drawing in a
single function, such as the display callback function used in our previous examples.
In interactive applications that is probably not the best choice, because we decide
what we draw interactively.
The reshape event returns in its measure the height and width of the new win-
dow. We can use these values to rescale the data that we use to specify the geometry.
Thus, we have the callback
GLint windowHeight, windowWidth;

```cpp
void reshape(GLsizei w, GLsizei h)
{
```

windowWidth = w;
windowHeight = h;
glViewport(0, 0, windowWidth, windowHeight);

```cpp
}
```

This function creates a new viewport that covers the entire resized window and copies
the returned values of the new window width and height to the global variables
windowWidth and windowHeight so they can be used by the mouse callback. Note
that because the reshape callback generates a display callback, we do not need to call
glutPostRedisplay.

#### 2.11.3 Keyboard Events

We can also use the keyboard as an input device. Keyboard events can be generated
when the mouse is in the window and one of the keys is depressed or released.10
The GLUT function glutKeyboardFunc is the callback for events generated by
depressing a key, whereas glutKeyboardUpFunc is the callback for events generated
by release of a key.
When a keyboard event occurs, the ASCII code for the key that generated the
event and the location of the mouse are returned. All the key-press callbacks are
registered in a single callback function, such as
glutKeyboardFunc(myKey);
For example, if we wish to use the keyboard only to exit the program, we can use the
10. Depending on the operating system, the mouse focus may have to set by ﬁrst clicking inside the
window before events are recognized.
                                                                                   2.11 Adding Interaction                    103

```cpp
void myKey(unsigned char key, int x, int y)
{
```

if(key==’q’ || key == ’Q’) exit(0);

```cpp
}
```

GLUT includes a function glutGetModifiers that allows the user to deﬁne actions
using the meta keys, such as the Control and Alt keys. These special keys can be
important when we are using one- or two-button mice because we can then deﬁne
the same functionality as having left, right, and middle buttons as we have assumed
in this chapter. More information about these functions is in the Suggested Readings
section at the end of the chapter.

#### 2.11.4 The Idle Callback                                                                                (x′, y ′)

The idle callback is invoked when there are no other events. Its default is the null
function pointer. A typical use of the idle callback is to continue to generate graphical           ρ               (x, y )
primitives through a display function while nothing else is happening. Another is to                    ϕ ρ
produce an animated display.                                                                            θ
Let’s do a simple extension to our triangle program that rotates the triangle about                                  x
the center of the window. Consider the two-dimensional rotation in Figure 2.45. A           FIGURE 2.45 Two-dimensional
point at (x, y) when rotated by φ degrees about the origin moves to a point (x  , y ).    rotation.
We obtain the equations of rotation by expressing both points in polar coordinates.
x = r cos(θ ),
y = r sin(θ ),
x  = r cos(θ + φ) = r(cos(θ ) cos(φ) − sin(θ ) sin(φ)),
y  = r sin(θ + φ) = r(cos(θ ) sin(φ) + sin(θ ) cos(φ)),
x  = x cos(φ) − y sin(φ),
y  = x sin(φ) + y cos(φ).
Instead of displaying a triangle using the entered vertex positions, ﬁrst we will rotate
the positions by a small angle each time. The idle callback need only post a redisplay.
In main, we specify an idle callback,
glutIdleFunc(idle);
The display callback not only changes the vertex positions but must also send the new
vertex data to the GPU as in the code for which the angle is 1/1000 of a degree:
104   Chapter 2   Graphics Programming

```cpp
const float     DegreesToRadians = M_PI / 180.0;
float angle = 0.001*DegreesToRadians;           // small angle in radians
void display()
{
```

for( int i = 0; i < 3; i++)

```cpp
{
float x = cos(angle)*points[i].x - sin(angle)*points[i].y;
float y = sin(angle)*points[i].x + cos(angle)*points[i].y;
```

points[i].x = x;
points[i].y = y;

```cpp
}
```

glBufferData(GL_ARRAY_BUFFER, sizeof(points), points,
GL_STATIC_DRAW);
glClear(GL_COLOR_BUFFER_BIT); // clear the window
glDrawArrays(GL_TRIANGLES, 0, 3);
glFlush();

```cpp
}
```

The idle function is just

```cpp
void idle()
{
```

glutPostRedisplay();

```cpp
}
```

Alternately, we could have incremented the angle by a small amount in the idle
callback and always applied the rotation to the original points in the display callback.
We can change most callback functions during program execution by simply
specifying a new callback function. We can also disable a callback by setting its call-
back function to NULL. In our example, we want to stop the rotation while we are
collecting data and then restart it once a new triangle is completely speciﬁed. We can
modify the display callback to accomplish this change:

```cpp
void mouse(int button, int state, int x, int y)
{
```

if(button == GLUT_RIGHT_BUTTON && state == GLUT_DOWN)

```cpp
{
```

exit(0);

```cpp
}
```

if(button == GLUT_LEFT_BUTTON && state == GLUT_DOWN)

```cpp
{
```

glutIdleFunc(NULL);
points[count].x = (float) x / (w/2) - 1.0;
points[count].y = (float) (h-y) / (h/2) - 1.0;
count++;

```cpp
}
```

                                                                                 2.11 Adding Interaction   105
if(count == 3)

```cpp
{
```

glutIdleFunc(idle);
glutPostRedisplay();
count = 0;

```cpp
}
}
```


#### 2.11.5 Double Buffering

Although we have a complete program, depending on the speed of your computer
and how much you increment the angle in the idle callback, you may see a display
that does not show a rotating triangle but rather a somewhat broken-up display with
pieces of the triangle showing. This problem can be far more severe if you try to
generate a display with many objects in motion.
The reason for this behavior is the decoupling of the automatic display of the
contents of the frame buffer from the application code that changes values in the
frame buffer. Typically the frame buffer is redisplayed at a regular rate, known as
the refresh rate, which is in the range of 60 to 100 Hz (or frames per second).
However, an application program operates asynchronously and can cause changes
to the frame buffer at any time. Hence, a redisplay of the frame buffer can occur
when its contents are still being altered by the application and the viewer will see
only a partially drawn display. There are a couple of solutions to this problem. Some
operating systems give the user a parameter to set that will couple or sync the drawing

```cpp
into and display of the frame buffer.
```

The more common solution is double buffering. Instead of a single frame buffer,
the hardware has two frame buffers. One, called the front buffer, is one that is
displayed. The other, called the back buffer, is then available for constructing what
we would like to display. Once the drawing is complete, we swap the front and back
buffers. We then clear the new back buffer and can start drawing into it. Thus, rather
than using glFlush at the end of the display callback, we use
glutSwapBuffers();
We have to make one other change to use double buffering. In our initialization, we
have to request a double buffer. Hence, in main we use
glutInitDisplayMode(GLUT_RGBA | GLUT_DOUBLE | GLUT_DEPTH);
Note that the default in GLUT is equivalent to using GLUT_SINGLE rather than
GLUT_DOUBLE. However, modern graphics hardware has sufﬁcient memory that we
can always use double rather than single buffering. Most graphics cards will also allow
you to synchronize the display refresh with the application program.
106       Chapter 2   Graphics Programming

#### 2.11.6 Window Management

GLUT also supports both multiple windows and subwindows of a given window. We
can open a second top-level window (with the label “second window”) by
uint id = glutCreateWindow("second window");
The returned integer value allows us to select this window as the current window into
glutSetWindow(id);
We can make this window have properties different from those of other windows by
invoking the glutInitDisplayMode before glutCreateWindow. Furthermore,
each window can have its own set of callback functions because callback speciﬁcations
refer to the present window.

### 2.12    MENUS

We could use our graphics primitives and our mouse callbacks to construct various

*FIGURE 2.46 Slidebar.     graphical input devices. For example, we could construct a slidebar (Figure 2.46)*

using ﬁlled rectangles for the device, text for any labels, and the mouse to get the po-
sition. However, much of the code would be tedious to develop, especially if we tried
to create visually appealing and effective graphical devices (widgets). Most window
systems provide a toolkit that contains a set of widgets, but because our philosophy
is not to restrict our discussion to any particular window system, we shall not discuss
the speciﬁcs of such widget sets. Fortunately, GLUT provides one additional feature,
pop-up menus, that we can use with the mouse to create sophisticated interactive
applications.
Using menus involves taking a few simple steps. We must specify the actions
corresponding to each entry in the menu. We must link the menu to a particular
mouse button. Finally, we must register a callback function for each menu. We can
demonstrate simple menus with the example of a pop-up menu that has three entries.
The ﬁrst selection allows us to exit our program. The second and third start and stop
the rotation. The function calls to set up the menu and to link it to the right mouse
button should be placed in our main function. They are
glutCreateMenu(demo_menu);
glutAddMenuEntry("quit", 1);
glutAddMenuEntry("start rotation", 2);
glutAddMenuEntry("stop rotation", 3);
glutAttachMenu(GLUT_RIGHT_BUTTON);
The function glutCreateMenu registers the callback function demo_menu.
The second argument in each entry’s deﬁnition is the identiﬁer passed to the
callback when the entry is selected. Hence, our callback function is
                                                                                       2.12 Menus   107

*FIGURE 2.47 Structure of hierarchical menus.*


```cpp
void demo_menu(int id)
{
```

switch(id)

```cpp
{
```

case 1:
exit(0);
break;
case 2:
glutIdleFunc(idle);
break;
case 3:
glutIdleFunc(NULL);
break;

```cpp
}
```

glutPostRedisplay();

```cpp
}
```

The call to glutPostRedisplay requests a redraw through the glutDisplayFunc
callback, so that the screen is drawn again without the menu.
GLUT also supports hierarchical menus, as shown in Figure 2.47. For example,
suppose that we want the main menu that we create to have only two entries. The ﬁrst
entry still causes the program to terminate, but now the second causes a submenu to
pop up. The submenu contains the two entries for turning the rotation on and off.
The following code for the menu (which is in main) should be clear:
sub_menu = glutCreateMenu(rotation_menu);
glutAddMenuEntry("start rotation", 2);
glutAddMenuEntry("stop rotation", 3);
glutCreateMenu(top_menu);
glutAddMenuEntry("Quit", 1);
glutAddSubMenu("start/stop rotation", sub_menu);
glutAttachMenu(GLUT_RIGHT_BUTTON);
Writing the callback functions, rotation_menu and top_menu, should be a simple
exercise.
108   Chapter 2   Graphics Programming
In this chapter, we introduced just enough of the OpenGL API to apply the basic
concepts that we learned in Chapter 1. Although the ﬁrst application we used to
develop our ﬁrst program was two dimensional, we took the path of looking at two-
dimensional graphics as a special case of three-dimensional graphics. We then were
able to extend the example to three dimensions with minimal work.
The Sierpinski gasket provides a nontrivial beginning application. A few exten-
sions and mathematical issues are presented in the exercises at the end of this chapter.
The texts in the Suggested Readings section provide many other examples of interest-
ing curves and surfaces that can be generated with simple programs.
The historical development of graphics APIs and graphical models illustrates the
importance of starting in three dimensions. The pen-plotter model from Chapter 1
was used for many years and is the basis of many important APIs, such as PostScript.
Work to deﬁne an international standard for graphics APIs began in the 1970s and
culminated with the adoption of GKS by the International Standards Organization
(ISO) in 1984. However, GKS had its basis in the pen-plotter model and as a two-
dimensional API was of limited utility in the CAD community. Although the standard
was extended to three dimensions with GKS-3D, the limitations imposed by the orig-
inal underlying model led to a standard that was lacking in many aspects. The PHIGS
and PHIGS+ APIs, started in the CAD community, are inherently three dimensional
and are based on the synthetic-camera model.
OpenGL is derived from the IrisGL API, which is based on implementing the
synthetic-camera model with a pipeline architecture. IrisGL was developed for Sil-
icon Graphics, Inc. (SGI) workstations, which incorporated a pipeline architecture
originally implemented with special-purpose VLSI chips. Hence, although PHIGS
and GL have much in common, GL was designed speciﬁcally for high-speed real-
time rendering. OpenGL was a result of application users realizing the advantages of
GL programming and wanting to carry these advantages to other platforms. Because
it removed input and windowing functions from GL and concentrated on rendering,
OpenGL emerged as a new API that was portable while retaining the features that
make GL such a powerful API.
Although most application programmers who use OpenGL prefer to program
in C, there is a fair amount of interest in higher-level interfaces. Using C++ rather
than C requires minimal code changes but does not provide a true object-oriented

```cpp
interface to OpenGL. Among object-oriented programmers, there has been much
interest in both OpenGL and higher-level APIs. Although there is no ofﬁcial Java
```

binding to OpenGL, there have been multiple efforts to come up with one. The
problem is not simple, because application users want to make use of the object
orientation of Java and various Java toolkits, together with a non–object-oriented
OpenGL speciﬁcation. There are a few bindings available on the Internet, and Sun
Microsystems recently released their Java bindings. Many Java programmers use the
JOGL bindings.
                                                                                     Suggested Readings   109
In Chapter 8, we will introduce scene graphs, which provide a much higher-level,
object-oriented interface to graphics hardware. Most scene graph APIs are built on
top of OpenGL.
Within the game community, the dominance of Windows makes it possible for
game developers to write code for a single platform. DirectX runs only on Windows
platforms and is optimized for speed on these systems. Although much DirectX code
looks like OpenGL code, the coder can use device-dependent features that are avail-
able in commodity graphics cards. Consequently, applications written in DirectX
do not have the portability and stability of OpenGL applications. Thus, we see Di-
rectX dominating the game world, whereas scientiﬁc and engineering applications
generally are written in OpenGL. For OpenGL programmers who want to use fea-
tures speciﬁc to certain hardware, OpenGL has an extension mechanism for accessing
these features but at the cost of portability. Programming pipelines that are accessi-
ble through the OpenGL Shading Language and Cg are leading to small performance
differences between OpenGL and DirectX for high-end applications.
Our examples and programs have shown how we describe and display geomet-
ric objects in a simple manner. In terms of the modeling–rendering paradigm that
we presented in Chapter 1, we have focused on the modeling. However, our mod-
els are completely unstructured. Representations of objects are lists of vertices and
attributes. In Chapter 8, we will learn to construct hierarchical models that can rep-
resent relationships among objects. Nevertheless, at this point, you should be able
to write interesting programs. Complete the exercises at the end of the chapter and
extend a few of the two-dimensional problems to three dimensions.
The Sierpinski gasket provides a good introduction to the mysteries of fractal geome-
try; there are good discussions in several texts [Bar93, Hil01, Man82, Pru90].
The pen-plotter API is used by PostScript [Ado85] and LOGO [Pap81]. LOGO
provides turtle graphics, an API that is both simple to learn and capable of describing
several of the two-dimensional mathematical curves that we use in Chapter 11 (see
Exercise 2.4).
GKS [ANSI85], GKS-3D [ISO88], PHIGS [ANSI88], and PHIGS+[PHI89] are
both U.S. and international standards. Their formal descriptions can be obtained
from the American National Standards Institute (ANSI) and from ISO. Numerous
textbooks use these APIs [Ang90, End84, Fol94, Hea04, Hop83, Hop91].
The X Window System [Sch88] has become the standard on UNIX workstations
and has inﬂuenced the development of window systems on other platforms. The
RenderMan interface is described in [Ups89].
The standard reference for OpenGL is the OpenGL Programming Guide [Shr10].
The OpenGL Reference Manual [Ope04] has the man pages for older versions. There
is also a formal speciﬁcation of OpenGL [Seg92]. The OpenGL Shading Language is
110   Chapter 2   Graphics Programming
described in [Ros09]. The standards documents as well as many other references and
pointers to code examples are on the OpenGL Web site, www.opengl.org.
Starting with the second edition and continuing through the present edition,
the Programming Guide uses the GLUT library that was developed by Mark Kil-
gard [Kil94b]. The Programming Guide provides many more code examples using
OpenGL. GLUT was developed for use with the X Window System [Kil96], but there
are also versions for Windows and the Macintosh. Much of this information and
many of the example programs are available over the Internet. Representative sites
are listed at the beginning of Appendix A.
OpenGL: A Primer [Ang08], the companion book to this text, contains details of
the OpenGL functions used here and more example programs. Windows users can
ﬁnd more examples in [Wri11] and [Fos97]. Details for Mac OS X users are [Kue08]
The graphics part of the DirectX API was originally known as Direct3D. The
present version is Version 11.0.

### 2.1   A slight variation on generating the Sierpinski gasket with triangular polygons

yields the fractal mountains used in computer-generated animations. After you
ﬁnd the midpoint of each side of the triangle, perturb this location before sub-
division. Generate these triangles without ﬁll. Later, you can do this exercise in
three dimensions and add shading. After a few subdivisions, you should have
generated sufﬁcient detail that your triangles look like a mountain.

### 2.2   The Sierpinski gasket, as generated in Exercise 2.1, demonstrates many of the

geometric complexities that are studied in fractal geometry [Man82]. Suppose
that you construct the gasket with mathematical lines that have length but
no width. In the limit, what percentage of the area of the original triangle
remains after the central triangle has been removed after each subdivision?
Consider the perimeters of the triangles remaining after each central triangle
is removed. In the limit, what happens to the total perimeter length of all
remaining triangles?

### 2.3   At the lowest level of processing, we manipulate bits in the frame buffer.

OpenGL has pixel-oriented commands that allow users to access the frame
buffer directly. You can experiment with simple raster algorithms, such as
drawing lines or circles, through a function that generates a single point. Write
a small library that will allow you to work in a virtual frame buffer that you cre-
ate in memory. The core functions should be WritePixel and ReadPixel.
Your library should allow you to set up and display your virtual frame buffer
and to run a user program that reads and writes pixels.

### 2.4   Turtle graphics is an alternative positioning system that is based on the concept

of a turtle moving around the screen with a pen attached to the bottom of its
shell. The turtle’s position can be described by a triplet (x, y, θ), giving the
                                                                                             Exercises          111
1       1
1      1       1                     1       1       1

*FIGURE 2.48 Generation of the Koch snowflake.*

location of the center and the orientation of the turtle. A typical API for such
a system includes functions such as the following:
init(x,y,theta); /* initialize position and orientation
of turtle */
forward(distance);
right(angle);
left(angle);
pen(up_down);
Implement a turtle-graphics library using OpenGL.

### 2.5   Use your turtle-graphics library from Exercise 2.4 to generate the Sierpinski

gasket and fractal mountains of Exercises 2.1 and 2.2.

### 2.6   Space-ﬁlling curves have interested mathematicians for centuries. In the limit,

these curves have inﬁnite length, but they are conﬁned to a ﬁnite rectangle
and never cross themselves. Many of these curves can be generated iteratively.
Consider the “rule” pictured in Figure 2.48 that replaces a single line segment
with four shorter segments. Write a program that starts with a triangle and
iteratively applies the replacement rule to all the line segments. The object that
you generate is called the Koch snowﬂake. For other examples of space-ﬁlling
curves, see [Hil07] and [Bar93].

### 2.7   You can generate a simple maze starting with a rectangular array of cells. Each

cell has four sides. You remove sides (except from the perimeter of all the
cells) until all the cells are connected. Then you create an entrance and an
exit by removing two sides from the perimeter. A simple example is shown in
Figure 2.49. Write a program using OpenGL that takes as input the two integers        FIGURE 2.49 Maze.
N and M and then draws an N × M maze.

### 2.8   Describe how you would adapt the RGB-color model in OpenGL to allow you

to work with a subtractive color model.

### 2.9   We saw that a fundamental operation in graphics systems is to map a point

(x, y) that lies within a clipping rectangle to a point (xs , ys ) that lies in the
viewport of a window on the screen. Assume that the two rectangles are de-
ﬁned by the viewport speciﬁed by
glViewport(u, v, w, h);
and a viewing rectangle speciﬁed by
112   Chapter 2   Graphics Programming

*FIGURE 2.50 Polygonal mesh.*

xmin ≤ x ≤ xmax ,
ymin ≤ y ≤ ymax .
Find the mathematical equations that map (x, y) into (xs , ys ).

### 2.10 Many graphics APIs use relative positioning. In such a system, the API contains

move_rel(x,y);
line_rel(x,y);
for drawing lines and polygons. The move_rel function moves an internal
position, or cursor, to a new position; the line_rel function moves the
cursor and deﬁnes a line segment between the old cursor position and the new
position. What are the advantages and disadvantages of relative positioning
as compared to the absolute positioning used in OpenGL? Describe how you
would add relative positioning to OpenGL.

### 2.11 In practice, testing each point in a polygon to determine whether it is inside

or outside the polygon is extremely inefﬁcient. Describe the general strategies
that you might pursue to avoid point-by-point testing.

### 2.12 Devise a test to determine whether a two-dimensional polygon is simple.


### 2.13 Figure 2.50 shows a set of polygons called a mesh; these polygons share some

edges and vertices. Find one or more simple data structures that represent the
mesh. A good data structure should include information on shared vertices
and edges. Using OpenGL, ﬁnd an efﬁcient method for displaying a mesh
represented by your data structure. Hint: Start with an array or linked list that
contains the locations of the vertices.

### 2.14 In Section 2.4, we saw that in OpenGL we specify polygons using lists of ver-

tices. Why might it be better to deﬁne polygons by their edges? Hint: Consider
how you might represent a mesh efﬁciently.

### 2.15 In OpenGL, we can associate a color with each vertex. If the endpoints of a line

segment have different colors assigned to them, OpenGL will interpolate be-
tween the colors as it renders the line segment. It will do the same for polygons.
                                                                                          Exercises   113
Use this property to display the Maxwell triangle: an equilateral triangle whose
vertices are red, green, and blue. What is the relationship between the Maxwell
triangle and the color cube?

### 2.16 We can simulate many realistic effects using computer graphics by incorporat-

ing simple physics in the model. Simulate a bouncing ball in two dimensions
incorporating both gravity and elastic collisions with a surface. You can model
the ball with a closed polygon that has a sufﬁcient number of sides to look
smooth.

### 2.17 An interesting but difﬁcult extension of Exercise 2.16 is to simulate a game of

pool or billiards. You will need to have multiple balls that can interact with the
sides of the table and with one another. Hint: Start with two balls and consider
how to detect possible collisions.

### 2.18 A certain graphics system with a CRT display is advertised to display any four

out of 64 colors. What does this statement tell you about the frame buffer and
about the quality of the monitor?

### 2.19 Devise a test for the convexity of a two-dimensional polygon.


### 2.20 Another approach to the three-dimensional gasket is based on subdividing

only the faces of an initial tetrahedron. Write a program that takes this ap-
proach. How do the results differ from the program that we developed in
Section 2.10?

### 2.21 Each time that we subdivide the tetrahedron and keep only the four smaller

tetrahedrons corresponding to the original vertices, we decrease the volume
by a factor f . Find f . What is the ratio of the new surface area of the four
tetrahedrons to the surface area of the original tetrahedron?

### 2.22 Creating simple games is a good way to become familiar with interactive

graphics programming. Program the game of checkers. You can look at each
square as an object that can be picked by the user. You can start with a program
in which the user plays both sides.

### 2.23 Write an interactive program that will allow you to guide a graphical rat

through the maze you generated in Exercise 2.7. You can use the left and right
buttons to turn the rat and the middle button to move him forward.

### 2.24 Plotting packages offer a variety of methods for displaying data. Write an


```cpp
interactive plotting application for two-dimensional curves. Your application
```

should allow the user to choose the mode (polyline display of the data, bar

```cpp
chart, or pie chart), colors, and line styles.
```


### 2.25 The required refresh rate for CRT displays of 50 to 85 Hz is based on the use of

short-persistence phosphors that emit light for extremely short intervals when
excited. Long-persistence phosphors are available. Why are long-persistence
phosphors not used in most workstation displays? In what types of applica-
tions might such phosphors be useful?
This page intentionally left blank
                                                                    CHA P TE R           3
W       e are now ready to concentrate on three-dimensional graphics. Much of this
chapter is concerned with such matters as how to represent basic geometric
types, how to convert between various representations, and what statements we can
make about geometric objects independent of a particular representation.
We begin with an examination of the mathematical underpinnings of computer
graphics. This approach should avoid much of the confusion that arises from a lack
of care in distinguishing among a geometric entity, its representation in a particular
reference system, and a mathematical abstraction of it.
We use the notions of afﬁne and Euclidean vector spaces to create the necessary
mathematical foundation for later work. One of our goals is to establish a method for
dealing with geometric problems that is independent of coordinate systems. The ad-
vantages of such an approach will be clear when we worry about how to represent the
geometric objects with which we would like to work. The coordinate-free approach
will prove to be far more robust than one based on representing the objects in a par-
ticular coordinate system or frame. This coordinate-free approach also leads to the
use of homogeneous coordinates, a system that not only enables us to explain this
approach but also leads to efﬁcient implementation techniques.
We use the terminology of abstract data types to reinforce the distinction be-
tween an object and its representation. Our development will show that the mathe-
matics arise naturally from our desire to manipulate a few basic geometric objects.
Much of what we present here is an application of vector spaces, geometry, and linear
algebra. Appendices B and C summarize the formalities of vector spaces and matrix
algebra, respectively.
In a vein similar to the approach we took in Chapter 2, we develop a simple
application program to illustrate the basic principles and to see how the concepts are
realized within an API. In this chapter, our example is focused on the representation
and transformations of a cube. We also consider how to specify transformations

```cpp
interactively and apply them smoothly. Because transformations are key to both
```

modeling and implementation, we will develop transformation capabilities that can
be carried out in both the application code and in the shaders.
116         Chapter 3       Geometric Objects and Transformations

### 3.1     SCALARS, POINTS, AND VECTORS

In computer graphics, we work with sets of geometric objects, such as lines, polygons,
and polyhedra. Such objects exist in a three-dimensional world and have properties
that can be described using concepts such as length and angles. As we discovered
working in two dimensions, we can deﬁne most geometric objects using a limited set
of simple entities. These basic geometric objects and the relationships among them
can be described using three fundamental types: scalars, points, and vectors.
Although we will consider each type from a geometric perspective, each of these
types also can be deﬁned formally, as in Appendix B, as obeying a set of axioms.
Although ultimately we will use the geometric instantiation of each type, we want to
take great care in distinguishing between the abstract deﬁnition of each entity and any
particular example, or implementation, of it. By taking care here, we can avoid many
subtle pitfalls later. Although we will work in three-dimensional spaces, virtually all
our results will hold in n-dimensional spaces.

#### 3.1.1 Geometric Objects

Our fundamental geometric object is a point. In a three-dimensional geometric sys-
tem, a point is a location in space. The only property that a point possesses is that
point’s location; a mathematical point has neither a size nor a shape.
Points are useful in specifying geometric objects but are not sufﬁcient by them-
selves. We need real numbers to specify quantities such as the distance between two
points. Real numbers—and complex numbers, which we will use occasionally—are
P         examples of scalars. Scalars are objects that obey a set of rules that are abstractions
of the operations of ordinary arithmetic. Thus, addition and multiplication are de-
ﬁned and obey the usual rules such as commutivity and associativity. Every scalar has
multiplicative and additive inverses, which implicitly deﬁne subtraction and division.
We need one additional type—the vector—to allow us to work with directions.1
Physicists and mathematicians use the term vector for any quantity with direction and
magnitude. Physical quantities, such as velocity and force, are vectors. A vector does
not, however, have a ﬁxed location in space.

*FIGURE 3.1 Directed line               In computer graphics, we often connect points with directed line segments, as*

segment that connects points.     shown in Figure 3.1. A directed line segment has both magnitude—its length—
and direction—its orientation—and thus is a vector. Because vectors have no ﬁxed
position, the directed line segments shown in Figure 3.2 are identical because they
have the same direction and magnitude. We will often use the terms vector and
directed line segment synonymously.
Vectors can have their lengths altered by real numbers. Thus, in Figure 3.3(a),
line segment A has the same direction as line segment B, but B has twice the length
1. The types such as vec3 used by GLSL and that we have used in our classes are not geometric types
but rather storage types. Hence, we can use a vec3 to store information about a point, a vector, or a

*FIGURE 3.2 Identical vectors.     color. Unfortunately, the choice of names by GLSL can cause some confusion.*

                                                                         3.1 Scalars, Points, and Vectors             117
D=A+C
A               B = 2A
(a)                                (b)
FIGURE 3.3 (a) Parallel line segments. (b) Addition of line segments.
that A has, so we can write B = 2A. We can also combine directed line segments by
the head-to-tail rule, as shown in Figure 3.3(b). Here, we connect the head of vector
A to the tail of vector C to form a new vector D, whose magnitude and direction are
determined by the line segment from the tail of A to the head of C. We call this new
vector, D, the sum of A and C and write D = A + C. Because vectors have no ﬁxed
positions, we can move any two vectors as necessary to form their sum graphically.
Note that we have described two fundamental operations: the addition of two vectors                      E = –A
and the multiplication of a vector by a scalar.
If we consider two directed line segments, A and E, as shown in Figure 3.4, with     FIGURE 3.4 Inverse vectors.
the same length but opposite directions, their sum as deﬁned by the head-to-tail
addition has no length. This sum forms a special vector called the zero vector, which
we denote 0, that has a magnitude of zero. Because it has no length, the orientation of
this vector is undeﬁned. We say that E is the inverse of A and we can write E = −A.
Using inverses of vectors, scalar-vector expressions such as A + 2B − 3C make sense.                              P
Although we can multiply a vector by a scalar to change its length, there are no
obvious sensible operations between two points that produce another point. Nor are
there operations between a point and a scalar that produce a point. There is, however,
an operation between points and directed line segments (vectors), as illustrated in            Q
Figure 3.5. We can use a directed line segment to move from one point to another.         FIGURE 3.5 Point-vector
We call this operation point-vector addition, and it produces a new point. We write       addition.
this operation as P = Q + v. We can see that the vector v displaces the point Q to the
new location P.
Looking at things slightly differently, any two points deﬁne a directed line seg-
ment or vector from one point to the second. We call this operation point-point
subtraction, and we can write it as v = P − Q. Because vectors can be multiplied
by scalars, some expressions involving scalars, vectors, and points make sense, such
as P + 3v, or 2P − Q + 3v (because it can be written as P + (P − Q) + 3v, a sum of
a point and a vector), whereas others, such as P + 3Q − v, do not.

#### 3.1.2 Coordinate-Free Geometry

Points exist in space regardless of any reference or coordinate system. Thus, we do not
need a coordinate system to specify a point or a vector. This fact may seem counter
118            Chapter 3      Geometric Objects and Transformations
(3, 3)     to your experiences, but it is crucial to understanding geometry and how to build
graphics systems. Consider the two-dimensional example shown in Figure 3.6. Here
we see a coordinate system deﬁned by two axes, an origin, and a simple geometric
object, a square. We can refer to the point at the lower-left corner of the square as
(1, 1)
having coordinates (1, 1) and note that the sides of the square are orthogonal to each
other and that the point at (3, 1) is 2 units from the point at (1, 1). Now suppose
that we remove the axes as shown in Figure 3.7. We can no longer specify where the

*FIGURE 3.6 Object and               points are. But those locations were relative to an arbitrary location of the origin*

coordinate system.                  and the orientation of the axes. What is more important is that the fundamental
geometric relationships are preserved. The square is still a square, orthogonal lines
are still orthogonal, and distances between points remain the same.
Of course, we may ﬁnd it inconvenient, at best, to refer to a speciﬁc point as “that
2              point over there” or “the blue point to the right of the red one.” Coordinate systems
and frames (see Section 3.3) solve this reference problem, but for now we want to see
just how far we can get following a coordinate-free approach that does not require an
2                         arbitrary reference system.

*FIGURE 3.7 Object without*

coordinate system.

#### 3.1.3 The Mathematical View: Vector and Affine Spaces

If we view scalars, points, and vectors as members of mathematical sets, then we
can look at a variety of abstract spaces for representing and manipulating these
sets of objects. Mathematicians have explored a variety of such spaces for applied
problems, ranging from the solution of differential equations to the approximation
of mathematical functions. The formal deﬁnitions of the spaces of interest to us—
vector spaces, afﬁne spaces, and Euclidean spaces—are given in Appendix B. We are
concerned with only those examples in which the elements are geometric types.
We start with a set of scalars, any pair of which can be combined to form another
scalar through two operations, called addition and multiplication. If these operations
obey the closure, associativity, commutivity, and inverse properties described in Ap-
pendix B, the elements form a scalar ﬁeld. Familiar examples of scalars include the
real numbers, complex numbers, and rational functions.
Perhaps the most important mathematical space is the (linear) vector space. A
vector space contains two distinct types of entities: vectors and scalars. In addition
to the rules for combining scalars, within a vector space, we can combine scalars and
vectors to form new vectors through scalar–vector multiplication and vectors with
vectors through vector–vector addition. Examples of mathematical vector spaces
include n-tuples of real numbers and the geometric operations on our directed line
segments.
In a linear vector space, we do not necessarily have a way of measuring a scalar
quantity. A Euclidean space is an extension of a vector space that adds a measure of
size or distance and allows us to deﬁne such things as the length of a line segment.
An afﬁne space is an extension of the vector space that includes an additional
type of object: the point. Although there are no operations between two points or
between a point and a scalar that yield points, there is an operation of vector–point
addition that produces a new point. Alternately, we can say there is an operation
                                                                         3.1 Scalars, Points, and Vectors   119
called point–point subtraction that produces a vector from two points. Examples of
afﬁne spaces include the geometric operations on points and directed line segments
that we introduced in Section 3.1.1.
In these abstract spaces, objects can be deﬁned independently of any particular
representation; they are simply members of various sets. One of the major vector-
space concepts is that of representing a vector in terms of one or more sets of basis
vectors. Representation (Section 3.3) provides the tie between abstract objects and
their implementation. Conversion between representations leads us to geometric
transformations.

#### 3.1.4 The Computer Science View

Although the mathematician may prefer to think of scalars, points, and vectors as
members of sets that can be combined according to certain axioms, the computer
scientist prefers to see them as abstract data types (ADTs). An ADT is a set of op-
erations on data; the operations are deﬁned independently of how the data are rep-
resented internally or of how the operations are implemented. The notion of data
abstraction is fundamental to modern computer science. For example, the operation
of adding an element to a list or of multiplying two polynomials can be deﬁned in-
dependently of how the list is stored or of how real numbers are represented on a
particular computer. People familiar with this concept should have no trouble dis-
tinguishing between objects (and operations on objects) and objects’ representations
(or implementations) in a particular system. From a computational point of view, we
should be able to declare geometric objects through code such as
vector u,v;
point p,q;
scalar a,b;
regardless of the internal representation or implementation of the objects on a partic-
ular system. In object-oriented languages, such as C++, we can use language features,
such as classes and overloading of operators, so we can write lines of code, such as
q = p+a*v;
using our geometric data types. Of course, ﬁrst we must deﬁne functions that per-
form the necessary operations; so that we can write them, we must look at the math-
ematical functions that we wish to implement. First, we will deﬁne our objects. Then
we will look to certain abstract mathematical spaces to help us with the operations
among them.

#### 3.1.5 Geometric ADTs

The three views of scalars, points, and vectors leave us with a mathematical and
computational framework for working with our geometric entities. In summary, for
computer graphics our scalars are the real numbers using ordinary addition and
120        Chapter 3     Geometric Objects and Transformations
multiplication. Our geometric points are locations in space, and our vectors are
directed line segments. These objects obey the rules of an afﬁne space. We can also
create the corresponding ADTs in a program.
Our next step is to show how we can use our types to form geometrical objects
and to perform geometric operations among them. We will use the following no-
tation:
Greek letters α, β , γ , . . . denote scalars;
uppercase letters P, Q, R, . . . denote points;
lowercase letters u, v, w, . . . denote vectors.
We have not as yet introduced any reference system, such as a coordinate system; thus,
for vectors and points, this notation refers to the abstract objects, rather than to these
objects’ representations in a particular reference system. We use boldface letters for
the latter in Section 3.3. The magnitude of a vector v is a real number denoted by |v|.
The operation of vector–scalar multiplication (see Appendix B) has the property that
|αv| = |α||v|,
and the direction of αv is the same as the direction of v if α is positive and the opposite
direction if α is negative.
We have two equivalent operations that relate points and vectors. First, there
is the subtraction of two points, P and Q—an operation that yields a vector v de-
v = P − Q.
As a consequence of this operation, given any point Q and vector v, there is a unique
point, P, that satisﬁes the preceding relationship. We can express this statement as
follows: Given a point Q and a vector v, there is a point P such that
P = Q + v.

*FIGURE 3.8 Point–point         Thus, P is formed by a point–vector addition operation. Figure 3.8 shows a visual*

subtraction.                   interpretation of this operation. The head-to-tail rule gives us a convenient way of
visualizing vector–vector addition. We obtain the sum u + v as shown in Figure 3.9(a)
by drawing the sum vector as connecting the tail of u to the head of v. However, we
can also use this visualization, as demonstrated in Figure 3.9(b), to show that for any
three points P, Q, and R,
(P − Q) + (Q − R) = P − R.

#### 3.1.6 Lines

The sum of a point and a vector (or the subtraction of two points) leads to the notion
of a line in an afﬁne space. Consider all points of the form
P(α) = P0 + αd,
                                                                           3.1 Scalars, Points, and Vectors              121
u+v          v                PR            PQ
u                            QR
(a)                             (b)

*FIGURE 3.9 Use of the head-to-tail rule. (a) For vectors. (b) For points.*

where P0 is an arbitrary point, d is an arbitrary vector, and α is a scalar that can
vary over some range of values. Given the rules for combining points, vectors, and                                P()
scalars in an afﬁne space, for any value of α, evaluation of the function P(α) yields
a point. For geometric vectors (directed line segments), these points lie on a line, as                d
shown in Figure 3.10. This form is known as the parametric form of the line because
we generate points on the line by varying the parameter α. For α = 0, the line passes
through the point P0, and as α is increased, all the points generated lie in the direction        P0
of the vector d. If we restrict α to nonnegative values, we get the ray emanating from
P0 and going in the direction of d. Thus, a line is inﬁnitely long in both directions, a     FIGURE 3.10 Line in an affine
line segment is a ﬁnite piece of a line between two points, and a ray is inﬁnitely long      space.
in one direction.

#### 3.1.7 Affine Sums

Whereas in an afﬁne space the addition of two vectors, the multiplication of a vector                               P()
=1
by a scalar, and the addition of a vector and a point are deﬁned, the addition of two
arbitrary points and the multiplication of a point by a scalar are not. However, there
is an operation called afﬁne addition that has certain elements of these latter two                    v
operations. For any point Q, vector v, and positive scalar α,
=0
P = Q + αv                                                                                         Q
describes all points on the line from Q in the direction of v, as shown in Figure 3.11.      FIGURE 3.11 Affine addition.
However, we can always ﬁnd a point R such that
v = R − Q;
thus,
P = Q + α(R − Q) = αR + (1 − α)Q.
This operation looks like the addition of two points and leads to the equivalent form
P = α1R + α2Q,
122               Chapter 3          Geometric Objects and Transformations
α1 + α2 = 1.
P()                 3.1.8 Convexity
A convex object is one for which any point lying on the line segment connecting any
two points in the object is also in the object. We saw the importance of convexity for
polygons in Chapter 2. We can use afﬁne sums to help us gain a deeper understanding
of convexity. For 0 ≤ α ≤ 1, the afﬁne sum deﬁnes the line segment connecting R and

*FIGURE 3.12 Line segment                   Q, as shown in Figure 3.12; thus, this line segment is a convex object. We can extend*

that connects two points.                  the afﬁne sum to include objects deﬁned by n points P1, P2 , . . . , Pn. Consider the
P = α1P1 + α2P2 + . . . + αnPn .
We can show, by induction (see Exercise 3.29), that this sum is deﬁned if and only if
α1 + α2 + . . . + αn = 1.
The set of points formed by the afﬁne sum of n points, under the additional restric-
αi ≥ 0,    i = 1, 2, . . . , n,

*FIGURE 3.13 Convex hull.                  is called the convex hull of the set of points (Figure 3.13). It is easy to verify that the*

convex hull includes all line segments connecting pairs of points in {P1, P2 , . . . , Pn}.
Geometrically, the convex hull is the set of points that we form by stretching a tight-
ﬁtting surface over the given set of points—shrink-wrapping the points. It is the
smallest convex object that includes the set of points. The notion of convexity is
extremely important in the design of curves and surfaces; we will return to it in

## Chapter 10.


#### 3.1.9 Dot and Cross Products

Many of the geometric concepts relating the orientation between two vectors are in
terms of the dot (inner) and cross (outer) products of two vectors. The dot product
of u and v is written u . v (see Appendix B). If u . v = 0, u and v are said to be
orthogonal. In a Euclidean space, the magnitude of a vector is deﬁned. The square of
the magnitude of a vector is given by the dot product
u                              |u|2 = u . u.
The cosine of the angle between two vectors is given by
θ
u.v
v             cos θ =        .
|u| cos θ
|u||v|

*FIGURE 3.14 Dot product                    In addition, |u| cos θ = u . v/|v| is the length of the orthogonal projection of u onto*

and projection.                            v, as shown in Figure 3.14. Thus, the dot product expresses the geometric result that
                                                                          3.1 Scalars, Points, and Vectors                123
the shortest distance from a point (the end of the vector u) to the line segment v is
obtained by drawing the vector orthogonal to v from the end of u. We can also see
that the vector u is composed of the vector sum of the orthogonal projection of u on
v and a vector orthogonal to v.
u×v
In a vector space, a set of vectors is linearly independent if we cannot write one
of the vectors in terms of the others using scalar-vector addition. A vector space has a
dimension, which is the maximum number of linearly independent vectors that we
can ﬁnd. Given any three linearly independent vectors in a three-dimensional space,                                   v
we can use the dot product to construct three vectors, each of which is orthogonal
to the other two. This process is outlined in Appendix B. We can also use two non-
parallel vectors, u and v, to determine a third vector n that is orthogonal to them
(Figure 3.15). This vector is the cross product                                            FIGURE 3.15 Cross product.
n = u × v.
Note that we can use the cross product to derive three mutually orthogonal vectors
in a three-dimensional space from any two nonparallel vectors. Starting again with u
and v, we ﬁrst compute n as before. Then, we can compute w by
w = u × n,
and u, n, and w are mutually orthogonal.
The cross product is derived in Appendix C, using the representation of the
vectors that gives a direct method for computing it. The magnitude of the cross
product gives the magnitude of the sine of the angle θ between u and v,
|u × v|
| sin θ| =           .
|u||v|
Note that the vectors u, v, and n form a right-handed coordinate system; that is, if
u points in the direction of the thumb of the right hand and v points in the direction
of the index ﬁnger, then n points in the direction of the middle ﬁnger.

#### 3.1.10 Planes

A plane in an afﬁne space can be deﬁned as a direct extension of the parametric line.
From simple geometry, we know that three points not on the same line determine a
unique plane. Suppose that P, Q, and R are three such points in an afﬁne space. The
line segment that joins P and Q is the set of points of the form
T (,  )
S(α) = αP + (1 − α)Q,         0 ≤ α ≤ 1.
P               S()          Q
Suppose that we take an arbitrary point on this line segment and form the line
segment from this point to R, as shown in Figure 3.16. Using a second parameter            FIGURE 3.16 Formation of a
β, we can describe points along this line segment as                                       plane.
T(β) = βS + (1 − β)R,          0 ≤ β ≤ 1.
124        Chapter 3   Geometric Objects and Transformations
Such points are determined by both α and β and form the plane determined by P, Q,
and R. Combining the preceding two equations, we obtain one form of the equation
of a plane:
T(α, β) = β[αP + (1 − α)Q] + (1 − β)R.
We can rearrange this equation in the following form:
T(α, β) = P + β(1 − α)(Q − P) + (1 − β)(R − P).
Noting that Q − P and R − P are arbitrary vectors, we have shown that a plane can
also be expressed in terms of a point, P0, and two nonparallel vectors, u and v, as
T(α, β) = P0 + αu + βv.
T(α, β) = βαP + β(1 − α)Q + (1 − β)R,
this form is equivalent to expressing T as
T(α, β  , γ ) = α P + β Q + γ R,
α  + β  + γ  = 1.
The representation of a point by (α  , β  , γ ) is called its barycentric coordinate
representation.
We can also observe that for 0 ≤ α, β ≤ 1, all the points T(α, β) lie in the triangle
formed by P, Q, and R. If a point P lies in the plane, then
P − P0 = αu + βv.

*FIGURE 3.17 Normal to a      We can ﬁnd a vector w that is orthogonal to both u and v, as shown in Figure 3.17. If*

plane.                       we use the cross product
n = u × v,
n . (P − P0) = 0.
The vector n is perpendicular, or orthogonal, to the plane; it is called the normal to
the plane. The forms P(α), for the line, and T(α, β), for the plane, are known as
parametric forms because they give the value of a point in space for each value of the
parameters α and β.
                                                                       3.2 Three-Dimensional Primitives          125

### 3.2     THREE-DIMENSIONAL PRIMITIVES

In a three-dimensional world, we can have a far greater variety of geometric objects
than we can in two dimensions. When we worked in a two-dimensional plane in

## Chapter 2, we considered objects that were simple curves, such as line segments, and

ﬂat objects with well-deﬁned interiors, such as simple polygons. In three dimensions,
we retain these objects, but they are no longer restricted to lie in the same plane.
Hence, curves become curves in space (Figure 3.18), and objects with interiors can
become surfaces in space (Figure 3.19). In addition, we can have objects with vol-        FIGURE 3.18 Curves in three
umes, such as parallelepipeds and ellipsoids (Figure 3.20).                               dimensions.
We face two problems when we expand our graphics system to incorporate all
these possibilities. First, the mathematical deﬁnitions of these objects can become
complex. Second, we are interested in only those objects that lead to efﬁcient imple-
mentations in graphics systems. The full range of three-dimensional objects cannot
be supported on existing graphics systems, except by approximate methods.
Three features characterize three-dimensional objects that ﬁt well with existing
graphics hardware and software:
1. The objects are described by their surfaces and can be thought of as being

*FIGURE 3.19 Surfaces in*

hollow.                                                                          three dimensions.
2. The objects can be speciﬁed through a set of vertices in three dimensions.
3. The objects either are composed of or can be approximated by ﬂat, convex
polygons.
We can understand why we set these conditions if we consider what most mod-
ern graphics systems do best: They render triangles or meshes of triangles. Com-
modity graphics cards can render over 100 million small, ﬂat triangles per second.
Performance measurements for graphics systems usually are quoted for small three-
dimensional triangles that can be generated by triangle strips. In addition, these

*FIGURE 3.20 Volumetric*

triangles are shaded, lit, and texture mapped, features that are implemented in the       objects.
hardware of modern graphics cards.
The ﬁrst condition implies that we need only two-dimensional primitives to
model three-dimensional objects because a surface is a two- rather than a three-
dimensional entity. The second condition is an extension of our observations in
Chapters 1 and 2. If an object is speciﬁed by vertices, we can use a pipeline architec-
ture to process these vertices at high rates, and we can use the hardware to generate
the images of the objects only during rasterization. The ﬁnal condition is an exten-
sion from our discussion of two-dimensional polygons. Most graphics systems are
optimized for the processing of points, line segments, and triangles. In three dimen-
sions, a triangle is speciﬁed by an ordered list of three vertices.
However, for general polygons speciﬁed with more than three vertices, the ver-
tices do not have to lie in the same plane. If they do not, there is no simple way to
deﬁne the interior of the object. Consequently, most graphics systems require that
the application either specify simple planar polygons or triangles. If a system allows
126   Chapter 3   Geometric Objects and Transformations
polygons and the application does not specify a ﬂat polygon, then the results of ras-
terizing the polygon are not guaranteed to be what the programmer might desire.
Because triangular polygons are always ﬂat, either the modeling system is designed to
always produce triangles, or the graphics system provides a method to divide, or tes-
sellate, an arbitrary polygon into triangular polygons. If we apply this same argument
to a curved object, such as a sphere, we realize that we should use an approximation
to the sphere composed of small, ﬂat polygons. Hence, even if our modeling system
provides curved objects, we assume that a triangle mesh approximation is used for
implementation.
The major exception to this approach is constructive solid geometry (CSG). In
such systems, we build objects from a small set of volumetric objects through a set of
operations such as union and intersection. We consider CSG models in Chapter 8.
Although this approach is an excellent one for modeling, rendering CSG models
is more difﬁcult than is rendering surface-based polygonal models. Although this
situation may not hold in the future, we discuss in detail only surface rendering.
All the primitives with which we work can be speciﬁed through a set of vertices.
As we move away from abstract objects to real objects, we must consider how we
represent points in space in a manner that can be used within our graphics systems.

### 3.3   COORDINATE SYSTEMS AND FRAMES

So far, we have considered vectors and points as abstract objects, without representing
them in an underlying reference system. In a three-dimensional vector space, we can
represent any vector w uniquely in terms of any three linearly independent vectors,
v1, v2 , and v3 (see Appendix B), as
w = α1v1 + α2v2 + α3v3 .
The scalars α1, α2, and α3 are the components of w with respect to the basis v1, v2 ,
and v3. These relationships are shown in Figure 3.21. We can write the representation
v2
w = α1v 1 + α2v 2 + α3v 3
α1
v1
v3

*FIGURE 3.21 Vector derived from three basis vectors.*

                                                                   3.3 Coordinate Systems and Frames   127
of w with respect to this basis as the column matrix
⎡ ⎤
α1
⎣
a = α2 ⎦ ,
α3
where boldface letters denote a representation in a particular basis, as opposed to the
original abstract vector w. We can also write this relationship as
⎡ ⎤
v1
T ⎣
w=a        v2 ⎦ = a T v,
v3
⎡     ⎤
v1
v = ⎣ v2 ⎦ .
v3
We usually think of the basis vectors, v1, v2 , v3, as deﬁning a coordinate system.
However, for dealing with problems using points, vectors, and scalars, we need a
more general method. Figure 3.22 shows one aspect of the problem. The three vectors
form a coordinate system that is shown in Figure 3.22(a) as we would usually draw
it, with the three vectors emerging from a single point. We could use these three
basis vectors as a basis to represent any vector in three dimensions. Vectors, however,
have direction and magnitude but lack a position attribute. Hence, Figure 3.22(b) is
equivalent, because we have moved the basis vectors, leaving their magnitudes and
directions unchanged. Most people ﬁnd this second ﬁgure confusing, even though
mathematically it expresses the same information as the ﬁrst ﬁgure. We are still left
with the problem of how to represent points—entities that have ﬁxed positions.
(a)                               (b)

*FIGURE 3.22 Coordinate systems. (a) Vectors emerging from a common*

point. (b) Vectors moved.
128            Chapter 3     Geometric Objects and Transformations
Because an afﬁne space contains points, once we ﬁx a particular reference
point—the origin—in such a space, we can represent all points unambiguously. The
usual convention for drawing coordinate axes as emerging from the origin, as shown
in Figure 3.22(a), makes sense in the afﬁne space where both points and vectors have
representations. However, this representation requires us to know both the reference
point and the basis vectors. The origin and the basis vectors determine a frame.
Loosely, this extension ﬁxes the origin of the vector coordinate system at some point
P0. Within a given frame, every vector can be written uniquely as
w = α1v1 + α2v2 + α3v3 = a T v,
just as in a vector space; in addition, every point can be written uniquely as
P = P0 + β1v1 + β2v2 + β3v3 = P0 + bT v.
p = (x, y, z)   Thus, the representation of a particular vector in a frame requires three scalars; the
representation of a point requires three scalars and the knowledge of where the origin
is located. As we will see in Section 3.3.4, by abandoning the more familiar notion of
v                   a coordinate system and a basis in that coordinate system in favor of the less familiar
notion of a frame, we avoid the difﬁculties caused by vectors having magnitude and
x     direction but no ﬁxed position. In addition, we are able to represent points and
vectors in a manner that will allow us to use matrix representations but that maintains
a distinction between the two geometric types.
Because points and vectors are two distinct geometric types, graphical represen-
z                                   tations that equate a point with a directed line segment drawn from the origin to that

*FIGURE 3.23 A dangerous             point (Figure 3.23) should be regarded with suspicion. Thus, a correct interpretation*

representation of a vector.         of Figure 3.23 is that a given vector can be deﬁned as going from a ﬁxed reference
point (the origin) to a particular point in space. Note that a vector, like a point, exists
regardless of the reference system, but as we will see with both points and vectors,
eventually we have to work with their representation in a particular reference system.

#### 3.3.1 Representations and N-Tuples

Suppose that vectors e1, e2, and e3 form a basis. The representation of any vector, v, is
given by the component (α1, α2 , α3) of a vector a where
v = α1e1 + α2e2 + α3e3 .
The basis vectors2 must themselves have representations that we can denote e1, e2,
and e3, given by
2. Many textbooks on vectors refer to these vectors as the unit basis i, j, k and write other vectors in
the form v = α1i + α2j + α3k.
                                                                    3.3 Coordinate Systems and Frames   129
e1 = (1, 0, 0)T ,
e2 = (0, 1, 0)T ,
e3 = (0, 0, 1)T .
In other words, the 3-tuple (1, 0, 0) is the representation of the ﬁrst basis vector.
Consequently, rather than thinking in terms of abstract vectors, we can work with
3-tuples and we can write the representation of any vector v as a column matrix a or
the 3-tuple (α1, α2 , α3), where
a = α1e1 + α2e2 + α3e3 .
The basis 3-tuples e1, e2, and e3 are vectors in the familiar Euclidean space R 3 . The
vector space R 3 is equivalent (or homomorphic) to the vector space of our original
geometric vectors. From a practical perspective, it is almost always easier to work with
3-tuples (or more generally n-tuples) than with other representations.

#### 3.3.2 Change of Coordinate Systems

Frequently, we are required to ﬁnd how the representation of a vector changes when
we change the basis vectors. For example, in OpenGL, we specify our geometry using
the coordinate system or frame that is natural for the model, which is known as the
object or model frame. Models are then brought into the world frame. At some
point, we want to know how these objects appear to the camera. It is natural at that
point to convert from the world frame to the camera or eye frame. The conversion
from the object frame to the eye frame is done by the model-view matrix.
Let’s consider changing representations for vectors ﬁrst. Suppose that

```cpp
{v1, v2 , v3} and {u1, u2 , u3} are two bases. Each basis vector in the second set can
```

be represented in terms of the ﬁrst basis (and vice versa). Hence, there exist nine
scalar components, {γij }, such that
u1 = γ11v1 + γ12v2 + γ13v3 ,
u2 = γ21v1 + γ22v2 + γ23v3 ,
u3 = γ31v1 + γ32v2 + γ33v3 .
The 3 × 3 matrix
⎡             ⎤
γ11 γ12 γ13
M = ⎣ γ21 γ22 γ23 ⎦
γ31 γ32 γ33
is deﬁned by these scalars, and
⎡ ⎤         ⎡ ⎤
u1          v1
⎣ u2 ⎦ = M ⎣ v 2 ⎦ ,
u3          v3
130   Chapter 3   Geometric Objects and Transformations
u = Mv.
The matrix M contains the information to go from a representation of a vector in
one basis to its representation in the second basis. The inverse of M gives the matrix
representation of the change from {u1, u2 , u3} to {v1, v2 , v3}. Consider a vector w that
has the representation {α1, α2 , α3} with respect to {v1, v2 , v3}; that is,
w = α1v1 + α2v2 + α3v3 .
Equivalently,
w = a T v,
⎡     ⎤
α1
a = ⎣ α2 ⎦ ,
α3
⎡ ⎤
v1
v = ⎣ v2 ⎦ .
v3
Assume that b is the representation of w with respect to {u1, u2 , u3}; that is,
w = β1u1 + β2u2 + β3u3 ,
⎡   ⎤
u1
w = bT ⎣ u2 ⎦ = bT u,
u3
⎡     ⎤
β1
b = ⎣ β2 ⎦ .
β3
Then, using our representation of the second basis in terms of the ﬁrst, we ﬁnd that
⎡ ⎤    ⎡ ⎤       ⎡ ⎤
u1        v1        v1
T ⎣    ⎦    ⎣    ⎦  T ⎣
w=b    u2 = b M v2 = a
v2 ⎦ .
u3        v3        v3
                                                                   3.3 Coordinate Systems and Frames            131
Thus,
a = MT b.
The matrix
T = (MT )−1
takes us from a to b, through the simple matrix equation
v3
b = Ta.                                                                                        v3′
Thus, rather than working with our original vectors, typically directed line segments,
we can work instead with their representations, which are 3-tuples or elements of                                v1′
R 3 . This result is important because it moves us from considering abstract vectors to
working with column matrices of scalars—the vectors’ representations. The impor-                                 v1
tant point to remember is that whenever we work with columns of real numbers as
“vectors,” there is an underlying basis of which we must not lose track, lest we end up
working in the wrong coordinate system.
v2
These changes in basis leave the origin unchanged. We can use them to represent
rotation and scaling of a set of basis vectors to derive another basis set, as shown in         v2′
Figure 3.24. However, a simple translation of the origin, or change of frame as shown

*FIGURE 3.24 Rotation and*

in Figure 3.25, cannot be represented in this way. After we complete a simple example,    scaling of a basis.
we introduce homogeneous coordinates, which allow us to change frames yet still use
matrices to represent the change.
v2
v1
v3

*FIGURE 3.25 Translation of a basis.*

132   Chapter 3   Geometric Objects and Transformations

#### 3.3.3 Example Change of Representation

Suppose that we have a vector w whose representation in some basis is
⎡ ⎤
a= 2⎦.
⎣
We can denote the three basis vectors as v1, v2, and v3. Hence,
w = v1 + 2v2 + 3v3 .
Now suppose that we want to make a new basis from the three vectors v1, v2, and v3
u1 = v1,
u2 = v 1 + v 2 ,
u 3 = v1 + v2 + v3 .
The matrix M is
⎡           ⎤
1 0 0
M=⎣1 1 0⎦.
1 1 1
The matrix that converts a representation in v1, v2, and v3 to one in which the basis
vectors are u1, u2, and u3 is
T = (MT )−1
⎡       ⎤
1 1 1 −1
= ⎣ 0 1 1⎦
0 0 1
⎡          ⎤
1 −1 0
= ⎣ 0 1 −1 ⎦ .
0 0    1
In the new system, the representation of w is
⎡ ⎤
−1
b = Ta = ⎣ −1 ⎦ .
That is,
w = −u1 − u2 + 3u3 .
                                                                       3.3 Coordinate Systems and Frames   133
If we are working in the space of 3-tuples (R 3), rather than in an abstract setting,
then we can associate v1, v2, and v3 with the unit basis in R 3:
⎡ ⎤               ⎡ ⎤                 ⎡ ⎤
1                 0                  0
⎣
e1 = 0 ,  ⎦            ⎣
e2 = 1 ,   ⎦        e3 = 0 ⎦ .
⎣
0                  0                  1
We can make this example a little more concrete by considering the following
variant. Suppose that we are working with the default (x, y, z) coordinate system,
which happens to be orthogonal. We are given the three direction vectors whose
representations are (1, 0, 0), (1, 1, 0), and (1, 1, 1). Thus, the ﬁrst vector points along
the x-axis, the second points in a direction parallel to the plane z = 0, and the third
points in a direction symmetric to the three basis directions. These three new vectors,
although they are not mutually orthogonal, are linearly independent and thus form
a basis for a new coordinate system that we can call the x  , y  , z  system. The original
directions have representations in the x  , y  , z  system given by the columns of the
matrix T.

#### 3.3.4 Homogeneous Coordinates

The potential confusion between a vector and a point that we illustrated in Fig-
ure 3.23 still exists with a three-dimensional representation. Suppose that we start
with the frame deﬁned by the point P0 and the vectors v1, v2, and v3. Usually, our ﬁrst
inclination is to represent a point P located at (x, y, z) with the column matrix
⎡ ⎤
p= y⎦,
⎣
where x, y, and z are the components of the basis vectors for this point, so that
P = P0 + xv1 + yv2 + zv3 .
If we represent the point this way, then its representation is of the same form as the
w = xv1 + yv2 + zv3 .
Homogeneous coordinates avoid this difﬁculty by using a four-dimensional repre-
sentation for both points and vectors in three dimensions. In the frame speciﬁed by
(v1, v2 , v3 , P0), any point P can be written uniquely as
P = α1v1 + α2v2 + α3v3 + P0 .
If we agree to deﬁne the “multiplication” of a point by the scalars 0 and 1 as
0 . P = 0,
1 . P = P,
134   Chapter 3   Geometric Objects and Transformations
then we can express this relation formally, using a matrix product, as
⎡   ⎤
v1
⎢v ⎥
⎢ ⎥
P = [ α 1 α2    α3   1 ]⎢ 2 ⎥ .
⎣ v3 ⎦
P0
Strictly speaking, this expression is not a dot or inner product, because the elements
of the matrices are dissimilar; nonetheless, the expression is computed as though
it were an inner product by multiplying corresponding elements and summing the
results. The four-dimensional row matrix on the right side of the equation is the
homogeneous-coordinate representation of the point P in the frame determined by
v1, v2 , v3, and P0. Equivalently, we can say that P is represented by the column matrix
⎡  ⎤
α1
⎢α ⎥
⎢ ⎥
p=⎢ 2⎥.
⎣ α3 ⎦
In the same frame, any vector w can be written as
w = δ1v1 + δ2v2 + δ3v3
⎡     ⎤
v1
⎢v ⎥
⎢ ⎥
= [ δ1 δ2     δ3   0 ]T ⎢ 2 ⎥ .
⎣ v3 ⎦
P0
Thus, w can be represented by the column matrix
⎡ ⎤
δ1
⎢δ ⎥
⎢ ⎥
w=⎢ 2⎥.
⎣ δ3 ⎦
There are numerous ways to interpret this formulation geometrically.
We simply note that we can carry out operations on points and vectors using their
homogeneous-coordinate representations and ordinary matrix algebra. Consider, for
example, a change of frames—a problem that caused difﬁculties when we used three-
dimensional representations. If (v1, v2 , v3 , P0) and (u1, u2 , u3 , Q 0) are two frames,
then we can express the basis vectors and reference point of the second frame in terms
of the ﬁrst as
                                                                 3.3 Coordinate Systems and Frames   135
u1 = γ11v1 + γ12v2 + γ13v3 ,
u2 = γ21v1 + γ22v2 + γ23v3 ,
u3 = γ31v1 + γ32v2 + γ33v3 ,
Q 0 = γ41v1 + γ42v2 + γ43v3 + P0 .
These equations can be written in the form
⎡    ⎤  ⎡ ⎤
u1      v1
⎢u ⎥    ⎢v ⎥
⎢ 2⎥    ⎢ ⎥
⎢    ⎥=M⎢ 2 ⎥,
⎣ u3 ⎦  ⎣ v3 ⎦
Q0      P0
where now M is the 4 × 4 matrix
⎡                    ⎤
γ11 γ12 γ13 0
⎢γ     γ     γ23 0 ⎥
⎢                    ⎥
M = ⎢ 21 22              ⎥.
⎣ γ31 γ32 γ33 0 ⎦
γ41 γ42 γ43 1
M is called the matrix representation of the change of frames.
We can also use M to compute the changes in the representations directly. Sup-
pose that a and b are the homogeneous-coordinate representations either of two
points or of two vectors in the two frames. Then
⎡   ⎤       ⎡ ⎤         ⎡ ⎤
u1           v1          v1
⎢u ⎥         ⎢v ⎥        ⎢v ⎥
⎢    ⎥       ⎢ ⎥         ⎢ ⎥
bT ⎢ 2 ⎥ = bT M ⎢ 2 ⎥ = a T ⎢ 2 ⎥ .
⎣ u3 ⎦       ⎣ v3 ⎦      ⎣ v3 ⎦
Q0           P0          P0
Hence,
a = MT b.
When we work with representations, as is usually the case, we are interested in MT ,
⎡                    ⎤
α11 α12 α13 α14
⎢α       α22 α23 α24 ⎥
⎢                    ⎥
MT = ⎢ 21                  ⎥
⎣ α31 α32 α33 α34 ⎦
0      0   0  1
and is determined by 12 coefﬁcients.
136   Chapter 3   Geometric Objects and Transformations
There are other advantages to using homogeneous coordinates that we explore
extensively in later chapters. Perhaps the most important is that all afﬁne (line-
preserving) transformations can be represented as matrix multiplications in homo-
geneous coordinates. Although we have to work in four dimensions to solve three-
dimensional problems when we use homogeneous-coordinate representations, less
arithmetic work is involved. The uniform representation of all afﬁne transformations
makes carrying out successive transformations (concatenation) far easier than in
three-dimensional space. In addition, modern hardware implements homogeneous-
coordinate operations directly, using parallelism to achieve high-speed calculations.

#### 3.3.5 Example Change in Frames

Consider again the example of Section 3.3.3. If we again start with the basis vectors
v1, v2, and v3 and convert to a basis determined by the same u1, u2, and u3, then the
three equations are the same:
u1 = v1,
u2 = v1 + v2 ,
u3 = v1 + v2 + v3 .
The reference point does not change, so we add the equation
Q 0 = P0 .
Thus, the matrices in which we are interested are the matrix
⎡                ⎤
1 0 0 0
⎢1 1 0 0⎥
⎢                ⎥
M=⎢                  ⎥,
⎣1 1 1 0⎦
0 0 0 1
its transpose, and their inverses.
Suppose that in addition to changing the basis vectors, we also want to move
the reference point to the point that has the representation (1, 2, 3, 1) in the original
system. The displacement vector v = v1 + 2v2 + 3v3 moves P0 to Q 0. The fourth
component identiﬁes this entity as a point. Thus, we add to the three equations from
Q 0 = P0 + v1 + 2v2 + 3v3 ,
⎡             ⎤
1 1 1 1
⎢0 1 1 2⎥
⎢             ⎥
MT = ⎢              ⎥.
⎣0 0 1 3⎦
0 0 0 1
                                                                  3.3 Coordinate Systems and Frames   137
⎡        ⎤
1 −1 0  1
⎢ 0 1 −1 1 ⎥
⎢           ⎥
T = (MT )−1 = ⎢           ⎥.
⎣0 0   1 −3 ⎦
0 0  0  1
This pair of matrices allows us to move back and forth between representations in
the two frames. Note that T takes the point (1, 2, 3) in the original frame, whose
⎡ ⎤
⎢2⎥
⎢ ⎥
p=⎢ ⎥,
⎣3⎦
⎡ ⎤
⎢0⎥
⎢ ⎥
p = ⎢ ⎥ ,
⎣0⎦
the origin in the new system. However, the vector (1, 2, 3), which is represented as
⎡ ⎤
⎢2⎥
⎢ ⎥
a=⎢ ⎥
⎣3⎦
in the original system, is transformed to
⎡     ⎤
−1
⎢ −1 ⎥
⎢     ⎥
b=⎢        ⎥,
⎣ 3 ⎦
a transformation that is consistent with the results from our example of change
in coordinate systems and that also demonstrates the importance of distinguishing
between points and vectors.

#### 3.3.6 Working with Representations

Application programs almost always work with representations rather than abstract
points. Thus, when we specify a point—for example, by putting its coordinates in
an array—we are doing so with respect to some frame. In our earlier examples, we
138   Chapter 3   Geometric Objects and Transformations
avoided dealing with changes in frames by specifying data in clip coordinates, a
normalized system that OpenGL uses for its rendering. However, applications pro-
grams prefer to work in frames that have a relationship to the problem on which
they are working and thus want to place the origin, orient the axes, and scale the
units so they make sense in the problem space. Because OpenGL eventually needs
its data in clip coordinates, at least one change of representation is required. As
we shall see, in fact there are additional frames that we will ﬁnd useful both for
modeling and rendering. Hence, we will carry out a sequence of changes in repre-
sentation.
Changes of representation are thus speciﬁed by a matrix of the form
a = Cb,
where a and b are the two representations of a point or vector in homogeneous
coordinates. As we have seen in Section 3.3.4, this matrix must be a homogeneous
form so C is the transpose of a matrix M and is given by
⎡                        ⎤
α11 α12        α13   α14
⎢α    α22        α23   α24 ⎥
⎢                          ⎥
C = MT = ⎢ 21                       ⎥.
⎣ α31 α32        α33   α34 ⎦
0   0          0     1
The problem is how to ﬁnd C when we are working with representations. It turns out
to be quite easy. Suppose that we are working in some frame and we specify another
frame by its representation in this frame. Thus, if in the original system we specify a
frame by the representations of three vectors, u, v, and n, and give the origin of the
new frame as the point p, in homogeneous coordinates all four of these entities are
4-tuples or elements of R 4 .
Let’s consider the inverse problem. The matrix
T = C−1
converts from representations in the (u, v, n, p) frame to representations in the orig-
inal frame. Thus, we must have
⎡ ⎤   ⎡ ⎤
1      u1
⎢0⎥   ⎢u ⎥
⎢ ⎥   ⎢ 2⎥
T⎢ ⎥=u=⎢ ⎥.
⎣0⎦   ⎣ u3 ⎦
0      0
                                                                               3.4 Frames in OpenGL   139
Likewise,
⎡ ⎤         ⎡ ⎤
0           v1
⎢ 1⎥        ⎢v ⎥
⎢ ⎥         ⎢ ⎥
T⎢ ⎥=v=⎢ 2⎥,
⎣0⎦         ⎣ v3 ⎦
0           0
⎡ ⎤         ⎡ ⎤
0           n1
⎢0⎥         ⎢n ⎥
⎢ ⎥         ⎢ ⎥
T⎢ ⎥=n=⎢ 2⎥,
⎣ 1⎦        ⎣ n3 ⎦
0            0
⎡ ⎤         ⎡ ⎤
0           p1
⎢0⎥         ⎢p ⎥
⎢ ⎥         ⎢ ⎥
T⎢ ⎥=p=⎢ 2⎥.
⎣0⎦         ⎣ p3 ⎦
1            1
Putting these results together, we ﬁnd
⎡              ⎤
u1 v1 n1 p1
⎢u v n p ⎥
⎢            2⎥
TI = T = [ u v n p ] = ⎢ 2 2           2
⎥,
⎣ u3 v3 n3 p3 ⎦
0 0 0 1
⎡             ⎤−1
u1 v1 n1 p1
⎢u v n p ⎥
⎢           2⎥
C =[u    v   n   p ]−1 = ⎢ 2 2    2
⎥ .
⎣ u3 v3 n3 p3 ⎦
0 0 0 1
Thus, the representation of a frame in terms of another frame gives us the inverse
of the matrix we need to convert from representations in the ﬁrst frame to represen-
tations in the second. Of course, we must compute this inverse, but computing the
inverse of a 4 × 4 matrix of this form should not present a problem.

### 3.4   FRAMES IN OPENGL

As we have seen, OpenGL is based on a pipeline model, the ﬁrst part of which is a
sequence of operations on vertices, many of which are geometric. We can characterize
such operations by a sequence of transformations or, equivalently, as a sequence of
changes of frames for the objects speciﬁed by an application program.
140   Chapter 3   Geometric Objects and Transformations
In versions of OpenGL with a ﬁxed-function pipeline and immediate-mode ren-
dering, six frames were speciﬁed in the pipeline. With programmable shaders, we
have a great deal of ﬂexibility to add additional frames or avoid some traditional
frames. Although as we demonstrated in our ﬁrst examples, we could use some
knowledge of how the pipeline functions to avoid using all these frames, that would
not be the best way to build our applications. Rather, each of the six frames we will
discuss will prove to be useful, either for developing our applications or for imple-
mentation of the pipeline. Some will be applied in the application code, others in our
shaders. Some may not be visible to the application. In each of these frames, a vertex
has different coordinates. The following is the usual order in which the frames occur
in the pipeline:
1. Object (or model) coordinates
2. World coordinates
3. Eye (or camera) coordinates
4. Clip coordinates
5. Normalized device coordinates
6. Window (or screen) coordinates
Let’s consider what happens when an application program speciﬁes a vertex. This
vertex may be speciﬁed directly in the application program or indirectly through an
instantiation of some object. In most applications, we tend to specify or use an object
with a convenient size, orientation, and location in its own frame called the model or
object frame. For example, a cube would typically have its faces aligned with axes
of the frame, its center at the origin, and have a side length of 1 or 2 units. The
coordinates in the corresponding function calls are in object or model coordinates.
An individual scene may comprise hundreds or even thousands of individual
objects. The application program generally applies a sequence of transformations
to each object to size, orient, and position it within a frame that is appropriate for
the particular application. For example, if we were using an instance of a square
for a window in an architectural application, we would scale it to have the correct
proportions and units, which would probably be in feet or meters. The origin of
application coordinates might be a location in the center of the bottom ﬂoor of the
building. This application frame is called the world frame, and the values are in
world coordinates. Note that if we do not model with predeﬁned objects or apply
any transformations before we specify our geometry, object and world coordinates
are the same.
Object and world coordinates are the natural frames for the application pro-
gram. However, the image that is produced depends on what the camera or viewer
sees. Virtually all graphics systems use a frame whose origin is the center of the
camera’s lens3 and whose axes are aligned with the sides of the camera. This frame
3. For a perspective view, the center of the lens is the center of projection (COP), whereas for an
orthogonal view, the direction of projection is aligned with the sides of the camera.
                                                                                   3.4 Frames in OpenGL   141
is called the camera frame or eye frame. Because there is an afﬁne transformation
that corresponds to each change of frame, there are 4 × 4 matrices that represent
the transformation from model coordinates to world coordinates and from world
coordinates to eye coordinates. These transformations usually are concatenated to-
gether into the model-view transformation, which is speciﬁed by the model-view
matrix. Usually, the use of the model-view matrix instead of the individual matrices
should not pose any problems for the application programmer. In Chapter 5, where
we discuss lighting and shading, we will see situations where we must separate the
two transformations.
The last three representations are used primarily in the implementation of the
pipeline, but, for completeness, we introduce them here. Once objects are in eye co-
ordinates, OpenGL must check whether they lie within the view volume. If an object
does not, it is clipped from the scene prior to rasterization. OpenGL can carry out this
process most efﬁciently if it ﬁrst carries out a projection transformation that brings
all potentially visible objects into a cube centered at the origin in clip coordinates.
We will study this transformation in Chapter 4. After this transformation, vertices
are still represented in homogeneous coordinates. The division by the w component,
called perspective division, yields three-dimensional representations in normalized
device coordinates. The ﬁnal transformation takes a position in normalized device
coordinates and, taking into account the viewport, creates a three-dimensional rep-
resentation in window coordinates. Window coordinates are measured in units of
pixels on the display but retain depth information. If we remove the depth coordi-
nate, we are working with two-dimensional screen coordinates.
The application programmer usually works with two frames: the eye frame and
the object frame. By concatenating them together to form the model-view matrix,
we have a transformation that positions the object frame relative to the eye frame.
Thus, the model-view matrix converts the homogeneous-coordinate representations
of points and vectors to their representations in the application space to their repre-
sentations in the eye frame.
Although an application does not require us to use the model-view matrix, the
model-view matrix is so important to most applications that we will almost always
include it in our examples. One of the issues we will discuss in some detail is where
we specify our transformations and where they are applied. For example, we could
specify a transformation in the application and apply it to the data there. We could
also deﬁne the parameters of a transformation in the application and send these
parameters to the shaders and let the GPU carry out the transformations. We examine
these approaches in the following sections.
Let’s assume that we allocate a model-view matrix in our applications and ini-
tialize it to an identity matrix. Now the object frame and eye frame are identical.
Thus, if we do not change the model-view matrix, we are working in eye coordi-
nates. As we saw in Chapter 2, the camera is at the origin of its frame, as shown in
Figure 3.26(a). The three basis vectors in eye space correspond to (1) the up direction
of the camera, the y direction; (2) the direction the camera is pointing, the negative
z direction; and (3) a third orthogonal direction, x, placed so that the x, y, z direc-
tions form a right-handed coordinate system. We obtain other frames in which to
142          Chapter 3        Geometric Objects and Transformations
y, yc                                                 yc
x, xc                                               xc
z, zc                                                zc
(a)                                                  (b)

*FIGURE 3.26 Camera and object frames. (a) In default positions. (b) After applying model-view matrix.*

place objects by performing homogeneous coordinate transformations that specify
new frames relative to the camera frame. In Section 3.5, we will learn how to specify
these transformations; in Section 3.3, we used them to position the camera relative
to our objects.
Because frame changes are represented by model-view matrices that can be
stored, we can save frames and move between frames by changing the current model-
view matrix. In Chapter 7, we will see that creating a data structure such as a stack to
store transformations will be helpful in working with complex models.
When ﬁrst working with multiple frames, there can be some confusion about
which frames are ﬁxed and which are varying. Because the model-view matrix po-
sitions the camera relative to the objects, it is usually a matter of convenience as to
which frame we regard as ﬁxed. Most of the time, we will regard the camera as ﬁxed
and the other frames as moving relative to the camera, but you may prefer to adopt a
different view.
Before beginning a detailed discussion of transformations and how we use them
in OpenGL, we present two simple examples. In the default settings shown in Fig-
                                                                                     3.4 Frames in OpenGL   143
ure 3.26(a), the camera and object frames coincide with the camera pointing in the
negative z-direction. In many applications, it is natural to specify objects near the
origin, such as a square centered at the origin or perhaps a group of objects whose
center of mass is at the origin. It is also natural to set up our viewing conditions so
that the camera sees only those objects that are in front of it. Consequently, to form
images that contain all these objects, we must either move the camera away from the
objects or move the objects away from the camera. Equivalently, we move the cam-
era frame relative to the object frame. If we regard the camera frame as ﬁxed and the
model-view matrix as positioning the object frame relative to the camera frame, then
the model-view matrix,
⎡                  ⎤
1 0 0 0
⎢0 1 0 0 ⎥
⎢                  ⎥
A=⎢                     ⎥,
⎣ 0 0 1 −d ⎦
0 0 0 1
moves a point (x, y, z) in the object frame to the point (x, y, z − d) in the camera
frame. Thus, by making d a suitably large positive number, we “move” the objects
in front of the camera by moving the world frame relative to the camera frame, as
shown in Figure 3.26(b). Note that, as far as the user—who is working in world
coordinates—is concerned, she is positioning objects as before. The model-view ma-
trix takes care of the relative positioning of the object and eye frames. This strategy is
almost always better than attempting to alter the positions of the objects by changing
their vertex positions to place them in front of the camera.
Let’s look at another example. When we deﬁne our objects using vertices, we
are working in the application frame (or world frame). The vertex positions spec-
iﬁed there are the representation of points in that frame. Thus, we do not use the
world frame directly but rather implicitly by representing points (and vectors) in it.
Consider the situation illustrated in Figure 3.27.

*FIGURE 3.27 Camera at (1, 0, 1) pointing toward the origin.*

144   Chapter 3   Geometric Objects and Transformations
Here we see the camera positioned in the object frame. Using homogeneous
coordinates, it is centered at a point p = (1, 0, 1, 1)T in world coordinates and points
at the origin in the world frame. Thus, the vector whose representation in the world
frame is n = (−1, 0, −1, 0)T is orthogonal to the back of the camera and points
toward the origin. The camera is oriented so that its up direction is the same as the up
direction in world coordinates and has the representation v = (0, 1, 0, 0)T . We can
form an orthogonal coordinate system for the camera by using the cross product to
determine a third orthogonal direction for the camera, which is u = (1, 0, −1, 0)T .
We can now proceed as we did in Section 3.3.6 and derive the matrix M that converts
the representation of points and vectors in the world frame to their representations
in the camera frame. The transpose of this matrix in homogeneous coordinates is
obtained by the inverse of a matrix containing the coordinates of the camera,
⎡                ⎤    ⎡              ⎤−1 ⎡ 1                          ⎤
uT                1          0 −1 1        2             0 − 21    0
⎢ vT           ⎥ ⎢ 0           1 0 0⎥     ⎢ 0                        0⎥
⎢              ⎥ ⎢                    ⎥   ⎢                1 0         ⎥
(MT )−1 = ⎢ T            ⎥=⎢                    ⎥ =⎢ 1                           ⎥.
⎣n             ⎦ ⎣ −1          0 −1 1 ⎦   ⎣ −2             0 − 21    1⎦
0     0 0 1     0           0 0 1        0              0 0       1
Note that the origin in the original frame is now one unit in the n direction from
the origin in the camera frame or, equivalently, at the point whose representation is
(0, 0, 1, 1) in the camera frame.
In OpenGL, we can set a model-view matrix by sending an array of 16 elements
to the vertex shader. For situations in which we have the representation of one frame
in terms of another through the speciﬁcation of the basis vectors and the origin, it
is a direct exercise to ﬁnd the required coefﬁcients. However, such is not usually the
case. For most geometric problems, we usually go from one frame to another by a
sequence of geometric transformations such as rotations, translations, and scales. We
will follow this approach in subsequent sections. But ﬁrst, we will introduce some
helpful C++ classes.

### 3.5    MATRIX AND VECTOR CLASSES

In Chapter 2, we saw how using some new data types could clarify our application
code and were necessary for GLSL. Let’s expand and formalize these notions by

```cpp
introducing the C++ classes that we will use in our applications. The code is in two
```

ﬁles, mat.h and vec.h, that can both be included in your application through the

```cpp
#include "mat.h"
```

The basic types are mat2, mat3, mat4, vec2, vec3, and vec4. The matrix classes
are for 2 × 2, 3 × 3, and 4 × 4 matrices whereas the vector types are for 2-, 3-, and
4-element arrays. The arithmetic operators are overloaded, so we can write code
                                                                           3.5 Matrix and Vector Classes   145

```cpp
#include "mat.h"
```

vec4 x, y = vec4(1.0, 2.0, 3.0, 1.0); // use of constructor
x = 2.0*y;
x[2] = 5.0;
mat4 a, b = mat4(vec4(y), vec4(x), vec4(y), vec4(x)); //matrix constructor

```cpp
float s = 2.5;
```

a[2][1] = 3.5;
b = s*a;
vec4 z = b*x;
y = x*b;
Thus, we can reference individual elements of either type and carry out the standard
matrix operations, as described in Appendix B. Note that the products b*x and x*b
will, in general, yield different results. The ﬁrst is the product of a 1 × 4 row matrix
times a 4 × 4 square matrix, whereas the second is the product of a 4 × 4 square
matrix times a 4 × 1 column matrix. Both yield four elements and so can be stored
as vec4s.
In light of our previous deﬁnition of points and vectors, the use of the names
vec2, vec3, and vec4 may be a bit disconcerting. GLSL uses these types to store any
quantity that has three or four elements, including vectors (directions) and points in
homogeneous coordinates, colors (either RGB or RGBA), and, as we shall see later,
texture coordinates. The advantage is that we can write code for all these types that
uses the same operations. By using these same GLSL types in the classes in vec.h
and mat.h, the code that manipulates points, vectors, and transformations in our
applications will look similar to GLSL code. We will see that we will often have a
choice as to where we carry out our algorithms, in the application or in one of the
shaders. By having the same types available, we will be able to transfer an algorithm
easily from an application to one of the shaders.
One trick that can make the application code a little clearer is to use a typedef
to assign names to types that will make the code easier to understand. Some helpful
typedef vec3 color3;
typedef vec4 color4;
typedef vec3 point3;
typedef vec4 point4;
Of course we have not really created any new classes, but we prefer code such as
color3 red = color3(1.0, 0.0, 0.0);
vec3 red =    vec3(1.0, 0.0, 0.0);
146        Chapter 3   Geometric Objects and Transformations

### 3.6    MODELING A COLORED CUBE

We now have most of the basic conceptual and practical knowledge we need to build
three-dimensional graphical applications. We will use them to produce a program
that draws a rotating cube. One frame of an animation might be as shown in Fig-

*FIGURE 3.28 One frame of     ure 3.28. However, before we can rotate the cube, we will consider how we can*

cube animation.              model it efﬁciently. Although three-dimensional objects can be represented, like two-
dimensional objects, through a set of vertices, we will see that data structures will help
us to incorporate the relationships among the vertices, edges, and faces of geometric
objects. Such data structures are supported in OpenGL through a facility called vertex
arrays, which we introduce at the end of this section.
After we have modeled the cube, we can animate it by using afﬁne transforma-
tions. We introduce these transformations in Section 3.7 and then use them to alter
a model-view matrix. In Chapter 4, we use these transformations again as part of the
viewing process. Our pipeline model will serve us well. Vertices will ﬂow through a
number of transformations in the pipeline, all of which will use our homogeneous-
coordinate representation. At the end of the pipeline awaits the rasterizer. At this
point, we can assume it will do its job automatically, provided we perform the pre-
liminary steps correctly.

#### 3.6.1 Modeling the Faces

The cube is as simple a three-dimensional object as we might expect to model and
display. There are a number of ways, however, to model it. A CSG system would re-
gard it as a single primitive. At the other extreme, the hardware processes the cube as
an object deﬁned by eight vertices. Our decision to use surface-based models implies
that we regard a cube either as the intersection of six planes or as the six polygons,
called facets, that deﬁne its faces. A carefully designed data structure should support
both the high-level application view of the cube and the low-level view needed for the
implementation.
We start by assuming that the vertices of the cube are available through an array
of vertices. We will work with homogeneous coordinates, so
point4 vertices[8] = {
point4(-1.0,-1.0,-1.0,1.0),point4(1.0,-1.0,-1.0,1.0),
point4(1.0,1.0,-1.0,1.0), point4(-1.0,1.0,-1.0,1.0),
point4(-1.0,-1.0,1.0,1.0), point4(1.0,-1.0,1.0,1.0)
point4(1.0,1.0,1.0,1.0), point4(-1.0,1.0,1.0,1.0)};
We can then use the list of points to specify the faces of the cube. For example,
one face is given by the sequence of vertices (0, 3, 2, 1). We can specify the other ﬁve
faces similarly.

#### 3.6.2 Inward- and Outward-Pointing Faces

We have to be careful about the order in which we specify our vertices when we
are deﬁning a three-dimensional polygon. We used the order 0, 3, 2, 1 for the ﬁrst
                                                                                      3.6 Modeling a Colored Cube                  147
face. The order 1, 0, 3, 2 would be the same, because the ﬁnal vertex in a polygon
speciﬁcation is always linked back to the ﬁrst. However, the order 0, 1, 2, 3 is different.
Although it describes the same boundary, the edges of the polygon are traversed in the
reverse order—0, 3, 2, 1—as shown in Figure 3.29. The order is important because
each polygon has two sides. Our graphics systems can display either or both of them.                                       2
From the camera’s perspective, we need a consistent way to distinguish between the
two faces of a polygon. The order in which the vertices are speciﬁed provides this
information.                                                                                                 0
We call a face outward facing if the vertices are traversed in a counterclockwise
order when the face is viewed from the outside. This method is also known as the                        FIGURE 3.29 Traversal of the
right-hand rule because if you orient the ﬁngers of your right hand in the direction                    edges of a polygon.
the vertices are traversed, the thumb points outward.
In our example, the order 0, 3, 2, 1 speciﬁes an outward face of the cube, whereas
the order 0, 1, 2, 3 speciﬁes the back face of the same polygon. Note that each face of
an enclosed object, such as our cube, is an inside or outside face, regardless of from
where we view it, as long as we view the face from outside the object. By specifying
front and back carefully, we will be able to eliminate (or cull) faces that are not visible
or to use different attributes to display front and back faces. We will consider culling
further in Chapter 6.

#### 3.6.3 Data Structures for Object Representation

We could now describe our cube through a set of vertex speciﬁcations. For example,
we could use a two-dimensional array of positions
point3 faces[6][4];
or we could use a single array of 24 vertices
point3 cube_vertices[24];
where cube_vertices[i] contains the x, y, z coordinates of the ith vertex in the
list. Both of these methods work, but they both fail to capture the essence of the
cube’s topology, as opposed to the cube’s geometry. If we think of the cube as a
polyhedron, we have an object—the cube—that is composed of six faces. The faces
are each quadrilaterals that meet at vertices; each vertex is shared by three faces. In
addition, pairs of vertices deﬁne edges of the quadrilaterals; each edge is shared by
two faces. These statements describe the topology of a six-sided polyhedron. All are
true, regardless of the location of the vertices—that is, regardless of the geometry of
the object.4
Throughout the rest of this book, we will see that there are numerous advan-
tages to building for our objects data structures that separate the topology from the
4. We are ignoring special cases (singularities) that arise, for example, when three or more vertices
lie along the same line or when the vertices are moved so that we no longer have nonintersecting
faces.
148   Chapter 3   Geometric Objects and Transformations
Polygon         Faces      Vertex lists         Vertices
x 0, y0
5              6
Cube             A             0
x1, y1
B             3
1                                                C             2               x 2, y2
D             1
4              7                                                         x 3, y3
F             3               x 4, y4
0           3                                                                 x 5, y5
2               x 6, y6
x 7, y 7

*FIGURE 3.30 Vertex-list representation of a cube.*

geometry. In this example, we use a structure, the vertex list, that is both simple and
useful and can be expanded later.
The data specifying the location of the vertices contain the geometry and can be
stored as a simple list or array, such as in vertices[8]—the vertex list. The top-
level entity is a cube; we regard it as being composed of six faces. Each face consists of
four ordered vertices. Each vertex can be speciﬁed indirectly through its index. This
data structure is shown in Figure 3.30. One of the advantages of this structure is that
each geometric location appears only once, instead of being repeated each time it is
used for a facet. If, in an interactive application, the location of a vertex is changed,
the application needs to change that location only once, rather than searching for
multiple occurrences of the vertex.

#### 3.6.4 The Color Cube

We can use the vertex list to deﬁne a color cube. We use a function quad that takes
as input the indices of four vertices in outward pointing order and adds data to two
arrays, as in Chapter 2, to store the vertex positions and the corresponding colors for
vec4 quad_colors[36], vertices[36];

```cpp
int i = 0; /* vertex and color index */
```

Note that because we can only display triangles, the quad function must generate
two triangles for each face and thus six vertices. If we want each vertex to have its own
color, then we need 24 vertices and 24 colors for our data. Using this quad function,
we can specify our cube through the function
                                                                          3.6 Modeling a Colored Cube   149

```cpp
void colorcube()
{
```

quad(0,3,2,1);
quad(2,3,7,6);
quad(3,0,4,7);
quad(1,2,6,5);
quad(4,5,6,7);
quad(5,4,0,1);

```cpp
}
```

We will assign the colors to the vertices using the colors of the corners of the
color solid from Chapter 2 (black, white, red, green, blue, cyan, magenta, yellow). We
assign a color for each vertex using the index of the vertex. Alternately, we could use
the ﬁrst index of the ﬁrst vertex speciﬁed by quad to ﬁx the color for the entire face.
Here are the RGBA colors,
color4 colors[8] = {color4(0.0,0.0,0.0,1.0),
color4(1.0,0.0,0.0,1.0),
color4(1.0,1.0,0.0,1.0),
color4(0.0,1.0,0.0,1.0),
color4(0.0,0.0,1.0,1.0),
color4(1.0,0.0,1.0,1.0),
color4(0.0,1.0,1.0,1.0),
color4(1.0,1.0,1.0,1.0)};
and the vertices of a cube that corresponds to the clipping area in clip coordinates,
point4 vertices[8] = {point4(-1.0,-1.0,1.0,1.0), point4(-1.0,1.0,1.0,1.0),
point4(1.0,1.0,1.0,1.0), point4(1.0,-1.0,1.0,1.0),
point4(-1.0,-1.0,-1.0,1.0), point4(-1.0,1.0,-1.0,1.0),
point4(1.0,1.0,-1.0,1.0), point4(1.0,-1.0,-1.0,1.0)};
Here is the quad function that uses the ﬁrst three vertices to specify one triangle
and the ﬁrst, third, and fourth to specify the second:

```cpp
int i = 0;
void quad(int a, int b, int c, int d)
{
```

quad_color[i] = colors[a];
points[i] = vertices[a];
i++;
quad_color[i] = colors[b];
points[i] = vertices[b];
i++;
quad_color[i] = colors[c];
points[i] = vertices[c];
i++;
150          Chapter 3     Geometric Objects and Transformations
quad_color[i] = colors[a];
points[i] = vertices[a];
i++;
quad_color[i] = colors[c];
points[i] = vertices[c];
i++;
quad_color[i] = colors[d];
points[i] = vertices[d];
i++;

```cpp
}
```

Note the initialization of i outside the quad function. If, as in later examples, we
invoke quad multiple times, either because we change the colors or locations of the
same vertices or we have multiple cubes, we must be careful as to where we start
placing data in the points array. Our program is almost complete, but ﬁrst we
examine how the colors and other vertex attributes can be assigned to fragments by
the rasterizer.

#### 3.6.5 Interpolation

C2
Although we have speciﬁed colors for the vertices of the cube, the graphics system
must decide how to use this information to assign colors to points inside the polygon.
There are many ways to use the colors of the vertices to ﬁll in, or interpolate, colors
C4                across a polygon. Probably the most common method used in computer graphics is
based on the barycentric coordinate representation of triangles that we introduced in
C1
C0               C3           Section 3.1. One of the major reasons for this approach is that triangles are the key
object that we work with in rendering.

*FIGURE 3.31 Interpolation             Consider the polygon shown in Figure 3.31. The colors C0, C1, and C2 are the*

using barycentric coordinates.   ones assigned to the vertices in the application program. Assume that we are using
RGB color and that the interpolation is applied individually to each primary color.
We ﬁrst use linear interpolation to interpolate colors, along the edges between vertices
0 and 1, creating RGB colors along the edges through the parametric equations as
follows:
C01(α) = (1 − α)C0 + αC1.
As α goes from 0 to 1, we generate colors, C01(α) along this edge. For a given value
of α, we obtain the color C3. We can now interpolate colors along the line connecting
C3 with the color C2 at the third vertex as follows:
C32(β) = (1 − β)C3 + βC2 ,
which for a given value of β gives the color C4 at an interior point. As the barycentric
coordinates α and β range from 0 to 1, we get interpolated colors for all the interior
                                                                                   3.6 Modeling a Colored Cube   151
points and thus a color for each fragment generated by the rasterizer. The same

```cpp
interpolation method can be used on any vertex attribute.5
```

We now have an object that we can display much as we did with the three-
dimensional Sierpinski gasket in Section 2.9, using a basic orthographic projection.
In Section 3.7, we introduce transformations, enabling us to animate the cube and
also to construct more complex objects. First, however, we introduce an OpenGL
feature that not only reduces the overhead of generating our cube but also gives us
a higher-level method of working with the cube and with other polyhedral objects.

#### 3.6.6 Displaying the Cube

The complete program is given in Appendix A. The parts of the application program
to display the cube and the shaders are almost identical to the the code we used to
display the three-dimensional gasket in Chapter 2. The differences are entirely in how
we place data in the arrays for the vertex positions and vertex colors. The OpenGL
parts, including the shaders, are the same.
However, the display of the cube is not very informative. Because the sides of the
cube are aligned with the clipping volume, we see only the front face. The display also
occupies the entire window. We could get a more interesting display by changing the
data so that it corresponds to a rotated cube. We could scale the data to get a smaller
cube. For example, we could scale the cube by half by changing the vertex data to
point4 vertices[8] = {point4(-0.5,-0.5,0.5,1.0),
point4(-0.5,0.5,0.5,1.0),
point4(0.5,0.5,0.5,1.0),
point4(0.5,-0.5,0.5,1.0),
point4(-0.5,-0.5,-0.5,1.0),
point4(-0.5,0.5,-0.5,1.0),
point4(0.5,0.5,-0.5,1.0),
point4(0.5,-0.5,-0.5,1.0)};
but that would not be a very ﬂexible solution. We could put the scale factor in the
quad function. A better solution might be to change the vertex shader to
in vec4 vPosition;
in vec4 vColor;
out vec4 color;

```cpp
void main()
{
```

gl_Position = 0.5*vPosition;
color = vColor;

```cpp
}
```

5. Modern graphics cards support interpolation methods that are correct under perspective viewing.
152           Chapter 3   Geometric Objects and Transformations
Note that we also changed the vertex shader to use input data in four-dimensional
homogeneous coordinates. We also can simplify the fragment shader to
in vec4 color;
out vec4 fragColor;

```cpp
void main()
{
```

fragColor = color;

```cpp
}
```

Rather than looking at these ad hoc approaches, we will develop a transformation ca-
u      v     pability that will enable us to rotate, scale, and translate data either in the application
or in the shaders. We will also examine in greater detail how we convey data among
the application and shaders that enable us to carry out transformations in the GPU
P                               and alter transformations dynamically.

### 3.7    AFFINE TRANSFORMATIONS

A transformation is a function that takes a point (or vector) and maps it into another

*FIGURE 3.32 Transformation.     point (or vector). We can picture such a function by looking at Figure 3.32 or by*

Q = T(P)
for points, or
v = R(u)
for vectors. If we use homogeneous coordinate representations, then we can represent
both vectors and points as four-dimensional column matrices and we can deﬁne the
transformation with a single function,
q = f (p),
v = f (u),
that transforms the representations of both points and vectors in a given frame.
This formulation is too general to be useful, as it encompasses all single-valued
mappings of points and vectors. In practice, even if we were to have a convenient de-
scription of the function f , we would have to carry out the transformation on every
point on a curve. For example, if we transform a line segment, a general transforma-
tion might require us to carry out the transformation for every point between the two
endpoints.
Consider instead a restricted class of transformations. Let’s assume that we are
working in four-dimensional, homogeneous coordinates. In this space, both points
                                                                                         3.7 Affine Transformations   153
and vectors are represented as 4-tuples.6 We can obtain a useful class of transfor-
mations if we place restrictions on f . The most important restriction is linearity. A
function f is a linear function if and only if, for any scalars α and β and any two
vertices (or vectors) p and q,
f (αp + βq) = αf (p) + βf (q).
The importance of such functions is that if we know the transformations of p and
q, we can obtain the transformations of linear combinations of p and q by taking
linear combinations of their transformations. Hence, we avoid having to calculate
transformations for every linear combination.
Using homogeneous coordinates, we work with the representations of points and
vectors. A linear transformation then transforms the representation of a given point
(or vector) into another representation of that point (or vector) and can always be
written in terms of the two representations, u and v, as a matrix multiplication:
v = Cu,
where C is a square matrix. Comparing this expression with the expression we ob-
tained in Section 3.3 for changes in frames, we can observe that as long as C is
nonsingular, each linear transformation corresponds to a change in frame. Hence,
we can view a linear transformation in two equivalent ways: (1) as a change in the
underlying representation, or frame, that yields a new representation of our vertices,
or (2) as a transformation of the vertices within the same frame.
When we work with homogeneous coordinates, C is a 4 × 4 matrix that leaves
unchanged the fourth (w) component of a representation. The matrix C is of the
⎡                       ⎤
α11 α12 α13 α14
⎢α       α22 α23 α24 ⎥
⎢                       ⎥
C = ⎢ 21                     ⎥
⎣ α31 α32 α33 α34 ⎦
0     0    0     1
and is the transpose of the matrix M that we derived in Section 3.3.4. The 12 values
can be set arbitrarily, and we say that this transformation has 12 degrees of free-
dom. However, points and vectors have slightly different representations in our afﬁne
space. Any vector is represented as
⎡ ⎤
α1
⎢α ⎥
⎢ ⎥
u=⎢ 2⎥.
⎣ α3 ⎦
6. We consider only those functions that map vertices to other vertices and that obey the rules for
manipulating points and vectors that we have developed in this chapter and in Appendix B.
154   Chapter 3   Geometric Objects and Transformations
⎡ ⎤
β1
⎢β ⎥
⎢ ⎥
p=⎢ 2⎥.
⎣ β3 ⎦
If we apply an arbitrary C to a vector,
v = Cu,
we see that only nine of the elements of C affect u, and thus there are only nine
degrees of freedom in the transformation of vectors. Afﬁne transformations of points
have the full 12 degrees of freedom.
We can also show that afﬁne transformations preserve lines. Suppose that we
P(α) = P0 + αd,
where P0 is a point and d is a vector. In any frame, the line can be expressed as
p(α) = p0 + αd,
where p0 and d are the representations of P0 and d in that frame. For any afﬁne
transformation matrix A,
Cp(α) = Cp0 + αCd.
Thus, we can construct the transformed line by ﬁrst transforming p0 and d and
using whatever line-generation algorithm we choose when the line segment must be
displayed. If we use the two-point form of the line,
p(α) = αp0 + (1 − α)p1,
a similar result holds. We transform the representations of p0 and p1 and then con-

```cpp
struct the transformed line. Because there are only 12 elements in C that we can select
```

arbitrarily, there are 12 degrees of freedom in the afﬁne transformation of a line or
line segment.
We have expressed these results in terms of abstract mathematical spaces. How-
ever, their importance in computer graphics is practical. We need only to transform
the homogeneous-coordinate representation of the endpoints of a line segment to de-
termine completely a transformed line. Thus, we can implement our graphics systems
as a pipeline that passes endpoints through afﬁne-transformation units and generates
the interior points at the rasterization stage.
Fortunately, most of the transformations that we need in computer graphics are
afﬁne. These transformations include rotation, translation, and scaling. With slight
                                                                   3.8 Translation, Rotation, and Scaling   155
modiﬁcations, we can also use these results to describe the standard parallel and
perspective projections discussed in Chapter 4.

### 3.8    TRANSLATION, ROTATION, AND SCALING

We have been going back and forth between looking at geometric objects as abstract
entities and working with their representation in a given frame. When we work with
application programs, we have to work with representations. In this section, ﬁrst we
show how we can describe the most important afﬁne transformations independently
of any representation. Then, we ﬁnd matrices that describe these transformations by
acting on the representations of our points and vectors. In Section 3.8, we will see
how these transformations can be implemented in OpenGL.
We look at transformations as ways of moving the points that describe one or
more geometric objects to new locations. Although there are many transformations
that will move a particular point to a new location, there will almost always be only
a single way to transform a collection of points to new locations while preserving
the spatial relationships among them. Hence, although we can ﬁnd many matrices
that will move one corner of our color cube from P0 to Q0, only one of them, when
applied to all the vertices of the cube, will result in a displaced cube of the same size
and orientation.

#### 3.8.1 Translation

Translation is an operation that displaces points by a ﬁxed distance in a given di-
rection, as shown in Figure 3.33. To specify a translation, we need only to specify a
displacement vector d, because the transformed points are given by
P = P + d
for all points P on the object. Note that this deﬁnition of translation makes no
reference to a frame or representation. Translation has three degrees of freedom
because we can specify the three components of the displacement vector arbitrarily.
(a)                         (b)

*FIGURE 3.33 Translation. (a) Object in original position. (b) Object*

translated.
156           Chapter 3              Geometric Objects and Transformations
y                                    3.8.2 Rotation
(x ′, y ′)
Rotation is more difﬁcult to specify than translation because we must specify more
parameters. We start with the simple example of rotating a point about the origin
in a two-dimensional plane, as shown in Figure 3.34. Having speciﬁed a particular
(x, y )         point—the origin—we are in a particular frame. A two-dimensional point at (x, y)
θ                                in this frame is rotated about the origin by an angle θ to the position (x  , y ). We
ϕ                            can obtain the standard equations describing this rotation by representing (x, y) and
(x  , y ) in polar form:

*FIGURE 3.34 Two-dimensional*

rotation.                                  x = ρ cos φ,
y = ρ sin φ,
x  = ρ cos(θ + φ),
y  = ρ sin(θ + φ).
Expanding these terms using the trigonometric identities for the sine and cosine of
the sum of two angles, we ﬁnd
x  = ρ cos φ cos θ − ρ sin φ sin θ = x cos θ − y sin θ ,
y  = ρ cos φ sin θ + ρ sin φ cos θ = x sin θ + y cos θ .
These equations can be written in matrix form as
                        
x        cos θ − sin θ      x
  =                          .
y        sin θ   cos θ      y
We expand this form to three dimensions in Section 3.9.
Note three features of this transformation that extend to other rotations:
1. There is one point—the origin, in this case—that is unchanged by the ro-
tation. We call this point the ﬁxed point of the transformation. Figure 3.35
shows a two-dimensional rotation about a ﬁxed point in the center of the ob-
ject rather than about the origin of the frame.
2. Knowing that the two-dimensional plane is part of three-dimensional space,
we can reinterpret this rotation in three dimensions. In a right-handed sys-
tem, when we draw the x- and y-axes in the standard way, the positive z-axis
comes out of the page. Our deﬁnition of a positive direction of rotation is
counterclockwise when we look down the positive z-axis toward the origin.
We use this deﬁnition to deﬁne positive rotations about other axes.
3. Rotation in the two-dimensional plane z = 0 is equivalent to a three-
dimensional rotation about the z-axis. Points in planes of constant z all rotate
in a similar manner, leaving their z values unchanged.
                                                                  3.8 Translation, Rotation, and Scaling   157
y                                       y

*FIGURE 3.35 Rotation about a fixed point.*



*FIGURE 3.36 Three-dimensional rotation.*

We can use these observations to deﬁne a general three-dimensional rotation that
is independent of the frame. We must specify the three entities shown in Figure 3.36:
a ﬁxed point (Pf ), a rotation angle (θ), and a line or vector about which to rotate.
For a given ﬁxed point, there are three degrees of freedom: the two angles necessary
to specify the orientation of the vector and the angle that speciﬁes the amount of
rotation about the vector.
Rotation and translation are known as rigid-body transformations. No combi-
nation of rotations and translations can alter the shape or volume of an object; they
can alter only the object’s location and orientation. Consequently, rotation and trans-
lation alone cannot give us all possible afﬁne transformations. The transformations
shown in Figure 3.37 are afﬁne, but they are not rigid-body transformations.
158   Chapter 3   Geometric Objects and Transformations

*FIGURE 3.37 Non–rigid-body transformations.*


*FIGURE 3.38 Uniform and nonuniform scaling.*


#### 3.8.3 Scaling

Scaling is an afﬁne non–rigid-body transformation by which we can make an object
bigger or smaller. Figure 3.38 illustrates both uniform scaling in all directions and
scaling in a single direction. We need nonuniform scaling to build up the full set of
                                                      3.9 Transformations in Homogeneous Coordinates                159

*FIGURE 3.40 Reflection.*

afﬁne transformations that we use in modeling and viewing by combining a properly
chosen sequence of scalings, translations, and rotations.                                   z
Scaling transformations have a ﬁxed point, as we can see from Figure 3.39.             FIGURE 3.39 Effect of scale
Hence, to specify a scaling, we can specify the ﬁxed point, a direction in which we         factor.
wish to scale, and a scale factor (α). For α > 1, the object gets longer in the speciﬁed
direction; for 0 ≤ α < 1, the object gets smaller in that direction. Negative values of α
give us reﬂection (Figure 3.40) about the ﬁxed point, in the scaling direction. Scaling
has six degrees of freedom because we can specify an arbitrary ﬁxed point and three
independent scaling factors.

### 3.9    TRANSFORMATIONS IN HOMOGENEOUS

All graphics APIs force us to work within some reference system. Hence, we cannot
work with high-level expressions such as
Q = P + αv.
Instead, we work with representations in homogeneous coordinates and with expres-
q = p + αv.
Within a frame, each afﬁne transformation is represented by a 4 × 4 matrix of the
160   Chapter 3   Geometric Objects and Transformations
⎡                         ⎤
α11 α12       α13    α14
⎢α    α22       α23    α24 ⎥
⎢                          ⎥
A = ⎢ 21                       ⎥.
⎣ α31 α32       α33    α34 ⎦
0   0         0      1

#### 3.9.1 Translation

Translation displaces points to new positions deﬁned by a displacement vector. If we
move the point p to p by displacing by a distance d, then
p = p + d.
Looking at their homogeneous-coordinate forms
⎡ ⎤                      ⎡   ⎤              ⎡  ⎤
x                        x                αx
⎢y⎥                     ⎢ y ⎥            ⎢α ⎥
⎢ ⎥                     ⎢ ⎥               ⎢ ⎥
p=⎢ ⎥,               p = ⎢  ⎥ ,         d=⎢ y ⎥,
⎣z⎦                     ⎣z ⎦              ⎣ αz ⎦
1                        1                 0
we see that these equations can be written component by component as
x  = x + αx ,
y  = y + αy ,
z  = z + αz .
This method of representing translation using the addition of column matrices does
not combine well with our representations of other afﬁne transformations. However,
we can also get this result using the matrix multiplication:
p = Tp,
⎡                 ⎤
1       0    0 αx
⎢0             0 αy ⎥
⎢         1         ⎥
T=⎢                   ⎥.
⎣0        0    1 αz ⎦
0       0    0 1
T is called the translation matrix. We sometimes write it as T(αx , αy , αz ) to empha-
size the three independent parameters.
It might appear that using a fourth ﬁxed element in the homogeneous represen-
tation of a point is not necessary. However, if we use the three-dimensional forms
                                                   3.9 Transformations in Homogeneous Coordinates   161
⎡ ⎤                         ⎡ ⎤
x                        x
q=⎣y⎦,               q = ⎣ y  ⎦ ,
z                        z
it is not possible to ﬁnd a 3 × 3 matrix D such that q  = Dq for the given displacement
vector d. For this reason, the use of homogeneous coordinates is often seen as a clever
trick that allows us to convert the addition of column matrices in three dimensions
to matrix–matrix multiplication in four dimensions.
We can obtain the inverse of a translation matrix either by applying an inversion
algorithm or by noting that if we displace a point by the vector d, we can return to
the original position by a displacement of −d. By either method, we ﬁnd that
⎡                  ⎤
1 0 0 −αx
⎢ 0 1 0 −α ⎥
⎢                 y⎥
T−1(αx , αy , αz ) = T(−αx , −αy , −αz ) = ⎢                   ⎥.
⎣ 0 0 1 −αz ⎦
0 0 0         1

#### 3.9.2 Scaling

For both scaling and rotation, there is a ﬁxed point that is unchanged by the trans-
formation. We let the ﬁxed point be the origin, and we show how we can concatenate
transformations to obtain the transformation for an arbitrary ﬁxed point.
A scaling matrix with a ﬁxed point of the origin allows for independent scaling
along the coordinate axes. The three equations are
x  = βx x,
y  = βy y,
z  = βz z.
These three equations can be combined in homogeneous form as
p = Sp,
⎡                        ⎤
βx       0    0    0
⎢ 0        βy        0⎥
⎢               0      ⎥
S = S(βx , βy , βz ) = ⎢                      ⎥.
⎣ 0        0    βz   0⎦
0        0    0    1
As is true of the translation matrix and, indeed, of all homogeneous coordinate trans-
formations, the ﬁnal row of the matrix does not depend on the particular transfor-
mation, but rather forces the fourth component of the transformed point to retain
the value 1.
162   Chapter 3   Geometric Objects and Transformations
We obtain the inverse of a scaling matrix by applying the reciprocals of the scale
factors:
1 1 1
S−1(βx , βy , βz ) = S     ,  ,           .
βx βy βz

#### 3.9.3 Rotation

We ﬁrst look at rotation with a ﬁxed point at the origin. There are three degrees
of freedom corresponding to our ability to rotate independently about the three
coordinate axes. We have to be careful, however, because matrix multiplication is
not a commutative operation (Appendix C). Rotation about the x-axis by an angle
θ followed by rotation about the y-axis by an angle φ does not give us the same result
as the one that we obtain if we reverse the order of the rotations.
We can ﬁnd the matrices for rotation about the individual axes directly from
the results of the two-dimensional rotation that we developed in Section 3.7.2. We
saw that the two-dimensional rotation was actually a rotation in three dimensions
about the z-axis and that the points remained in planes of constant z. Thus, in three
dimensions, the equations for rotation about the z-axis by an angle θ are
x  = x cos θ − y sin θ ,
y  = x sin θ + y cos θ ,
z = z ,
or, in matrix form,
p = Rz p,
⎡                               ⎤
cos θ      − sin θ   0       0
⎢ sin θ       cos θ            0⎥
⎢                      0         ⎥
Rz = Rz (θ ) = ⎢                                ⎥.
⎣ 0             0      1       0⎦
0           0      0       1
We can derive the matrices for rotation about the x- and y-axes through an identical
argument. If we rotate about the x-axis, then the x values are unchanged, and we
have a two-dimensional rotation in which points rotate in planes of constant x; for
rotation about the y-axis, the y values are unchanged. The matrices are
⎡                          ⎤
1    0          0      0
⎢ 0 cos θ − sin θ 0 ⎥
⎢                          ⎥
Rx = Rx (θ ) = ⎢                          ⎥,
⎣ 0 sin θ      cos θ     0⎦
0    0          0      1
                                                    3.9 Transformations in Homogeneous Coordinates   163
⎡                        ⎤
cos θ     0 sin θ     0
⎢ 0                      0⎥
⎢            1   0         ⎥
Ry = Ry (θ ) = ⎢                          ⎥.
⎣ − sin θ    0 cos θ     0⎦
0       0   0       1
The signs of the sine terms are consistent with our deﬁnition of a positive rotation in
a right-handed system.
Suppose that we let R denote any of our three rotation matrices. A rotation by θ
can always be undone by a subsequent rotation by −θ; hence,
R −1(θ ) = R(−θ ).
In addition, noting that all the cosine terms are on the diagonal and the sine terms
are off-diagonal, we can use the trigonometric identities
cos(−θ ) = cos θ
sin(−θ ) = − sin θ
to ﬁnd
R −1(θ ) = R T (θ ).
In Section 3.10.1, we show how to construct any desired rotation matrix, with a
ﬁxed point at the origin, as a product of individual rotations about the three axes
R = Rz Ry Rx .
Using the fact that the transpose of a product is the product of the transposes in the
reverse order, we see that for any rotation matrix,
R −1 = R T .
A matrix whose inverse is equal to its transpose is called an orthogonal matrix.
Normalized orthogonal matrices correspond to rotations about the origin.

#### 3.9.4 Shear

Although we can construct any afﬁne transformation from a sequence of rota-
tions, translations, and scalings, there is one more afﬁne transformation—the shear
transformation—that is of such importance that we regard it as a basic type rather
than deriving it from the others. Consider a cube centered at the origin, aligned with
the axes, and viewed from the positive z-axis, as shown in Figure 3.41. If we pull the
top to the right and the bottom to the left, we shear the object in the x-direction. Note
that neither the y nor the z values are changed by the shear, so we can call this op-
eration x shear to distinguish it from shears of the cube in other possible directions.
164                 Chapter 3        Geometric Objects and Transformations

*FIGURE 3.41 Shear.*

y                                    Using simple trigonometry on Figure 3.42, we see that each shear is characterized by
a single angle θ ; the equations for this shear are
(x, y )         (x , y )
x  = x + y cot θ ,
y  = y,

z = z ,

*FIGURE 3.42 Computation of                 leading to the shearing matrix*

the shear matrix.                                    ⎡                 ⎤
1 cot θ 0 0
⎢0           0 0⎥
⎢       1         ⎥
Hx (θ ) = ⎢                 ⎥.
⎣0      0    1 0⎦
0     0    0 1
We can obtain the inverse by noting that we need to shear in only the opposite
direction; hence,
Hx−1(θ ) = Hx (−θ).
Shearing in the x-direction followed by a shear in z-direction, leaves the y values
unchanged and can be regarded as a shear in the x − z plane.

### 3.10       CONCATENATION OF TRANSFORMATIONS

In this section, we create examples of afﬁne transformations by multiplying together,
or concatenating, sequences of the basic transformations that we just introduced.
Using this strategy is preferable to attempting to deﬁne an arbitrary transformation
directly. The approach ﬁts well with our pipeline architectures for implementing
graphics systems.
Suppose that we carry out three successive transformations on a point p, cre-
ating a new point q. Because the matrix product is associative, we can write the
                                                                3.10 Concatenation of Transformations             165
p              A                   B                    C              q

*FIGURE 3.43 Application of transformations one at a time.*

q = CBAp,
without parentheses. Note that here the matrices A, B, and C (and thus M) can be
arbitrary 4 × 4 matrices, although in practice they will most likely be afﬁne. The order
in which we carry out the transformations affects the efﬁciency of the calculation. In
one view, shown in Figure 3.43, we can carry out A, followed by B, followed by C—an
q = (C(B(Ap))).
If we are to transform a single point, this order is the most efﬁcient because each
matrix multiplication involves multiplying a column matrix by a square matrix. If we
have many points to transform, then we can proceed in two steps. First, we calculate
M = CBA.
Then, we use this matrix on each point
q = Mp.                                                                                    p            M            q
This order corresponds to the pipeline shown in Figure 3.44, where we compute M

*FIGURE 3.44 Pipeline trans-*

ﬁrst, then load it into a pipeline transformation unit. If we simply count operations,     formation.
we see that although we do a little more work in computing M initially, because M
may be applied to tens of thousands of points, this extra work is insigniﬁcant com-
pared with the savings we obtain by using a single matrix multiplication for each
point. We now derive examples of computing M.

#### 3.10.1 Rotation About a Fixed Point

Our ﬁrst example shows how we can alter the transformations that we deﬁned with
a ﬁxed point at the origin (rotation, scaling, shear) to have an arbitrary ﬁxed point.
We demonstrate for rotation about the z-axis; the technique is the same for the other
cases.
Consider a cube with its center at pf and its sides aligned with the axes. We
want to rotate the cube about the z-axis, but this time about its center pf , which
becomes the ﬁxed point of the transformation, as shown in Figure 3.45. If pf were
the origin, we would know how to solve the problem: We would simply use Rz (θ ).
This observation suggests the strategy of ﬁrst moving the cube to the origin. We can
then apply Rz (θ ) and ﬁnally move the object back such that its center is again at pf .
This sequence is shown in Figure 3.46. In terms of our basic afﬁne transformations,
166          Chapter 3   Geometric Objects and Transformations

z                                      z
(a)                                    (b)

*FIGURE 3.45 Rotation of a cube about its center.*

y                            y                             y                          y
x                              x                            x                      x
z                        z                              z                            z

*FIGURE 3.46 Sequence of transformations.*

the ﬁrst is T(−pf ), the second is Rz (θ ), and the ﬁnal is T(pf ). Concatenating them
together, we obtain the single matrix
M = T(pf )Rz (θ )T(−pf ).
If we multiply out the matrices, we ﬁnd that
⎡                                              ⎤
cos θ       − sin θ     0 xf − xf cos θ + yf sin θ
⎢ sin θ        cos θ      0 yf − xf sin θ − yf cos θ ⎥
⎢                                                    ⎥
M=⎢                                                    ⎥.
⎣ 0              0        1             0            ⎦
0            0        0             1
                                                                3.10 Concatenation of Transformations   167

(a)                         (b)

*FIGURE 3.47 Rotation of a cube about the z-axis. (a) Cube before*

rotation. (b) Cube after rotation.

(a)                         (b)

*FIGURE 3.48 Rotation of a cube about the y-axis.*


#### 3.10.2 General Rotation

We now show that an arbitrary rotation about the origin can be composed of three
successive rotations about the three axes. The order is not unique (see Exercise 3.10),
although the resulting rotation matrix is. We form the desired matrix by ﬁrst doing
a rotation about the z-axis, then doing a rotation about the y-axis, and concluding
with a rotation about the x-axis.
Consider the cube, again centered at the origin with its sides aligned with the
axes, as shown in Figure 3.47(a). We can rotate it about the z-axis by an angle α to
orient it, as shown in Figure 3.47(b). We then rotate the cube by an angle β about the
y-axis, as shown in a top view in Figure 3.48. Finally, we rotate the cube by an angle γ
about the x-axis, as shown in a side view in Figure 3.49. Our ﬁnal rotation matrix is
R = Rx Ry Rz .
168        Chapter 3   Geometric Objects and Transformations
(a)                          (b)

*FIGURE 3.49 Rotation of a cube about the x-axis.*

A little experimentation should convince you that we can achieve any desired orien-
tation by proper choice of α, β, and γ , although, as we will see in the example of
Section 3.10.4, ﬁnding these angles can be tricky.

#### 3.10.3 The Instance Transformation

Our example of a cube that can be rotated to any desired orientation suggests a
generalization appropriate for modeling. Consider a scene composed of many simple
objects, such as those shown in Figure 3.50. One option is to specify each of these

*FIGURE 3.50 Scene of simple*

objects, through its vertices, in the desired location with the desired orientation and
objects.                      size. An alternative is to specify each of the object types once at a convenient size, in
a convenient place, and with a convenient orientation. Each occurrence of an object
in the scene is an instance of that object’s prototype, and we can obtain the desired
size, orientation, and location by applying an afﬁne transformation—the instance
transformation—to the prototype. We can build a simple database to describe a
scene from a list of object identiﬁers (such as 1 for a cube and 2 for a sphere) and
of the instance transformation to be applied to each object.
The instance transformation is applied in the order shown in Figure 3.51. Objects
are usually deﬁned in their own frames, with the origin at the center of mass and
the sides aligned with the model frame axes. First, we scale the object to the desired
size. Then we orient it with a rotation matrix. Finally, we translate it to the desired
orientation. Hence, the instance transformation is of the form
M = TRS.
Modeling with the instance transformation works well not only with our pipeline
architectures but also with other methods for retaining objects such as scene graphs
that we will introduce in Chapter 8. A complex object that is used many times can
be loaded into the server once as a display list. Displaying each instance of it requires
only sending the appropriate instance transformation to the server before executing
the display list.
                                                                3.10 Concatenation of Transformations                 169
M = TRS                            R

*FIGURE 3.51 Instance transformation.*


#### 3.10.4 Rotation About an Arbitrary Axis                                                                y

p2
Our ﬁnal rotation example illustrates not only how we can achieve a rotation about
an arbitrary point and line in space but also how we can use direction angles to specify                   u
orientations. Consider rotating a cube, as shown in Figure 3.52. We need three entities                          
to specify this rotation. There is a ﬁxed point p0 that we assume is the center of the                     p1
cube, a vector about which we rotate, and an angle of rotation. Note that none of these
entities relies on a frame and that we have just speciﬁed a rotation in a coordinate-                                     x
free manner. Nonetheless, to ﬁnd an afﬁne matrix to represent this transformation,                                   p0
we have to assume that we are in some frame.
The vector about which we wish to rotate the cube can be speciﬁed in various
ways. One way is to use two points, p1 and p2, deﬁning the vector                           z

*FIGURE 3.52 Rotation of a*

u = p2 − p1.                                                                               cube about an arbitrary axis.
Note that the order of the points determines the positive direction of rotation for θ
and that even though we draw u as passing through p0, only the orientation of u
matters. Replacing u with a unit-length vector
⎡ ⎤
αx
v=         ⎣
= αy ⎦
|u|
αz
in the same direction simpliﬁes the subsequent steps. We say that v is the result of
normalizing u. We have already seen that moving the ﬁxed point to the origin is
a helpful technique. Thus, our ﬁrst transformation is the translation T(−p0), and
the ﬁnal one is T(p0). After the initial translation, the required rotation problem
is as shown in Figure 3.53. Our previous example (see Section 3.10.2) showed that
we could get an arbitrary rotation from three rotations about the individual axes.
This problem is more difﬁcult because we do not know what angles to use for the
170              Chapter 3                Geometric Objects and Transformations
p2  p1

*FIGURE 3.53 Movement of the fixed point to the origin.*

y                                            y                                y                        y
x                              x                          x                             x
x
z
y
z                                         z                              z                           z

*FIGURE 3.54 Sequence of rotations.*

individual rotations. Our strategy is to carry out two rotations to align the axis of
rotation, v, with the z-axis. Then we can rotate by θ about the z-axis, after which we
can undo the two rotations that did the aligning. Our ﬁnal rotation matrix will be of
R = Rx (−θx )Ry (−θy )Rz (θ )Ry (θy )Rx (θx ).
( x,  y ,  z )
This sequence of rotations is shown in Figure 3.54. The difﬁcult part of the process is
determining θx and θy .
We proceed by looking at the components of v. Because v is a unit-length vector,
z                        αx2 + αy2 + αz2 = 1.
We draw a line segment from the origin to the point (αx , αy , αz ). This line segment
z                                             has unit length and the orientation of v. Next, we draw the perpendiculars from the

*FIGURE 3.55 Direction                             point (αx , αy , αz ) to the coordinate axes, as shown in Figure 3.55. The three direction*

angles.                                           angles—φx , φy , φz —are the angles between the line segment (or v) and the axes. The
                                                                    3.10 Concatenation of Transformations                            171
cos φx = αx ,                                                                                                  y
cos φy = αy ,
z
cos φz = αz .
( x,  y,  z )
Only two of the direction angles are independent, because                                                  d       1
x                                  x
cos2 φx + cos2 φy + cos2 φz = 1.                                                                 y                    x
We can now compute θx and θy using these angles. Consider Figure 3.56. It shows that
the effect of the desired rotation on the point (αx , αy , αz ) is to rotate the line segment

```cpp
into the plane y = 0. If we look at the projection of the line segment (before the
```

rotation) on the plane x = 0, we see a line segment of length d on this plane. Another          FIGURE 3.56 Computation of
the x rotation.
way to envision this ﬁgure is to think of the plane x = 0 as a wall and consider a distant
light source located far down the positive x-axis. The line that we see on the wall is the
shadow of the line segment from the origin to (αx , αy , αz ). Note that the length of the
shadow is less than the length of the line segment. We can say the line segment has
been foreshortened to d = αy2 + αz2. The desired angle of rotation is determined
by the angle that this shadow makes with the z-axis. However, the rotation matrix is
determined by the sine and cosine of θx . Thus, we never need to compute θx ; rather,
⎡                         ⎤
1     0       0      0                                                                                    x
⎢ 0 α /d −α /d 0 ⎥                                                                              d 
⎢       z         y       ⎥                                                                         y            1
Rx (θx ) = ⎢                         ⎥.
⎣ 0 αy /d αz /d 0 ⎦
0     0       0      1
We compute Ry in a similar manner. Figure 3.57 shows the rotation. This angle is                FIGURE 3.57 Computation of
clockwise about the y-axis; therefore, we have to be careful of the sign of the sine            the y rotation.
terms in the matrix, which is
⎡                  ⎤
d 0 −αx 0
⎢ 0 1           0⎥
⎢          0       ⎥
Ry (θy ) = ⎢                  ⎥.
⎣ αx 0     d    0⎦
0 0      0     1
Finally, we concatenate all the matrices to ﬁnd
M = T(p0)Rx (−θx )Ry (−θy )Rz (θ )Ry (θy )Rx (θx )T(−p0).
Let’s look at a speciﬁc example. Suppose that we wish to rotate an object by
45 degrees about the line passing through the origin and the point (1, 2, 3). We
leave the ﬁxed point at the origin. The ﬁrst step is to ﬁnd the point along the line
that√is a unit
√ distance
√ from the√origin. √    We obtain√ it by normalizing (1, 2, 3) to
(1/ 14, 2/ 14, 3/ 14), or (1/ 14, 2/ 14, 3/ 14, 1) in homogeneous coordi-
nates. The ﬁrst part of the rotation takes this point to (0, 0, 1, 1). We ﬁrst rotate about
172   Chapter 3   Geometric Objects and Transformations
√        √      √
the x-axis by the angle cos−1 √3 . This matrix carries (1/ 14, 2/ 14, 3/ 14, 1) to
√         √                   14
(1/ 14, 0, 13/14,
√     1), which  is in the plane y = 0. The y rotation must be by the
angle − cos−1( 13/14). This rotation aligns the object with the z-axis, and now we
can rotate about the z-axis by the desired 45 degrees. Finally, we undo the ﬁrst two
rotations. If we concatenate these ﬁve transformations into a single rotation matrix
R, we ﬁnd that
                                       
−1 3                −1    13                   −1    13
R = Rx − cos √              Ry cos             Rz (45)Ry − cos
13                  14                         14

Rx cos−1 √
⎡ 2+13√2            √     √        √    √       ⎤
2− 2−3 7      6−3 2+4 7
28            14            28        0
⎢ 2−√2+3√7              √          √ √          ⎥
⎢                   4+5 2       6−3 2− 7
0⎥
⎢       14            14            14          ⎥
=⎢       √    √         √ √              √         ⎥.
⎢ 6−3 2−4 7 6−3 2+ 7             18+5 2
0 ⎥
⎣       28            14            28          ⎦
0             0             0         1
This matrix does not change any point on the line passing through the origin and the
point (1, 2, 3). If we want a ﬁxed point other than the origin, we form the matrix
M = T(pf )RT(−pf ).
This example is not simple. It illustrates the powerful technique of applying
many simple transformations to get a complex one. The problem of rotation about
an arbitrary point or axis arises in many applications. The major variants lie in the
manner in which the axis of rotation is speciﬁed. However, we can usually employ
techniques similar to the ones that we have used here to determine direction angles
or direction cosines.

### 3.11     TRANSFORMATION MATRICES IN OPENGL

We can now focus on the implementation of a homogeneous-coordinate transforma-
tion package and of that package’s interface to the user. We have introduced a set of
frames, including the world frame and the camera frame, that should be important
for developing applications. In a shader-based implementation of OpenGL, the exis-
tence or nonexistence of these frames is entirely dependent on what the application
programmer decides to do.7 In a modern implementation of OpenGL, the applica-
tion programmer not only can choose which frames to use but also where to carry out
7. In earlier versions of OpenGL that relied on the ﬁxed-function pipeline, these frames were part of
the speciﬁcation and their state was part of the environment.
                                                              3.11 Transformation Matrices in OpenGL   173
the transformations between frames. Some will best be carried out in the application,
others in a shader.
As we develop a method for specifying and carrying out transformations, we
should emphasize the importance of state. Although very few state variables are pre-
deﬁned in OpenGL, once we specify various attributes and matrices, they effectively
deﬁne the state of the system. Thus, when a vertex is processed, how it is processed is
determined by the values of these state variables.
The two transformations we will use most often are the model-view transfor-
mation and the projection transformation. The model-view transformation brings
representations of geometric objects from application or model frame to the cam-
era frame. The projection matrix will both carry out the desired projection and also
change the representation to clip coordinates. We will use only the model-view matrix
in this chapter. The model-view matrix normally is an afﬁne-transformation matrix
and has only 12 degrees of freedom, as discussed in Section 3.7. The projection ma-
trix, as we will see in Chapter 4, is also a 4 × 4 matrix, but it is not afﬁne.

#### 3.11.1 Current Transformation Matrices

The generalization common to most graphics systems is of a current transformation
matrix (CTM). The CTM is part of the pipeline (Figure 3.58); thus, if p is a vertex
speciﬁed in the application, then the pipeline produces Cp. Note that Figure 3.58 does
not indicate where in the pipeline the current transformation matrix is applied. If we
use a CTM, we can regard it as part of the state of the system.
First we will introduce a simple set of functions that we can use to form and
manipulate 4 × 4 afﬁne transformation matrices. Let C denote the CTM (or any
other 4 × 4 afﬁne matrix). Initially, we will set it to the 4 × 4 identity matrix; it
can be reinitialized as needed. If we use the symbol ← to denote replacement, we can
C ← I.
The functions that alter C are of two forms: those that load it with some matrix and
those that modify it by premultiplication or postmultiplication by a matrix. The three
transformations supported in most systems are translation, scaling with a ﬁxed point
of the origin, and rotation with a ﬁxed point of the origin. Symbolically, we can write
these operations in postmultiplication form as
C ← CT,
C ← CS,
Vertices                                                        Vertices

*FIGURE 3.58 Current transformation matrix (CTM).*

174   Chapter 3   Geometric Objects and Transformations
C ← CR,
C ← T,
C ← S,
C ← R.
Most systems allow us to load the CTM with an arbitrary matrix M,
C ← M,
or to postmultiply by an arbitrary matrix M,
C ← CM.
Although we will occasionally use functions that set a matrix, most of the time
we will alter an existing matrix; that is, the operation
C ← CR,
C ← R.

#### 3.11.2 Rotation, Translation, and Scaling

In our applications and shaders, the matrix that is most often applied to all vertices
is the product of the model-view matrix and the projection matrix. We can think of
the CTM as the product of these matrices (Figure 3.59), and we can manipulate each
individually by working with the desired matrix.
Using the matrix and vector classes, we can form afﬁne matrices for rotation,
translation, and scaling by creating the following ﬁve functions:
RotateX(float xangle);
RotateY(float yangle);
RotateZ(float zangle);
Translate(float dx, float dy, float dz);
Scale(float sx, float sy, float sz);
Vertices                                     Vertices
Model–view           Projection

*FIGURE 3.59 Model-view and projection matrices.*

                                                                        3.11 Transformation Matrices in OpenGL   175
For example, the code for Rotatez is
mat4 Rotatez(const float theta)

```cpp
{
float s = M_PI/180.0*theta; //convert degrees to radians
```

mat4 c;                      // an identity matrix
c[2][2] = c[3][3] = 1.0;
c[0][0] = c[1][1] = cos(s);
c[1][0] = sin(s);
c[0][1] = -c[1][0];
return c;

```cpp
}
```

For rotation, the angles are speciﬁed in degrees and the rotations are for a ﬁxed point
at the origin. In the translation function, the variables are the components of the
displacement vector; for scaling, the variables determine the scale factors along the
coordinate axes and the ﬁxed point is the origin.

#### 3.11.3 Rotation About a Fixed Point

In Section 3.10, we showed that we can perform a rotation about a ﬁxed point, other
than the origin, by ﬁrst moving the ﬁxed point to the origin, then rotating about
the origin, and ﬁnally moving the ﬁxed point back to its original location. Using the
example from Section 3.11, the following sequence sets the matrix mode, then forms
the required matrix for a 45-degree rotation about the line through the origin and
the point (1, 2, 3) with a ﬁxed point of (4, 5, 6):
mat4 R, ctm;

```cpp
float thetax, thetay;
const float Radians_To_Degrees = 180.0/M_PI;
```

thetax = Radians_To_Degrees*acos(3.0/sqrt(14.0));
thetay = Radians_To_Degrees*acos(sqrt(13.0/14.0));
R = RotateX(-thetax)*RotateY(-thetay)*RotateZ(-45.0)
*RotateY(-thetax)*RotateX(thetax);
ctm = Translate(4.0, 5.0, 6.0)*R* Translate(-4.0, -5.0, -6.0);
Because we want to do arbitrary rotations so often, it is a good exercise (Exercise 3.31)
to write functions Rotate(float theta, vec3 d) and Rotate(float theta,

```cpp
float dx, float dy, float dz) that will form an arbitrary rotation matrix for a
```

rotation of theta degrees about a line in the direction of the vector d = (dx, dy, dz).8
8. Although it is a good exercise to write the various matrix functions yourself, we also provide them
in a separate include ﬁle angel.h that can be downloaded from the book’s Web site.
176   Chapter 3   Geometric Objects and Transformations

#### 3.11.4 Order of Transformations

You might be bothered by what appears to be a reversal of the required function
calls. The rule in OpenGL is this: The transformation speciﬁed last is the one applied
ﬁrst. A little examination shows that this order is a consequence of multiplying the
CTM on the right by the speciﬁed afﬁne transformation and thus is both correct and
reasonable. The sequence of operations that we speciﬁed was
C ← I,
C ← CT(4.0, 5.0, 6.0),
C ← CR(45.0, 1.0, 2.0, 3.0),
C ← CT(−4.0, −5.0, −6.0).
In each step, we postmultiply at the end of the existing CTM, forming the matrix
C = T(4.0, 5.0, 6.0)R(45.0, 1.0, 2.0, 3.0)T(−4.0, −5.0, −6.0),
which is the matrix that we expect from Section 3.11. Each vertex p that is speciﬁed
after the model-view matrix has been set will be multiplied by C, thus forming the
q = Cp.
There are other ways to think about the order of operations. One way is in terms
of a stack. Altering the CTM is similar to pushing matrices onto a stack; when we
apply the ﬁnal transformation, the matrices are popped off the stack in the reverse
order in which they were placed there. The analogy is conceptual rather than exact
because when we use a transformation function, the matrix is altered immediately.

### 3.12      SPINNING OF THE CUBE

We will now examine how we can manipulate the color cube interactively. We will
take the cube that we deﬁned in Section 3.6 and we rotate it using the three buttons
of the mouse. Our program will be based on the following three callback functions:
glutDisplayFunc(display);
glutIdleFunc(spincube);
glutMouseFunc(mouse)
We will examine two fundamentally different ways of doing the updates to the display.
In the ﬁrst, we will form a new model-view matrix in the display callback and apply
it to the vertex data to get new vertex positions. We must then send the new data to
the GPU. In the second, we will send the model-view matrix to the vertex shader and
apply it there. The mouse and idle callbacks will be the same in both cases, so let’s
examine them ﬁrst.
                                                                            3.12 Spinning of the Cube   177
The mouse callback selects the axis for rotation, using 0, 1, and 2 to denote
rotation about the x, y, and z axes, respectively:

```cpp
int axis = 0;
void mouse(int button, int state, int x, int y)
{
```

if(button == GLUT_LEFT_BUTTON && state == GLUT_DOWN) axis = 0;
if(button == GLUT_MIDDLE_BUTTON && state == GLUT_DOWN) axis = 1;
if(button == GLUT_RIGHT_BUTTON && state == GLUT_DOWN) axis = 2;

```cpp
}
```

The idle callback increments the angle associated with the chosen axis by 0.1 degrees
each time:

```cpp
void spinCube()
{
```

theta[axis] += 0.1;
if( theta[axis] > 360.0 ) theta[axis] -= 360.0;
glutPostRedisplay();

```cpp
}
```


#### 3.12.1 Updating in the Display Callback

The function display starts by clearing the frame and depth buffers and then forms
a model-view matrix using the values of three angles determined by the mouse call-
glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
mat4 ctm = RotateX(theta[0])*RotateY(theta[1])*RotateZ(theta[2]);
The problem is how to apply this matrix. Suppose that we set up the data arrays
as in our previous examples by executing the colorcube function as part of our
initialization. Thus, we have color and position data in the arrays quad_colors and
points for 36 vertices. We can use a second array
point4 new_points[36];
to hold the transformed points and then apply the rotations in the display callback
for(i=0; i<36; i++)

```cpp
{
```

new_points[i] = ctm*points[i];

```cpp
}
```

However, these transformed positions are on the CPU, not on the GPU. To get them

```cpp
into the pipeline, we can initialize the vertex array to be new_points rather than
```

points initially,
178   Chapter 3   Geometric Objects and Transformations
loc = glGetAttribLocation(program, "vPosition");
glEnableVertexAttribArray(loc);
glVertexAttribPointer(loc, 4, GL_FLOAT, GL_FALSE, 0,
BUFFER_OFFSET(0));
and then in the display callback send these points to the GPU:
glBindVertexArray(abuffer);
glBindBuffer(GL_ARRAY_BUFFER, buffers[0]);
glBufferData(GL_ARRAY_BUFFER, sizeof(new_points), new_points,
GL_STATIC_DRAW);
glDrawArrays(GL_TRIANGLES, 0, N);
glutSwapBuffers();
There is, however, a major weakness in this approach. We are applying the current
transformation in the application program and sending the vertex positions to the
GPU every time we want to update the display. Consequently, we are not using the
power of the GPU and having the performance determined by how fast we can send
data from the CPU to the GPU. In applications, where we have complex geometry,
this approach will lead to poor performance.
Our second approach will be to send the vertex data to the GPU once. Every time
we update the transformation matrix, we will send a new transformation matrix to
the GPU and update the vertex positions on the GPU. First, we must examine how to
get such data from the CPU to the GPU.

#### 3.12.2 Uniform Variables

In a given application, a variable may change in a variety of ways. When we send
vertex attributes to a shader, these attributes can be different for each vertex in a
primitive. We may also want parameters that will remain the same for all vertices
in a primitive or equivalently for all the vertices that are displayed when we execute
a function such as glDrawArrays. Such variables are called uniform qualiﬁed vari-
ables. For example, in our present example, we want the same rotation matrix to
apply to all the vertices in the points array.
We set up uniform variables in much the same way as we did for vertex at-
tributes. Suppose that we want to send the elapsed time from the application to the
vertex shader. In the application, we can use GLUT to get the elapsed time in milli-

```cpp
float etime;
```

etime = 0.001*glutGet(GLUT_ELAPSED_TIME);
In the vertex shader, we might have a corresponding variable time. For example, the
following shader varies the x component of each vertex sinusoidally:
                                                                           3.12 Spinning of the Cube   179
uniform float time;
attribute vec4 vPosition;

```cpp
void main()
{
```

vPosition.x *= (1+sin(time));
gl_Position = vPosition;

```cpp
}
```

We still must establish a connection between the corresponding time variables in the
application and the shader and get the values from the application to the shader.
After the shaders have been compiled and linked, we can get the correspondence
in the application in a manner similar to vertex attributes. We get the location by
GLint timeParam;
timeParam = glGetUniformLocation(program, "time");
Now whenever we want to send the elapsed time to the shader, we execute
glUniform1f(timeParam, etime);
There are forms of glUniform corresponding to all the types supported by GLSL,
including ﬂoats, ints, and two-, three-, and four-dimensional vectors and matrices.
For the 4 × 4 rotation matrix, we use the form
glUniformMatrix4fv(matrix_loc, 1, GL_TRUE, ctm);
Here, matrix_loc is determined by
GLint matrix_loc;
matrix_loc = getUniformLocation(program, "rotation");
in vec4 vPosition;
in vec4 vColor;
out vec4 color;
uniform mat4 rotation;

```cpp
void main()
{
```

gl_Position = rotation*vPosition;
color = vColor;

```cpp
}
```

180   Chapter 3   Geometric Objects and Transformations
The second parameter, glUniformMatrix, is the number of elements of ctm that
are sent. The third parameter declares that the data should be sent in row-major
order. The display callback is now
mat4 ctm;

```cpp
void display()
{
```

glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
ctm = RotateX(theta[0])*RotateY(theta[1])*RotateZ(theta[2]);
glUniformMatrix4fv(matrix_loc, 1, GL_TRUE, ctm);
glDrawArrays(GL_TRIANGLES, 0, N);
glutSwapBuffers();

```cpp
}
```

Alternately, we could have computed the rotation matrix in the vertex shader by
sending only the rotation angles to the vertex shader. A program that takes this
approach is in Appendix A.

### 3.13    INTERFACES TO THREE-DIMENSIONAL APPLICATIONS

In Section 3.12, we used a three-button mouse to control the direction of rotation of
our cube. This interface is limited. Rather than use all three mouse buttons to control
rotation, we might want to use mouse buttons to control functions, such as pulling
down a menu, that we would have had to assign to keys in our previous example.
In Section 3.10, we noted that there were many ways to obtain a given orienta-
tion. Rather than do rotations about the x-, y-, and z-axes in that order, we could do
a rotation about the x-axis, followed by a rotation about the y-axis, and ﬁnish with
another rotation about the x-axis. If we do our orientation this way, we can obtain
our desired orientation using only two mouse buttons. However, there is still a prob-
lem: Our rotations are in a single direction. It would be easier to orient an object if
we could rotate either forward or backward about an axis and could stop the rotation
once we reached a desired orientation.
GLUT allows us to use the keyboard in combination with the mouse. We could,
for example, use the left mouse button for a forward rotation about the x-axis and
the Control key in combination with the left mouse button for a backward rotation
about the x-axis.
However, neither of these options provides a good user interface, which should
be more intuitive and less awkward. Let’s consider a few options that provide a more

```cpp
interesting and smoother interaction.
```


#### 3.13.1 Using Areas of the Screen

Suppose that we want to use one mouse button for orienting an object, one for getting
closer to or farther from the object, and one for translating the object to the left or
right. We can use the motion callback to achieve all these functions. The callback
                                                    3.13 Interfaces to Three-Dimensional Applications   181
returns which button has been activated and where the mouse is located. We can use
the location of the mouse to control how fast and in which direction we rotate or
translate and to move in or out.
As just noted, we need the ability to rotate about only two axes to achieve any
orientation. We could then use the left mouse button and the mouse position to
control orientation. We can use the distance from the center of the screen to control
the x and y rotations. Thus, if the left mouse button is held down but the mouse is
located in the center of the screen, there will be no rotation; if the mouse is moved up,
the object will be rotated about the y-axis in a clockwise manner; and if the mouse
is moved down, the object will be rotated about the y-axis in a counterclockwise
manner. Likewise, motion to the right or left will cause rotation about the x-axis.
The distance from the center can control the speed of rotation. Motion toward the
corners can cause simultaneous rotations about the x- and y-axes.
Using the right mouse button in a similar manner, we can translate the object
right to left and up to down. We might use the middle mouse button to move the
object toward or away from the viewer by having the mouse position control a trans-
lation in the z-direction. The code for such an interface is straightforward in GLUT;
we leave it as an exercise (Exercise 3.20).

#### 3.13.2 A Virtual Trackball

The use of the mouse position to control rotation about two axes provides us with
most of the functionality of a trackball. We can go one step further and create a
graphical or virtual trackball using our mouse and the display. One of the beneﬁts
of such a device is that we can create a frictionless trackball that, once we start
it rotating, will continue to rotate until stopped by the user. Thus, the device will
support continuous rotations of objects but will still allow changes in the speed
and orientation of the rotation. We can also do the same for translation and other
parameters that we can control from the mouse.
We start by mapping the position of a trackball to that of a mouse. Consider the
trackball shown in Figure 3.60. We assume that the ball has a radius of 1 unit. We can

*FIGURE 3.60 Trackball frame.*

182   Chapter 3   Geometric Objects and Transformations
(x, y, z)
(x, 0, z)

*FIGURE 3.61 Projection of the trackball position to the plane.*

map a position on its surface to the plane y = 0 by doing an orthogonal projection to
the plane, as shown in Figure 3.61. The position (x, y, z) on the surface of the ball is
mapped to (x, 0, z) on the plane. This projection is reversible because we know that
the three-dimensional point that is projected to the point on the plane must satisfy
x 2 + y 2 + z 2 = 1.
Thus, given the point on the plane (x, 0, z), the corresponding point on the hemi-
sphere must be (x, y, z), where

y = 1 − x2 − z2.
We can compute the three-dimensional information and track it as the mouse
moves. Suppose that we have two positions on the hemisphere p1 and p2; then the
vectors from the origin to these points determine the orientation of a plane, as shown
in Figure 3.62, whose normal is deﬁned by their cross product
n = p1 × p2 .
The motion of the trackball that moves from p1 to p2 can be achieved by a rotation
about n. The angle of rotation is the angle between the vectors p1 and p2, which we
can compute using the magnitude of the cross product. Because both p1 and p2 have
unit length,
| sin θ| = |n|.
If we are tracking the mouse at a high rate, then the changes in position that we detect
will be small; rather than use an inverse trigonometric function to ﬁnd θ , we can use
                                                       3.13 Interfaces to Three-Dimensional Applications   183
p2
p1

*FIGURE 3.62 Computation of the plane of rotation.*

sin θ ≈ θ .
We can implement the virtual trackball through use of the idle, motion, and
mouse callbacks in GLUT. We can think of the process in terms of three logical
variables, or ﬂags, that control the tracking of the mouse and of the display redrawing.
These are set initially as follows:9
bool trackingMouse = false;
bool trackballMove = false;
bool redrawContinue = false;
If redrawContinue is true, the idle function posts a redisplay. If tracking-
Mouse is true, we update the trackball position as part of the motion callback. If
trackballMove is true, we update the rotation matrix that we use in our display
routine.
The changes in these variables are controlled through the mouse callback. When
we push a mouse button—either a particular button or any button, depending on
exactly what we want—we start updating the trackball position by initializing it, and
then letting the motion callback update it and post redisplays in response to changes
in the position of the mouse. When the mouse button is released, we stop tracking the
mouse. We can use the two most recent mouse positions to deﬁne a velocity vector
so that we can continually update the rotation matrix. Thus, once the mouse button
is released, the object will continue to rotate at a constant velocity—an effect that we
9. If the compiler does not support the Boolean type, we can use typedef bool int; and then
deﬁne true and false as 1 and 0, respectively.
184   Chapter 3   Geometric Objects and Transformations
could achieve with an ideal frictionless trackball but not directly with either a real
mouse or a real trackball.

#### 3.13.3 Smooth Rotations

Our approach to orienting objects has been based on angles (the Euler angles) mea-
sured with respect to the three coordinate axes. This perspective led to our forming
rotation matrices by concatenating simple rotations about the x-, y-, and z-axes to
obtain a desired rotation about an arbitrary axis. Although OpenGL allows us to
rotate about an arbitrary axis, we usually employ our concatenation strategy to de-
termine this axis and the corresponding angle of rotation.10
Consider what happens if we wish to move between two orientations as part of
an animation. In principle, we can determine an appropriate rotation matrix as the
product of rotations about the three axes,
R(θ ) = Rx (θx )Ry (θy )Rz (θz ).
If we want to create a sequence of images that move between the two orientations, we
can change the individual angles in small increments, either individually or simulta-
neously. Such a sequence would not appear smooth to a viewer; she would detect the
individual rotations about each of the three axes.
With a device such as the trackball, we saw that we could rotate the cube
smoothly about any axis. We did so by exploiting the equivalence between the two
orientations of the cube and two points on a unit circle. A smooth rotation between
the two orientations corresponds to a great circle on the surface of a sphere. This
circle corresponds to a single rotation about a suitable axis that is the normal to the
plane determined by the two points on the sphere and that sphere’s center. If we
increase this angle smoothly, our viewer will see a smooth rotation.
In one sense, what has failed us is our mathematical formulation, which relies
on the use of coordinate axes. However, a deeper and less axis-dependent method is
embedded within the matrix formulation. Suppose that we start with an arbitrary ro-
tation matrix R. All points on a line in the direction d are unaffected by the rotation.
Thus, for any such point p,
Rp = p.
In terms of the matrix R, the column matrix p is an eigenvector of the matrix
corresponding to the eigenvalue 1 (see Appendix C). In addition, for the direction d,
Rd = d,
10. This section and the next may be skipped on a ﬁrst reading.
                                                     3.13 Interfaces to Three-Dimensional Applications   185
so that its direction is unaffected by the rotation. Consequently, d is also an eigen-
vector of R corresponding to another eigenvalue of 1. The point p must be the ﬁxed
point of the rotation, and the vector d must be the normal to a plane perpendicular
to the direction of rotation. In terms of the trackball, computing the axis of rota-
tion was equivalent to ﬁnding a particular eigenvector of the desired rotation matrix.
We could also go the other way. Given an arbitrary rotation matrix, by ﬁnding its
eigenvalues and eigenvectors, we also determine the axis of rotation and the ﬁxed
point.

#### 3.13.4 Incremental Rotation

Suppose that we are given two orientations of an object, such as a camera, and we
want to go smoothly from one to the other. One approach is to ﬁnd the great circle
path as we did with the virtual trackball and make incremental changes in the angle
that rotates us along this path. Thus, we start with the axis of rotation, a start angle, a
ﬁnal angle, and a desired increment in the angle determined by the number of steps
we wish to take. The main loop in the code will be of the following form:
mat4 ctm;
for(i=0, i<imax; i++)

```cpp
{
```

thetax += dx;
thetay += dy;
thetaz += dz;
ctm = RotateXm(thetax)*RotateYm(thetay)*RotateZm(thetaz);
draw_object();

```cpp
}
```

One problem with this approach is that the calculation of the rotation matrix
requires the evaluation of the sines and cosines of three angles. We would do better if
we compute the rotation matrix once and reuse it. We could also use the small angle
sin θ ≈ θ ,
cos θ ≈ 1.
If we form an arbitrary rotation matrix through the Euler angles
R = Rz (ψ)Ry (φ)Rx (θ ),
then we can use the approximations to write R as
186   Chapter 3   Geometric Objects and Transformations
⎡                      ⎤⎡                             ⎤
cos ψ − sin ψ 0 0           cos φ         0 sin φ    0
⎢ sin ψ     cos ψ         ⎥ ⎢                          0⎥
⎢                    0 0⎥⎢ 0                1   0        ⎥
R= ⎢                         ⎥⎢                             ⎥
⎣ 0           0      1 0 ⎦ ⎣ − sin φ        0 cos φ    0⎦
0        0      0 1        0           0   0      1
⎡                        ⎤
1     0        0     0
⎢ 0 cos θ − sin θ 0 ⎥
⎢                        ⎥
⎢                        ⎥
⎣ 0 sin θ      cos θ   0⎦
0     0        0     1
⎡                     ⎤
1 −ψ φ 0
⎢ ψ            −θ 0 ⎥
⎢         1           ⎥
≈ ⎢                     ⎥.
⎣ −φ      θ     1 0⎦
0      0     0 1

### 3.14      QUATERNIONS

Quaternions are an extension of complex numbers that provide an alternative
method for describing and manipulating rotations. Although less intuitive than our
original approach, quaternions provide advantages for animation and hardware im-
plementation of rotation.

#### 3.14.1 Complex Numbers and Quaternions

In two dimensions, the use of complex numbers to represent operations such as
rotation is well known to most students of engineering and science. For example,
suppose that we let i denote the pure imaginary number such that i2 = −1. Recalling
Euler’s identity,
e iθ = cos θ + i sin θ ,
we can write the polar representation of a complex number c as
c = a + ib = re iθ ,
√
where r = a2 + b2 and θ = tan−1 b/a.
If we rotate c about the origin by φ to c, then we can ﬁnd c using a rotation
matrix, or we can use the polar representation to write
c = re i(θ+φ) = re iθ e iφ .
Thus, e iφ is a rotation operator in the complex plane and provides an alternative to
using transformations that may prove more efﬁcient in practice.
                                                                                              3.14 Quaternions   187
However, we are really interested in rotations in a three-dimensional space. In
three dimensions, the problem is more difﬁcult because to specify a rotation about
the origin we need to specify both a direction (a vector) and the amount of rotation
about it (a scalar). One solution is to use a representation that consists of both a vector
and a scalar. Usually, this representation is written as the quaternion
a = (q0 , q1, q2 , q3) = (q0 , q),
where q = (q1, q2 , q3). The operations among quaternions are based on the use of
three “complex” numbers i, j, and k with the properties
i2 = j2 = k 2 = ijk = −1.
These numbers are analogous to the unit vectors in three dimensions, and we can
q = q1i + q2j + q3k.
Now we can use the relationships among i, j, and k to derive quaternion addition and
multiplication. If the quaternion b is given by
b = (p0 , p),
then using the dot and cross products for vectors,
a + b = (p0 + q0 , p + q),
ab = (p0q0 − q . p, q0p + p0q + q × p).
We can also deﬁne a magnitude for quaternions in the normal manner as
|a|2 = q02 + q12 + q22 + q32 = q02 + q . q.
Quaternions have a multiplicative identity, the quaternion (1, 0), and it is easy to
verify that the inverse of a quaternion is given by
a−1 =       (q , −q).
|a|2 0

#### 3.14.2 Quaternions and Rotation

So far, we have only deﬁned a new mathematical object. For it to be of use to us,
we must relate it to our geometric entities and show how it can be used to carry out
operations such as rotation. Suppose that we use the vector part of a quaternion to
p = (0, p).
188   Chapter 3   Geometric Objects and Transformations
Thus, the components of p = (x, y, z) give the location of the point. Consider the

θ    θ
r = cos , sin v ,
2     2
where v has unit length. We can then show that the quaternion r is a unit quaternion
(|r| = 1), and therefore

θ        θ
r −1 = cos , − sin v .
2         2
If we consider the quaternion product of the quaternion p that represents a point with
r, we obtain the quaternion
p = rpr −1.
This quaternion has the form (0, p), where
θ         θ                θ   θ             θ
p = cos2     p + sin2 (p . v)v + 2 sin cos (v × p) − sin (v × p) × v
2         2                2   2             2
and thus p is the representation of a point. What is less obvious is that p is the result
of rotating the point p by θ degrees about the vector v. However, we can verify that
this indeed is the case by comparing terms in p with those of the general rotation.
Before doing so, consider the implication of this result. Because we get the same
result, the quaternion product formed from r and p is an alternate to transformation
matrices as a representation of rotation with a ﬁxed point of the origin about an
arbitrary axis. If we count operations, quaternions are faster and have been built into
both hardware and software implementations.
Let’s consider a few examples. Suppose that we consider the rotation about the z-
axis by θ with a ﬁxed point at the origin. The desired unit vector v is (0, 0, 1), yielding
θ      θ
r = cos     + sin (0, 0, 1).
2      2
The rotation of an arbitrary point p = (x, y, z) yields the quaternion
p = rpr −1 = r(0, p)r −1 = (0, p),
p = (x cos θ − y sin θ , x sin θ + y cos θ , z).
Thus, we get the expected result but with fewer operations. If we consider a sequence
of rotations about the coordinate axes that in matrix form yields the matrix R =
Rx (θx )Ry (θy )Rz (θz ), we instead can use the product of the corresponding quaternions
to form rx ry rz .
                                                                                           3.14 Quaternions   189
Returning to the rotation about an arbitrary axis, in Section 3.10.4, we derived a
M = T(p0)Rx (−θx )Ry (−θy )Rz (θz )Ry (θy )Rx (θx )T(−p0).
Because of the translations at the beginning and end, we cannot use quaternions
for the entire operation. We can, however, recognize that the elements of p = rpr −1
can be used to ﬁnd the elements of the homogeneous coordinate rotation matrix
embedded in M. Thus, if again r = (cos θ2 , sin θ2 v), then
⎡
1 − 2 sin2 θ2 (vy2 + vz2)              2 sin2 θ2 vx vy − 2 cos θ2 sin θ2 vz
⎢ 2 sin2 θ v v + 2 cos θ sin θ v                   1 − 2 sin2 θ2 (vx2 + vz2)
⎢        2 x y                   2 z
R=⎢                          2
⎣ 2 sin2 θ vx vz − 2 cos θ sin θ vy           2 sin2 θ2 vy vz + 2 cos θ2 sin θ2 vx
2                2      2
0                                            0
⎤
2 sin2 θ2 vx vz + 2 cos θ2 sin θ2 vy   0
2 sin2 θ2 vy vz − 2 cos θ2 sin θ2 vx   0⎥⎥
⎥.
1 − 2 sin2 θ2 (vx2 + vy2)         0⎦
0                     1
This matrix can be made to look more familiar if we use the trigonometric identities
θ       θ            θ
cos θ = cos2      − sin2 = 1 − 2 sin2 ,
2       2            2
θ    θ
sin θ = 2 cos      sin ,
2    2
and recall that v is a unit vector so that
vx2 + vy2 + vz2 = 1.
Thus, we can use quaternion products to form r and then form the rotation part
of M by matching terms between R and r. We then use our normal transformation
operations to add in the effect of the two translations.
Alternately, we can use the vec4 type to create quaternions either in the applica-
tion (Exercise 3.26) or in the shaders (Exercise 3.30). In either case, we can carry out
the rotation directly without converting back to a rotation matrix.
In addition to the efﬁciency of using quaternions instead of rotation matrices,
quaternions can be interpolated to obtain smooth sequences of rotations for ani-
mation.
190   Chapter 3   Geometric Objects and Transformations
In this chapter, we have presented two different—but ultimately complementary—
points of view regarding the mathematics of computer graphics. One is that mathe-
matical abstraction of the objects with which we work in computer graphics is nec-
essary if we are to understand the operations that we carry out in our programs. The
other is that transformations—and the techniques for carrying them out, such as
the use of homogeneous coordinates—are the basis for implementations of graphics
systems.
Our mathematical tools come from the study of vector analysis and linear alge-
bra. For computer-graphics purposes, however, the order in which we have chosen
to present these tools is the reverse of the order that most students learn them. In
particular, linear algebra is studied ﬁrst, and then vector-space concepts are linked to
the study of n-tuples in R n. In contrast, our study of representation in mathematical
spaces led to our use of linear algebra as a tool for implementing abstract types.
We pursued a coordinate-free approach for two reasons. First, we wanted to
show that all the basic concepts of geometric objects and of transformations are inde-
pendent of the ways the latter are represented. Second, as object-oriented languages
become more prevalent, application programmers will work directly with the objects,
instead of with those objects’ representations. The references in Suggested Readings
contain examples of geometric programming systems that illustrate the potential of
this approach.
Homogeneous coordinates provided a wonderful example of the power of math-
ematical abstraction. By going to an abstract mathematical space—the afﬁne space—
we were able to ﬁnd a tool that led directly to efﬁcient software and hardware
methods.
Finally, we provided the set of afﬁne transformations supported in OpenGL and
discussed ways that we could concatenate them to provide all afﬁne transformations.
The strategy of combining a few simple types of matrices to build a desired transfor-
mation is a powerful one; you should use it for a few of the exercises at the end of
this chapter. In Chapter 4, we build on these techniques to develop viewing for three-
dimensional graphics; in Chapter 8, we use our transformations to build hierarchical
models.
There are many texts on vector analysis and linear algebra, although most treat
the topics separately. Within the geometric-design community, the vector-space ap-
proach of coordinate-free descriptions of curves and surfaces has been popular; see
the book by Faux and Pratt [Fau80]. See DeRose [DeR88, DeR89] for an introduction
to geometric programming. Homogeneous coordinates arose in geometry [Max51]
and were later discovered by the graphics community [Rob63, Rie81]. Their use in
hardware started with Silicon Graphics’ Geometry Engine [Cla82]. Modern hard-
                                                                                           Exercises   191
ware architectures use application-speciﬁc integrated circuits (ASICs) that include
homogeneous coordinate transformations.
Quaternions were introduced to computer graphics by Shoemake [Sho85] for
use in animation. See the book by Kuipers [Kui99] for many examples of the use of
rotation matrices and quaternions.
Software tools such as Mathematica [Wol91] and MATLAB [Mat95] are excellent
aids for learning to manipulate transformation matrices.

### 3.1   Show that the following sequences commute:

a. A rotation and a uniform scaling
b. Two rotations about the same axis
c. Two translations

### 3.2   Twist is similar to rotation about the origin except that the amount of rotation

increases by a factor f the farther a point is from the origin. Write a program to
twist the triangle-based Sierpinski gasket by a user-supplied value of f . Observe
how the shape of the gasket changes with the number of subdivisions.

### 3.3   Write a library of functions that will allow you to do geometric programming.

Your library should contain functions for manipulating the basic geometric
types (points, lines, vectors) and operations on those types, including dot
and cross products. It should allow you to change frames. You can also create
functions to interface with OpenGL so that you can display the results of
geometric calculations.

### 3.4   If we are interested in only two-dimensional graphics, we can use three-

dimensional homogeneous coordinates by representing a point as p = [x y 1]T
and a vector as v = [a b 0]T . Find the 3 × 3 rotation, translation, scaling, and
shear matrices. How many degrees of freedom are there in an afﬁne transfor-
mation for transforming two-dimensional points?

### 3.5   We can specify an afﬁne transformation by considering the location of a small

number of points both before and after these points have been transformed. In
three dimensions, how many points must we consider to specify the transfor-
mation uniquely? How does the required number of points change when we
work in two dimensions?

### 3.6   How must we change the rotation matrices if we are working in a left-handed

system and we retain our deﬁnition of a positive rotation?

### 3.7   Show that any sequence of rotations and translations can be replaced by a

single rotation about the origin followed by a translation.

### 3.8   Derive the shear transformation from the rotation, translation, and scaling

transformations.
192        Chapter 3   Geometric Objects and Transformations

### 3.9   In two dimensions, we can specify a line by the equation y = mx + h. Find an

afﬁne transformation to reﬂect two-dimensional points about this line. Extend
your result to reﬂection about a plane in three dimensions.

### 3.10 In Section 3.10, we showed that an arbitrary rotation matrix could be com-

posed from successive rotations about the three axes. How many ways can we
compose a given rotation if we can do only three simple rotations? Are all three
of the simple rotation matrices necessary?

### 3.11 Add shear to the instance transformation. Show how to use this expanded

instance transformation to generate parallelepipeds from a unit cube.

### 3.12 Find a homogeneous-coordinate representation of a plane.


### 3.13 Determine the rotation matrix for a rotation of the form Rx Ry Rz . Assume that

the ﬁxed point is the origin and the angles are θx , θy , and θz .

### 3.14 Consider the solution of either constant-coefﬁcient linear differential or dif-

ference equations (recurrences). Show that the solutions of the homogeneous
equations form a vector space. Relate the solution for a particular inhomoge-
neous equation to an afﬁne space.

### 3.15 Write a program to generate a Sierpinski gasket as follows. Start with a white

triangle. At each step, use transformations to generate three similar triangles
that are drawn over the original triangle, leaving the center of the triangle white
and the three corners black.

### 3.16 Start with a cube centered at the origin and aligned with the coordinate axes.

Find a rotation matrix that will orient the cube symmetrically, as shown in

*FIGURE 3.63 Symmetric ori-        Figure 3.63.*

entation of cube.            3.17 We have used vertices in three dimensions to deﬁne objects such as three-
dimensional polygons. Given a set of vertices, ﬁnd a test to determine whether
the polygon that they determine is planar.

### 3.18 Three vertices determine a triangle if they do not lie in the same line. Devise a

test for collinearity of three vertices.

### 3.19 We deﬁned an instance transformation as the product of a translation, a rota-

tion, and a scaling. Can we accomplish the same effect by applying these three
types of transformations in a different order?

### 3.20 Write a program that allows you to orient the cube with one mouse button, to

translate it with a second, and to zoom in and out with a third.

### 3.21 Given two nonparallel, three-dimensional vectors u and v, how can we form

an orthogonal coordinate system in which u is one of the basis vectors?

### 3.22 An incremental rotation about the z-axis can be approximated by the matrix

⎡               ⎤
1 −θ 0 0
⎢θ     1 0 0⎥
⎢               ⎥
⎢               ⎥.
⎣0 0 1 0⎦
0 0 0 1
                                                                                         Exercises   193
What negative aspects are there if we use this matrix for a large number of
steps? Can you suggest a remedy? Hint: Consider points a distance of 1 from
the origin.

### 3.23   Find the quaternions for 90-degree rotations about the x- and y-axes. Deter-

mine their product.

### 3.24   Determine the rotation matrix R = R(θx )R(θy )R(θz ). Find the corresponding

quaternion.

### 3.25   Redo the trackball program using quaternions instead of rotation matrices.


### 3.26   Using the vec4 class, create a set of quaternion operations that can be carried

out in an application. For example, you might start with the prototypes
typedef vec4 quaternion;
quaternion multq(const quaternion &, const quaternion &);
quaternion addq(const quaternion &, const quaternion &);
quaternion iverseq(const quaternion &);
point4 rotateq(float theta, const point4 &);

### 3.27 Write a vertex shader that takes as input an angle and an axis of rotation and

rotates vertices about this axis.

### 3.28 In principle, an object-oriented system could provide scalars, vectors, and

points as basic types. None of the popular APIs does so. Why do you think
this is the case?

### 3.29 Show that the sum

P = α1P1 + α2P2 + . . . + αnPn
is deﬁned if and only if
α1 + α2 + . . . + αn = 1.
Hint: Start with the ﬁrst two terms and write them as
P = α1P1 + α2P2 + . . . = α1P1 + (α2 + α1 − α1)P2 + . . .
= α1(P1 − P2) + (α1 + α2)P2 + . . . ,
and then proceed inductively.

### 3.30 Write a vertex shader whose input is a quaternion and rotates the input vertex

using quaternion rotation.

### 3.31 Write a function Rotate(float theta, vec3d) that will rotate by theta

degrees about the axis d with a ﬁxed point at the origin.
This page intentionally left blank
                                                                     CHA P TE R            4
W       e have completed our discussion of the ﬁrst half of the synthetic camera
model—specifying objects in three dimensions. We now investigate the multi-
tude of ways in which we can describe our virtual camera. Along the way, we examine
related topics, such as the relationship between classical viewing techniques and com-
puter viewing and how projection is implemented using projective transformations.
There are three parts to our approach. First, we look at the types of views that we
can create and why we need more than one type of view. Then we examine how an
application program can specify a particular view within OpenGL. We will see that
the viewing process has two parts. In the ﬁrst, we use the model-view matrix to switch
vertex representations from the object frame in which we deﬁned our objects to their
representation in the eye frame, in which the camera is at the origin. This represen-
tation of the geometry will allow us to use canonical viewing procedures. The second
part of the process deals with the type of projection we prefer (parallel or perspec-
tive) and the part of the world we wish to image (the clipping or view volume). These
speciﬁcations will allow us to form a projection matrix that is concatenated with the
model-view matrix. Finally, we derive the projection matrices that describe the most
important parallel and perspective views and investigate how to carry out these pro-
jections in OpenGL.

### 4.1    CLASSICAL AND COMPUTER VIEWING

Before looking at the interface between computer-graphics systems and application
programs for three-dimensional viewing, we take a slight diversion to consider classi-
cal viewing. There are two reasons for examining classical viewing. First, many of the
jobs that were formerly done by hand drawing—such as animation in movies, archi-
tectural rendering, drafting, and mechanical-parts design—are now routinely done
with the aid of computer graphics. Practitioners of these ﬁelds need to be able to pro-
duce classical views—such as isometrics, elevations, and various perspectives—and
thus must be able to use the computer system to produce such renderings. Second,
the relationships between classical and computer viewing show many advantages of,
and a few difﬁculties with, the approach used by most APIs.
196   Chapter 4   Viewing

*FIGURE 4.1 Viewing.*


*FIGURE 4.2 Movement of the center of projection (COP) to infinity.*

When we introduced the synthetic-camera model in Chapter 1, we pointed
out the similarities between classical and computer viewing. The basic elements in
both cases are the same. We have objects, a viewer, projectors, and a projection
plane (Figure 4.1). The projectors meet at the center of projection (COP). The COP
corresponds to the center of the lens in the camera or in the eye, and in a computer-
graphics system, it is the origin of the camera frame for perspective views. All
standard graphics systems follow the model that we described in Chapter 1, which
is based on geometric optics. The projection surface is a plane, and the projectors are
straight lines. This situation is the one we usually encounter and is straightforward
to implement, especially with our pipeline model.
Both classical and computer graphics allow the viewer to be an inﬁnite distance
from the objects. Note that as we move the COP to inﬁnity, the projectors become
parallel and the COP can be replaced by a direction of projection (DOP), as shown
in Figure 4.2. Note also that as the COP moves to inﬁnity, we can leave the projection
plane ﬁxed and the size of the image remains about the same, even though the COP is
inﬁnitely far from the objects. Views with a ﬁnite COP are called perspective views;
views with a COP at inﬁnity are called parallel views. For parallel views, the origin
of the camera frame usually lies in the projection plane.
                                                                    4.1 Classical and Computer Viewing   197
Color Plates 9 and 10 show a parallel and a perspective rendering, respectively.
These plates illustrate the importance of having both types of view available in appli-
cations such as architecture; in an API that supports both types of viewing, the user
can switch easily between various viewing modes. Most modern APIs support both
parallel and perspective viewing. The class of projections produced by these systems
is known as planar geometric projections because the projection surface is a plane
and the projectors are lines. Both perspective and parallel projections preserve lines;
they do not, in general, preserve angles. Although the parallel views are the limiting
case of perspective viewing, both classical and computer viewing usually treat them as
separate cases. For classical views, the techniques that people use to construct the two
types by hand are different, as anyone who has taken a drafting class surely knows.
From the computer perspective, there are differences in how we specify the two types
of views. Rather than looking at a parallel view as the limit of the perspective view,
we derive the limiting equations and use those equations directly to form the corre-
sponding projection matrix. In modern pipeline architectures, the projection matrix
corresponding to either type of view can be loaded into the pipeline.
Although computer-graphics systems have two fundamental types of viewing
(parallel and perspective), classical graphics appears to permit a host of different
views, ranging from multiview orthographic projections to one-, two-, and three-
point perspectives. This seeming discrepancy arises in classical graphics as a result of
the desire to show a speciﬁc relationship among an object, the viewer, and the projec-
tion plane, as opposed to the computer-graphics approach of complete independence
of all speciﬁcations.

#### 4.1.1 Classical Viewing

When an architect draws an image of a building, she knows which side she wishes to
display and thus where she should place the viewer in relationship to the building.
Each classical view is determined by a speciﬁc relationship between the objects and
the viewer.
In classical viewing, there is the underlying notion of a principal face. The
types of objects viewed in real-world applications, such as architecture, tend to be
composed of a number of planar faces, each of which can be thought of as a principal
face. For a rectangular object, such as a building, there are natural notions of the
front, back, top, bottom, right, and left faces. In addition, many real-world objects
have faces that meet at right angles; thus, such objects often have three orthogonal
directions associated with them.
Figure 4.3 shows some of the main types of views. We start with the most re-
strictive view for each of the parallel and perspective types, and then move to the less
restrictive conditions.

#### 4.1.2 Orthographic Projections

Our ﬁrst classical view is the orthographic projection shown in Figure 4.4. In all or-
thographic (or orthogonal) views, the projectors are perpendicular to the projection
plane. In a multiview orthographic projection, we make multiple projections, in
198   Chapter 4   Viewing
Front elevation       Elevation oblique            Plan oblique
Isometric        One-point perspective     Three-point perspective

*FIGURE 4.3 Classical views.*


*FIGURE 4.4 Orthographic projections.*

each case with the projection plane parallel to one of the principal faces of the object.
Usually, we use three views—such as the front, top, and right—to display the object.
The reason that we produce multiple views should be clear from Figure 4.5. For a
box-like object, only the faces parallel to the projection plane appear in the image. A
viewer usually needs more than two views to visualize what an object looks like from
its multiview orthographic projections. Visualization from these images can require
skill on the part of the viewer. The importance of this type of view is that it preserves
both distances and angles, and because there is no distortion of either distance or
shape, multiview orthographic projections are well suited for working drawings.

#### 4.1.3 Axonometric Projections

If we want to see more principal faces of our box-like object in a single view, we
must remove one of our restrictions. In axonometric views, the projectors are still
                                                                  4.1 Classical and Computer Viewing   199

*FIGURE 4.5 Temple and three multiview orthographic projections.*

Projection plane                  Projection plane                Projection plane
(a)                               (b)                        (c)

*FIGURE 4.6 Axonometric projections. (a) Construction of trimetric-view*

projections. (b) Top view. (c) Side view.
orthogonal to the projection plane, as shown in Figure 4.6, but the projection plane
can have any orientation with respect to the object. If the projection plane is placed
symmetrically with respect to the three principal faces that meet at a corner of our
rectangular object, then we have an isometric view. If the projection plane is placed
symmetrically with respect to two of the principal faces, then the view is dimetric.
The general case is a trimetric view. These views are shown in Figure 4.7. Note that
in an isometric view, a line segment’s length in the image space is shorter than its
length measured in the object space. This foreshortening of distances is the same
200   Chapter 4   Viewing
Dimetric                Trimetric                      Isometric

*FIGURE 4.7 Axonometric views.*

Projection plane                         Projection plane
(a)                                      (b)                       (c)

*FIGURE 4.8 Oblique view. (a) Construction. (b) Top view. (c) Side view.*

in the three principal directions, so we can still make distance measurements. In the
dimetric view, however, there are two different foreshortening ratios; in the trimetric
view, there are three. Also, although parallel lines are preserved in the image, angles
are not. A circle is projected into an ellipse. This distortion is the price we pay for the
ability to see more than one principal face in a view that can be produced easily either
by hand or by computer. Axonometric views are used extensively in architectural and
mechanical design.

#### 4.1.4 Oblique Projections

The oblique views are the most general parallel views. We obtain an oblique projec-
tion by allowing the projectors to make an arbitrary angle with the projection plane,
as shown in Figure 4.8. Consequently, angles in planes parallel to the projection plane
are preserved. A circle in a plane parallel to the projection plane is projected into a cir-
cle, yet we can see more than one principal face of the object. Oblique views are the
most difﬁcult to construct by hand. They are also somewhat unnatural. Most physi-
                                                                    4.1 Classical and Computer Viewing   201
cal viewing devices, including the human visual system, have a lens that is in a ﬁxed
relationship with the image plane—usually, the lens is parallel to the plane. Although
these devices produce perspective views, if the viewer is far from the object, the views
are approximately parallel, but orthogonal, because the projection plane is parallel
to the lens. The bellows camera that we used to develop the synthetic-camera model
in Section 1.6 has the ﬂexibility to produce approximations to parallel oblique views.
One use of such a camera is to create images of buildings in which the sides of the
building are parallel rather than converging as they would be in an image created
with an orthogonal view with the camera on the ground.
From the application programmer’s point of view, there is no signiﬁcant differ-
ence among the different parallel views. The application programmer speciﬁes a type
of view—parallel or perspective—and a set of parameters that describe the camera.
The problem for the application programmer is how to specify these parameters in
the viewing procedures so as best to view an object or to produce a speciﬁc classical
view.

#### 4.1.5 Perspective Viewing

All perspective views are characterized by diminution of size. When objects are
moved farther from the viewer, their images become smaller. This size change gives
perspective views their natural appearance; however, because the amount by which a
line is foreshortened depends on how far the line is from the viewer, we cannot make
measurements from a perspective view. Hence, the major use of perspective views is
in applications such as architecture and animation, where it is important to achieve
natural-looking images.
In the classical perspective views, the viewer is located symmetrically with respect
to the projection plane, as shown in Figure 4.9. Thus, the pyramid determined by the
window in the projection plane and the center of projection is a symmetric or right

*FIGURE 4.9 Perspective viewing.*

202   Chapter 4   Viewing
(a)                    (b)                    (c)

*FIGURE 4.10 Classical perspective views. (a) Three-point. (b) Two-point.*

(c) One-point.
pyramid. This symmetry is caused by the ﬁxed relationship between the back (retina)
and lens of the eye for human viewing, or between the back and lens of a camera
for standard cameras, and by similar ﬁxed relationships in most physical situations.
Some cameras, such as the bellows camera, have movable ﬁlm backs and can produce
general perspective views. The model used in computer graphics includes this general
case.
The classical perspective views are usually known as one-, two-, and three-point
perspectives. The differences among the three cases are based on how many of the
three principal directions in the object are parallel to the projection plane. Consider
the three perspective projections of the building shown in Figure 4.10. Any corner
of the building includes the three principal directions. In the most general case—
the three-point perspective—parallel lines in each of the three principal directions
converges to a ﬁnite vanishing point (Figure 4.10(a)). If we allow one of the principal
directions to become parallel to the projection plane, we have a two-point projection
(Figure 4.10(b)), in which lines in only two of the principal directions converge.
Finally, in the one-point perspective (Figure 4.10(c)), two of the principal directions
are parallel to the projection plane, and we have only a single vanishing point. As
with parallel viewing, it should be apparent from the programmer’s point of view that
the three situations are merely special cases of general perspective viewing, which we
implement in Section 4.4.

### 4.2    VIEWING WITH A COMPUTER

We can now return to three-dimensional graphics from a computer perspective. Be-
cause viewing in computer graphics is based on the synthetic-camera model, we
should be able to construct any of the classical views. However, there is a fundamen-
tal difference. All the classical views are based on a particular relationship among the
objects, the viewer, and the projectors. In computer graphics, we stress the indepen-
dence of the object speciﬁcations and camera parameters. Hence, to create one of the

```cpp
classical views, the application program must use information about the objects to
```

create and place the proper camera.
                                                                                        4.2 Viewing with a Computer   203
Using OpenGL, we will have many options on how and where we carry out
viewing. All our approaches will use the powerful transformation capabilities of the
GPU. Because every transformation is equivalent to a change of frames, we can
develop viewing in terms of the frames and coordinate systems we introduced in

## Chapter 3. In particular, we will work with object coordinates, camera coordinates,

and clip coordinates.
A good starting point is the output of the vertex shader. In Chapters 2 and 3, we
used the fact that as long as the vertices output by the vertex shader were within the
clipping volume, they continued onto the rasterizer. Hence, in Chapter 2 we were able
to specify vertex positions inside the default viewing cube. In Chapter 3, we learned
how to scale positions using afﬁne transformations so they would be mapped inside
the cube. We also relied on the fact that objects that are sent to the rasterizer are
projected with a simple orthographic projection.
Hidden-surface removal, however, occurs after the fragment shader. Conse-
quently, although an object might be blocked from the camera by other objects, even
with hidden-surface removal enabled, the rasterizer will still generate fragments for
blocked objects within the clipping volume. However, we need more ﬂexibility in
both how we specify objects and how we view them. There are four major additions
to address:
1. We need the ability to work in the units of the application.
2. We need to position the camera independently of the objects.
3. We want to be able to specify a clipping volume in units related to the appli-
cation.
4. We want to be able to do either parallel or perspective projections.
We can accomplish all these additions by careful use of transformations: the ﬁrst three
using afﬁne transformations, and the last using a process called perspective normal-
ization. All of these transformations must be carried out either in the application code
or in the vertex shader.
We approach all these tasks through the transformation capabilities we devel-
oped in Chapter 3. Of the frames that are used in OpenGL, three are important in the
viewing process: the object frame, the camera frame, and the clip coordinate frame.
In Chapters 2 and 3, we were able to avoid explicitly specifying the ﬁrst two by using a
default in which all three frames were identical. We either directly speciﬁed vertex po-
sitions in clip coordinates or used an afﬁne transformation to scale objects we wanted
to be visible to lie within the clipping cube in clip coordinates. The camera was ﬁxed
to be at the origin and pointing in the negative z-direction in clip coordinates.1
To get a more ﬂexible way to do viewing, we will separate the process into two
fundamental operations. First, we must position and orient the camera. This oper-
ation is the job of the model-view transformation. After vertices pass through this
1. The default camera can “see” objects behind it if they are in the clipping volume.
204         Chapter 4   Viewing
Object                      Camera                    Clip
coordinates                 coordinates              coordinates
Model-view            Projection
Vertices                                                        Vertices

*FIGURE 4.11 Viewing transformations.*

transformation, they will be represented in eye or camera coordinates. The second
step is the application of the projection transformation. This step applies the speciﬁed
projection—orthographic or perspective—to the vertices and puts objects within the
speciﬁed clipping volume into the same clipping cube in clip coordinates. One of the
functions of either projection will be to allow us to specify a view volume in camera
coordinates rather than having to scale our object to ﬁt into the default view volume.
These transformations are shown in Figure 4.11.
What we have called the current transformation matrix will be the product of
two matrices: the model-view matrix and the projection matrix. The model-view
matrix will take vertices in object coordinates and convert them to a representation
in camera coordinates and thus must encapsulate the positioning and orientation
of the camera. The projection matrix will both carry out the desired projection—
either orthogonal or perspective—and convert a viewing volume speciﬁed in camera
coordinates to ﬁt inside the viewing cube in clip coordinates.

### 4.3    POSITIONING OF THE CAMERA

In this section, we deal with positioning and orientation of the camera; in Section 4.4,
we discuss how we specify the desired projection. Although we will focus on an API
that will work well with OpenGL, we also will examine brieﬂy a few other APIs to
specify a camera.

#### 4.3.1 Positioning of the Camera Frame

As we saw in Chapter 3, we can specify vertices in any units we choose, and we can
deﬁne a model-view matrix by a sequence of afﬁne transformations that repositions
these vertices. The model-view transformation is the concatenation of a modeling
x     transformation that takes instances of objects in object coordinates and brings them

```cpp
into the world frame. The second part transforms world coordinates to eye coordi-
```

nates. Because we usually do not need to access world coordinates, we can use the
model-view matrix rather than separate modeling and viewing matrices.
Initially, we set the model-view matrix to an identity matrix, so the camera
frame and the object frame are identical. Hence, the camera is initially pointing in

*FIGURE 4.12 Initial camera    the negative z-direction (Figure 4.12). In most applications, we model our objects*

position.                     as being located around the origin, so a camera located at the default position with
the default orientation does not see all the objects in the scene. Thus, either we must
                                                                                 4.3 Positioning of the Camera   205
y, yc                                   yc                                   x
x, xc                                       xc
z, zc                                    zc
(a)                                                (b)

*FIGURE 4.13 Movement of the camera and object frames. (a) Initial*

configuration. (b) Configuration after change in the model-view matrix.
move the camera away from the objects that we wish to have in our image, or the
objects must be moved in front of the camera. These are equivalent operations, as
either can be looked at as positioning the frame of the camera with respect to the
frame of the objects.
It might help to think of a scene in which we have initially speciﬁed several ob-
jects by specifying all vertices and putting their positions into an array. We start with
the model-view matrix set to an identity matrix. Changes to the model-view matrix
move the object frame relative to the camera and affect the camera’s view of all ob-
jects deﬁned afterward, because their vertices are speciﬁed relative to the repositioned
object frame. Equivalently, in terms of the ﬂow of an application program, the pro-
jection and model-view matrices are part of its state. We will either apply them to
the vertex positions in the application or, more likely, we will send them to the vertex
shader where they will be applied automatically whenever vertex data is sent to the
shader.
In either case, the sequence illustrated in Figure 4.13 shows the process. In part
(a), we have the initial conﬁguration. A vertex speciﬁed at p has the same represen-
tation in both frames. In part (b), we have changed the model-view matrix to C by a
sequence of transformations. The two frames are no longer the same, although C
contains the information to move from the camera frame to the object frame or,
equivalently, contains the information that moves the camera away from its initial
position at the origin of the object frame. A vertex speciﬁed at q after the change to
the model-view matrix is at q in the object frame. However, its position in the camera
frame is Cq and can be stored internally within the application or sent to the GPU,
206   Chapter 4   Viewing
where it will be converted to camera coordinates. The viewing transformation will
assume that vertex data it starts with is in camera coordinates.
An equivalent view is that the camera is still at the origin of its own frame, and
the model-view matrix is applied to primitives speciﬁed in this system. In practice,
you can use either view. But be sure to take great care regarding where in your
program the primitives are speciﬁed relative to changes in the model-view matrix.
At any given time, the model-view matrix encapsulates the relationship between
the camera frame and the object frame. Although combining the modeling and view-
ing transformations into a single matrix may initially cause confusion, on closer ex-
amination this approach is a good one. If we regard the camera as an object with
geometric properties, then transformations that alter the position and orientation of
objects should also affect the position and orientation of the camera relative to these
objects.
The next problem is how we specify the desired position of the camera and then
implement camera positioning in OpenGL. We outline three approaches, one in this
section and two in Section 4.3.2. Two others are given as exercises (Exercises 4.2
and 4.3).
Our ﬁrst approach is to specify the position indirectly by applying a sequence
of rotations and translations to the model-view matrix. This approach is a direct
application of the instance transformation that we presented in Chapter 3, but we
must be careful for two reasons. First, we usually want to specify the camera’s position
and orientation before we position any objects in the scene.2 Second, the order of
transformations on the camera may appear to be backward from what you might
expect.
Consider an object centered at the origin. The camera is in its initial position,
also at the origin, pointing down the negative z-axis. Suppose that we want an image
of the faces of the object that point in the positive z-direction. We must move the
camera away from the origin. If we allow the camera to remain pointing in the
negative z-direction, then we want to move the camera backward along the positive
z-axis, and the proper transformation is
⎡                  ⎤
1 0 0 0
⎢0 1 0 0 ⎥
⎢                  ⎥
T=⎢                     ⎥,
⎣ 0 0 1 −d ⎦
0 0 0 1
where d is a positive number.
Many people ﬁnd it helpful to interpret this operation as moving the camera
frame relative to the object frame. This point of view has a basis in classical viewing.
In computer graphics, we usually think of objects as being positioned in a ﬁxed frame,
2. In an animation, where in the program we specify the position of the camera depends on whether
we wish to attach the camera to a particular object or to place the camera in a ﬁxed position in the
scene (see Exercise 4.3).
                                                                           4.3 Positioning of the Camera           207
and it is the viewer who must move to the right position to achieve the desired view.
In classical viewing, the viewer dominates. Conceptually, we do viewing by picking up
the object, orienting it as desired, and bringing it to the desired location. One con-
sequence of the classical approach is that distances are measured from the viewer to
the object, rather than—as in most physically based systems—from the object to the
viewer. Classical viewing often resulted in a left-handed camera frame. Early graphics
systems followed the classical approach by having modeling in right-handed coordi-                   y
nates and viewing in left-handed coordinates—a decision that, although technically
correct, caused confusion among users. When we are working in camera coordinates,
we will measure distances from the camera, which is consistent with classical viewing.
In OpenGL, the internal frames are right handed. Fortunately, because the applica-
tion program works primarily in object coordinates, the application programmer
usually does not see any of the internal representations and thus does not have to
worry about these alternate perspectives on viewing.                                             T
Suppose that we want to look at the same object from the positive x-axis. Now,                      R
not only do we have to move away from the object, but we also have to rotate the           z
camera about the y-axis, as shown in Figure 4.14. We must do the translation after         FIGURE 4.14 Positioning of
we rotate the camera by 90 degrees about the y-axis. In the program, the calls must        the camera.
be in the reverse order, as we discussed in Section 3.11, so we expect to see code like
the following:
mat4 model_view;
model_view = Translate(0.0, 0.0, -d)*RotateX(-90.0);
In terms of the two frames, ﬁrst we rotate the object frame relative to the camera
frame, and then we move the two frames apart.
In Chapters 2 and 3, we were able to show simple three-dimensional examples by
using an identity matrix as the default projection matrix. That default setting has the
effect of creating an orthographic projection with the camera at the origin, pointed
in the negative z-direction. In our cube example in Chapter 3, we rotated the cube to
see the desired faces. As we just discussed, rotating the cube is equivalent to rotating
the frame of the cube with respect to the frame of the camera; we could have achieved
the same view by rotating the camera relative to the cube. We can extend this strategy
of translating and rotating the camera to create other orthographic views. Perspective
views require changes to the default projection.
Consider creating an isometric view of the cube. Suppose that again we start with
a cube centered at the origin and aligned with the axes. Because the default camera
is in the middle of the cube, we want to move the camera away from the cube by a
translation. We obtain an isometric view when the camera is located symmetrically
with respect to three adjacent faces of the cube; for example, anywhere along the line
from the origin through the point (1, 1, 1). We can move the cube away from the
camera and then rotate the cube to achieve the desired view or, equivalently, move
the camera away from the cube and then rotate it to point at the cube.
Starting with the default camera, suppose that we are now looking at the cube
from somewhere on the positive z-axis. We can obtain one of the eight isometric
208   Chapter 4   Viewing
y                                                       y
(a)                                    (b)

*FIGURE 4.15 Cube after rotation about x-axis. (a) View from positive*

z-axis. (b) View from positive y-axis.
views—there is one for each vertex—by ﬁrst rotating the cube about the x-axis until
we see the two faces symmetrically, as shown in Figure 4.15(a). Clearly, we obtain this
view by rotating the cube by 45 degrees. The second rotation is about the y-axis. We
rotate the cube until we get the desired isometric. The required angle of rotation is
−35.26 degrees about the y-axis. This second angle of rotation may not seem obvious.
Consider what happens to the cube after the ﬁrst rotation. From our position on
the positive z-axis, the cube appears as shown in Figure
√ 4.15(a). The original corner
vertex at (−1, 1, 1) has been transformed to (−1, 0, 2). If we look at the cube from
the x-axis, as in Figure 4.15(b), we see that we want to rotate the right vertex
√ to the
y-axis. The right triangle that determines this angle has sides of 1 and 2, which
correspond to an angle of 35.26 degrees. However, we need a clockwise rotation, so
the angle must be negative. Finally, we move the camera away from the origin. Thus,
our strategy is ﬁrst to rotate the frame of the camera relative to the frame of the object
and then to separate the two frames; the model-view matrix is of the form
M = TR x Ry .
We obtain this model-view matrix for an isometric by multiplying the matrices
in homogeneous coordinates. The concatenation of the rotation matrices yields
⎡                       ⎤⎡ √               √         ⎤
1      0       0    0          2/2 0       2/2 0
⎢0    √        √
⎢        6/3 − 3/3 0 ⎥     ⎢
⎥ ⎢ √0         1     0     0⎥⎥
R = Rx Ry = ⎢     √       √         ⎥⎢                 √         ⎥
⎣0       3/3     6/3 0 ⎦ ⎣ − 2/2 0           2/2 0 ⎦
0      0       0    1          0     0     0     1
⎡  √                √           ⎤
2/2     0       2/2 0
√
⎢ 6/6       √        √
⎢             6/3 − 6/6 0 ⎥     ⎥
=⎢ √          √       √           ⎥.
⎣ − 3/3       3/3     3/3 0 ⎦
0       0       0       1
                                                                            4.3 Positioning of the Camera         209
It is simple
√     to verify that the original vertex (−1, 1, 1) is correctly transformed to
(0, 0, 3) by this matrix. If we concatenate in the translation by (0, 0, −d), the
⎡ √                   √            ⎤
2/2       0        2/2     0
⎢ √6/6 √6/3 −√6/6 0 ⎥
⎢                                  ⎥
TR = ⎢ √             √        √            ⎥.
⎣ − 3/3        3/3      3/3 −d ⎦
0         0        0       1
In OpenGL, the code for setting the model-view matrix is as follows:
mat4 model_view;
model_view = Translate(0.0, 0.0, -d)*RotateX(35.26)*RotateY(45.0);
We have gone from a representation of our objects in object coordinates to one
in camera coordinates. Rotation and translation do not affect the size of an object
nor, equivalently, the size of its orthographic projection. However, these transforma-
tions can affect whether or not objects are clipped. Because the clipping volume is
measured relative to the camera, if, for example, we translate the object away from
the camera, it may no longer lie within the clipping volume. Hence, even though
the projection of the object is unchanged and the camera still points at it, the object
would not be in the image.

#### 4.3.2 Two Viewing APIs

The construction of the model-view matrix for an isometric view is a little unsatis-
fying. Although the approach was intuitive, an interface that requires us to compute
the individual angles before specifying the transformations is a poor one for an ap-
plication program. We can take a different approach to positioning the camera—an
approach that is similar to that used by PHIGS, one of the original standard APIs for
three-dimensional graphics. Our starting point is again the object frame. We describe                       n   VUP
the camera’s position and orientation in this frame. The precise type of image that
we wish to obtain—perspective or parallel—is determined separately by the speciﬁ-                           v
cation of the projection matrix. This second part of the viewing process is often called            VRP
the normalization transformation. We approach this problem as one of a change in
frames. Again, we think of the camera as positioned initially at the origin, pointed
in the negative z-direction. Its desired location is centered at a point called the view-                             u
reference point (VRP; Figure 4.16), whose position is given in the object frame. The        FIGURE 4.16 Camera frame.
set_view_reference_point(x, y, z);
to specify this position. Next, we want to specify the orientation of the camera. We
can divide this speciﬁcation into two parts: speciﬁcation of the view-plane normal
(VPN) and speciﬁcation of the view-up vector (VUP). The VPN (n in Figure 4.16)
gives the orientation of the projection plane or back of the camera. The orientation
210         Chapter 4   Viewing
of a plane is determined by that plane’s normal, and thus part of the API is a function
set_view_plane_normal(nx, ny, nz);
The orientation of the plane does not specify what direction is up from the camera’s
v                     perspective. Given only the VPN, we can rotate the camera with its back in this plane.
VUP             The speciﬁcation of the VUP ﬁxes the camera and is performed by a function such as
set_view_up(vup_x, vup_y, vup_z);
We project the VUP vector on the view plane to obtain the up-direction vector v

*FIGURE 4.17 Determination     (Figure 4.17). Use of the projection allows the user to specify any vector not parallel*

of the view-up vector.        to v, rather than being forced to compute a vector lying in the projection plane. The
vector v is orthogonal to n. We can use the cross product to obtain a third orthogonal
direction u. This new orthogonal coordinate system usually is referred to as either the
viewing-coordinate system or the u-v-n system. With the addition of the VRP, we
have the desired camera frame. The matrix that does the change of frames is the view-
orientation matrix and is equivalent to the viewing component of the model-view
matrix.
We can derive this matrix using rotations and translations in homogeneous
coordinates. We start with the speciﬁcations of the view-reference point,
⎡ ⎤
⎢y⎥
⎢ ⎥
p=⎢ ⎥,
⎣z⎦
the view-plane normal,
⎡ ⎤
⎢n ⎥
⎢ ⎥
n=⎢ y ⎥,
⎣ nz ⎦
and the view-up vector,
⎡       ⎤
⎢v ⎥
⎢       ⎥
vup = ⎢ upy ⎥ .
⎣ vup ⎦
We construct a new frame with the view-reference point as its origin, the view-
plane normal as one coordinate direction, and two other orthogonal directions that
                                                                          4.3 Positioning of the Camera   211
we call u and v. Our default is that the original x, y, z axes become u, v, n, re-
spectively. The view-reference point can be handled through a simple translation
T(−x, −y, −z) from the viewing frame to the original origin. The rest of the model-
view matrix is determined by a rotation so that the model-view matrix V is of the
V = TR.
The direction v must be orthogonal to n; hence,
n . v = 0.
Figure 4.17 shows that v is the projection of vup into the plane formed by n and vup
and thus must be a linear combination of these two vectors,
v = αn + βvup .
If we temporarily ignore the length of the vectors, then we can set β = 1 and solve for
vup . n
α=−
n.n
vup . n
v = vup −             n.
n.n
We can ﬁnd the third orthogonal direction by taking the cross product
u = v × n.
These vectors do not generally have unit length. We can normalize each indepen-
dently, obtaining three unit-length vectors u, v , and n. The matrix
⎡                      ⎤
ux      vx   nx   0
⎢ u       vy   ny   0⎥
⎢                        ⎥
A = ⎢ y                     ⎥
⎣ uz       vz   nz   0⎦
0        0     0     1
is a rotation matrix that orients a vector in the uv n system with respect to the
original system. However, we really want to go in the opposite direction to obtain the
representation of vectors in the original system in the uv n system. We want A −1,
but because A is a rotation matrix, the desired matrix R is
R = A−1 = A T .
212   Chapter 4   Viewing
Finally, multiplying by the translation matrix T, we have
⎡                                   ⎤
ux uy uz −xux − yuy − zuz
⎢                                   ⎥
⎢ vx vy vz −xvx − yvy − zvz ⎥
⎢
V = RT = ⎢                                     ⎥.
                       ⎥
⎣ nx ny nz −xnx − yny − znz ⎦
0 0 0                   1
Note that, in this case, the translation matrix is on the right, whereas in our ﬁrst
derivation it was on the left. One way to interpret this difference is that in our ﬁrst
derivation, we ﬁrst rotated one of the frames and then pushed the frames apart in
a direction represented in the camera frame. In the second derivation, the camera
position was speciﬁed in the object frame. Another way to understand this difference
is to note that the matrices RT and TR have similar forms. The rotation parts of the
product—the upper-left 3 × 3 submatrices—are identical, as are the bottom rows.
The top three elements in the right column differ because the frame of the rotation
affects the translation coefﬁcients in RT and does not affect them in TR. For our
isometric example,
⎡      ⎤
−1
⎢ 1 ⎥
⎢      ⎥
n=⎢         ⎥,
⎣ 1 ⎦
⎡ ⎤
⎢ 1⎥
⎢ ⎥
vup = ⎢ ⎥ .
⎣0⎦
The camera position must be along a diagonal in the original frame. If we use
⎡     ⎤
−d
√ ⎢
3⎢ d ⎥ ⎥
p=     ⎢     ⎥,
3 ⎣ d ⎦
we obtain the same model-view matrix that we derived in Section 4.3.1.

#### 4.3.3 The Look-At Function

The use of the VRP, VPN, and VUP is but one way to provide an API for specifying
the position of a camera. In many situations, a more direct method is appropriate.
Consider the situation illustrated in Figure 4.18. Here a camera is located at a point e
called the eye point, speciﬁed in the object frame, and it is pointed at a second point
a, called the at point. These points determine a VPN and a VRP. The VPN is given by
                                                                             4.3 Positioning of the Camera   213
(atx , aty , atz )
(upx , upy , upz )
(eyex , eyey , eyez )

*FIGURE 4.18 Look-at positioning.*

the vector formed by point subtraction between the eyepoint and the at point,
vpn = a − e,
and normalizing it,
n=          .
|vpn|
The view-reference point is the eye point. Hence, we need only to add the desired
up direction for the camera, and a function to construct the desired matrix LookAt
could be of the form3
mat4 LookAt(point4 eye, point4 at, vec4 up)
mat4 LookAt(GLfloat eyex, GLfloat eyey, GLfloat eyez,
GLfloat atx, GLfloat aty, GLfloat atz,
GLfloat upx, GLfloat upy, GLfloat upz);
Note that once we have computed the vector vpn, we can proceed as we did with
forming the transformation in the previous section. A slightly simpler computation
would be to form a vector perpendicular to n and vup by taking their cross product
and normalizing it,
vup × n
u=                   .
|vup × n|
3. Because we are working in homogeneous coordinates, vup can be a vec4 type if the fourth
component is a zero.
214          Chapter 4       Viewing
z                                                                      z
Roll                    Pitch                  Yaw

*FIGURE 4.19 Roll, pitch, and yaw.*

Finally, we get the normalized projection of the up vector onto the camera plane by
n×u
v=           .
|n × u|
Note that we can use the standard rotations, translations, and scalings as part of
deﬁning our objects. Although these transformations will also alter the model-view
matrix, it is often helpful conceptually to consider the use of LookAt as positioning
the objects and subsequent operations that affect the model-view matrix as position-
ing the camera.
Note that whereas functions, such as LookAt, that position the camera alter
the model-view matrix and are speciﬁed in object coordinates, the functions that we

```cpp
introduce to form the projection matrix will be speciﬁed in eye coordinates.
```


#### 4.3.4 Other Viewing APIs

In many applications, neither of the viewing interfaces that we have presented is
appropriate. Consider a ﬂight-simulation application. The pilot using the simulator
usually uses three angles—roll, pitch, and yaw—to specify her orientation. These
angles are speciﬁed relative to the center of mass of the vehicle and to a coordinate
n                         system aligned along the axes of the vehicle, as shown in Figure 4.19. Hence, the
pilot sees an object in terms of the three angles and of the distance from the object
to the center of mass of her vehicle. A viewing transformation can be constructed
Elevation         (Exercise 4.2) from these speciﬁcations from a translation and three simple rotations.
Viewing in many applications is most naturally speciﬁed in polar—rather than
rectilinear—coordinates. Applications involving objects that rotate about other ob-
Azimuth                     jects ﬁt this category. For example, consider the speciﬁcation of a star in the sky. Its

*FIGURE 4.20 Elevation and          direction from a viewer is given by its elevation and azimuth (Figure 4.20). The eleva-*

azimuth.                           tion is the angle above the plane of the viewer at which the star appears. By deﬁning a
normal at the point that the viewer is located and using this normal to deﬁne a plane,
we deﬁne the elevation, regardless of whether or not the viewer is actually standing
on a plane. We can form two other axes in this plane, creating a viewing-coordinate
system. The azimuth is the angle measured from an axis in this plane to the projec-
                                                                                    4.4 Parallel Projections              215
tion onto the plane of the line between the viewer and the star. The camera can still
be rotated about the direction it is pointed by a twist angle.

### 4.4    PARALLEL PROJECTIONS

A parallel projection is the limit of a perspective projection in which the center of
projection is inﬁnitely far from the objects being viewed, resulting in projectors that
are parallel rather than converging at the center of projection. Equivalently, a parallel
projection is what we would get if we had a telephoto lens with an inﬁnite focal length.
Rather than ﬁrst deriving the equations for a perspective projection and computing
their limiting behavior, we will derive the equations for parallel projections directly
using the fact that we know in advance that the projectors are parallel and point in a
direction of projection.                                                                                            (x, y, z )
(xp, yp, 0)
z=0

#### 4.4.1 Orthogonal Projections                                                                                        x

Orthogonal or orthographic projections are a special case of parallel projections,
in which the projectors are perpendicular to the view plane. In terms of a camera,
orthogonal projections correspond to a camera with a back plane parallel to the lens,        z
which has an inﬁnite focal length. Figure 4.21 shows an orthogonal projection with          FIGURE 4.21 Orthogonal
the projection plane z = 0. As points are projected into this plane, they retain their x    projection.
and y values, and the equations of projection are
xp = x,
yp = y,
zp = 0.
We can write this result using our original homogeneous coordinates:
⎡ ⎤ ⎡                     ⎤⎡ ⎤
xp       1 0 0 0             x
⎢y ⎥ ⎢0 1 0 0⎥⎢y ⎥
⎢ p⎥ ⎢                    ⎥⎢ ⎥
⎢ ⎥=⎢                     ⎥⎢ ⎥.
⎣ zp ⎦ ⎣ 0 0 0 0 ⎦ ⎣ z ⎦
1        0 0 0 1             1
To prepare ourselves for a more general orthogonal projection, we can write this
q = MIp,
⎡ ⎤
⎢y⎥
⎢ ⎥
p=⎢ ⎥,
⎣z⎦
216   Chapter 4   Viewing
I is a 4 × 4 identity matrix, and
⎡              ⎤
1 0 0 0
⎢0 1 0 0⎥
⎢              ⎥
M=⎢                   ⎥.
⎣0 0 0 0⎦
0 0 0 1
The projection described by M is carried out by the hardware after the vertex shader.
Hence, only those objects inside the cube of side length 2 centered at the origin will
be projected and possibly visible. If we want to change which objects are visible, we
can replace the identity matrix by a transformation N that we can carry out either in
the application or in the vertex shader, which will give us control over the clipping
volume. For example, if we replace I with a scaling matrix, we can see more or fewer
objects.

#### 4.4.2 Parallel Viewing with OpenGL

We will focus on a single orthogonal viewing function in which the view volume is
a right parallelepiped, as shown in Figure 4.22. The sides of the clipping volume are
x = right ,
x = left ,
y = top,
y = bottom.
The near (front) clipping plane is located a distance near from the origin, and the
far (back) clipping plane is at a distance far from the origin. All these values are in
camera coordinates. We will derive a function
mat4 Ortho(GLfloat left, GLfloat right, GLfloat bottom, GLfloat top,
GLfloat near, GLfloat far)
which will form the projection matrix N.4
Although mathematically we get a parallel view by moving the camera to inﬁn-
ity, because the projectors are parallel, we can slide this camera in the direction of
projection without changing the projection. Consequently, it is helpful to think of an
orthogonal camera located initially at the origin in camera coordinates with the view
4. Users of Microsoft Windows may have to change the identiﬁers near and far because far is a
reserved word in Visual C++.
                                                                                 4.4 Parallel Projections   217
y                               (right, top, – far)
z = – far
z = – near
(left, bottom, – near )

*FIGURE 4.22 Orthographic viewing.*

x = ±1,
y = ±1,
z = ±1
as the default behavior. Equivalently, we are applying an identity projection matrix
for N. We will derive a nonidentity matrix N using translation and scaling that will
transform vertices in camera coordinates to ﬁt inside the default view volume, a
process called projection normalization. This matrix is what will be produced by
Ortho. Note that we are forced to take this approach because the ﬁnal projection
carried out by the GPU is ﬁxed. Nevertheless, the normalization process is efﬁcient
and will allow us to carry out parallel and perspective projections with the same
pipeline.

#### 4.4.3 Projection Normalization

When we introduced projection in Chapter 1 and looked at classical projection earlier
in this chapter, we viewed it as a technique that took the speciﬁcation of points
in three dimensions and mapped them to points on a two-dimensional projection
surface. Such a transformation is not invertible, because all points along a projector
map into the same point on the projection surface.
In computer graphics systems, we adopt a slightly different approach. First, we
work in four dimensions using homogeneous coordinates. Second, we retain depth
information—distance along a projector—as long as possible so that we can do
hidden-surface removal later in the pipeline. Third, we use projection normalization,
to convert all projections into orthogonal projections by ﬁrst distorting the objects
such that the orthogonal projection of the distorted objects is the same as the de-
sired projection of the original objects. This technique is shown in Figure 4.23. The
concatenation of the normalization matrix, which carries out the distortion and the
218   Chapter 4   Viewing
(a)                            (b)

*FIGURE 4.23 Predistortion of objects. (a) Perspective view. (b) Ortho-*

graphic projection of distorted object.
(normalize)           projection

*FIGURE 4.24 Normalization transformation.*

simple orthogonal projection matrix from Section 4.4.2, as shown in Figure 4.24,
yields a homogeneous coordinate matrix that produces the desired projection.
One advantage of this approach is that we can design the normalization matrix
so that view volume is distorted into the canonical view volume, which is the cube
deﬁned by the planes
x ± 1,
y ± 1,
z ± 1.
Besides the advantage of having both perspective and parallel views supported by the
same pipeline by loading in the proper normalization matrix, the canonical view vol-
ume simpliﬁes the clipping process because the sides are aligned with the coordinate
axes.
The normalization process deﬁnes what most systems call the projection matrix.
The projection matrix brings objects into four-dimensional clip coordinates, and
the subsequent perspective division converts vertices to a representation in three-
dimensional normalized device coordinates. Values in normalized device coordinates
are later mapped to window coordinates by the viewport transformation. Here we are
concerned with the ﬁrst step—deriving the projection matrix.
                                                                                   4.4 Parallel Projections   219

#### 4.4.4 Orthogonal-Projection Matrices

Although parallel viewing is a special case of perspective viewing, we start with or-
thogonal parallel viewing and later extend the normalization technique to perspective
viewing.
In OpenGL, the default projection matrix is an identity matrix, or equivalently,
what we would get from the following code:
mat4 N = Ortho(-1.0, 1.0, -1.0, 1.0, -1.0, 1.0);
The view volume is in fact the canonical view volume. Points within the cube deﬁned
by the sides x ± 1, y ± 1, and z ± 1 are mapped to the same cube. Points outside this
cube remain outside the cube. As trivial as this observation may seem, it indicates that
we can get the desired projection matrix for the general orthogonal view by ﬁnding a
matrix that maps the right parallelepiped speciﬁed by Ortho to this same cube.
Before we do so, recall that the last two parameters in Ortho are distances to
the near and far planes measured from a camera at the origin pointed in the negative
z-direction. Thus, the near plane is at z = 1.0, which is behind the camera, and the
far plane is at z = −1.0, which is in front of the camera. Although the projectors are
parallel and an orthographic projection is conceptually akin to having a camera with
a long telephoto lens located far from the objects, the importance of the near and far
distances in Ortho is that they determine which objects are clipped out.
Now suppose that, instead, we set the Ortho parameters by the following func-
tion call:
mat4 N = Ortho(left, right, bottom, top, near, far);
We now have speciﬁed a right parallelepiped view volume whose right side (relative
to the camera) is the plane x = left, whose left side is the plane x = right, whose top
is the plane y = top, and whose bottom is the plane y = bottom. The front is the near
clipping plane z = −near, and the back is the far clipping plane z = −far. The pro-
jection matrix that OpenGL sets up is the matrix that transforms this volume to the
cube centered at the origin with sides of length 2, which is shown in Figure 4.25. This
matrix converts the vertices that specify our objects to vertices within this canonical
view volume, by scaling and translating them. Consequently, vertices are transformed
such that vertices within the speciﬁed view volume are transformed to vertices within
the canonical view volume, and vertices outside the speciﬁed view volume are trans-
formed to vertices outside the canonical view volume. Putting everything together,
we see that the projection matrix is determined by the type of view and the view vol-
ume speciﬁed in Ortho, and that these speciﬁcations are relative to the camera. The
positioning and orientation of the camera are determined by the model-view matrix.
These two matrices are concatenated together, and objects have their vertices trans-
formed by this matrix product.
We can use our knowledge of afﬁne transformations to ﬁnd this projection ma-
trix. There are two tasks that we need to do. First, we must move the center of the
220   Chapter 4   Viewing
(right, top, –far)
(1, 1, –1)
(left, bottom, –near)                      (–1, –1, 1)

*FIGURE 4.25 Mapping a view volume to the canonical view volume.*


*FIGURE 4.26 Affine transformations for normalization.*

speciﬁed view volume to the center of the canonical view volume (the origin) by do-
ing a translation. Second, we must scale the sides of the speciﬁed view volume to each
have a length of 2 (see Figure 4.25). Hence, the two transformations are
T = T(−(right + left)/2, −(top + bottom)/2, +(far + near)/2)
S = S(2/(right − left), 2/(top − bottom), 2/(near − far)),
and they can be concatenated together (Figure 4.26) to form the projection matrix
⎡                                                    left+right   ⎤
right−left         0               0           − right−left
⎢                                                                 ⎥
⎢          0             2
0          − top−bottom ⎥
top+bottom
⎢                    top−bottom                                   ⎥
N = ST = ⎢                                                                 ⎥.
⎢                                     − far−near
2           far+near ⎥
− far−near ⎦
⎣          0              0
0              0               0                  1
This matrix maps the near clipping plane, z = −near , to the plane z = −1 and the
far clipping plane, z = −far , to the plane z = 1. Because the camera is pointing in the
negative z-direction, the projectors are directed from inﬁnity on the negative z-axis
toward the origin.

#### 4.4.5 Oblique Projections

Using Ortho, we have only a limited class of parallel projections—namely, only those
for which the projectors are orthogonal to the projection plane. As we saw earlier in
                                                                                          4.4 Parallel Projections               221
(x, y, z )

*FIGURE 4.28 Oblique clipping volume.*

(xp, yp, zp )
this chapter, oblique parallel projections are useful in many ﬁelds.5 We could develop                                               x
an oblique projection matrix directly; instead, however, we follow the process that
we used for the general orthogonal projection. We convert the desired projection to
a canonical orthogonal projection of distorted objects.
An oblique projection can be characterized by the angle that the projectors make
with the projection plane, as shown in Figure 4.27. In APIs that support general par-             FIGURE 4.27 Oblique
allel viewing, the view volume for an oblique projection has the near and far clipping            projection.
planes parallel to the view plane, and the right, left, top, and bottom planes parallel
to the direction of projection, as shown in Figure 4.28. We can derive the equations
for oblique projections by considering the top and side views in Figure 4.29, which
shows a projector and the projection plane z = 0. The angles θ and φ characterize the
degree of obliqueness. In drafting, projections such as the cavalier and cabinet pro-
jections are determined by speciﬁc values of these angles. However, these angles are
not the only possible interface (see Exercises 4.9 and 4.10).
If we consider the top view, we can ﬁnd xp by noting that
tan θ =          ,
xp − x
xp = x + z cot θ .
Likewise,
yp = y + z cot φ.
5. Note that without oblique projections we cannot draw coordinate axes in the way that we have
been doing in this book (see Exercise 4.15).
222   Chapter 4   Viewing
(x, z)

(z, y )
(xp, 0)
(0, yp )
(a)                        (b)

*FIGURE 4.29 Oblique projection. (a) Top view. (b) Side view.*

Using the equation for the projection plane
zp = 0,
we can write these results in terms of a homogeneous-coordinate matrix
⎡                   ⎤
1 0 cot θ 0
⎢ 0 1 cot φ 0 ⎥
⎢                   ⎥
P=⎢                     ⎥.
⎣0 0        0     0⎦
0 0       0     1
Following our strategy of the previous example, we can break P into the product
⎡              ⎤⎡                    ⎤
1 0 0 0           1 0 cot θ 0
⎢ 0 1 0 0 ⎥ ⎢ 0 1 cot φ 0 ⎥
⎢              ⎥⎢                    ⎥
P = Morth H(θ , φ) = ⎢              ⎥⎢                    ⎥,
⎣0 0 0 0⎦⎣0 0                1     0⎦
0 0 0 1           0 0      0     1
where H(θ , φ) is a shearing matrix. Thus, we can implement an oblique projection
by ﬁrst doing a shear of the objects by H(θ , φ) and then doing an orthographic
projection. Figure 4.30 shows the effect of H(θ , φ) on an object—a cube—inside an
oblique view volume. The sides of the clipping volume become orthogonal to the
view plane, but the sides of the cube become oblique as they are affected by the same
shear transformation. However, the orthographic projection of the distorted cube is
identical to the oblique projection of the undistorted cube.
We are not ﬁnished, because the view volume created by the shear is not our
canonical view volume. We have to apply the same scaling and translation matrices
that we used in Section 4.4.4. Hence, the transformation
                                                                                       4.4 Parallel Projections   223

*FIGURE 4.30 Effect of shear transformation.*

⎡                                                   right+left   ⎤
right−left       0                0        − right−left
⎢                                                            ⎥
⎢        0            2
0        − top−bottom ⎥
top+bottom
⎢                 top−bottom                                 ⎥
ST = ⎢                                                            ⎥
⎢                                  − far−near
2         far+near ⎥
− near−far ⎦
⎣        0            0
0            0                0                1
must be inserted after the shear and before the ﬁnal orthographic projection, so the
ﬁnal matrix is
N = Morth STH.
The values of left, right, bottom, and top are the vertices of the right parallelepiped
view volume created by the shear. These values depend on how the sides of the
original view volume are communicated through the application program; they may
have to be determined from the results of the shear to the corners of the original view
volume. One way to do this calculation is shown in Figure 4.29.
The speciﬁcation for an oblique projection can be through the angles θ and ψ
that projectors make with the projection plane. The parameters near and far are not
changed by the shear. However, the x and y values of where the sides of the view
volume intersect the near plane are changed by the shear and become left , right , top,
and bottom. If these points of intersection are (xmin , near), (xmax , near), (ymin , near),
and (ymax , near), then our derivation of shear in Chapter 3 yields the relationships
224   Chapter 4   Viewing
left = xmin − near ∗ cot θ ,
right = xmax − near ∗ cot θ ,
top = ymax − near ∗ cot φ,
bottom = ymin − near ∗ cot φ.

#### 4.4.6 An Interactive Viewer

In this section, we extend the rotating cube program to include both the model-view
matrix and an orthogonal projection matrix whose parameters can be set interac-
tively. As in our previous examples with the cube, we have choices as to where to
apply our transformations. In this example, we will send the model-view and pro-
jection matrices to the vertex shader. Because the model-view matrix can be used to
both transform an object and position the camera, in this example we will not use the
mouse function and instead focus on camera position and the orthogonal projection.
It will be straightforward to bring back the mouse and idle callbacks later to restart
the rotation of the cube.
The colored cube is centered at the origin in object coordinates so wherever we
place the camera, the at point is at the origin. Let’s position the camera in polar
coordinates so the eye point has coordinates
⎡               ⎤
r cos θ
eye = ⎣ r sin θ cos φ ⎦ ,
r sin θ sin φ
where the radius r is the distance from the origin. We can let the up direction be the y-
direction in object coordinates. These values specify a model-view matrix through the
LookAt function. In this example, we will send both a model-view and a projection
matrix to the vertex shader with the following display callback:

```cpp
void display( )
{
```

glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
eye[0] = radius*cos(theta);
eye[1] = radius*sin(theta)*cos(phi);
eye[2] = radius*sin(theta)*sin(phi);
model_view = LookAt(eye, at, up);
projection = Ortho(left, right, bottom, top, near, far);
glUniformMatrix4fv(matrix_loc, 1, GL_TRUE, model_view);
glUniformMatrix4fv(projection_loc, 1, GL_TRUE, projection);
glDrawArrays(GL_TRIANGLES, 0, N);
glutSwapBuffers();

```cpp
}
```

The corresponding vertex shader is
                                                                                4.4 Parallel Projections   225
in vec4 vPosition;
in vec4 vColor;
out vec4 color;
uniform mat4 model_view;
uniform mat4 projection;

```cpp
void main()
{
```

gl_Position = projection*model_view*vPosition;
color = vColor;

```cpp
}
```

We can use the keyboard callback to alter the projection matrix and the camera
position. The position of the camera is controlled by the r, o, and p keys, and the
sides of the viewing parallelpiped by the x, y, and z keys.

```cpp
void mykey(unsigned char key, int mousex, int mousey)
{
float dr = M_PI/180.0*5.0; // 5 degrees in radians
```

if(key==’q’||key==’Q’) exit(0);
if(key == ’x’) {left *= 1.1; right *= 1.1;}
if(key == ’X’) {left *= 0.9; right *= 0.9;}
if(key == ’y’) {bottom *= 1.1; top *= 1.1;}
if(key == ’Y’) {bottom *= 0.9; top *= 0.9;}
if(key == ’z’) {near *= 1.1; far *= 1.1;}
if(key == ’Z’) {near *= 0.9; far *= 0.9;}
if(key == ’r’) radius *= 2.0;
if(key == ’R’) radius *= 0.5;
if(key == ’o’) theta += dr;
if(key == ’O’) theta -= dr;
if(key == ’p’) phi += dr;
if(key == ’P’) phi -= dr;
glutPostRedisplay();

```cpp
}
```

Note that as we move the camera around, the size of the image of the cube does
not change, which is a consequence of using an orthogonal projection. However,
depending on the radius, some or even all of the cube can be clipped out. This
behavior is a consequence of the parameters in Ortho being measured relative to
the camera. Hence, if we move the camera back by increasing the radius but leave
the near and far distances unchanged, ﬁrst the back of the cube will be clipped out.
Eventually, as the radius becomes larger, the entire cube will be clipped out.
Now consider what happens as we change the parameters in Ortho. As we in-
crease right and left, the cube elongates in the x-direction. A similar phenomenon
occurs when we increase bottom and top in the y-direction. Although this distortion
of the cube’s image may be annoying, it is a consequence of using an x − y rectan-
gle in Ortho that is not square. This rectangle is mapped to the full viewport, which
226   Chapter 4   Viewing
has been unchanged. We can alter the program so that we increase or decrease all of
left, right, bottom, and top simultaneously or we can alter the viewport as part
of any change to Ortho (see Exercise 4.28).

### 4.5    PERSPECTIVE PROJECTIONS

We now turn to perspective projections, which are what we get with a camera whose
lens has a ﬁnite focal length or, in terms of our synthetic camera model, the center of
projection is ﬁnite.
As with parallel projections, we will separate perspective viewing into two parts:
the positioning of the camera and the projection. Positioning will be done the same
way, and we can use the LookAt function. The projection part is equivalent to select-
ing a lens for the camera. As we saw in Chapter 1, it is the combination of the lens and
the size of the ﬁlm (or of the back of the camera) that determines how much of the
world in front of a camera appears in the image. In computer graphics, we make an
equivalent choice when we select the type of projection and the viewing parameters.
With a physical camera, a wide-angle lens gives the most dramatic perspectives,
with objects near the camera appearing large compared to objects far from the lens.
A telephoto lens gives an image that appears ﬂat and is close to a parallel view.
First, we consider the mathematics for a simple projection. We can extend our
use of homogeneous coordinates to the projection process, which allows us to char-
acterize a particular projection with a 4 × 4 matrix.

#### 4.5.1 Simple Perspective Projections

Suppose that we are in the camera frame with the camera located at the origin,
pointed in the negative z-direction. Figure 4.31 shows two possibilities. In Fig-
ure 4.31(a), the back of the camera is orthogonal to the z-direction and is parallel
to the lens. This conﬁguration corresponds to most physical situations, including
those of the human visual system and of simple cameras. The situation shown in
Figure 4.31(b) is more general; the back of the camera can have any orientation with
respect to the front. We consider the ﬁrst case in detail because it is simpler. How-
ever, the derivation of the general result follows the same steps and should be a direct
exercise (Exercise 4.6).
As we saw in Chapter 1, we can place the projection plane in front of the center
of projection. If we do so for the conﬁguration of Figure 4.32(a), we get the views
shown in Figure 4.33. A point in space (x, y, z) is projected along a projector into the
point (xp , yp , zp). All projectors pass through the origin, and, because the projection
plane is perpendicular to the z-axis,
zp = d.
Because the camera is pointing in the negative z-direction, the projection plane is in
the negative half-space z < 0, and the value of d is negative.
                                                                                              4.5 Perspective Projections   227
z                               z
x                                        x
(a)                              (b)

*FIGURE 4.31 Two cameras. (a) Back parallel to front. (b) Back not parallel*

to front.
y                                                        (x, z )
(xp , d )                                              (y, z )
(x, y, z )
z=d           (yp , d )
(xp , yp , zp )                                            x     z
z=d
z                                             z
(a)                                                (b)                        (c)

*FIGURE 4.32 Three views of perspective projection. (a) Three-dimensional view. (b) Top view. (c) Side view.*

From the top view shown in Figure 4.32(b), we see two similar triangles whose
tangents must be the same. Hence,
= ,
xp =       .
z/d
Using the side view shown in Figure 4.32(c), we obtain a similar result for yp:
yp =       .
z/d
These equations are nonlinear. The division by z describes nonuniform foreshort-
ening: The images of objects farther from the center of projection are reduced in size
(diminution) compared to the images of objects closer to the COP.
228   Chapter 4   Viewing
Model-view            Projection

*FIGURE 4.33 Projection pipeline.*

We can look at the projection process as a transformation that takes points
(x, y, z) to other points (xp , yp , zp). Although this perspective transformation pre-
serves lines, it is not afﬁne. It is also irreversible. Because all points along a projector
project into the same point, we cannot recover a point from its projection. In Sec-
tions 4.7 and 4.8, we will develop an invertible variant of the projection transforma-
tion that preserves distances that are needed for hidden-surface removal.
We can extend our use of homogeneous coordinates to handle projections. When
we introduced homogeneous coordinates, we represented a point in three dimensions
(x, y, z) by the point (x, y, z , 1) in four dimensions. Suppose that, instead, we replace
(x, y, z) by the four-dimensional point
⎡      ⎤
⎢ wy ⎥
⎢      ⎥
p=⎢         ⎥.
⎣ wz ⎦
As long as w = 0, we can recover the three-dimensional point from its four-
dimensional representation by dividing the ﬁrst three components by w. In this new
homogeneous-coordinate form, points in three dimensions become lines through
the origin in four dimensions. Transformations are again represented by 4 × 4 ma-
trices, but now the ﬁnal row of the matrix can be altered—and thus w can be changed
by such a transformation.
Obviously, we would prefer to keep w = 1 to avoid the divisions otherwise nec-
essary to recover the three-dimensional point. However, by allowing w to change,
we can represent a larger class of transformations, including perspective projections.
⎡                ⎤
1 0 0 0
⎢0 1 0 0⎥
⎢                ⎥
M=⎢                    ⎥.
⎣0 0 1 0⎦
0 0 1/d 0
The matrix M transforms the point
⎡ ⎤
⎢y⎥
⎢ ⎥
p=⎢ ⎥
⎣z⎦
                                                               4.6 Perspective Projections with OpenGL   229
⎡     ⎤
⎢ y ⎥
⎢     ⎥
q=⎢        ⎥.
⎣ z ⎦
z/d
At ﬁrst glance, q may not seem sensible; however, when we remember that we have
to divide the ﬁrst three components by the fourth to return to our original three-
dimensional space, we obtain the results
xp =       ,
z/d
yp =       ,
z/d
zp =       = d,
z/d
which are the equations for a simple perspective projection. In homogeneous coor-
dinates, dividing q by its w component replaces q by the equivalent point
⎡ x ⎤ ⎡ ⎤
z/d        xp
⎢ y ⎥ ⎢y ⎥
⎢     ⎥ ⎢ ⎥
q  = ⎢ z/d ⎥ = ⎢ p ⎥ .
⎣ d ⎦ ⎣ zp ⎦
1          1
We have shown that we can do at least a simple perspective projection, by deﬁn-
ing a 4 × 4 projection matrix that we apply after the model-view matrix. However,
we must perform a perspective division at the end. This division can be made a part
of the pipeline, as shown in Figure 4.33.

### 4.6     PERSPECTIVE PROJECTIONS WITH OPENGL

The projections that we developed in Section 4.4 did not take into account the proper-
ties of the camera—the focal length of its lens or the size of the ﬁlm plane. Figure 4.34
shows the angle of view for a simple pinhole camera, like the one that we discussed in

## Chapter 1. Only those objects that ﬁt within the angle of view of the camera appear in

the image. If the back of the camera is rectangular, only objects within a semi-inﬁnite
pyramid—the view volume—whose apex is at the COP can appear in the image. Ob-
jects not within the view volume are said to be clipped out of the scene. Hence, our
description of simple projections has been incomplete; we did not include the effects
of clipping.
With most graphics APIs, the application program speciﬁes clipping parameters
through the speciﬁcation of a projection. The inﬁnite pyramid in Figure 4.34 becomes
230   Chapter 4   Viewing

*FIGURE 4.34 Specification of a view volume.*


*FIGURE 4.35 Front and back clipping planes.*

a ﬁnite clipping volume by adding front and back clipping planes, in addition to the
angle of view, as shown in Figure 4.35. The resulting view volume is a frustum—a
truncated pyramid. We have ﬁxed only one parameter by specifying that the COP is
at the origin in the camera frame. In principle, we should be able to specify each of
the six sides of the frustum to have almost any orientation. If we did so, however,
we would make it difﬁcult to specify a view in the application and complicate the
implementation. In practice, we rarely need this ﬂexibility, and usually we can get by
with only two perspective viewing functions. Other APIs differ in their function calls
but incorporate similar restrictions.

#### 4.6.1 Perspective Functions

We will develop two functions for specifying perspective views and one for specifying
parallel views. Alternatively, we can form the projection matrix directly, either by
                                                                      4.6 Perspective Projections with OpenGL   231
y                z = –far
z = –near
(right, top, –near)
(left, bottom, –near)

*FIGURE 4.36 Specification of a frustum.*

loading it or by applying rotations, translations, and scalings to an initial identity
matrix. We can specify a perspective camera view by the function
mat4 Frustum(GLfloat left, GLfloat right, GLfloat bottom, GLfloat top,
GLfloat near, GLfloat far);
whose parameters are similar to those in Ortho. These parameters are shown in Fig-
ure 4.36 in the camera frame. The near and far distances are measured from the COP
(the origin in eye coordinates) to front and back clipping planes, both of which are
parallel to the plane z = 0. Because the camera is pointing in the negative z-direction,
the front (near) clipping plane is the plane z = −near and the back (far) clipping
plane is the plane z = −far. The left, right, top, and bottom values are measured in
the near (front clipping) plane. The plane x = left is to the left of the camera as viewed
from the COP in the direction the camera is pointing. Similar statements hold for
right, bottom, and top. Although in virtually all applications far > near > 0, as
long as near = far , the resulting projection matrix is valid, although objects behind
the center of projection—the origin—will be inverted in the image if they lie between
the near and far planes.
Note that these speciﬁcations do not have to be symmetric with respect to the
z-axis and that the resulting frustum also does not have to be symmetric (a right
frustum). In Section 4.7, we show how the projection matrix for this projection can
be derived from the simple perspective-projection matrix.
In many applications, it is natural to specify the angle of view, or ﬁeld of view.
However, if the projection plane is rectangular, rather than square, then we see a
different angle of view in the top and side views (Figure 4.37). The angle fovy is the
angle between the top and bottom planes of the clipping volume. The function
mat4 Perspective(GLfloat fovy, GLfloat aspect, GLfloat near, GLfloat far);
232   Chapter 4   Viewing

*FIGURE 4.37 Specification using the field of view.*

allows us to specify the angle of view in the up (y) direction, as well as the aspect
ratio—width divided by height—of the projection plane. The near and far planes are
speciﬁed as in Frustum.

### 4.7    PERSPECTIVE-PROJECTION MATRICES

For perspective projections, we follow a path similar to the one that we used for
parallel projections: We ﬁnd a transformation that allows us, by distorting the vertices
of our objects, to do a simple canonical projection to obtain the desired image.
Our ﬁrst step is to decide what this canonical viewing volume should be. We then

```cpp
introduce a new transformation, the perspective-normalization transformation,
```

that converts a perspective projection to an orthogonal projection. Finally, we derive
the perspective-projection matrix we will use in OpenGL.

#### 4.7.1 Perspective Normalization

In Section 4.5, we introduced a simple perspective-projection matrix. For the pro-
jection plane at z = −1 and the center of the projection at the origin, the projection
⎡               ⎤
1 0 0 0
⎢0 1 0 0⎥
⎢               ⎥
M=⎢                   ⎥.
⎣0 0 1 0⎦
0 0 −1 0
To form an image, we also need to specify a clipping volume. Suppose that we ﬁx
the angle of view at 90 degrees by making the sides of the viewing volume intersect
the projection plane at a 45-degree angle. Equivalently, the view volume is the semi-
inﬁnite view pyramid formed by the planes
                                                                  4.7 Perspective-Projection Matrices   233
z = z min
(1, 1, 1)
(1, 1, 1)

*FIGURE 4.38 Simple perspective projection.*

x = ±z ,
y = ±z ,
shown in Figure 4.38. We can make the volume ﬁnite by specifying the near plane to
be z = −near and the far plane to be z = −far, where both near and far, the distances
from the center of projection to the near and far planes, satisfy
0 < near < far.
⎡               ⎤
1 0 0 0
⎢0 1 0 0 ⎥
⎢               ⎥
N=⎢                 ⎥,
⎣0 0 α β ⎦
0 0 −1 0
which is similar to M but is nonsingular. For now, we leave α and β unspeciﬁed (but
nonzero). If we apply N to the homogeneous-coordinate point p = [ x y z 1 ]T ,
we obtain the new point q = [ x  y  z  w  ]T , where
x  = x,
y  = y,
z  = αz + β ,
w  = −z.
After dividing by w , we have the three-dimensional point
234   Chapter 4   Viewing
x  = − ,
y  = − ,

  β
z =− α+             .
If we apply an orthographic projection along the z-axis to N, we obtain the matrix
⎡               ⎤
1 0 0 0
⎢0 1 0 0⎥
⎢               ⎥
Morth N = ⎢                ⎥,
⎣0 0 0 0⎦
0 0 −1 0
which is a simple perspective-projection matrix, and the projection of the arbitrary
⎡     ⎤
⎢ y ⎥
⎢     ⎥
p = Morth Np = ⎢     ⎥.
⎣ 0 ⎦
−z
After we do the perspective division, we obtain the desired values for xp and yp:
xp = − ,
yp = − .
We have shown that we can apply a transformation N to points, and after an or-
thogonal projection, we obtain the same result as we would have for a perspective
projection. This process is similar to how we converted oblique projections to or-
thogonal projections by ﬁrst shearing the objects.
The matrix N is nonsingular and transforms the original viewing volume into a
new volume. We choose α and β such that the new volume is the canonical clipping
volume. Consider the sides
x = ±z.
They are transformed by x  = −x/z to the planes
x  = ±1.
Likewise, the sides y = ±z are transformed to
y  = ±1.
                                                                    4.7 Perspective-Projection Matrices   235
z=1
z = far
x = 1              x=1
z = near
COP                                     z = 1

*FIGURE 4.39 Perspective normalization of view volume.*

The front clipping plane z = −near is transformed to the plane

β
z  = − α −                  .
Finally, the far plane z = −far is transformed to the plane

β
z  = − α −              .
near + far
α=−                ,
near − far
2 ∗ near ∗ far
β =−                   ,
near − far
then the plane z = −near is mapped to the plane z  = −1, the plane z = −far is
mapped to the plane z  = 1, and we have our canonical clipping volume. Figure 4.39
shows this transformation and the distortion to a cube within the volume. Thus, N
has transformed the viewing frustum to a right parallelepiped, and an orthographic
projection in the transformed volume yields the same image as does the perspective
projection. The matrix N is called the perspective-normalization matrix. The map-

     β
z =− α+
is nonlinear but preserves the ordering of depths. Thus, if z1 and z2 are the depths of
two points within the original viewing volume and
z1 > z 2 ,
236   Chapter 4   Viewing
z = –far
(right, top, –near)
(left, bottom, –near)

*FIGURE 4.40 OpenGL perspective.*

z1 > z2 .
Consequently, hidden-surface removal works in the normalized volume, although
the nonlinearity of the transformation can cause numerical problems because the
depth buffer has a limited depth resolution. Note that although the original projec-
tion plane we placed at z = −1 has been transformed by N to the plane z  = β − α,
there is little consequence to this result because we follow N by an orthographic pro-
jection.
Although we have shown that both perspective and parallel transformations
can be converted to orthographic transformations, the effects of this conversion are
greatest in implementation. As long as we can put a carefully chosen projection
matrix in the pipeline before the vertices are deﬁned, we need only one viewing
pipeline for all possible views. In Chapter 6, where we discuss implementation in
detail, we will see how converting all view volumes to right parallelepipeds by our
normalization process simpliﬁes both clipping and hidden-surface removal.

#### 4.7.2 OpenGL Perspective Transformations

The function Frustum does not restrict the view volume to a symmetric (or right)
frustum. The parameters are as shown in Figure 4.40. We can form the perspective
matrix by ﬁrst converting this frustum to the symmetric frustum with 45-degree
sides (see Figure 4.39). The process is similar to the conversion of an oblique parallel
view to an orthogonal view. First, we do a shear to convert the asymmetric frustum
to a symmetric one. Figure 4.40 shows the desired transformation. The shear an-
gle is determined by our desire to skew (shear) the point ((left + right)/2, (top +
bottom)/2, −near) to (0, 0, −near). The required shear matrix is
                             
left + right                  top + bottom
H(θ , φ) = H cot−1                         , cot−1                        .
−2near                         −2near
The resulting frustum is described by the planes
                                                                     4.7 Perspective-Projection Matrices   237
right − left
x=±                 ,
−2 ∗ near
top − bottom
y=±                 ,
−2 ∗ near
z = −near ,
z = −far.
The next step is to scale the sides of this frustum to
x = ±z,
y = ±z,
without changing either the near plane or the far plane. The required scaling matrix
is S(−2 ∗ near/(right − left), −2 ∗ near/(top − bottom), 1). Note that this transfor-
mation is determined uniquely without reference to the location of the far plane
z = −far because in three dimensions, an afﬁne transformation is determined by the
results of the transformation on four points. In this case, these points are the four
vertices where the sides of the frustum intersect the near plane.
To get the far plane to the plane z = −1 and the near plane to z = 1 after applying
a projection normalization, we use the projection-normalization matrix N:
⎡             ⎤
1       0 0 0
⎢0        1 0 0⎥
⎢                ⎥
N=⎢                ⎥,
⎣0        0 α β⎦
0       0 −1 0
with α and β as in Section 4.7.1. The resulting projection matrix is in terms of the
near and far distances,
⎡ 2∗near                  right+left            ⎤
right−left     0       right−left      0
⎢                                               ⎥
⎢      0        2∗near   top+bottom
0     ⎥
⎢             top−bottom top−bottom             ⎥
P = NSH = ⎢                                                  ⎥.
⎢                           far+near
− far−near
−2far∗near ⎥
⎣      0          0                   far−near ⎦
0          0           −1            0
We obtain the projection matrix corresponding to Persective(fovy,
aspect, near, far) by using symmetry in P so
left = −right ,
bottom = −top,
238   Chapter 4   Viewing
top = near ∗ tan(fovy),
right = top ∗ aspect ,
⎡ near                                   ⎤
right     0        0           0
⎢                                            ⎥
⎢ 0            near
0           0       ⎥
⎢               top                          ⎥
P = NSH = ⎢                                            ⎥.
⎢ 0             0
−far+near   −2far∗near ⎥
⎣                     far−near     far−near ⎦
0             0       −1             0

#### 4.7.3 Perspective Example

We have to make almost no changes to our previous example to go from an orthogo-
nal projection to a perspective projection. We can substitute Frustum for Ortho and
the parameters are the same. However, for a perspective view we should have
far > near > 0.
Note that if we want to see the foreshortening we associate with perspective views, we
can either move the cube off the z-axis or add additional cubes to the right or left.
We can add the perspective division to our vertex shader, so it becomes
in vec4 vPosition;
in vec4 vColor;
out vec4 color;
uniform mat4 model_view;
uniform mat4 projection;

```cpp
void main()
{
```

gl_Position = projection*model_view*vPosition/vPosition.w;
color = vColor;

```cpp
}
```

The full program is in Appendix A.

### 4.8    HIDDEN-SURFACE REMOVAL

Before introducing a few additional examples and extensions of viewing, we need to
deepen our understanding of the hidden-surface–removal process. Let’s start with the
cube we have been using in our examples. When we look at a cube that has opaque
sides, depending on its orientation, we see only one, two, or three front-facing sides.
From the perspective of our basic viewing model, we can say that we see only these
faces because they block the projectors from reaching any other surfaces.
                                                                               4.8 Hidden-Surface Removal   239
From the perspective of computer graphics, however, all six faces of the cube
have been speciﬁed and travel down the graphics pipeline; thus, the graphics sys-
tem must be careful about which surfaces it displays. Conceptually, we seek algo-
rithms that either remove those surfaces that should not be visible to the viewer,
called hidden-surface–removal algorithms, or ﬁnd which surfaces are visible, called
visible-surface algorithms. There are many approaches to the problem, several of
which we investigate in Chapter 6. OpenGL has a particular algorithm associated with
it, the z-buffer algorithm, to which we can interface through three function calls.
Hence, we introduce that algorithm here, and we return to the topic in Chapter 6.
Hidden-surface–removal algorithms can be divided into two broad classes.
Object-space algorithms attempt to order the surfaces of the objects in the scene
such that rendering surfaces in a particular order provides the correct image. For
example, for our cube, if we were to render the back-facing surfaces ﬁrst, we could
“paint” over them with the front surfaces and would produce the correct image. This

```cpp
class of algorithms does not work well with pipeline architectures in which objects
```

are passed down the pipeline in an arbitrary order. In order to decide on a proper
order in which to render the objects, the graphics system must have all the objects
available so it can sort them into the desired back-to-front order.
Image-space algorithms work as part of the projection process and seek to deter-
mine the relationship among object points on each projector. The z-buffer algorithm
is of the latter type and ﬁts in well with the rendering pipeline in most graphics sys-
tems because we can save partial information as each object is rendered.
The basic idea of the z-buffer algorithm is shown in Figure 4.41. A projector from
the COP passes through two surfaces. Because the circle is closer to the viewer than to
the triangle, it is the circle’s color that determines the color placed in the color buffer
at the location corresponding to where the projector pierces the projection plane. The
difﬁculty is determining how we can make this idea work regardless of the order in
which the triangle and the circle pass through the pipeline.
z1
z2

*FIGURE 4.41 The z-buffer algorithm.*

240   Chapter 4   Viewing
Let’s assume that all the objects are polygons. If, as the polygons are rasterized,
we can keep track of the distance from the COP or the projection plane to the closest
point on each projector that already has been rendered, then we can update this
information as successive polygons are projected and ﬁlled. Ultimately, we display
only the closest point on each projector. The algorithm requires a depth buffer, or z-
buffer, to store the necessary depth information as polygons are rasterized. Because
we must keep depth information for each pixel in the color buffer, the z-buffer has
the same spatial resolution as the color buffers. Its depth resolution is usually 32 bits
with recent graphics cards that store this information as ﬂoating-point numbers. The
z-buffer is one of the buffers that constitute the frame buffer and is usually part of
the memory on the graphics card.
The depth buffer is initialized to a value that corresponds to the farthest distance
from the viewer. When each polygon inside the clipping volume is rasterized, the
depth of each fragment—how far the corresponding point on the polygon is from the
viewer—is calculated. If this depth is greater than the value at that fragment’s location
in the depth buffer, then a polygon that has already been rasterized is closer to the
viewer along the projector corresponding to the fragment. Hence, for this fragment
we ignore the color of the polygon and go on to the next fragment for this polygon,
where we make the same test. If, however, the depth is less than what is already in the
z-buffer, then along this projector the polygon being rendered is closer than any one
we have seen so far. Thus, we use the color of the polygon to replace the color of the
pixel in the color buffer and update the depth in the z buffer.6
For the example shown in Figure 4.41, we see that if the triangle passes through
the pipeline ﬁrst, its colors and depths will be placed in the color and z-buffers. When
the circle passes through the pipeline, its colors and depths will replace the colors and
depths of the triangle where they overlap. If the circle is rendered ﬁrst, its colors and
depths will be placed in the buffers. When the triangle is rendered, in the areas where
there is overlap the depths of the triangle are greater than the depth of the circle, and
at the corresponding pixels no changes will be made to the color or depth buffers.
Major advantages of this algorithm are that its complexity is proportional to the
number of fragments generated by the rasterizer and that it can be implemented
with a small number of additional calculations over what we have to do to project
and display polygons without hidden-surface removal. We will return to this issue in

## Chapter 6.

From the application programmer’s perspective, she must initialize the depth
buffer and enable hidden-surface removal by using
glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGB | GLUT_DEPTH);
glEnable(GL_DEPTH_TEST);
6. The color of the polygon is determined by shading (Chapter 5) and texture mapping (Chapter 7)
if these features are enabled.
                                                                                   4.9 Displaying Meshes         241
Here we use the GLUT library for the initialization and specify a depth buffer in
addition to our usual RGB color and double buffering. The programmer can clear
the color and depth buffers as necessary for a new rendering by using
glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);

#### 4.8.1 Culling

For a convex object, such as the cube, faces whose normals point away from the viewer
are never visible and can be eliminated or culled before the rasterizer. We can turn on
culling in OpenGL by enabling it as follows:
glEnable(GL_CULL_FACE);
However, culling is guaranteed to produce a correct image only if we have a single
convex object. Often we can use culling in addition to the z-buffer algorithm (which
works with any collection of polygons). For example, suppose that we have a scene
composed of a collection of n cubes. If we use only the z-buffer algorithm, we pass
6n polygons through the pipeline. If we enable culling, half the polygons can be
eliminated early in the pipeline, and thus only 3n polygons pass through all stages
of the pipeline. We consider culling further in Chapter 6.

### 4.9    DISPLAYING MESHES

We now have the tools to walk through a scene interactively by having the camera
parameters change in response to user input. Before introducing a simple interface,
let’s consider another example of data display: mesh plots. A mesh is a set of polygons
that share vertices and edges. A general mesh, as shown in Figure 4.42, may contain          FIGURE 4.42 Mesh.
polygons with any number of vertices and require a moderately sophisticated data

```cpp
structure to store and display efﬁciently. Rectangular and triangular meshes, such as
```

we introduced in Chapter 2 for modeling a sphere, are much simpler to work with and
are useful for a wide variety of applications. Here we introduce rectangular meshes for
the display of height data. Height data determine a surface, such as terrain, through
either a function that gives the heights above a reference value, such as elevations
above sea level, or through samples taken at various points on the surface.
Suppose that the heights are given by y through a function
y = f (x, z),
where x and z are the points on a two-dimensional surface such as a rectangle.
Thus, for each x, z we get exactly one y, as shown in Figure 4.43. Such surfaces are
sometimes called 2-1/2–dimensional surfaces or height ﬁelds. Although all surfaces
cannot be represented this way, they have many applications. For example, if we use
an x, z coordinate system to give positions on the surface of the earth, then we can use
such a function to represent the height or altitude at each location. In many situations
242   Chapter 4   Viewing
y = f (x,z )

*FIGURE 4.43 Height field.*

the function f is known only discretely, and we have a set of samples or measurements
yij = f (xi , zj ).
We assume that these data points are equally spaced such that
xi = x0 + i x,            i = 0, . . . , N ,
z j = z0 + j z ,          j = 0, . . . , M ,
where x and z are the spacing between the samples in the x- and z-directions,
respectively. If f is known analytically, then we can sample it to obtain a set of discrete
data with which to work.
Probably the simplest way to display the data is to draw a line strip for each value
of x and another for each value of z, thus generating N + M line strips. Suppose
that the height data are in an array data. We can form a single array with the data
converted to vertices arranged by rows with the code

```cpp
float data[N][M];
```

point4 vertices[2*N*M]

```cpp
int k =0;
```

for(int i = 0; i<N; i++) for(int j=0; j<M; j++)

```cpp
{
```

vertices[k] = vec4(i, data[i][j], j, 1.0);
k++;

```cpp
}
```

We can form an array for the vertices by column by switching roles of i and j as
follows:
                                                                                  4.9 Displaying Meshes   243
point4 vertices[N*M];

```cpp
int k =0;
```

for(int i = 0; i<M; i++) for(int j=0; j<N; j++)

```cpp
{
```

vertices[k] = point4(j, data[j][i],i,1.0);
k++;

```cpp
}
```

We usually will want to scale the data to be over a convenient range, such as (0, 1),
and scale the x and z values to make them easier to display as part of the model-view
matrix or, equivalently, by adjusting the size of the view volume.
We can display these vertices by sending both arrays to the vertex shader. So in
the initialization we set up the vertex buffer object with the correct size but without
sending any data:
GLuint buffer;
glBindVertexArray(buffer);
loc = glGetAttribLocation(program, "vPosition");
glEnableVertexAttribArray(loc);
glVertexAttribPointer(loc, 4, GL_FLOAT, GL_FALSE, 0,
BUFFER_OFFSET(0));
glGenBuffers(1, &buffer);
glBindBuffer(GL_ARRAY_BUFFER, buffer);
glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), NULL,
GL_DYNAMIC_DRAW)
In the display callback, we load the two vertex arrays successively and display them:
/* form array of vertices by row here */
glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices,
GL_DYNAMIC_DRAW);
glDrawArrays(GL_LINE_STRIP, 0, N*M);
/* form array of vertices by column here */
glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices,
GL_DYNAMIC_DRAW);
glDrawArrays(GL_LINE_STRIP, 0, N*M);
You should now be able to complete a program to display the data. Figure 4.44
shows a rectangular mesh from height data for a part of Honolulu, Hawaii. These data
are available on the Web site for the book. There are a few problems with this simple
244   Chapter 4   Viewing

*FIGURE 4.44 Mesh plot of Honolulu data using line strips.*

approach. One is that we have to load data onto the GPU twice every time we want to
display the mesh and thus are wasting a lot of time moving data from the CPU to the
GPU. A second problem is that we are not doing any hidden-surface removal, so we
see the lines from parts of the original surface that should be hidden from the viewer.
Third, there are some annoying “extra” lines that appear from the end of one row (or
column) to the next row (or column). These lines are a consequence of putting all
rows (and columns) into a single line strip.
We can get around the last problem by assigning vertex colors carefully. The
ﬁrst two problems can be avoided by displaying the data as a surface using polygons.
An added advantage in using polygons is that we will be able to extend our code to
displaying the mesh with lights and material properties in Chapter 5.

#### 4.9.1 Displaying Meshes as a Surface

One simple way to generate a surface is through a triangular mesh. We can use the
four points yij , yi+1, j , yi+1, j+1, and yi, j+1 to generate two triangles. Thus, the height
data specify a mesh of 2NM triangles.
The basic OpenGL program is straightforward. We can form the array of triangle
data in the main function or as part of initialization as in the following code, which
normalizes the data to be in the range (0, 1), the x values to be over (−1, 1), and the
z values to be over (−1, 0):

```cpp
float data[N][M]; // all values assumed non negative
float fmax;       // maximum of data
```

point4 triangles[6*N*M] // vertex positions

```cpp
float fn = float(N);
float fm = float(M);
int k =0;
```

for(i=0; i<N-1; i++) for(j=0; j<M-1;j++)

```cpp
//NM quads, 2 triangles/quad
```

                                                                                  4.9 Displaying Meshes   245

```cpp
{
```

triangles[k] = vec4(2.0*(i/fn-0.5), data[i][j]/fmax, -j/fm,
1.0); k++;
triangles[k] = vec4(2.0*((i+1)/fn-0.5), data[i+1][j]/fmax,
-j/fm, 1.0); k++;
triangles[k] = vec4(2.0*((i+1)/fn-0.5), data[i+1][j+1]/fmax,
-(j+1)/fm,1.0); k++;
triangles[k] = vec4(2.0*((i+1)/fn-0.5),data[i+1][j]/fmax,
-j/fm, 1.0); k++;
triangles[k] = vec4(2.0*((i+1)/fn-0.5), data[i+1][j+1]/fmax,
-(j+1)/fm,1.0); k++;
triangles[k] = vec4(2.0*(i/fn-0.5), data[i][j+1]/fmax,
-(j+1)/fm, 1.0); k++;

```cpp
}
```

We initialize the vertex array as before,
glBindVertexArray(abuffer);
loc = glGetAttribLocation(program, "vPosition");
glEnableVertexAttribArray(loc);
glVertexAttribPointer(loc, 4, GL_FLOAT, GL_FALSE, 0,
BUFFER_OFFSET(0));
glGenBuffers(1, buffers);
glBindBuffer(GL_ARRAY_BUFFER, buffers[0]);
glBufferData(GL_ARRAY_BUFFER, sizeof(triangles), triangles,
GL_STATIC_DRAW);
glPolygonMode(GL_FRONT_AND_BACK, GL_LINE);
glDrawArrays(GL_TRIANGLES, 0, 6*N*M);
If we integrate this code with our previous example using line strips, the output
will look almost identical. Although we have designed a surface, by choosing to
display only the edges by using a polygon mode of GL_LINE, we do not generate any
fragments corresponding to the inside of the polygon, and thus we see the edges of
polygons that would be hidden if the mode were GL_FILL. We can ﬁx this problem
by rendering the data twice, ﬁrst as a ﬁlled white surface and second as black lines.
Because the data are already on the GPU, we do not have to send any vertex data to
the GPU for the second rendering. We can specify two colors in the initialization that
color4 white = vec4(1.0, 1.0, 1.0, 1.0);
color4 black = vec4(0.0, 0.0, 0.0, 1.0);
color_loc = glGetUniformLocation(program, "fcolor");
246   Chapter 4   Viewing
and then modify to the display callback to have the code
glPolygonMode(GL_FRONT_AND_BACK, GL_FILL);
glUniform4fv(color_loc, 1, white);
glDrawArrays(GL_TRIANGLES, 0, 6*N*M);
glPolygonMode(GL_FRONT_AND_BACK, GL_LINE);
glUniform4fv(color_loc, 1, black);
glDrawArrays(GL_TRIANGLES, 0, 6*N*M);
The modiﬁed fragment shader is
uniform vec4 fcolor;

```cpp
void main()
{
```

gl_FragColor = fcolor;

```cpp
}
```


#### 4.9.2 Polygon Offset

There are interesting aspects to this OpenGL program, and we can make various
modiﬁcations. First, if we use all the data, the resulting plot may contain many small
polygons. The resulting density of lines in the display may be annoying and can
contain moiré patterns. Hence, we might prefer to subsample the data either by using
every kth point for some k or by averaging groups of data points to obtain a new set
of samples with smaller N and M.
There is one additional trick that we used in the display of Figure 4.44. If we
draw both a polygon and a line loop with the code in the previous section, then each
triangle is rendered twice in the same plane, once ﬁlled and once by its edges. Even
though the second rendering of just the edges is done with ﬁlled rendering, numer-
ical inaccuracies in the renderer often cause parts of second rendering to lie behind
the corresponding fragments in the ﬁrst rendering. We can avoid this problem by
enabling the polygon offset mode and setting the offset parameters using glPoly-
gonOffset. Polygon ﬁll offset moves fragments slightly away from the viewer, so all
the desired lines should be visible. In initialization, we can set up polygon offset by
glEnable(GL_POLYGON_OFFSET_FILL);
glPolygonOffset(1.0, 1.0);
The two parameters in PolygonOffset are combined with the slope of the polygon
and an implementation-dependent constant. Consequently, you may have to do a
little experimentation to ﬁnd the best values.
Perhaps the greatest weakness of our code is that we are sending too much data
to the GPU and not using the most efﬁcient rendering method. Consider a mesh
consisting of a single row of N quadrilaterals. If we render it as 2N triangles using
                                                                                   4.9 Displaying Meshes   247
point4 vertices[6*N];
glDrawArrays(GL_TRIANGLES, 0, 6*N);
we send 6N vertices to the GPU. If we instead set our vertices as a triangle strip, then
point4 vertices[2*N-2];
glDrawArrays(GL_TRIANGLE_STRIP, 0, 2*N-2);
Not only are we sending less data and requiring less of the GPU’s memory, but the
GPU will render the triangles much faster as a triangle strip as opposed to individual
triangles. For a 1 × M mesh, we can easily construct the array for a triangle strip.
For an N × M mesh, the process is more complex. Although it would be simple
to repeat the process for the 1 × M mesh N times, setting up N triangle strips,
this approach would have us repeatedly sending data to the GPU. What we need
is a single triangle strip for the entire mesh. Exercises 4.22 and 4.23 outline two
approaches.

#### 4.9.3 Walking Through a Scene

The next step is to specify the camera and add interactivity. In this version, we use
perspective viewing, and we allow the viewer to move the camera by pressing the x,
X, y, Y, z, and Z keys on the keyboard, but we have the camera always pointing at
the center of the cube. The LookAt function provides a simple way to reposition and
reorient the camera.
The changes that we have to make to our previous program in Section 4.4 are
minor. We deﬁne an array viewer[3] to hold the camera position. Its contents are
altered by the simple keyboard callback function keys as follows:

```cpp
void keys(unsigned char key, int x, int y)
{
```

if(key == ’x’) viewer[0]-= 1.0;
if(key == ’X’) viewer[0]+= 1.0;
if(key == ’y’) viewer[1]-= 1.0;
if(key == ’Y’) viewer[1]+= 1.0;
if(key == ’z’) viewer[2]-= 1.0;
if(key == ’Z’) viewer[2]+= 1.0;
glutPostRedisplay();

```cpp
}
```

The display function calls LookAt using viewer for the camera position and uses the
origin for the at position. The cube is rotated, as before, based on the mouse input.
Note the order of the function calls in display that alter the model-view matrix:
248   Chapter 4   Viewing
mat4 model_view;

```cpp
void display()
{
```

glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
model_view = LookAt(viewer[0],viewer[1],viewer[2],
0.0, 0.0, 0.0, 0.0, 1.0, 0.0)
*RotateX(theta[0])*RotateY(theta[1])*RotateZ(theta[2]);
/* draw mesh or other objects here */
glutSwapBuffers();

```cpp
}
```

We can invoke Frustum from the reshape callback to specify the camera lens through
the following code:
mat4 projection;

```cpp
void myReshape(int w, int h)
{
```

glViewport(0, 0, w, h);
GLfloat left = -2.0, right = 2.0, bottom = -2.0, top = 2.0;
GLfloat aspect = (GLfloat) w / h;
if ( aspect <= 1.0 ) {
bottom /= aspect;
top /= aspect;

```cpp
} else {
```

left *= aspect;
right *= aspect;

```cpp
}
```

projection = Frustum(left, right, bottom, top, 2.0, 20.0);
glUniformMatrix4fv(projection_loc, 1, GL_TRUE, projection);

```cpp
}
```

Note that we chose the values of the parameters in Frustum based on the aspect ratio
of the window. Other than the added speciﬁcation of a keyboard callback function in
main, the rest of the program is the same as the program in Section 4.4. If you run
this program, you should note the effects of moving the camera, the lens, and the
sides of the viewing frustum. Note what happens as you move toward the mesh. You
should also consider the effect of always having the viewer look at the center of the
mesh as she is moving.
Note that we could have used the mouse buttons to move the viewer. We could
use the mouse buttons to move the user forward or to turn her right or left (see
                                                                            4.10 Projections and Shadows   249
Exercise 4.14). However, by using the keyboard for moving the viewer, we can use
the mouse to move the object as with the rotating cube in Chapter 3.
In this example, we are using direct positioning of the camera through LookAt.
There are other possibilities. One is to use rotation and translation matrices to alter
the model-view matrix incrementally. If we want to move the viewer through the
scene without having her looking at a ﬁxed point, this option may be more appealing.
We could also keep a position variable in the program and change it as the viewer
moves. In this case, the model-view matrix would be computed from scratch rather
than changed incrementally. Which option we choose depends on the particular
application, and often on other factors as well, such as the possibility that numerical
errors might accumulate if we were to change the model-view matrix incrementally
many times.
The basic mesh rendering can be extended in many ways. In Chapter 5, we
will learn to add lights and surface properties to create a more realistic image; in

## Chapter 7, we will learn to add a texture to the surface. The texture map might be

an image of the terrain from a photograph or other data that might be obtained by
digitization of a map. If we combine these techniques, we can generate a display in
which we can make the image depend on the time of day by changing the position
of the light source. It is also possible to obtain smoother surfaces by using the data
to deﬁne a smoother surface with the aid of one of the surface types that we will

```cpp
introduce in Chapter 10.
```


### 4.10     PROJECTIONS AND SHADOWS

The creation of simple shadows is an interesting application of projection matrices.
Although shadows are not geometric objects, they are important components of
realistic images and give many visual clues to the spatial relationships among the
objects in a scene. Starting from a physical point of view, shadows require a light
source to be present. A point is in shadow if it is not illuminated by any light source
or, equivalently, if a viewer at that point cannot see any light sources. However, if the
only light source is at the center of projection, there are no visible shadows, because
any shadows are behind visible objects. This lighting strategy has been called the
“ﬂashlight in the eye” model and corresponds to the simple lighting we have used
thus far.
To add physically correct shadows, we must understand the interaction between
light and materials, a topic that we investigate in Chapter 5. There we show that global
calculations are difﬁcult; normally, they cannot be done in real time.
Nevertheless, the importance of shadows in applications such as ﬂight simulators
led to a number of special approaches that can be used in many circumstances.
Consider the shadow generated by the point source in Figure 4.45. We assume for
simplicity that the shadow falls on the ground or the surface,
y = 0.
250   Chapter 4   Viewing
( xl, yl, zl )

*FIGURE 4.45 Shadow from a single polygon.*

Not only is the shadow a ﬂat polygon, called a shadow polygon, but it also is the
projection of the original polygon onto the surface. Speciﬁcally, the shadow polygon
is the projection of the polygon onto the surface with the center of projection at the
light source. Thus, if we do a projection onto the plane of a surface in a frame in
which the light source is at the origin, we obtain the vertices of the shadow polygon.
These vertices must then be converted back to a representation in the object frame.
Rather than do the work as part of an application program, we can ﬁnd a suitable
projection matrix and use it to compute the vertices of the shadow polygon.
Suppose that we start with a light source at (xl , yl , zl ), as shown in Figure 4.46(a).
If we reorient the ﬁgure such that the light source is at the origin, as shown in
Figure 4.46(b), by a translation matrix T(−xl , −yl , −zl ), then we have a simple
perspective projection through the origin. The projection matrix is
⎡                 ⎤
1 0 0 0
⎢0 1 0 0⎥
⎢                 ⎥
M=⎢                     ⎥.
⎣0 0 1 0⎦
0 −y      0 0
Finally, we translate everything back with T(xl , yl , zl ). The concatenation of this
matrix and the two translation matrices projects the vertex (x, y, z) to
x − xl
xp = xl −                         ,
(y − yl )/yl
yp = 0,
z − zl
zp = zl −                         .
(y − yl )/yl
                                                                           4.10 Projections and Shadows   251
(a)                                        (b)

*FIGURE 4.46 Shadow polygon projection. (a) From a light source.*

(b) With source moved to the origin.
However, with an OpenGL program, we can alter the model-view matrix to form the
desired polygon as follows. If the light source is ﬁxed, we can compute the shadow
projection matrix once as part of initialization. Otherwise, we need to recompute
it, perhaps in the idle callback function, if the light source is moving. The code for
setting up the matrix is as follows:

```cpp
float light[3];   // location of light
```

mat4 m;    // shadow projection matrix initially an identity matrix
m[3][1] = -1.0/light[1];
Let’s project a single square polygon parallel onto the plane y = 0. We can specify the
point4 square[4] = {vec4(-0.5, 0.5, -0.5, 1.0), vec4(-0.5, 0.5, 0.5, 1.0),
vec4(0.5, 0.5, -0.5, 1.0), vec4(0.5, 0.5, 0.5, 1.0)};
Note that the vertices are ordered so that we can render them using a triangle strip.
We initialize a red color for the square and a black color for its shadow, which we will
send to the fragment shader. We initialize a vertex array and a buffer object, as we did
in our previous examples:
252   Chapter 4   Viewing
GLuint abuffer, buffer;
glGenVertexArrays(1, &abuffer);
glBindVertexArray(abuffer);

```cpp
int loc = glGetAttribLocation(program, "vPosition");
```

glEnableVertexAttribArray(loc);
glVertexAttribPointer(loc, 4, GL_FLOAT, GL_FALSE, 0,
BUFFER_OFFSET(0));
color_loc = glGetUniformLocation(program, "fcolor");
glGenBuffers(1, &buffer);
glBindBuffer(GL_ARRAY_BUFFER, buffer);
glBufferData(GL_ARRAY_BUFFER, sizeof(square), square,
GL_STATIC_DRAW);
If the data do not change, we can also set the projection matrix and model-view
matrix as part of initialization and send them to the vertex shader:
model_view = LookAt(eye, at, up);
projection = Ortho(left, right, bottom, top, near, far);
glUniformMatrix4fv(matrix_loc, 1, GL_TRUE, model_view);
glUniformMatrix4fv(projection_loc, 1, GL_TRUE, projection);
The core of the display callback is

```cpp
void display()
{
```

mat4 mm;

```cpp
// clear the window
```

glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);

```cpp
// render red square
```

glUniform4fv(color_loc, 1, red);
glDrawArrays(GL_TRIANGLE_STRIP, 0, 4);

```cpp
//   matrix to compute vertices of shadow polygon
```

mm = model_view*Translate(light[0], light[1],
light[2])*m*Translate(-light[0], -light[1], -light[2]);
glUniformMatrix4fv(matrix_loc, 1, GL_TRUE, mm);

```cpp
// render shadow polygon
```

glUniform4fv(color_loc, 1, black);
glDrawArrays(GL_TRIANGLE_STRIP, 0, 4);
glutSwapBuffers();

```cpp
}
```

                                                                                     Summary and Notes   253
Note that although we are performing a projection with respect to the light
source, the matrix that we use is the model-view matrix. We render the same polygon
twice: the ﬁrst time as usual and the second time with an altered model-view matrix
that transforms the vertices. The same viewing conditions are applied to both the
polygon and its shadow polygon. The results of computing shadows for the colored
cube are shown in Color Plate 29.
For a simple environment, such as an airplane ﬂying over ﬂat terrain casting
a single shadow, this technique works well. It is also easy to convert from point
sources to distant (parallel) light sources (see Exercise 4.17). However, when objects
can cast shadows on other objects, this method becomes impractical. In Chapter 11,
we address more general, but slower, rendering methods that will create shadows
automatically as part of the rendering process.
We have come a long way. We can now write complete, nontrivial, three-dimensional
applications. Probably the most instructive activity that you can do now is to write
such an application. Developing skill with manipulating the model-view and projec-
tion functions takes practice.
We have presented the mathematics of the standard projections. Although most
APIs obviate the application programmer from writing projection functions, under-
standing the mathematics leads to understanding a pipeline implementation based
on concatenation of 4 × 4 matrices. Until recently, application programs had to do
the projections within the applications, and most hardware systems did not support
perspective projections.
There are three major themes in the remainder of this book. First, we explore
modeling further by expanding our basic set of primitives. In Chapter 8, we incorpo-
rate more complex relationships between simple objects through hierarchical models.
In Chapter 9, we explore approaches to modeling that allow us to describe objects
through procedures rather than as geometric objects. This approach allows us to
model objects with only as much detail as is needed, to incorporate physical laws into
our models, and to model natural phenomena that cannot be described by polygons.
In Chapter 10, we leave the world of ﬂat objects, adding curves and curved surfaces.
These objects are deﬁned by vertices, and we can implement them by breaking them

```cpp
into small ﬂat primitives, so we can use the same viewing pipeline.
```

The second major theme is realism. Although more complex objects allow us
to build more realistic models, we also explore more complex rendering options. In

## Chapter 5, we consider the interaction of light with the materials that characterize

our objects. We look more deeply at hidden-surface–removal methods, at shading
models, and in Chapter 7 at techniques such as texture mapping that allow us to create
complex images from simple objects using advanced rendering techniques.
Third, we look more deeply at implementation in Chapter 6. At this point, we
have introduced the major functional units of the graphics pipeline. We discuss the
254   Chapter 4   Viewing
details of the algorithms used in each unit. We will also see additional possibilities for
creating images by working directly in the frame buffer.
Carlbom and Paciorek [Car78] discuss the relationships between classical and com-
puter viewing. Rogers and Adams [Rog90] give many examples of the projection
matrices corresponding to the standard views used in drafting. Foley et al. [Fol90],
Watt [Wat00], and Hearn and Baker [Hea04] derive canonical projection transfor-
mations. All follow a PHIGS orientation, so the API is slightly different from the
one used here, although Foley derives the most general case. The references differ
in whether they use column or row matrices, in where the COP is located, and in
whether the projection is in the positive or negative z-direction. See the OpenGL Pro-
gramming Guide [Shr10] for a further discussion of the use of the model-view and
projection matrices in OpenGL.

### 4.1  Not all projections are planar geometric projections. Give an example of a

projection in which the projection surface is not a plane and another in which
the projectors are not lines.

### 4.2 Consider an airplane whose position is speciﬁed by the roll, pitch, and yaw and

by the distance from an object. Find a model-view matrix in terms of these
parameters.

### 4.3 Consider a satellite orbiting the earth. Its position above the earth is speciﬁed

in polar coordinates. Find a model-view matrix that keeps the viewer looking
at the earth. Such a matrix could be used to show the earth as it rotates.

### 4.4 Show how to compute u and v directions from the VPN, VRP, and VUP using

only cross products.

### 4.5 Can we obtain an isometric of the cube by a single rotation about a suitably

chosen axis? Explain your answer.

### 4.6 Derive the perspective-projection matrix when the COP can be at any point

and the projection plane can be at any orientation.

### 4.7 Show that perspective projection preserves lines.


### 4.8 Any attempt to take the projection of a point in the same plane as the COP

will lead to a division by zero. What is the projection of a line segment that has
endpoints on either side of the projection plane?

### 4.9 Deﬁne one or more APIs to specify oblique projections. You do not need to

write the functions; just decide which parameters the user must specify.

### 4.10 Derive an oblique-projection matrix from speciﬁcation of front and back clip-

ping planes and top-right and bottom-left intersections of the sides of the
clipping volume with the front clipping plane.
                                                                                         Exercises   255

### 4.11 Our approach of normalizing all projections seems to imply that we could

predistort all objects and support only orthographic projections. Explain any
problems we would face if we took this approach to building a graphics system.

### 4.12 How do the OpenGL projection matrices change if the COP is not at the

origin? Assume that the COP is at (0, 0, d) and the projection plane is z = 0.

### 4.13 We can create an interesting class of three-dimensional objects by extending

two-dimensional objects into the third dimension by extrusion. For example,
a circle becomes a cylinder, a line becomes a quadrilateral, and a quadrilateral
in the plane becomes a parallelepiped. Use this technique to convert the two-
dimensional maze from Exercise 2.7 to a three-dimensional maze.

### 4.14 Extend the maze program of Exercise 4.13 to allow the user to walk through

the maze. A click on the middle mouse button should move the user forward;
a click on the right or left button should turn the user 90 degrees to the right
or left, respectively.

### 4.15 If we were to use orthogonal projections to draw the coordinate axes, the x-

and y-axes would lie in the plane of the paper, but the z-axis would point out of
the page. Instead, we can draw the x- and y-axes meeting at a 90-degree angle,
with the z-axis going off at −135 degrees from the x-axis. Find the matrix that
projects the original orthogonal-coordinate axes to this view.

### 4.16 Write a program to display a rotating cube in a box with three light sources.

Each light source should project the cube onto one of the three visible sides of
the box.

### 4.17 Find the projection of a point onto the plane ax + by + cz + d = 0 from a light

source located at inﬁnity in the direction (dx , dy , dz ).

### 4.18 Using one of the three-dimensional interfaces discussed in Chapter 3, write a

program to move the camera through a scene composed of simple objects.

### 4.19 Write a program to ﬂy through the three-dimensional Sierpinski gasket formed

by subdividing tetrahedra. Can you prevent the user from ﬂying through walls?

### 4.20 In animation, often we can save effort by working with two-dimensional pat-

terns that are mapped onto ﬂat polygons that are always parallel to the camera,
a technique known as billboarding. Write a program that will keep a simple
polygon facing the camera as the camera moves.

### 4.21 Stereo images are produced by creating two images with the viewer in two

slightly different positions. Consider a viewer who is at the origin but whose
eyes are separated by x units. What are the appropriate viewing speciﬁcations
to create the two images?

### 4.22 In Section 4.9, we displayed a mesh by drawing two line strips. How would

you alter this approach to not draw the extra line from the end of one row (or
column) to the begining of the next row (or column)?

### 4.23 Derive a method for displaying a mesh using a single triangle strip.


### 4.24 Construct a fragment shader that does polygon offset during a perspective

projection.
256   Chapter 4   Viewing

### 4.25 Write a shader that modiﬁes the height of a mesh in the shader.


### 4.26 Render a rectangular mesh as a single triangle strip by creating a degenerate

triangle at the end of each row.

### 4.27 Write a program that will ﬂy around above a mesh. Your program should allow

the user to look around at the hills and valleys rather than always looking at a
single point.

### 4.28 Write a reshape callback that does not distort the shape of objects as the win-

dow is altered.
                                                                      CHA P TE R            5
W       e have learned to build three-dimensional graphical models and to display
them. However, if you render one of our models, you might be disappointed
to see images that look ﬂat and thus fail to show the three-dimensional nature of
the model. This appearance is a consequence of our unnatural assumption that each
surface is lit such that it appears to a viewer in a single color. Under this assumption,
the orthographic projection of a sphere is a uniformly colored circle, and a cube
appears as a ﬂat hexagon. If we look at a photograph of a lit sphere, we see not a
uniformly colored circle but rather a circular shape with many gradations, or shades,
of color. It is these gradations that give two-dimensional images the appearance of
being three-dimensional.
What we have left out is the interaction between light and the surfaces in our
models. This chapter begins to ﬁll that gap. We develop separate models of light
sources and of the most common light–material interactions. Our aim is to add
shading to a fast pipeline graphics architecture. Consequently, we develop only local
lighting models. Such models, as opposed to global lighting models, allow us to
compute the shade to assign to a point on a surface, independent of any other surfaces
in the scene. The calculations depend only on the material properties assigned to the
surface, the local geometry of the surface, and the locations and properties of the light
sources. In this chapter, we introduce the lighting models used most often in OpenGL
applications. We shall see that we have choices as to where to apply a given lighting
model: in the application, in the vertex shader, or in the fragment shader.
Following our previous development, we investigate how we can apply shading
to polygonal models. We develop a recursive approximation to a sphere that will allow
us to test our shading algorithms. We then discuss how light and material properties
are speciﬁed in OpenGL applications and can be added to our sphere-approximating
program.
We conclude the chapter with a short discussion of the two most important
methods for handling global lighting effects: ray tracing and radiosity.
258   Chapter 5   Lighting and Shading

*FIGURE 5.1 Reflecting surfaces.*


### 5.1    LIGHT AND MATTER

In Chapters 1 and 2, we presented the rudiments of human color vision, delaying
until now any discussion of the interaction between light and surfaces. Perhaps the
most general approach to rendering is based on physics, where we use principles such
as conservation of energy to derive equations that describe how light is reﬂected from
surfaces.
From a physical perspective, a surface can either emit light by self-emission,
as a light bulb does, or reﬂect light from other surfaces that illuminate it. Some
surfaces may both reﬂect light and emit light from internal physical processes. When
we look at a point on an object, the color that we see is determined by multiple

```cpp
interactions among light sources and reﬂective surfaces. These interactions can be
```

viewed as a recursive process. Consider the simple scene in Figure 5.1. Some light
from the source that reaches surface A is scattered. Some of this reﬂected light reaches
surface B, and some of it is then scattered back to A, where some of it is again reﬂected
back to B, and so on. This recursive scattering of light between surfaces accounts
for subtle shading effects, such as the bleeding of colors between adjacent surfaces.
Mathematically, the limit of this recursive process can be described using an integral
equation, the rendering equation, which in principle we could use to ﬁnd the shading
of all surfaces in a scene. Unfortunately, this equation generally cannot be solved
analytically. Numerical methods for computing a solution are not fast enough for
real-time rendering. There are various approximate approaches, such as radiosity and
ray tracing, each of which is an excellent approximation to the rendering equation
for particular types of surfaces. Although ray tracing can render moderately complex
scenes in real time, these methods cannot render scenes at the rate at which we can
pass polygons through the modeling-projection pipeline. Consequently, we focus on
a simpler rendering model, based on the Phong reﬂection model, that provides a
compromise between physical correctness and efﬁcient calculation. We will introduce
global methods in Section 5.10 and then consider the rendering equation, radiosity,
and ray tracing in greater detail in Chapter 11.
                                                                                      5.1 Light and Matter   259

*FIGURE 5.2 Light and surfaces.*

Rather than looking at a global energy balance, we follow rays of light from
light-emitting (or self-luminous) surfaces that we call light sources. We then model
what happens to these rays as they interact with reﬂecting surfaces in the scene. This
approach is similar to ray tracing, but we consider only single interactions between
light sources and surfaces. There are two independent parts of the problem. First, we
must model the light sources in the scene. Then we must build a reﬂection model that
deals with the interactions between materials and light.
To get an overview of the process, we can start following rays of light from a
point source, as shown in Figure 5.2. As we noted in Chapter 1, our viewer sees only
the light that leaves the source and reaches her eyes—perhaps through a complex
path and multiple interactions with objects in the scene. If a ray of light enters her
eye directly from the source, she sees the color of the source. If the ray of light hits
a surface visible to our viewer, the color she sees is based on the interaction between
the source and the surface material: She sees the color of the light reﬂected from the
surface toward her eyes.
In terms of computer graphics, we replace the viewer by the projection plane, as
shown in Figure 5.3. Conceptually, the clipping window in this plane is mapped to
the screen; thus, we can think of the projection plane as ruled into rectangles, each
corresponding to a pixel. The color of the light source and of the surfaces determines
the color of one or more pixels in the frame buffer.
We need to consider only those rays that leave the source and reach the viewer’s
eye, either directly or through interactions with objects. In the case of computer
viewing, these are the rays that reach the center of projection (COP) after passing
through the clipping rectangle. Note that in scenes for which the image shows a lot
260   Chapter 5   Lighting and Shading

*FIGURE 5.3 Light, surfaces, and computer imaging.*

(a)                         (b)                       (c)

*FIGURE 5.4 Light–material interactions. (a) Specular surface. (b) Diffuse*

surface. (c) Translucent surface.
of the background, most rays leaving a source do not contribute to the image and are
thus of no interest to us. We make use of this observation in Section 5.10.
Figure 5.2 shows both single and multiple interactions between rays and objects.
It is the nature of these interactions that determines whether an object appears red or
brown, light or dark, dull or shiny. When light strikes a surface, some of it is absorbed
and some of it is reﬂected. If the surface is opaque, reﬂection and absorption account
for all the light striking the surface. If the surface is translucent, some of the light is
transmitted through the material and emerges to interact with other objects. These

```cpp
interactions depend on wavelength. An object illuminated by white light appears red
```

because it absorbs most of the incident light but reﬂects light in the red range of
frequencies. A shiny object appears so because its surface is smooth. Conversely, a dull
object has a rough surface. The shading of objects also depends on the orientation of
their surfaces, a factor that we shall see is characterized by the normal vector at each
point. These interactions between light and materials can be classiﬁed into the three
groups depicted in Figure 5.4.
                                                                                              5.2 Light Sources          261
1. Specular surfaces appear shiny because most of the light that is reﬂected or
scattered is in a narrow range of angles close to the angle of reﬂection. Mirrors
are perfectly specular surfaces; the light from an incoming light ray may be
partially absorbed, but all reﬂected light from a given angle emerges at a single
angle, obeying the rule that the angle of incidence is equal to the angle of
reﬂection.
2. Diffuse surfaces are characterized by reﬂected light being scattered in all
directions. Walls painted with matte or ﬂat paint are diffuse reﬂectors, as are
many natural materials, such as terrain viewed from an airplane or a satellite.
Perfectly diffuse surfaces scatter light equally in all directions, and thus a ﬂat
perfectly diffuse surface appears the same to all viewers.
3. Translucent surfaces allow some light to penetrate the surface and to emerge
from another location on the object. This process of refraction characterizes
glass and water. Some incident light may also be reﬂected at the surface.
We shall model all these surfaces in Section 5.3. First, we consider light sources.


### 5.2     LIGHT SOURCES

Light can leave a surface through two fundamental processes: self-emission and re-
ﬂection. We usually think of a light source as an object that emits light only through                                    x

```cpp
internal energy sources. However, a light source, such as a light bulb, can also reﬂect
```

some light that is incident on it from the surrounding environment. We will usually
omit the emissive term in our simple models. When we discuss lighting in Section 5.7,
we will see that we can easily add a self-emission term.
If we consider a source such as the one in Figure 5.5, we can look at it as an              FIGURE 5.5 Light source.
object with a surface. Each point (x, y, z) on the surface can emit light that is char-
acterized by the direction of emission (θ , φ) and the intensity of energy emitted at
each wavelength λ. Thus, a general light source can be characterized by a six-variable
illumination function I(x, y, z, θ , φ, λ). Note that we need two angles to specify a
direction, and we are assuming that each frequency can be considered independently.
From the perspective of a surface illuminated by this source, we can obtain the total
contribution of the source (Figure 5.6) by integrating over its surface, a process that
accounts for the emission angles that reach this surface and must also account for the
distance between the source and the surface. For a distributed light source, such as a
light bulb, the evaluation of this integral is difﬁcult, whether we use analytic or nu-
merical methods. Often, it is easier to model the distributed source with polygons,
each of which is a simple source, or with an approximating set of point sources.
We consider four basic types of sources: ambient lighting, point sources, spot-
lights, and distant light. These four lighting types are sufﬁcient for rendering most
simple scenes.
262   Chapter 5   Lighting and Shading
p1
I(x1, y1, z1, 1,   1,  )
p2
I (x 2, y2, z 2,  2,   2,  )

*FIGURE 5.6 Adding the contribution from a source.*


#### 5.2.1 Color Sources

Not only do light sources emit different amounts of light at different frequencies,
but their directional properties can vary with frequency as well. Consequently, a
physically-correct model can be complex. However, our model of the human visual
system is based on three-color theory, which tells us we perceive three tristimulus
values rather than a full-color distribution. For most applications, we can thus model
light sources as having three components—red, green, and blue—and can use each of
the three color sources to obtain the corresponding color components that a human
observer sees.
We describe a source through a three-component intensity, or luminance, func-
⎡ ⎤
I = ⎣ Ig ⎦ ,
each of whose components is the intensity of the independent red, green, and blue
components. Thus, we use the red component of a light source for the calculation of
the red component of the image. Because light–material computations involve three
similar but independent calculations, we will tend to present a single scalar equation,
with the understanding that it can represent any of the three color components.

#### 5.2.2 Ambient Light

In many rooms, such as classrooms or kitchens, the lights have been designed and
positioned to provide uniform illumination throughout the room. Such illumination
is often achieved through large sources that have diffusers whose purpose is to scatter
light in all directions. We could create an accurate simulation of such illumination,
at least in principle, by modeling all the distributed sources and then integrating the
illumination from these sources at each point on a reﬂecting surface. Making such a
model and rendering a scene with it would be a daunting task for a graphics system,
especially one for which real-time performance is desirable. Alternatively, we can
look at the desired effect of the sources: to achieve a uniform light level in the room.
This uniform lighting is called ambient light. If we follow this second approach, we
can postulate an ambient intensity at each point in the environment. Thus, ambient
                                                                                              5.2 Light Sources        263
p0

*FIGURE 5.7 Point source illuminating a surface.*

illumination is characterized by an intensity, Ia , that is identical at every point in the
scene.
Our ambient source has three color components:
⎡     ⎤
Ia = ⎣ Iag ⎦ .
We will use the scalar Ia to denote any one of the red, green, or blue components of
Ia . Although every point in our scene receives the same illumination from Ia , each
surface can reﬂect this light differently.

#### 5.2.3 Point Sources

An ideal point source emits light equally in all directions. We can characterize a point
source located at a point p0 by a three-component color matrix:
⎡          ⎤
Ir (p0)
I(p0) = ⎣ Ig (p0) ⎦ .
Ib(p0)
The intensity of illumination received from a point source is proportional to the
inverse square of the distance between the source and surface. Hence, at a point p
(Figure 5.7), the intensity of light received from the point source is given by the matrix
i(p, p0) =            I(p ).
|p − p0|2 0
As with ambient light, we will use I(p0) to denote any of the components of I(p0).
The use of point sources in most applications is determined more by their ease
of use than by their resemblance to physical reality. Scenes rendered with only point
sources tend to have high contrast; objects appear either bright or dark. In the real
world, it is the large size of most light sources that contributes to softer scenes, as
we can see from Figure 5.8, which shows the shadows created by a source of ﬁnite                FIGURE 5.8 Shadows created
size. Some areas are fully in shadow, or in the umbra, whereas others are in partial            by finite-size light source.
264               Chapter 5          Lighting and Shading
shadow, or in the penumbra. We can mitigate the high-contrast effect from point-
source illumination by adding ambient light to a scene.
The distance term also contributes to the harsh renderings with point sources.
Although the inverse-square distance term is correct for point sources, in practice it
ps         is usually replaced by a term of the form (a + bd + cd 2)−1, where d is the distance
between p and p0. The constants a, b, and c can be chosen to soften the lighting. Note
that if the light source is far from the surfaces in the scene, the intensity of the light
                     from the source is sufﬁciently uniform that the distance term is constant over each
surface.

*FIGURE 5.9 Spotlight.                   5.2.4 Spotlights*

Spotlights are characterized by a narrow range of angles through which light is emit-
Intensity                  ted. We can construct a simple spotlight from a point source by limiting the angles at
which light from the source can be seen. We can use a cone whose apex is at ps , which
points in the direction ls , and whose width is determined by an angle θ , as shown in
Figure 5.9. If θ = 180, the spotlight becomes a point source.
More realistic spotlights are characterized by the distribution of light within the
                             cone—usually with most of the light concentrated in the center of the cone. Thus, the

```cpp
intensity is a function of the angle φ between the direction of the source and a vector
```


*FIGURE 5.10 Attenuation of                 s to a point on the surface (as long as this angle is less than θ ; Figure 5.10). Although*

a spotlight.                               this function could be deﬁned in many ways, it is usually deﬁned by cose φ, where the
exponent e (Figure 5.11) determines how rapidly the light intensity drops off.
Intensity                       As we shall see throughout this chapter, cosines are convenient functions for
lighting calculations. If u and v are any unit-length vectors, we can compute the
cosine of the angle θ between them with the dot product
cos θ = u . v,
               
a calculation that requires only three multiplications and two additions.

*FIGURE 5.11 Spotlight expo-*

nent.

#### 5.2.5 Distant Light Sources

Most shading calculations require the direction from the point on the surface to
the light source position. As we move across a surface, calculating the intensity at
each point, we should recompute this vector repeatedly—a computation that is a
signiﬁcant part of the shading calculation. However, if the light source is far from
the surface, the vector does not change much as we move from point to point, just as
the light from the sun strikes all objects that are in close proximity to one another
at the same angle. Figure 5.12 illustrates that we are effectively replacing a point
source of light with a source that illuminates objects with parallel rays of light—a

*FIGURE 5.12 Parallel light                 parallel source. In practice, the calculations for distant light sources are similar to the*

source.                                    calculations for parallel projections; they replace the location of the light source with
the direction of the light source. Hence, in homogeneous coordinates, a point light
source at p0 is represented internally as a four-dimensional column matrix:
                                                                          5.3 The Phong Reflection Model            265
⎡ ⎤
⎢y⎥
⎢ ⎥
p0 = ⎢ ⎥ .
⎣z⎦
In contrast, the distant light source is described by a direction vector whose represen-
tation in homogeneous coordinates is the matrix
⎡ ⎤
⎢y⎥
⎢ ⎥
p0 = ⎢ ⎥ .
⎣z⎦
The graphics system can carry out rendering calculations more efﬁciently for distant
light sources than for near ones. Of course, a scene rendered with distant light sources
looks different from a scene rendered with near sources. Fortunately, our models will
allow both types of sources.

### 5.3    THE PHONG REFLECTION MODEL

Although we could approach light–material interactions through physical models, we
have chosen to use a model that leads to efﬁcient computations, especially when we           I
use it with our pipeline-rendering model. The reﬂection model that we present was                                      r

```cpp
introduced by Phong and later modiﬁed by Blinn. It has proved to be efﬁcient and
```

to be a close-enough approximation to physical reality to produce good renderings
under a variety of lighting conditions and material properties.
The Phong model uses the four vectors shown in Figure 5.13 to calculate a color         FIGURE 5.13 Vectors used by
for an arbitrary point p on a surface. If the surface is curved, all four vectors can        the Phong model.
change as we move from point to point. The vector n is the normal at p; we discuss
its calculation in Section 5.4. The vector v is in the direction from p to the viewer
or COP. The vector l is in the direction of a line from p to an arbitrary point on the
source for a distributed light source or, as we are assuming for now, to the point-
light source. Finally, the vector r is in the direction that a perfectly reﬂected ray from
l would take. Note that r is determined by n and l. We calculate it in Section 5.4.
The Phong model supports the three types of material–light interactions—
ambient, diffuse, and specular—that we introduced in Section 5.1. Suppose that we
have a set of point sources. We assume that each source can have separate ambient,
diffuse, and specular components for each of the three primary colors. Although this
assumption may appear unnatural, remember that our goal is to create realistic shad-
ing effects in as close to real time as possible. We use a local model to simulate effects
that can be global in nature. Thus, our light-source model has ambient, diffuse, and
specular terms. We need nine coefﬁcients to characterize these terms at any point p
on the surface. We can place these nine coefﬁcients in a 3 × 3 illumination matrix for
the ith light source:
266   Chapter 5   Lighting and Shading
⎡                      ⎤
Li = ⎣ Lird       Ligd   Libd ⎦ .
The ﬁrst row of the matrix contains the ambient intensities for the red, green, and
blue terms from source i. The second row contains the diffuse terms; the third con-
tains the specular terms. We assume that any distance-attenuation terms have not yet
been applied. This matrix is only a simple way of storing the nine lighting terms we
need. In practice, we will use constructs such as
vec3 light_i_ambient, light_i_diffuse, light_i_specular;
vec4 light_i_ambient, light_i_diffuse, light_i_specular;
for each source in our code. The four-dimensional form will be useful when we
consider lighting with materials that are not opaque.
We construct the model by assuming that we can compute how much of each
of the incident lights is reﬂected at the point of interest. For example, for the red
diffuse term from source i, Lird , we can compute a reﬂection term Rird , and the latter’s
contribution to the intensity at p is Rird Lird . The value of Rird depends on the material
properties, the orientation of the surface, the direction of the light source, and the
distance between the light source and the viewer. Thus, for each point, we have nine
coefﬁcients that we can place in a matrix of reﬂection terms of the form
⎡                    ⎤
Ri = ⎣ Rird Rigd Ribd ⎦ .
We can then compute the contribution for each color source by adding the ambient,
diffuse, and specular components. For example, the red intensity that we see at p from
Iir = Rira Lira + Rird Lird + RirsLirs
= Iira + Iird + Iirs .
We obtain the total intensity by adding the contributions of all sources and, possibly,
a global ambient term. Thus, the red term is

Ir =     (Iira + Iird + Iirs) + Iar ,
where Iar is the red component of the global ambient light.
We can simplify our notation by noting that the necessary computations are the
same for each source and for each primary color. They differ depending on whether
                                                                          5.3 The Phong Reflection Model                 267
we are considering the ambient, diffuse, or specular terms. Hence, we can omit the
subscripts i, r, g, and b. We write
I = Ia + Id + Is = La Ra + Ld Rd + LsRs ,
with the understanding that the computation will be done for each of the primaries
and each source; the global ambient term can be added at the end. As with the lighting
terms, when we get to code we will use forms such as
vec4 reflect_i_ambient, reflect_i_diffuse, reflect_i_specular;
Note that these terms are all for a single surface and in general we will have different
reﬂectivity properties for each material.

#### 5.3.1 Ambient Reflection

The intensity of ambient light Ia is the same at every point on the surface. Some of this
light is absorbed and some is reﬂected. The amount reﬂected is given by the ambient
reﬂection coefﬁcient, Ra = ka . Because only a positive fraction of the light is reﬂected,
0 ≤ ka ≤ 1,
Ia = ka La .
Here La can be any of the individual light sources, or it can be a global ambient term.
A surface has, of course, three ambient coefﬁcients—kar , kag , and kab—and they
can differ. Hence, for example, a sphere appears yellow under white ambient light if
its blue ambient coefﬁcient is small and its red and green coefﬁcients are large.

#### 5.3.2 Diffuse Reflection

A perfectly diffuse reﬂector scatters the light that it reﬂects equally in all directions.   FIGURE 5.14 Rough surface.
Hence, such a surface appears the same to all viewers. However, the amount of
light reﬂected depends both on the material—because some of the incoming light
is absorbed—and on the position of the light source relative to the surface. Diffuse
reﬂections are characterized by rough surfaces. If we were to magnify a cross section
of a diffuse surface, we might see an image like that shown in Figure 5.14. Rays of            n              n
light that hit the surface at only slightly different angles are reﬂected back at markedly
different angles. Perfectly diffuse surfaces are so rough that there is no preferred angle
of reﬂection. Such surfaces, sometimes called Lambertian surfaces, can be modeled
mathematically with Lambert’s law.                                                                 (a)         (b)
Consider a diffuse planar surface, as shown in Figure 5.15, illuminated by the          FIGURE 5.15 Illumination of
sun. The surface is brightest at noon and dimmest at dawn and dusk because, ac-              a diffuse surface. (a) At noon.
cording to Lambert’s law, we see only the vertical component of the incoming light.          (b) In the afternoon.
268   Chapter 5   Lighting and Shading
(a)
d/cos
(b)

*FIGURE 5.16 Vertical contributions by Lambert’s law. (a) At noon. (b) In*

the afternoon.
One way to understand this law is to consider a small parallel light source striking
a plane, as shown in Figure 5.16. As the source is lowered in the (artiﬁcial) sky, the
same amount of light is spread over a larger area, and the surface appears dimmer.
Returning to the point source of Figure 5.15, we can characterize diffuse reﬂections
mathematically. Lambert’s law states that
Rd ∝ cos θ ,
where θ is the angle between the normal at the point of interest n and the direction
of the light source l. If both l and n are unit-length vectors,1 then
cos θ = l . n.
If we add in a reﬂection coefﬁcient kd representing the fraction of incoming diffuse
light that is reﬂected, we have the diffuse reﬂection term:
Id = kd (l . n)Ld .
If we wish to incorporate a distance term, to account for attenuation as the light
travels a distance d from the source to the surface, we can again use the quadratic
attenuation term:
Id =                 (l . n)Ld .
a + bd + cd 2
1. Direction vectors, such as l and n, are used repeatedly in shading calculations through the dot
product. In practice, both the programmer and the graphics software should seek to normalize all
such vectors as soon as possible.
                                                                          5.3 The Phong Reflection Model             269
There is a potential problem with this expression because (l . n)Ld will be neg-
ative if the light source is below the horizon. In this case, we want to use zero rather
than a negative value. Hence, in practice we use max((l . n)Ld , 0).

#### 5.3.3 Specular Reflection

If we employ only ambient and diffuse reﬂections, our images will be shaded and will
appear three-dimensional, but all the surfaces will look dull, somewhat like chalk.
What we are missing are the highlights that we see reﬂected from shiny objects. These
highlights usually show a color different from the color of the reﬂected ambient and          FIGURE 5.17 Specular high-
diffuse light. For example, a red plastic ball viewed under white light has a white           lights.
highlight that is the reﬂection of some of the light from the source in the direction
of the viewer (Figure 5.17).
Whereas a diffuse surface is rough, a specular surface is smooth. The smoother
the surface is, the more it resembles a mirror. Figure 5.18 shows that as the surface
gets smoother, the reﬂected light is concentrated in a smaller range of angles centered
about the angle of a perfect reﬂector—a mirror or a perfectly specular surface. Mod-
eling specular surfaces realistically can be complex because the pattern by which the         FIGURE 5.18 Specular sur-
light is scattered is not symmetric. It depends on the wavelength of the incident light,      face.
and it changes with the reﬂection angle.
Phong proposed an approximate model that can be computed with only a slight
increase over the work done for diffuse surfaces. The model adds a term for specular
reﬂection. Hence, we consider the surface as being rough for the diffuse term and
smooth for the specular term. The amount of light that the viewer sees depends on
the angle φ between r, the direction of a perfect reﬂector, and v, the direction of the
viewer. The Phong model uses the equation
Is = ksLs cosα φ.
The coefﬁcient ks (0 ≤ ks ≤ 1) is the fraction of the incoming specular light that is
reﬂected. The exponent α is a shininess coefﬁcient. Figure 5.19 shows how, as α is
increased, the reﬂected light is concentrated in a narrower region centered on the
angle of a perfect reﬂector. In the limit, as α goes to inﬁnity, we get a mirror; values in
the range 100 to 500 correspond to most metallic surfaces, and smaller values (< 100)
correspond to materials that show broad highlights.
The computational advantage of the Phong model is that if we have normalized r
and n to unit length, we can again use the dot product, and the specular term becomes
Is = ksLsmax((r . v)α , 0).
We can add a distance term, as we did with diffuse reﬂections. What is referred to as
the Phong model, including the distance term, is written
I=                (k L max(l . n, 0) + ksLsmax((r . v)α , 0)) + ka La .
a + bd + cd 2 d d
This formula is computed for each light source and for each primary.
270   Chapter 5   Lighting and Shading
=1
=2
=5

*FIGURE 5.19 Effect of shininess coefficient.*

It might seem to make little sense either to associate a different amount of ambi-
ent light with each source or to allow the components for specular and diffuse lighting
to be different. Because we cannot solve the full rendering equation, we must use var-
ious tricks in an attempt to obtain realistic renderings.
Consider, for example, an environment with many objects. When we turn on a
light, some of that light hits a surface directly. These contributions to the image can
be modeled with specular and diffuse components of the source. However, much of
the rest of the light from the source is scattered from multiple reﬂections from other
objects and makes a contribution to the light received at the surface under considera-
tion. We can approximate this term by having an ambient component associated with
the source. The shade that we should assign to this term depends on both the color of
the source and the color of the objects in the room—an unfortunate consequence of
our use of approximate models. To some extent, the same analysis holds for diffuse
light. Diffuse light reﬂects among the surfaces, and the color that we see on a partic-
ular surface depends on other surfaces in the environment. Again, by using carefully
chosen diffuse and specular components with our light sources, we can approximate
a global effect with local calculations.
We have developed the Phong model in object space. The actual shading, how-
ever, is not done until the objects have passed through the model-view and projection
transformations. These transformations can affect the cosine terms in the model (see
Exercise 5.19). Consequently, to make a correct shading calculation, we must either
preserve spatial relationships as vertices and vectors pass through the pipeline, per-
haps by sending additional information through the pipeline from object space, or
go backward through the pipeline to obtain the required shading information.

#### 5.3.4 The Modified Phong Model

If we use the Phong model with specular reﬂections in our rendering, the dot product
r . v should be recalculated at every point on the surface. We can obtain an interesting
approximation by using the unit vector halfway between the viewer vector and the
light-source vector:
                                                                                 5.4 Computation of Vectors           271
l+v
h=            .                                                                                           n
|l + v|
ψ
               r
Figure 5.20 shows all ﬁve vectors. Here we have deﬁned ψ as the angle between n and                                       v
h, the halfway angle. When v lies in the same plane as do l, n, and r, we can show
(see Exercise 5.7) that
2ψ = φ.

*FIGURE 5.20 Determination*

of the halfway vector.
If we replace r . v with n . h, we avoid calculation of r. However, the halfway angle ψ
is smaller than φ, and if we use the same exponent e in (n . h)e that we used in (r . v)e ,
then the size of the specular highlights will be smaller. We can mitigate this problem

by replacing the value of the exponent e with a value e  so that (n . h)e is closer to
(r . v)e . It is clear that avoiding recalculation of r is desirable. However, to appreciate
fully where savings can be made, you should consider all the cases of ﬂat and curved
surfaces, near and far light sources, and near and far viewers (see Exercise 5.8).
When we use the halfway vector in the calculation of the specular term, we
are using the Blinn-Phong, or modiﬁed Phong, lighting model. This model is the
default in systems with a ﬁxed-function pipeline and is the one we will use in our
ﬁrst shaders that carry out lighting.
Color Plate 17 shows a group of Utah teapots (Section 10.10) that have been ren-
dered in OpenGL using the modiﬁed Phong model. Note that it is only our ability to
control material properties that makes the teapots appear different from one another.
The various teapots demonstrate how the modiﬁed Phong model can create a variety
of surface effects, ranging from dull surfaces to highly reﬂective surfaces that look like
metal.

### 5.4     COMPUTATION OF VECTORS

The illumination and reﬂection models that we have derived are sufﬁciently general
that they can be applied to either curved or ﬂat surfaces, to parallel or perspective
views, and to distant or near surfaces. Most of the calculations for rendering a scene
involve the determination of the required vectors and dot products. For each special
case, simpliﬁcations are possible. For example, if the surface is a ﬂat polygon, the
normal is the same at all points on the surface. If the light source is far from the
surface, the light direction is the same at all points.
In this section, we examine how the vectors are computed for the general case. In
Section 5.5, we see what additional techniques can be applied when our objects are
composed of ﬂat polygons. This case is especially important because most renderers,
including OpenGL, render curved surfaces by approximating those surfaces with
many small, ﬂat polygons.
272   Chapter 5   Lighting and Shading

#### 5.4.1 Normal Vectors

For smooth surfaces, the vector normal to the surface exists at every point and gives
the local orientation of the surface. Its calculation depends on how the surface is
represented mathematically. Two simple cases—the plane and the sphere—illustrate
both how we compute normals and where the difﬁculties lie.
A plane can be described by the equation
ax + by + cz + d = 0.
As we saw in Chapter 3, this equation could also be written in terms of the normal to
the plane, n, and a point, p0, known to be on the plane as
n . (p − p0) = 0,
where p is any point (x, y, z) on the plane. Comparing the two forms, we see that the
⎡ ⎤
n=⎣b⎦,
or, in homogeneous coordinates,
⎡ ⎤
⎢b⎥
⎢ ⎥
n=⎢ ⎥.
⎣c⎦
However, suppose that instead we are given three noncollinear points—p0, p1, p2—
that are in this plane and thus are sufﬁcient to determine it uniquely. The vectors
p2 − p0 and p1 − p0 are parallel to the plane, and we can use their cross product to
ﬁnd the normal
n = (p2 − p0) × (p1 − p0).
We must be careful about the order of the vectors in the cross product: Reversing
the order changes the surface from outward pointing to inward pointing, and that
reversal can affect the lighting calculations. Some graphics systems use the ﬁrst three
vertices in the speciﬁcation of a polygon to determine the normal automatically.
OpenGL does not do so, but as we shall see in Section 5.5, forcing users to compute
normals creates more ﬂexibility in how we apply our lighting model.
For curved surfaces, how we compute normals depends on how we represent the
surface. In Chapter 10, we discuss three different methods for representing curves
and surfaces. We can see a few of the possibilities by considering how we represent a
unit sphere centered at the origin. The usual equation for this sphere is the implicit
                                                                              5.4 Computation of Vectors          273
f (x, y, z) = x 2 + y 2 + z 2 − 1 = 0,
or in vector form,
f (p) = p . p − 1 = 0.
The normal is given by the gradient vector, which is deﬁned by the column matrix
⎡ ∂f ⎤
∂x
⎡ ⎤
2x
⎢ ∂f ⎥
n=⎢      ⎥ = ⎣ 2y ⎦ = 2p.
⎣ ∂y ⎦
∂f         2z
∂z
The sphere could also be represented in parametric form. In this form, the x, y,
and z values of a point on the sphere are represented independently in terms of two
parameters u and v:
x = x(u, v),
y = y(u, v),
z = z(u, v).
As we shall see in Chapter 10, this form is preferable in computer graphics, especially
for representing curves and surfaces; although, for a particular surface, there may be
multiple parametric representations. One parametric representation for the sphere is
x(u, v) = cos u sin v,
y(u, v) = cos u cos v,
z(u, v) = sin u.                                                                                         p
As u and v vary in the range −π/2 < u < π/2, −π < v < π , we get all the points on
the sphere. When we are using the parametric form, we can obtain the normal from
the tangent plane, shown in Figure 5.21, at a point p(u, v) = [x(u, v) y(u, v) z(u, v)]T    FIGURE 5.21 Tangent plane
on the surface. The tangent plane gives the local orientation of the surface at a point;    to sphere.
we can derive it by taking the linear terms of the Taylor series expansion of the surface
at p. The result is that at p, lines in the directions of the vectors represented by
⎡ ∂x ⎤                     ⎡ ∂x ⎤
∂u                     ∂v
∂p ⎢ ∂y ⎥                 ∂p ⎢ ∂y ⎥
= ⎣ ∂u ⎦ ,                = ⎣ ∂v ⎦
∂u                        ∂v
∂z                     ∂z
∂u                     ∂v
lie in the tangent plane. We can use their cross product to obtain the normal
∂p ∂p
n=      ×    .
∂u   ∂v
For our sphere, we ﬁnd that
274            Chapter 5    Lighting and Shading
⎡            ⎤
n = cos u ⎣ cos u cos v ⎦ = (cos u)p.
We are interested in only the direction of n; thus, we can divide by cos u to obtain the
n = p.
In Section 5.9, we use this result to shade a polygonal approximation to a sphere.
Within a graphics system, we usually work with a collection of vertices, and the
normal vector must be approximated from some set of points close to the point where
the normal is needed. The pipeline architecture of real-time graphics systems makes
this calculation difﬁcult because we process one vertex at a time, and thus the graphics
system may not have the information available to compute the approximate normal at
a given point. Consequently, graphics systems often leave the computation of normals
to the user program.
In OpenGL, we will usually set up a normal as a vertex attribute by a mechanism
typedef normal vec4;
normal n = vec4(nx, ny, nz, 0.0);
n                  and then send the normal as needed to a vertex shader as an attribute qualiﬁed
i       r
variable.

#### 5.4.2 Angle of Reflection

Once we have calculated the normal at a point, we can use this normal and the
direction of the light source to compute the direction of a perfect reﬂection. An ideal
mirror is characterized by the following statement: The angle of incidence is equal

*FIGURE 5.22 A mirror.           to the angle of reﬂection. These angles are as pictured in Figure 5.22. The angle of*

incidence is the angle between the normal and the light source (assumed to be a point
source); the angle of reﬂection is the angle between the normal and the direction in
which the light is reﬂected. In two dimensions, there is but a single angle satisfying
the angle condition. In three dimensions, however, our statement is insufﬁcient to
compute the required angle: There is an inﬁnite number of angles satisfying our
condition. We must add the following statement: At a point p on the surface, the
incoming light ray, the reﬂected light ray, and the normal at the point must all lie in
the same plane. These two conditions are sufﬁcient for us to determine r from n and
l. Our primary interest is the direction, rather than the magnitude, of r. However,
many of our rendering calculations will be easier if we deal with unit-length vectors.
Hence, we assume that both l and n have been normalized such that
|l| = |n| = 1.
                                                                                    5.5 Polygonal Shading   275
We also want
|r| = 1.
If θi = θr , then
cos θi = cos θr .
Using the dot product, the angle condition is
cos θi = l . n = cos θr = n . r.
The coplanar condition implies that we can write r as a linear combination of l
and n:
r = αl + βn.
Taking the dot product with n, we ﬁnd that
n . r = αl . n + β = l . n.
We can get a second condition between α and β from our requirement that r also be
of unit length; thus,
1 = r . r = α 2 + 2αβl . n + β 2 .
Solving these two equations, we ﬁnd that
r = 2(l . n)n − l.
Some of the shaders we develop will use this calculation to compute a reﬂection
vector for use in the application; others that need the reﬂection vector only in a shader
can use the GLSL reflect function to compute it. Methods such as environment
maps will use the reﬂected-view vector (see Exercise 5.26) that is used to determine
what a viewer would see if she looked at a reﬂecting surface such as a highly polished
sphere.

### 5.5    POLYGONAL SHADING

Assuming that we can compute normal vectors, given a set of light sources and a
viewer, the lighting models that we have developed can be applied at every point on
a surface. Unfortunately, even if we have simple equations to determine normal vec-
tors, as we did in our example of a sphere (Section 5.4), the amount of computation
required can be large. We have already seen many of the advantages of using polyg-
onal models for our objects. A further advantage is that for ﬂat polygons, we can
signiﬁcantly reduce the work required for shading. Most graphics systems, including
276   Chapter 5   Lighting and Shading

*FIGURE 5.23 Polygonal mesh.*


*FIGURE 5.24 Distant source and viewer.*

OpenGL, exploit the efﬁciencies possible for rendering ﬂat polygons by decomposing
curved surfaces into many small, ﬂat polygons.
Consider a polygonal mesh, such as that shown in Figure 5.23, where each poly-
gon is ﬂat and thus has a well-deﬁned normal vector. We consider three ways to shade
the polygons: ﬂat shading, smooth or Gouraud shading, and Phong shading.

#### 5.5.1 Flat Shading

The three vectors—l, n, and v—can vary as we move from point to point on a surface.
For a ﬂat polygon, however, n is constant. If we assume a distant viewer, v is constant
over the polygon. Finally, if the light source is distant, l is constant. Here distant
could be interpreted in the strict sense of meaning that the source is at inﬁnity. The
necessary adjustments, such as changing the location of the source to the direction of
the source, could then be made to the shading equations and to their implementation.
Distant could also be interpreted in terms of the size of the polygon relative to how far
the polygon is from the source or viewer, as shown in Figure 5.24. Graphics systems
or user programs often exploit this deﬁnition.
If the three vectors are constant, then the shading calculation needs to be carried
out only once for each polygon, and each point on the polygon is assigned the same
shade. This technique is known as ﬂat, or constant, shading.
Flat shading will show differences in shading among the polygons in our mesh.
If the light sources and viewer are near the polygon, the vectors l and v will be dif-
                                                                                     5.5 Polygonal Shading        277

*FIGURE 5.25 Flat shading of polygonal mesh.*

ferent for each polygon. However, if our polygonal mesh has been designed to model
a smooth surface, ﬂat shading will almost always be disappointing because we can
see even small differences in shading between adjacent polygons, as shown in Fig-
ure 5.25. The human visual system has a remarkable sensitivity to small differences
in light intensity, due to a property known as lateral inhibition. If we see an increas-
ing sequence of intensities, as is shown in Figure 5.26, we perceive the increases in
brightness as overshooting on one side of an intensity step and undershooting on
the other, as shown in Figure 5.27. We see stripes, known as Mach bands, along the           FIGURE 5.26 Step chart.
edges. This phenomenon is a consequence of how the cones in the eye are connected
to the optic nerve, and there is little that we can do to avoid it, other than to look for
smoother shading techniques that do not produce large differences in shades at the
edges of polygons.

#### 5.5.2 Smooth and Gouraud Shading

In our rotating-cube example of Section 3.12, we saw that the rasterizer interpolates
colors assigned to vertices across a polygon. Suppose that the lighting calculation
is made at each vertex using the material properties and the vectors n, v, and l
computed for each vertex. Thus, each vertex will have its own color that the rasterizer
can use to interpolate a shade for each fragment. Note that if the light source is
distant, and either the viewer is distant or there are no specular reﬂections, then
smooth (or interpolative) shading shades a polygon in a constant color.

*FIGURE 5.27 Perceived and actual intensities at an edge.*

278         Chapter 5         Lighting and Shading
n                         If we consider our mesh, the idea of a normal existing at a vertex should cause
concern to anyone worried about mathematical correctness. Because multiple poly-
n2            n4         gons meet at interior vertices of the mesh, each of which has its own normal, the
n1            n3
normal at the vertex is discontinuous. Although this situation might complicate the
mathematics, Gouraud realized that the normal at the vertex could be deﬁned in such
a way as to achieve smoother shading through interpolation. Consider an interior ver-
tex, as shown in Figure 5.28, where four polygons meet. Each has its own normal. In
Gouraud shading, we deﬁne the normal at a vertex to be the normalized average of

*FIGURE 5.28 Normals near            the normals of the polygons that share the vertex. For our example, the vertex normal*


```cpp
interior vertex.                    is given by
```

n1 + n2 + n3 + n4
n=                              .
|n1 + n2 + n3 + n4|
From an OpenGL perspective, Gouraud shading is deceptively simple. We need only
to set the vertex normals correctly. Often, the literature makes no distinction between
smooth and Gouraud shading. However, the lack of a distinction causes a problem:
How do we ﬁnd the normals that we should average together? If our program is lin-
ear, specifying a list of vertices (and other properties), we do not have the necessary
information about which polygons share a vertex. What we need, of course, is a data

```cpp
structure for representing the mesh. Traversing this data structure can generate the
```

vertices with the averaged normals. Such a data structure should contain, at a mini-
mum, polygons, vertices, normals, and material properties. One possible structure is
the one shown in Figure 5.29. The key information that must be represented in the
data structure is which polygons meet at each vertex.
Color Plates 4 and 5 show the shading effects available in OpenGL. In Color
Plate 4, there is a single light source, but each polygon has been rendered with a
single shade (constant shading), computed using the Phong model. In Color Plate 5,

*FIGURE 5.29 Mesh data structure.*

                                                                                  5.5 Polygonal Shading          279

*FIGURE 5.30 Edge normals.*

normals have been assigned to all the vertices. OpenGL has then computed shades
for the vertices and has interpolated these shades over the faces of the polygons.
Color Plate 21 contains another illustration of the smooth shading provided
by OpenGL. We used this color cube as an example in both Chapters 2 and 3, and
the programs are in Appendix A. The eight vertices are colored black, white, red,
green, blue, cyan, magenta, and yellow. Once smooth shading is enabled, OpenGL

```cpp
interpolates the colors across the faces of the polygons automatically.
```


#### 5.5.3 Phong Shading

Even the smoothness introduced by Gouraud shading may not prevent the appear-
ance of Mach bands. Phong proposed that instead of interpolating vertex intensities,
as we do in Gouraud shading, we interpolate normals across each polygon. Consider
a polygon that shares edges and vertices with other polygons in the mesh, as shown                nC        nD
in Figure 5.30. We can compute vertex normals by interpolating over the normals of                     nB
the polygons that share the vertex. Next, we can use interpolation, as we did in Chap-
ter 3, to interpolate the normals over the polygon. Consider Figure 5.31. We can use     FIGURE 5.31 Interpolation of
the interpolated normals at vertices A and B to interpolate normals along the edge       normals in Phong shading.
between them:
nC (α) = (1 − α)nA + αnB .
We can do a similar interpolation on all the edges. The normal at any interior point
can be obtained from points on the edges by
n(α, β) = (1 − β)nC + βnD .
Once we have the normal at each point, we can make an independent shading calcu-
lation. Usually, this process can be combined with rasterization of the polygon. Until
recently, Phong shading could only be carried out off-line because it requires the in-
terpolation of normals across each polygon. In terms of the pipeline, Phong shading
280   Chapter 5   Lighting and Shading
requires that the lighting model be applied to each fragment, hence, the name per-
fragment shading. We will implement Phong shading through a fragment shader.

### 5.6     APPROXIMATION OF A SPHERE

We have used the sphere as an example curved surface to illustrate shading calcu-
lations. However, the sphere is not an object supported within OpenGL, so we will
generate approximations to a sphere using triangles through a process known as re-
cursive subdivision, a technique we introduced in Chapter 2 for constructing the
Sierpinski gasket. Recursive subdivision is a powerful technique for generating ap-
proximations to curves and surfaces to any desired level of accuracy. The sphere
approximation provides a basis for us to write simple programs that illustrate the

```cpp
interactions between shading parameters and polygonal approximations to curved
```

surfaces.
Our starting point is a tetrahedron, although we could start with any regular
polyhedron whose facets could be divided initially into triangles.2 The regular tetra-
hedron is composed of four equilateral√triangles, determined√    by √
four vertices. We
√ with √  the four vertices (0, 0, 1), (0, 2 2/3, −1/3), (− 6/3, − 2/3, −1/3), and
( 6/3, − 2/3, −1/3). All four lie on the unit sphere, centered at the origin. (Exer-
cise 5.6 suggests one method for ﬁnding these points.)
We get a ﬁrst approximation by drawing a wireframe for the tetrahedron. We
specify the four vertices as follows:
point4 v[4]= {vec4(0.0, 0.0, 1.0, 1.0),
vec4(0.0, 0.942809, -0.333333, 1.0),
vec4(-0.816497, -0.471405, -0.333333, 1.0),
vec4(0.816497, -0.471405, -0.333333, 1.0)};
We then put the vertices into an array
point4 data[12];
so we can display the triangles using line loops. Each triangle adds three points to this

```cpp
static int k = 0;
void triangle(point4 a, point4 b, point4 c)
{
```

data[k]= a;
2. The regular icosahedron is composed of 20 equilateral triangles; it makes a nice starting point for
generating spheres. See [Shr10].
                                            5.6 Approximation of a Sphere by Recursive Subdivision              281
k++;
data[k] = b;
k++;
data[k] = c;
k++

```cpp
}
```

All the data for the tetrahedron is put into data as follows:

```cpp
void tetrahedron()
{
```

triangle(v[0], v[1], v[2]);
triangle(v[3], v[2], v[1]);
triangle(v[0], v[3], v[1]);
triangle(v[0], v[2], v[3]);

```cpp
}
```

The order of vertices obeys the right-hand rule, so we can convert the code to draw
shaded polygons with little difﬁculty. If we add the usual code for initialization, set-
ting up a vertex buffer object and drawing the array, our program will generate an
image such as that in Figure 5.32: a simple regular polyhedron, but a poor approxi-        FIGURE 5.32 Tetrahedron.
mation to a sphere.
We can get a closer approximation to the sphere by subdividing each facet of the
tetrahedron into smaller triangles. Subdividing into triangles will ensure that all the
new facets will be ﬂat. There are at least three ways to do the subdivision, as shown
in Figure 5.33. We can bisect each of the angles of the triangle and draw the three
bisectors, which meet at a common point, thus generating three new triangles. We
can also compute the center of mass (centroid) of the vertices by simply averaging
them and then draw lines from this point to the three vertices, again generating three
triangles. However, these techniques do not preserve the equilateral triangles that
make up the regular tetrahedron. Instead—recalling a construction for the Sierpinski
gasket of Chapter 2—we can connect the bisectors of the sides of the triangle, forming
four equilateral triangles, as shown in Figure 5.33(c). We use this technique for our
example.
After we have subdivided a facet as just described, the four new triangles will
still be in the same plane as the original triangle. We can move the new vertices that
(a)                  (b)                   (c)

*FIGURE 5.33 Subdivision of a triangle by (a) bisecting angles,*

(b) computing the centroid, and (c) bisecting sides.
282   Chapter 5   Lighting and Shading
we created by bisection to the unit sphere by normalizing each bisected vertex, using
the normalization function normalize in vec.h. We can now subdivide a single
triangle, deﬁned by the vertices a, b, and c:
point4 v1, v2, v3;
v1 = normalize(a + b);
v2 = normalize(a + c);
v3 = normalize(b + c);
triangle(a, v2, v1);
triangle(c, v3, v2);
triangle(b, v1, v3);
triangle(v1, v2, v3);
We can use this code in our tetrahedron routine to generate 16 triangles rather than
4, but we would rather be able to repeat the subdivision process n times to generate
successively closer approximations to the sphere. By calling the subdivision routine
recursively, we can control the number of subdivisions.
First, we make the tetrahedron routine depend on the depth of recursion by
adding an argument n:

```cpp
void tetrahedron(int n)
{
```

divide_triangle(v[0], v[1], v[2], n);
divide_triangle(v[3], v[2], v[1], n);
divide_triangle(v[0], v[3], v[1], n);
divide_triangle(v[0], v[2], v[3], n);

```cpp
}
```

The divide_triangle function calls itself to subdivide further if n is greater than
zero but generates triangles if n has been reduced to zero. Here is the code:

```cpp
void divide_triangle(point4 a, point4 b, point4 c, int n)
{
```

point4 v1, v2, v3;
if (n>0)

```cpp
{
```

v1 = normalize(v[a] + v[b]);
v2 = normalize(v[a] + v[c]);
v3 = normalize(v[b] + v[c]);
divide_triangle(a ,v2, v1, n-1);
divide_triangle(c ,v3, v2, n-1);
divide_triangle(b ,v1, v3, n-1);
divide_triangle(v1 ,v2, v3, n-1);

```cpp
}
```

else triangle(a, b, c);

```cpp
}
```

                                                                      5.7 Specifying Lighting Parameters           283
Figure 5.34 shows an approximation to the sphere drawn with this code. We now turn
to adding lighting and shading to our sphere approximation.

### 5.7    SPECIFYING LIGHTING PARAMETERS

For many years, the Blinn-Phong lighting model was the standard in computer
graphics. It was implemented in hardware and was speciﬁed as part of the OpenGL
ﬁxed-functionality pipeline. With the present emphasis on shaders, we are free to im-
plement other lighting models with no loss of efﬁciency. We can also choose where           FIGURE 5.34 Sphere approx-
to apply a light model. Consequently, we must specify a group of lighting and mate-         imations using subdivision.
rial parameters and then either use them in the application code or send them to the
shaders.

#### 5.7.1 Light Sources

In Section 5.2. we introduced four types of light sources: ambient, point, spotlight,
and distant. However, because spotlights and distant light sources can be derived
from a point source, we will focus on point sources and ambient light. An ideal point
source emits light uniformly in all directions. To get a spotlight from a point source,
we need only limit the directions of the point source and make the light emissions
follow a desired proﬁle. To get a distant source from a point source, we need to allow
the location of the source to go to inﬁnity so the position of the source becomes
the direction of the source. Note that this argument is similar to the argument that
parallel viewing is the limit of perspective viewing as the center of projection moves
to inﬁnity. As we argued in deriving the equations for parallel projections, we will
ﬁnd it easier to derive the equations for lighting with distant sources directly rather
than by taking limits.
Although the contribution of ambient light is the same everywhere in a scene,
ambient light is dependent on the sources in the environment. For example, consider
a closed room with a single white point source. When the light is turned off, there is
no light in the room of any kind. When the light is turned on, at any point in the room
that can see the light source there is a contribution from the light hitting surfaces
directly contributing to the diffuse or specular reﬂection we see at that point. There
is also a contribution from the white light bouncing off of multiple surfaces in the
room and giving a contribution that is almost the same at every point in the room. It
is this latter contribution that we call ambient light. Its color depends not only on the
color of the source but also on the reﬂective properties of the surfaces in the room.
Thus if the room has red walls, we would expect the ambient component to have a
dominant red component. However, the existence of an ambient component to the
shade we see on a surface is ultimately tied to the light sources in the environment
and hence becomes part of the speciﬁcation of the sources.
For every light source, we must specify its color and either its location or its
direction. As in Section 5.2, the color of a source will have three components—
diffuse, specular, and ambient—that we can specify for a single light as
284   Chapter 5   Lighting and Shading
color4 light_diffuse, light_specular, light_ambient;
We can specify the position of the light as follows:
point4 light_position;
For a point source, its position will be in homogeneous coordinates, so a light might
be speciﬁed as
point4 light_position = vec4(1.0, 2.0, 3.0, 1.0);
If the fourth component is changed to zero as in
point4 light_position = vec4(1.0, 2.0, 3.0, 0.0);
the source becomes a directional source in the direction (1.0, 2.0, 3.0).
For positional light sources, we may also want to account for the attenuation
of light received due to its distance from the source. Although for an ideal source the
attenuation is inversely proportional to the square of the distance d, we can gain more
ﬂexibility by using the distance-attenuation model,
f (d) =                 ,
a + bd + cd 2
which contains constant, linear, and quadratic terms. We can use three ﬂoats for these
values,

```cpp
float attenuation_constant, attenuation_linear, attenuation_quadratic;
```

and use them in the application or send them to the shaders as uniform variables.
We can also convert a positional source to a spotlight by setting its direction, the
angle of the cone or the spotlight cutoff, and the drop off rate or spotlight exponent.
These three parameters can be speciﬁed by three ﬂoats.

#### 5.7.2 Materials

Material properties should match up directly with the supported light sources and
with the chosen reﬂection model. We may also want the ﬂexibility to specify different
material properties for the front and back faces of a surface.
For example, we might specify ambient, diffuse, and specular reﬂectivity coef-
ﬁcients (ka , kd , ks) for each primary color through three colors using either RGB or
color3 ambient = color3(0.2, 0.2, 0.2);
color3 diffuse = color3(1.0, 0.8, 0.0);
color3 specular = color3(1.0, 1.0, 1.0);
                                                                      5.7 Specifying Lighting Parameters   285
or, assuming the surface is opaque,
color4 ambient = color4(0.2, 0.2, 0.2, 1.0);
color4 diffuse = color4(1.0, 0.8, 0.0, 1.0);
color4 specular = color4(1.0, 1.0, 1.0, 1.0);
Here we have deﬁned a small amount of white ambient reﬂectivity, yellow diffuse
properties, and white specular reﬂections. Note that often the diffuse and specular
reﬂectivity are the same. For the specular component, we also need to specify its
shininess:

```cpp
float shininess;
```

If we have different reﬂectivity properties for the front and back faces, we can also
specify three additional parameters,
color4 back_ambient, back_diffuse, back_specular;
that can be used to render the back faces.
We also want to allow for scenes in which a light source is within the view volume
and thus might be visible. For example, for an outdoor night scene, we might see the
moon in an image. We could model the moon with a simple polygonal approximation
to a circle. However, when we render the moon, its color should be constant and
not be affected by other light sources. We can create such effects by including an
emissive component that models self-luminous sources. This term is unaffected by
any of the light sources, and it does not affect any other surfaces. It adds a ﬁxed color
to the surfaces and is speciﬁed in a manner similar to other material properties. For
example,
color4 emission = color4(0.0, 0.3, 0.3, 1.0);
specﬁes a small amount of blue-green (cyan) emission.
From an application programmer’s perspective, we would like to have material
properties that we can specify with a single function call. We can achieve this goal by
deﬁning material objects in the application using structs or classes. For example,
typedef struct materialStruct {
color4 ambient;
color4 diffuse;
color4 specular;
color4 emission;

```cpp
float shininess;
} materialStruct;
```

We can now deﬁne materials by code such as
286   Chapter 5   Lighting and Shading
materialStruct brassMaterials =

```cpp
{
{0.33, 0.22, 0.03, 1.0},
{0.78, 0.57, 0.11, 1.0},
{0.99, 0.91, 0.81, 1.0},
{0.0, 0.0, 0.0, 1.0},
```

27.8

```cpp
};
```

and access this code through a pointer,
currentMaterial = &brassMaterials;

### 5.8   IMPLEMENTING A LIGHTING MODEL

Thus far, we have only looked at parameters that we might use in a light model. We
have yet to build a particular model. Nor have we worried about where to apply
a lighting model. We focus on a simple version of the Blinn-Phong model using a
single point source. Because light from multiple sources is additive, we can repeat the
calculation for each source and add up the individual contributions. We have three
choices as to where we do the calculation: in the application, in the vertex shader,
or in the fragment shader. Although the basic model can be the same for each, there
will major differences both in efﬁciency and appearance, depending on where the
calculation is done.

#### 5.8.1 Applying the Lighting Model in the Application

We have used two methods to assign colors to ﬁlled triangles. In the ﬁrst, we sent
a single color for each polygon to the shaders as a uniform variable and used this
color for each fragment. In the second, we assigned a color to each vertex as a vertex
attribute. The rasterizer then interpolated these vertex colors across the polygon.
Both these approaches can be applied to lighting. In constant or ﬂat shading, we
apply a lighting model once for each polygon and use the computed color for the
entire polygon. In the interpolative shading, we apply the model at each vertex to
compute a vertex color attribute. The vertex shader can then output these colors and
the rasterizer will interpolate them to determine a color for each fragment.
Let’s do a simple example with ambient, diffuse, and specular lighting. Assume
that the following parameters have been speciﬁed for a single point light source:
color4 light_ambient, light_diffuse, light_specular;
point4 light_position;
Also assume that there is a single material whose parameters are
color4 reflect_ambient, reflect_diffuse, reflect_specular;
                                                                  5.8 Implementing a Lighting Model   287
The color we need to compute is the sum of the ambient, diffuse, and specular
contributions:
color4 color_out, ambient, diffuse, specular;
color_out = ambient + diffuse + specular;
Each component of the ambient term is the product of the corresponding terms from
the ambient light source and the material reﬂectivity. We can use the function
vec4 product(vec4 a, vec4 b)

```cpp
{
```

return vec4(a[0]*b[0], a[1]*b[1], a[2]*b[2], a[3]*b[3]);

```cpp
}
```

Hence,
ambient   = product(light_ambient, reflect_ambient);
We need the normal to compute the diffuse term. Because we are working with
triangles, we have the three vertices and these vertices determine a unique plane and
its normal. Suppose that we have three vertices v0, v1, and v2. The cross product
of v1-v0 and v2-v1 is perpendicular to the plane determined by the three vertices.
Thus, we get the desired unit normal as follows:
vec4 v0, v1, v2;
vec4 n = normalize(cross(v1-v0, v2-v1));
Note that the direction of the normal depends on the order of the vertices and
assumes we are using the right-hand rule to determine an outward face.
Next, we need to take the dot product of the unit normal with the vector in the
direction of the light source. There are four cases we must consider:
1. Constant shading with a distant source
2. Interpolative shading with a distant source
3. Constant shading with a ﬁnite source
4. Interpolative shading with a ﬁnite source
For constant shading, we only need to compute a single diffuse color for each
triangle. For a distant source, we have the direction of the source that is the same
for all points on the triangle. Hence, we can simply take the dot product of the unit
normal with a normalized source direction. The diffuse contribution is then
color4 diffuse = product(light_diffuse, reflect_ambient)*dot(n, normalize(light_position));
288   Chapter 5   Lighting and Shading
There is one additional step we should take. The diffuse term only makes sense if the
dot product is nonnegative, so we must modify the calculation to
color4 diffuse;

```cpp
float d = dot(n, normalize(light_position));
```

if (d>0) diffuse = product(light_diffuse, reflect_ambient)*d;
else diffuse = vec4(0.0, 0.0, 0.0, 1.0);
For a distant light source, the diffuse contribution at each vertex is identical,
so we need do only one diffuse calculation per polygon, and thus interpolative and

```cpp
constant diffuse shading are the same.
```

For a ﬁnite or near source, we have two choices: We either compute a single
diffuse contribution for the entire polygon and use constant shading, or we compute
the diffuse term at each vertex and use interpolative shading. Because we are working
with triangles, the normal is the same at each vertex, but with a near source the vector
from any point on the polygon to the light source will be different. If we use a single
color, we can use the point at the center of the triangle to compute the direction:
point4 v = (1.0/3.0)* (v0 + v1 + v2);
vector4 light_vector = light_position - v;

```cpp
float d = dot(n, normalize(light_vector));
```

if (d>0) diffuse = product(light_diffuse, reflect_ambient)*d;
else diffuse = vec4(0.0, 0.0, 0.0, 1.0);
The calculation for the specular term appears to be similar to the calculation for
the diffuse, but there is one tricky issue. We need to compute the halfway vector. For
a distance source, the light position becomes a direction, so
vec4 half = normalize(light_position + view_direction);
The view direction is a vector from a point on the surface to the eye. The default is
that the camera is at the origin in object space, so for a vertex v, the vector is
vec4 origin = vec4(0.0, 0.0, 0.0, 1.0);
vec4 view_direction = v - origin;
Thus, even though each triangle has a single normal, there is a different halfway
vector for each vertex, and consequently the specular term will vary across the surface
as the rasterizer interpolates the vertex shades. The specular term for vertex v can be
color4 specular;

```cpp
float s = dot(half, n);
```

                                                                     5.8 Implementing a Lighting Model             289
if (s>0.0) specular =
pow(s, material_shininess)*product(light_specular, material_specular);
else specular = vec4(0.0, 0.0, 0.0, 1.0);
where the expression exp(material_shininess*log(s)) evaluates s to the
power material_shininess.

#### 5.8.2 Efficiency

For a static scene, the lighting computation is done once so we can send the vertex
positions and vertex colors to the GPU once. Consider what happens if we add
lighting to our rotating-cube program. Each time the cube rotates about a coordinate
axis, the normal to four of the faces changes as does the position of each of the six
vertices. Hence, we must recompute the diffuse and specular components at each of
the vertices. If we do all the calculations in the CPU, both the vertex positions and
colors must then be sent to the GPU. For large data sets, this process is extremely
inefﬁcient. Not only are we doing a lot of computation on the CPU, but we are
also causing a potential bottleneck by sending so much vertex data to the GPU.
Consequently, we will almost always want to do lighting calculation in the shaders.
Before examining shaders for lighting, there are a few other efﬁciency measures
we can employ, either in the application or in a shader. We can obtain many efﬁcien-
cies if we assume that either or both of the viewer and the light source are far from the
polygon we are rendering. Hence, even if the source is a point source with a ﬁnite lo-
cation, it might be far enough away that the distances from the vertices to the source
are all about the same. In this case, the diffuse term at each vertex would be identical
and we would need do only one calculation per polygon. Note that a deﬁnition of far
and near in this context depends both on the distance to the light source and the size
of the polygon. A small polygon will not show much variation in the diffuse compo-
nent even if the source is fairly close to the polygon. The same argument holds for the
specular term when we consider the distance between vertices and the viewer. We can
add parameters that allow the application to specify if it wants to use these simpliﬁed
calculations.

*FIGURE 5.35 Shading of con-*

In Chapter 3, we saw that a surface has both a front face and a back face. For        vex objects.
polygons, we determine front and back by the order in which the vertices are spec-
iﬁed, using the right-hand rule. For most objects, we see only the front faces, so we
are not concerned with how OpenGL shades the back-facing surfaces. For example,
for convex objects, such as a sphere or a parallelepiped (Figure 5.35), the viewer can
never see a back face, regardless of where she is positioned. However, if we remove a
side from a cube or slice the sphere, as shown in Figure 5.36, a properly placed viewer

*FIGURE 5.36 Visible back*

may see a back face; thus, we must shade both the front and back faces correctly. In
surfaces.
many situations, we can ignore all back faces by either culling them out in the appli-
cation or by not rendering any face whose normal does not point toward the viewer.
If we render back faces, they may have different material properties than the front
faces, so we must specify a set of back face properties.
290   Chapter 5   Lighting and Shading
Light sources are special types of geometric objects and have geometric at-
tributes, such as position, just like polygons and points. Hence, light sources can
be affected by transformations. We can specify them at the desired position or specify
them in a convenient position and move them to the desired position by the model-
view transformation. The basic rule governing object placement is that vertices are
converted to eye coordinates by the model-view transformation in effect at the time
the vertices are deﬁned. Thus, by careful placement of the light-source speciﬁcations
relative to the deﬁnition of other geometric objects, we can create light sources that
remain stationary while the objects move, light sources that move while objects re-
main stationary, and light sources that move with the objects.
We also have choices as to which coordinate system to use for lighting computa-
tions. For now, we will do our lighting calculations in object coordinates. Depending
on whether or not the light source or objects are moving, it may be more efﬁcient to
use eye coordinates. Later when we add texture mapping to our skills, we will intro-
duce lighting methods that will use local coordinate systems.

#### 5.8.3 Lighting in the Vertex Shader

When we presented transformations, we saw that a transformation such as the
model-view transformation could be carried out either in the application or the
vertex shader, but for most applications it was far more efﬁcient to implement
the transformation in the shader. The same is true for lighting. To implement lighting
in the vertex shader, we must carry out three steps.
First, we must choose a lighting model. Do we use the Blinn-Phong or some
other model? Do we include distance attenuation? Do we want two-sided lighting?
Once we make these decisions, we can write a vertex shader to implement the model.
Finally, we have to transfer the necessary data to the shader. Some data can be trans-
ferred using uniform variables, and other data can be transferred as vertex attributes.
Let’s go through the process for the model we just developed, the Blinn-Phong
model without distance attenuation with a single point light source. We can transfer
the ambient, diffuse, and specular components of the source plus its position as
uniform variables. We can do likewise for the material properties. Rather than writing
the application code ﬁrst, because we know how to transfer information to a shader,
ﬁrst we will write the vertex shader.
The vertex shader must output a vertex position in clip coordinates and a vertex
color to the rasterizer. If we send a model-view matrix and a projection matrix to the
shader, then the computation of the vertex position is identical to our examples from
Chapters 3 and 4. Hence, this part of the code will look something like
in vec4 vPosition;
uniform mat4 ModelView;
uniform mat4 Projection;

```cpp
void main()
{
```

gl_Position = Projection*ModelView*vPosition;

```cpp
}
```

                                                                                5.8 Implementing a Lighting Model   291
The output color is the sum of the ambient, diffuse, and specular contributions,
out vec4 color
vec4 ambient, diffuse, specular;
color = ambient + diffuse + specular;
so the part we must address is computation of these three terms.
Rather than sending all the reﬂectivity and light colors separately to the shader,
we send only the product term for each contribution. Thus, in the ambient computa-
tion, we use the products of the red, green, and blue ambient light with the red, green,
and blue ambient reﬂectivities. We can compute these products and send them to the
in vec4 AmbientProduct;
We can do the same for the diffuse and specular products:
uniform vec4 DiffuseProduct, SpecularProduct;
The ambient term is then simply
ambient = AmbientProduct;
The diffuse term requires a normal for each vertex. Because triangles are ﬂat, the
normal is the same for each vertex in a triangle so we can send the normal to the
shader as a uniform variable.3 We can use the normalize function to get a unit-
length normal from the vec4 type we used in the application:
uniform vec4 Normal;
vec3 N = normalize(Normal.xyz);
The unit vector in the direction of the light source is given by
vec3 L = normalize(LightPosition - vPosition).xyz;
The diffuse term is then
diffuse =       max(dot(L, N), 0.0)*DiffuseProduct;
The specular term is computed in a similar manner. Because the viewer is at the origin
in object coordinates, the normalized vector in the direction of the viewer is
3. In Section 5.9, we will consider methods that assign a different normal to each vertex of a polygon.
292   Chapter 5   Lighting and Shading
vec3 E = -normalize(vPosition.xyz);
vec3 H = normalize(L+E);
The specular term is then
specular = pow(max(dot(N, H), 0.0), Shininess)*SpecularProduct;
However, if the light source is behind the surface, there cannot be a specular term, so
we add a simple test:
specular = max(pow(max(dot(N, H), 0.0), Shininess)
*SpecularProduct, 0.0);
Here is the full shader:
in vec4 vPosition;
in vec4 Normal;
out vec4 color;
uniform vec4 AmbientProduct, DiffuseProduct, SpecularProduct;
uniform mat4 ModelView;
uniform mat4 Projection;
uniform vec4 LightPosition;
uniform float Shininess;

```cpp
void main()
{
```

vec4 ambient, diffuse, specular;
gl_Position = Projection*ModelView*vPosition;
vec3 N = normalize(Normal.xyz);
vec3 L = normalize(LightPosition.xyz - (ModelView*vPosition).xyz);
vec3 E = -normalize((ModelView*vPosition).xyz);
vec3 H = normalize(L+E);

```cpp
float Kd = max(dot(L, N), 0.0);
float Ks = pow(max(dot(N, H), 0.0), Shininess);
```

ambient = AmbientProduct;
diffuse = Kd*DiffuseProduct;
specular = max(pow(max(dot(N, H), 0.0), Shininess)
*SpecularProduct, 0.0);
color = vec4((ambient + diffuse + specular).xyz, 1.0);

```cpp
}
```

Because the colors are set in the vertex shader, the simple fragment shader that we
have used previously,
                                                                     5.8 Implementing a Lighting Model   293
in vec4 color;

```cpp
void main()
{
```

gl_FragColor = color;

```cpp
}
```

will take the interpolated colors from the rasterizer and assign them to fragments.
Let’s return to the cube-shading example. The main change we have to make is to
set up uniform variables for the light and material parameters. Thus, for the ambient
component we might have an ambient light term and an ambient reﬂectivity given as
color4 light_ambient = color4(0.2, 0.2, 0.2, 1.0);
color4 material_ambient = color4(1.0, 0.0, 1.0, 1.0);
We compute the ambient product
color4 ambient_product = product(light_ambient, material_ambient);
We get these values to the shader as follows:
GLuint ambient_product_loc;
ambient_product_loc = glGetUniformLocation(program, "AmbientProduct");
glUniform4fv(ambient_product_loc, 4, ambient_product);
We can do the same for the rest of the uniform variables in the vertex shader. Note
that the normal vector depends on more than one vertex and so cannot be computed
in the shader, because the shader has the position information only for the vertex that
initiated its execution.
There is an additional issue that has to do with the fact that the cube is rotating.
As the cube rotates, the positions of all the vertices and all the normals to the surface
change. When we ﬁrst developed the program, we applied the rotation transforma-
tion in the application, and each time that the rotation matrix was updated we resent
the vertices to the GPU. Later, we argued that it was far more efﬁcient to send the
rotation matrix to the shader and let the transformation be carried out in the GPU.
The same is true with this example. We can send a projection matrix and a model-
view matrix as uniform variables. This example has only one object, the cube, and
thus the rotation matrix and the model-view matrix are identical. If we want to apply
the rotation to the normal vector in the GPU, then we need to make the following
change to the shader:
vec4 NN = ModelView*Normal;
vec3 N = normalize(NN.xyz);
The complete program is in Appendix A.
294        Chapter 5   Lighting and Shading
However, if there are multiple objects in the scene or the viewing parameters
change, we have to be a little careful with this construct. Suppose that there is a sec-
ond, nonrotating cube in the scene and we also use nondefault viewing parameters.
Now we have two different model-view matrices, one for each cube. One is constant
and the other is changing as one of the cubes rotates. What is really changing is the
modeling part of the model-view matrix and not the viewing part that positions the
camera. We can handle this complication in a number of ways. We could compute
the two model-view matrices in the application and send them to the vertex shader
each time there is a rotation. We could also use separate modeling and viewing trans-
formations and send only the modeling matrix—the rotation matrix—to the shader
after initialization. We would then form the model-view matrix in the shader. We
could also just send the rotation angles to the shader and do all the work there. If the
light source is also changing its position, we have even more options.

### 5.9    SHADING OF THE SPHERE MODEL

The rotating cube is a simple example to demonstrate lighting, but because there are
only six faces and they meet at right angles, it is not a good example for testing the
smoothness of a lighting model. Consider instead the sphere model that we developed
in Section 5.6. Although the model comprises many small triangles, unlike the cube,
we do not want to see the edges. Rather, we want to shade the polygons so that we
cannot see the edges where triangles meet and the smoother the shading, the fewer
polygons we need to model the sphere.
To shade the sphere model, we can start with the same shaders we used for
the rotating cube. The differences are in the application program. We replace the
generation of the cube with the tetrahedron subdivision from Section 5.6, adding the
computation of the normals, which are sent to the vertex shader as attribute qualiﬁed

*FIGURE 5.37 Shaded sphere    variables. The result is shown in Figure 5.37. Note that even as we increase the number*

model.                       of subdivisions so that the interiors of the spheres appear smooth, we can still see
edges of polygons around the outside of the sphere image. This type of outline is
called a silhouette edge.
The differences in this example between constant shading and smooth shading
are minor. Because each triangle is ﬂat, the normal is the same at each vertex. If the
source is far from the object, the diffuse component will be constant for each triangle.
Likewise, if the camera is far from the viewer, the specular term will be constant for
each triangle. However, because two adjacent triangles will have different normals
and thus are shaded with different colors, we still can see the lack of smoothness even
if we create many triangles.
One way to get an idea of how smooth a display we can get with relatively few
triangles is to use the actual normals of the sphere for each vertex in the approxima-
tion. In Section 5.4, we found that for the sphere centered at the origin, the normal
at a point p is simply p. Hence, in the triangle function, the position of a vertex
gives the normal:
                                                                              5.10 Per-Fragment Lighting            295

```cpp
void triangle(point4 a, point4 b, point4 c)
{
```

normals[k] = a;
data[k]= a;
k++;
normals[k] = b;
data[k] = b;
k++;
normals[k] = c;
data[k] = c;
k++;

```cpp
}
```

The results of this deﬁnition of the normals are shown in Figure 5.38 and Color            FIGURE 5.38 Shading of the
Plate 29.                                                                                  sphere with the true normals.
Although using the true normals produces a rendering more realistic than ﬂat
shading, the example is not a general one, because we have used normals that are
known analytically. We also have not provided a true Gouraud-shaded image. Sup-
pose we want a Gouraud-shaded image of our approximate sphere. At each vertex, we
need to know the normals of all polygons incident at the vertex. Our code does not
have a data structure that contains the required information. Try Exercises 5.9 and
5.10, in which you construct such a structure. Note that six polygons meet at a vertex
created by subdivision, whereas only three polygons meet at the original vertices of
the tetrahedron.

### 5.10    PER-FRAGMENT LIGHTING

There is another option we can use to obtain a smoother shading. We can do the light-
ing calculation on a per-fragment basis rather than on a per-vertex basis. When we
did all our lighting calculations in the vertex shader, visually there was no advantage
over doing the same computation in the application and then sending the computed
vertex colors to the vertex shader, which would then pass them on to the rasterizer.
Thus, whether we did lighting in the vertex shader or in the application, the rasterizer

```cpp
interpolated the same vertex colors to obtain fragment colors.
```

With a fragment shader, we can do an independent lighting calculation for each
fragment. The fragment shader needs to get the interpolated values of the normal
vector, light source position, and eye position from the rasterizer. The vertex shader
can compute these values and output them to the rasterizer. In addition, the vertex
shader must output the vertex position in clip coordinates. Here is the vertex shader:
in vec4 vPosition;
in vec4 Normal;
uniform mat4 ModelView;
uniform vec4 LightPosition;
uniform mat4 Projection;
296   Chapter 5   Lighting and Shading
out vec3 N;
out vec3 L;
out vec3 E;

```cpp
void main()
{
```

gl_Position = Projection*ModelView*vPosition;
N = Normal.xyz;
L = LightPosition.xyz - vPosition.xyz;
if (LightPosition.w == 0.0) L = LightPosition.xyz;
E = vPosition.xyz;

```cpp
}
```

The fragment shader can now apply the Blinn-Phong lighting model to each fragment
using the light and material parameters passed in from the application as uniform
variables and the interpolated vectors from the rasterizer. The following shader cor-
responds to the vertex shader we used in the previous example:
uniform vec4 AmbientProduct, DiffuseProduct, SpecularProduct;
uniform mat4 ModelView;
uniform vec4 LightPosition;
uniform float Shininess;
in vec3 N;
in vec3 L;
in vec3 E;

```cpp
void main()
{
```

vec3 NN = normalize(N);
vec3 EE = normalize(E);
vec3 LL = normalize(L);
vec4 ambient, diffuse, specular;
vec3 H = normalize(LL+EE);

```cpp
float Kd = max(dot(LL, NN), 0.0);
```

Kd = dot(LL, NN);

```cpp
float Ks = pow(max(dot(NN, H), 0.0), Shininess);
```

ambient = AmbientProduct;
diffuse = Kd*DiffuseProduct;
if (dot(LL, NN) < 0.0) specular = vec4(0.0, 0.0, 0.0, 1.0);
else specular = Ks*SpecularProduct;
gl_FragColor = vec4((ambient + diffuse + specular).xyz, 1.0);

```cpp
}
```

Note that we normalize vectors in the fragment shader rather than in the vertex
shaders. If we were to normalize a variable such as the normals in the vertex shader, it
would not guarantee that the interpolated normals produced by the rasterizer would
have the unit magnitude needed for the lighting computation.
                                                                                  5.11 Global Illumination   297

#### 5.10.1 Nonphotorealistic Shading

Programmable shaders make it possible to not only incorporate more realistic light-
ing models in real time but also to create interesting nonphotorealistic effects. Two
such examples are the use of only a few colors and emphasizing the edges in objects.
Both these effects are techniques that we might want to use to obtain a cartoonlike
effect in an image.
Suppose that we use only two colors in a vertex shader:
vec4 color1 = vec4(1.0, 1.0, 0.0, 1.0); // yellow
vec4 color2 = vec4(1.0, 0.0, 0.0, 1.0); // red
We could then switch between the colors based, for example, on the magnitude of the
diffuse color. Using the light and normal vectors, we could assign colors as
if (dot(lightv, norm) > 0.5) gl_FrontColor = color1;
else gl_FrontColor = color2;
Although we could have used two colors in simpler ways, by using the diffuse color to
determine a threshold, the color of the object changes with its shape and the position
of the light source.
We can also try to draw the silhouette edge of an object. One way to identify
such edges is to look at sign changes in dot(lightv, norm). This value should be
positive for any vertex facing the viewer and negative for a vertex pointed away from
the viewer. Thus, we can test for small values of this value and assign a color such as
black to the vertex:
vec4 color3 = vec4(0.0, 0.0, 0.0, 1.0); // black
if (abs(dot(viewv, norm) < 0.01)) glFrontColor = color3;

### 5.11    GLOBAL ILLUMINATION

There are limitations imposed by the local lighting model that we have used. Con-
sider, for example, an array of spheres illuminated by a distant source, as shown in
Figure 5.39(a). The spheres close to the source block some of the light from the source
from reaching the other spheres. However, if we use our local model, each sphere is
shaded independently; all appear the same to the viewer (Figure 5.39(b)). In addi-
tion, if these spheres are specular, some light is scattered among spheres. Thus, if the
spheres were very shiny, we should see the reﬂection of multiple spheres in some of
the spheres and possibly even the multiple reﬂections of some spheres in themselves.
Our lighting model cannot handle this situation. Nor can it produce shadows, except
by using the tricks for some special cases, as we saw in Chapter 4.
All of these phenomena—shadows, reﬂections, blockage of light—are global
effects and require a global lighting model. Although such models exist and can be
quite elegant, in practice they are incompatible with the pipeline model. With the
298        Chapter 5   Lighting and Shading
L                                              L
(a)                                             (b)

*FIGURE 5.39 Array of shaded spheres. (a) Global lighting model.*

(b) Local lighting model.
pipeline model, we must render each polygon independently of the other polygons,
and we want our image to be the same regardless of the order in which the application
produces the polygons. Although this restriction limits the lighting effects that we can
simulate, we can render scenes very rapidly.
There are alternative rendering strategies, including ray tracing and radiosity,
that can handle global effects. Each is best at different lighting conditions. Ray trac-
ing starts with the synthetic-camera model but determines for each projector that
strikes a polygon if that point is indeed illuminated by one or more sources before
computing the local shading at each point. Thus, in Figure 5.40, we see three poly-
gons and a light source. The projector shown intersects one of the polygons. A local
renderer might use the modiﬁed Phong model to compute the shade at the point of

```cpp
intersection. The ray tracer would ﬁnd that the light source cannot strike the point of
```

COP                          intersection directly but that light from the source is reﬂected from the third polygon

*FIGURE 5.40 Polygon blocked   and this reﬂected light illuminates the point of intersection. In Chapter 11, we shall*

from light source.            show how to ﬁnd this information and make the required calculations.
A radiosity renderer is based upon energy considerations. From a physical point
of view, all the light energy in a scene is conserved. Consequently, there is an energy
balance that accounts for all the light that radiates from sources and is reﬂected by
various surfaces in the scene. A radiosity calculation thus requires the solution of
a large set of equations involving all the surfaces. As we shall see in Chapter 11, a
ray tracer is best suited to a scene consisting of highly reﬂective surfaces, whereas
a radiosity renderer is best suited for a scene in which all the surfaces are perfectly
diffuse.
Although a pipeline renderer cannot take into account many global phenomena
exactly, this observation does not mean we cannot produce realistic imagery with
OpenGL or another API that is based upon a pipeline architecture. What we can do
is use our knowledge of OpenGL and of the effects that global lighting produces to
approximate what a global renderer would do. For example, our use of projective
shadows in Chapter 4 shows that we can produce simple shadows. Many of the most
exciting advances in computer graphics over the past few years have been in the use of
pipeline renderers for global effects. We will study many such techniques in the next
few chapters, including mapping methods, multipass rendering, and transparency.
                                                                                      Summary and Notes   299
We have developed a lighting model that ﬁts well with our pipeline approach to
graphics. With it, we can create a variety of lighting effects, and we can employ
different types of light sources. Although we cannot create the global effects of a
ray tracer, a typical graphics workstation can render a polygonal scene using the
modiﬁed Phong reﬂection model and smooth shading in the same amount of time
as it can render a scene without shading. From the perspective of an application
program, adding shading requires setting parameters that describe the light sources
and materials and can be implemented with programmable shaders. In spite of the
limitations of the local lighting model that we have introduced, our simple renderer
performs remarkably well; it is the basis of the reﬂection model supported by most
APIs.
Programmable shaders have changed the picture considerably. Not only can we
create new methods of shading each vertex, we can use fragment shaders to do the
lighting calculation for each fragment, thus avoiding the need to interpolate colors
across each polygon. Methods such as Phong shading that were not possible within
the standard pipeline can now be programmed by the user and will execute in about
the same amount of time as the modiﬁed Phong shader. It is also possible to create a
myriad of new shading effects.
The recursive-subdivision technique that we used to generate an approximation
to a sphere is a powerful one that will reappear in various guises in Chapter 10, where
we use variants of this technique to render curves and surfaces. It will also arise when
we introduce modeling techniques that rely on the self-similarity of many natural
objects.
This chapter concludes our development of polygonal-based graphics. You
should now be able to generate scenes with lighting and shading. Techniques for
creating even more sophisticated images, such as texture mapping and compositing,
involve using the pixel-level capabilities of graphics systems—topics that we consider
in Chapter 7.
Now is a good time for you to write an application program. Experiment with
various lighting and shading parameters. Try to create light sources that move, either
independently or with the objects in the scene. You will probably face difﬁculties
in producing shaded images that do not have small defects, such as cracks between
polygons through which light can enter. Many of these problems are artifacts of small
numerical errors in rendering calculations. There are many tricks of the trade for
mitigating the effects of these errors. Some you will discover on your own; others
are given in the Suggested Readings for this chapter.
We turn to rasterization issues in Chapter 6. Although we have seen some of
the ways in which the different modules in the rendering pipeline function, we have
not yet seen the details. As we develop these details, you will see how the pieces ﬁt
together such that each successive step in the pipeline requires only a small increment
of work.
300   Chapter 5   Lighting and Shading
The use of lighting and reﬂection in computer graphics has followed two parallel
paths: the physical and the computational. From the physical perspective, Kajiya’s
rendering equation [Kaj86] describes the overall energy balance in an environment
and requires knowledge of the reﬂectivity function for each surface. Reﬂection mod-
els, such as the Torrance-Sparrow model [Tor67] and Cook-Torrance model [Coo82],
are based on modeling a surface with small planar facets. See Hall [Hal89] and Foley
[Fol90] for discussions of such models.
Phong [Pho75] is credited with putting together a computational model that in-
cluded ambient, diffuse, and specular terms. The use of the halfway vector was ﬁrst
suggested by Blinn [Bli77]. The basic model of transmitted light was used by Whit-
ted [Whi80]. It was later modiﬁed by Heckbert and Hanrahan [Hec84]. Gouraud
[Gou71] introduced interpolative shading.
The OpenGL Programming Guide [Shr10] contains many good hints on effec-
tive use of OpenGL’s rendering capabilities and discusses the ﬁxed-function lighting
pipeline that uses functions that have been deprecated in shader-based OpenGL.

### 5.1   Most graphics systems and APIs use the simple lighting and reﬂection models

that we introduced for polygon rendering. Describe the ways in which each of
these models is incorrect. For each defect, give an example of a scene in which
you would notice the problem.

### 5.2   Often, when a large polygon that we expect to have relatively uniform shading

is shaded by OpenGL, it is rendered brightly in one area and more dimly in
others. Explain why the image is uneven. Describe how you can avoid this
problem.

### 5.3   In the development of the Phong reﬂection model, why do we not consider

light sources being obscured from the surface by other surfaces in our reﬂec-
tion model?

### 5.4   How should the distance between the viewer and the surface enter the render-

ing calculations?

### 5.5   We have postulated an RGB model for the material properties of surfaces. Give

an argument for using a subtractive color model instead.

### 5.6   Find four points equidistant from one another on a unit sphere. These points

determine a tetrahedron. Hint: You can arbitrarily let one of the points be at
(0, 1, 0) and let the other three be in the plane y = −d for some positive value
of d.

### 5.7   Show that if v lies in the same plane as l, n, and r, then the halfway angle

satisﬁes
2ψ = φ.
                                                                                          Exercises   301
What relationship is there between the angles if v is not coplanar with the other
vectors?

### 5.8   Consider all the combinations of near or far viewers, near or far light sources,

ﬂat or curved surfaces, and diffuse and specular reﬂections. For which cases
can you simplify the shading calculations? In which cases does the use of the
halfway vector help? Explain your answers.

### 5.9   Construct a data structure for representing the subdivided tetrahedron. Tra-

verse the data structure such that you can Gouraud-shade the approximation
to the sphere based on subdividing the tetrahedron.

### 5.10 Repeat Exercise 5.9 but start with an icosahedron instead of a tetrahedron.


### 5.11 Construct a data structure for representing meshes of quadrilaterals. Write a

program to shade the meshes represented by your data structure.

### 5.12 Write a program that does recursive subdivisions on quadrilaterals and quadri-

lateral meshes.

### 5.13 Consider two materials that meet along a planar boundary. Suppose that the

speed of light in the two materials are v1 and v2. Show that Snell’s law is a
statement that light travels from a point in one material to a point in the second
material in the minimum time.

### 5.14 Show that the halfway vector h is at the angle at which a surface must be

oriented so that the maximum amount of reﬂected light reaches the viewer.

### 5.15 Although we have yet to discuss frame-buffer operations, you can start con-


```cpp
structing a ray tracer using a single routine of the form write_pixel(x, y,
```

color) that places the value of color (either an RGB color or an intensity)
at the pixel located at (x, y) in the frame buffer. Write a pseudocode routine
ray that recursively traces a cast ray. You can assume that you have a function
available that will intersect a ray with an object. Consider how to limit how far
the original ray will be traced.

### 5.16 If you have a pixel-writing routine available on your system, write a ray tracer

that will ray-trace a scene composed of only spheres. Use the mathematical
equations for the spheres rather than a polygonal approximation.

### 5.17 Add light sources and shading to the maze program in Exercise 2.23.


### 5.18 Using the sphere-generation program in Appendix A as a starting point, con-


```cpp
struct an interactive program that will allow you to position one or more light
```

sources and to alter material properties. Use your program to try to generate
images of surfaces that match familiar materials, such as various metals, plas-
tic, and carbon.

### 5.19 As geometric data pass through the viewing pipeline, a sequence of rotations,

translations, scalings, and a projection transformation is applied to the vectors
that determine the cosine terms in the Phong reﬂection model. Which, if any,
of these operations preserve(s) the angles between the vectors? What are the
implications of your answer for implementation of shading?
302   Chapter 5   Lighting and Shading

### 5.20 Estimate the amount of extra calculations required for Phong shading as com-

pared to Gouraud shading. Take into account the results of Exercise 5.19.

### 5.21 If the light position is altered by an afﬁne transformation, such as a modeling

transformation, how must the normal vector be transformed so that the angle
between the normal and the light vector remains unchanged?

### 5.22 Redo the implementation of the Blinn-Phong shading model so the calcula-

tions are carried out in eye coordinates.

### 5.23 Generalize the shadow-generation algorithm (Section 4.10) to handle ﬂat sur-

faces at arbitrary orientations.

### 5.24 Convert the shadow-generation algorithm (Section 4.10) to an algorithm for

distant sources. Hint: The perspective projection should become a parallel
projection.

### 5.25 Compare the shadow-generation algorithm of Section 4.10 to the generation

of shadows by a global-rendering method. What types of shadows can be
generated by one method but not the other?

### 5.26 Consider a highly reﬂective sphere centered at the origin with a unit radius. If a

viewer is located at p, describe the points she would see reﬂected in the sphere
at a point on its surface.
                                                                     CHA P TE R            6
W        e now turn to the next steps in the pipeline: clipping, rasterization, and
hidden-surface removal. Although we have yet to consider some major parts
of OpenGL that are available to the application programmer, including discrete
primitives, texture mapping, and curves and surfaces, there are several reasons for
considering these topics at this point. First, you may be wondering how your pro-
grams are processed by the system that you are using: how lines are drawn on the
screen, how polygons are ﬁlled, and what happens to primitives that lie outside the
viewing volumes deﬁned in your program. Second, our contention is that if we are
to use a graphics system efﬁciently, we need to have a deeper understanding of the
implementation process: which steps are easy, and which tax our hardware and soft-
ware. Third, our discussion of implementation will open the door to new capabilities
that are supported by the latest hardware.
Learning implementation involves studying algorithms. As when we study any
algorithm, we must be careful to consider such issues as theoretical versus practical
performance, hardware versus software implementations, and the speciﬁc character-
istics of an application. Although we can test whether an OpenGL implementation
works correctly in the sense that it produces the correct pixels on the screen, there
are many choices for the algorithms employed. We focus on the basic operations that
are both necessary to implement a standard API and required whether the rendering
is done by a pipeline architecture or by another method, such as ray tracing. Conse-
quently, we present a variety of the basic algorithms for each of the principal tasks in
an implementation.
In this chapter, we are concerned with the basic algorithms that are used to im-
plement the rendering pipeline employed by OpenGL. We shall focus on three issues:
clipping, rasterization, and hidden-surface removal. Clipping involves eliminating
objects that lie outside the viewing volume and thus cannot be visible in the image.
Rasterization produces fragments from the remaining objects. These fragments can
contribute to the ﬁnal image. Hidden-surface removal determines which fragments
correspond to objects that are visible, namely, those that are in the view volume and
are not blocked from view by other objects closer to the camera.
304   Chapter 6   From Vertices to Fragments
Application              Graphics            Frame
program                  system             buffer

*FIGURE 6.1 High-level view of the graphics process.*


### 6.1    BASIC IMPLEMENTATION STRATEGIES

Let us begin with a high-level view of the implementation process. In computer
graphics, we start with an application program, and we end with an image. We
can again consider this process as a black box (Figure 6.1) whose inputs are the
vertices and states deﬁned in the program—geometric objects, attributes, camera
speciﬁcations—and whose output is an array of colored pixels in the frame buffer.
Within the black box, we must do many tasks, including transformations, clip-
ping, shading, hidden-surface removal, and rasterization of the primitives that can
appear on the display. These tasks can be organized in a variety of ways, but regard-
less of the strategy that we adopt, we must always do two things: We must pass every
geometric object through the system, and we must assign a color to every pixel in the
color buffer that is displayed.
Suppose that we think of what goes into the black box in terms of a single
program that carries out the entire process. This program takes as input a set of
vertices specifying geometric objects and produces as output pixels in the frame
buffer. Because this program must assign a value to every pixel and must process
every geometric primitive (and every light source), we expect this program to contain
at least two loops that iterate over these basic variables.
If we wish to write such a program, then we must immediately address the
following question: Which variable controls the outer loop? The answer we choose
determines the ﬂow of the entire implementation process. There are two fundamental
strategies, often called the image-oriented and the object-oriented approaches.
In the object-oriented approach, the outer loop is over the objects. We can think
of the program as controlled by a loop of this form:
for (each_object) render(object);
A pipeline renderer ﬁts this description. Vertices are deﬁned by the program and
ﬂow through a sequence of modules that transforms them, colors them, and deter-
mines whether they are visible. A polygon might ﬂow through the steps illustrated in
Figure 6.2. Note that after a polygon passes through geometric processing, the ras-
terization of this polygon can potentially affect any pixels in the frame buffer. Most
implementations that follow this approach are based on construction of a rendering
pipeline containing hardware or software modules for each of the tasks. Data (ver-
tices) ﬂow forward through the system.
In the past, the major limitations of the object-oriented approach were the large
amount of memory required and the high cost of processing each object indepen-
                                                                     6.1 Basic Implementation Strategies   305
y                      y                      y
Clipping               Projection            Rasterizing
x                       x                     x          Frame

*FIGURE 6.2 Object-oriented approach.*

dently. Any geometric primitive that emerges from the geometric processing can po-
tentially affect any set of pixels in the frame buffer; thus, the entire color buffer—and
various other buffers, such as the depth buffer used for hidden-surface removal—
must be of the size of the display and must be available at all times. Before memory
became both inexpensive and dense, this requirement was considered to be a serious
problem. Now various pipelined geometric processors are available that can process
tens of millions of polygons per second. In fact, precisely because we are doing the
same operations on every primitive, the hardware to build an object-based system is
fast and relatively inexpensive, with many of the functions implemented with special-
purpose chips.
Today, the main limitation of object-oriented implementations is that they can-
not handle most global calculations. Because each geometric primitive is processed
independently—and in an arbitrary order—complex shading effects that involve
multiple geometric objects, such as reﬂections, cannot be handled except by approx-
imate methods. The major exception is hidden-surface removal, where the z buffer is
used to store global information.
Image-oriented approaches loop over pixels, or rows of pixels called scanlines,
that constitute the frame buffer. In pseudocode, the outer loop of such a program is
of the following form:
for (each_pixel) assign_a_color(pixel);
For each pixel, we work backward, trying to determine which geometric prim-
itives can contribute to its color. The advantages of this approach are that we need
only limited display memory at any time and that we can hope to generate pixels
at the rate and in the order required to refresh the display. Because the results of
most calculations do not differ greatly from pixel to pixel (or scanline to scanline), we
can use this coherence in our algorithms by developing incremental forms for many
of the steps in the implementation. The main disadvantage of this approach is that,
unless we ﬁrst build a data structure from the geometric data, we do not know which
primitives affect which pixels. Such a data structure can be complex and may imply
that all the geometric data must be available at all times during the rendering process.
For problems with very large databases, even having a good data representation may
306          Chapter 6   From Vertices to Fragments
Geometry                                        Fragment
Modeling                                   Rasterization            Display              Frame buffer
processing                                     processing

*FIGURE 6.3 Implementation tasks.*

not avoid memory problems. However, because image-space approaches have access
to all objects for each pixel, they are well suited to handle global effects such as
shadows and reﬂections. Ray tracing (Chapter 11) is an example of the image-based
approach.
We lean toward the object-based approach, although we look at examples of
algorithms suited for both approaches.

### 6.2     FOUR MAJOR TASKS

We start by reviewing the blocks in the pipeline, focusing on those blocks that we
have yet to discuss in detail. There are four major tasks that any graphics system
must perform to render a geometric entity, such as a three-dimensional polygon, as
that entity passes from deﬁnition in a user program to possible display on an output
device:
1. Modeling
2. Geometry processing
3. Rasterization
4. Fragment processing
Figure 6.3 shows how these tasks might be organized in a pipeline implementation.
Regardless of the approach, all four tasks must be carried out.

#### 6.2.1 Modeling

The usual results of the modeling process are sets of vertices that specify a group of
geometric objects supported by the rest of the system. We have seen a few examples
that required some modeling by the user, such as the approximation of spheres in

## Chapter 5. In Chapters 8 and 9, we explore other modeling techniques.

We can look at the modeler as a black box that produces geometric objects
and is usually a user program. Yet, there are other tasks that the modeler might
perform. Consider, for example, clipping: the process of eliminating parts of objects
that cannot appear on the display because they lie outside the viewing volume. A
user can generate geometric objects in her program, and she can hope that the rest
of the system can process these objects at the rate at which they are produced, or
the modeler can attempt to ease the burden on the rest of the system by minimizing
the number of objects that it passes on. The latter approach often means that the
modeler may do some of the same jobs as the rest of the system, albeit with different
                                                                                      6.2 Four Major Tasks   307
algorithms. In the case of clipping, the modeler, knowing more about the speciﬁcs
of the application, can often use a good heuristic to eliminate many, if not most,
primitives before they are sent on through the standard viewing process.

#### 6.2.2 Geometry Processing

Geometry processing works with vertices. The goals of the geometry processor are to
determine which geometric objects can appear on the display and to assign shades
or colors to the vertices of these objects. Four processes are required: projection,
primitive assembly, clipping, and shading.
Usually, the ﬁrst step in geometry processing is to change representations from
object coordinates to camera or eye coordinates using the model-view transforma-
tion. As we saw in Chapter 3, the conversion to camera coordinates is only the ﬁrst
part of the viewing process. The second step is to transform vertices using the pro-
jection transformation to a normalized view volume in which objects that might be
visible are contained in a cube centered at the origin. Vertices are now represented in
clip coordinates. Not only does this normalization convert both parallel and ortho-
graphic projections to a simple orthographic projection in a simple volume but, in
addition, we simplify the clipping process, as we shall see in Section 6.7.
Geometric objects are transformed by a sequence of transformations that may
reshape and move them (modeling) or may change their representations (viewing).
Eventually, only those primitives that ﬁt within a speciﬁed volume, the view volume,
can appear on the display after rasterization. We cannot, however, simply allow all
objects to be rasterized, hoping that the hardware will take care of primitives that lie
wholly or partially outside the view volume. The implementation must carry out this
task before rasterization. One reason is that rasterizing objects that lie outside the
view volume is inefﬁcient because such objects cannot be visible. Another reason is
that when vertices reach the rasterizer, they can no longer be processed individually
and ﬁrst must be assembled into primitives. Primitives that lie partially in the viewing
volume can generate new primitives with new vertices for which we must carry out
shading calculations. Before clipping can take place, vertices must be grouped into
objects, a process known as primitive assembly.
Note that even though an object lies inside the view volume, it will not be vis-
ible if it is obscured by other objects. Algorithms for hidden-surface removal (or
visible-surface determination) are based on the three-dimensional spatial relation-
ships among objects. This step is normally carried out as part of fragment processing.
As we saw in Chapter 5, colors can be determined on either a per-vertex or
per-fragment basis. If they are assigned on a per-vertex basis, they can be sent from
the application as vertex attributes or computed in the vertex shader. If lighting is
enabled, vertex colors are computed using a lighting model that can be implemented
in the application or in the vertex shader.
After clipping takes place, the remaining vertices are still in four-dimensional
homogeneous coordinates. Perspective division converts them to three-dimensional
representation in normalized device coordinates.
308   Chapter 6   From Vertices to Fragments
Collectively, these operations constitute what has been called front-end process-
ing. All involve three-dimensional calculations, and all require ﬂoating-point arith-
metic. All generate similar hardware and software requirements. All are carried out
on a vertex-by-vertex basis. We will discuss clipping, the only geometric step that we
have yet to discuss, in Section 6.3.

#### 6.2.3 Rasterization

Even after geometric processing has taken place, we still need to retain depth infor-
mation for hidden-surface removal. However, only the x, y values of the vertices are
needed to determine which pixels in the frame buffer can be affected by the primitive.
For example, after perspective division, a line segment that was speciﬁed originally
in three dimensions by two vertices becomes a line segment speciﬁed by a pair of
three-dimensional vertices in normalized device coordinates. To generate a set of
fragments that give the locations of the pixels in the frame buffer corresponding to
these vertices, we only need their x, y components or, equivalently, the results of
the orthogonal projection of these vertices. We determine these fragments through
a process called rasterization or scan conversion. For line segments, rasterization
determines which fragments should be used to approximate a line segment between
the projected vertices. For polygons, rasterization determines which pixels lie inside
the two-dimensional polygon determined by the projected vertices.
The colors that we assign to these fragments can be determined by the vertex at-
tributes or obtained by interpolating the shades at the vertices that are computed, as
in Chapter 5. Objects more complex than line segments and polygons are usually ap-
proximated by multiple line segments and polygons, and thus most graphics systems
do not have special rasterization algorithms for them. We shall see exceptions to this
rule for some special curves and surfaces in Chapter 10.
The rasterizer starts with vertices in normalized device coordinates but outputs
fragments whose locations are in units of the display—window coordinates. As we
saw in Chapters 2 and 4, the projection of the clipping volume must appear in the
assigned viewport. In OpenGL, this ﬁnal transformation is done after projection
and is two-dimensional. The preceding transformations have normalized the view
volume such that its sides are of length 2 and line up with the sides of the viewport
(Figure 6.4), so this transformation is simply
x + 1.0
xv = xvmin +           (xvmax − xvmin ),
2.0
y + 1.0
yv = yvmin +           (yvmax − yvmin ),
2.0
z + 1.0
zv = zvmin +           (zvmax − zvmin ).
2.0
Recall that for perspective viewing, these z-values have been scaled nonlinearly
by perspective normalization. However, they retain their original depth order, so they
                                                                                      6.2 Four Major Tasks   309
(xmax, ymax )
(xv max, yv max )
(x, y )                                (xv, yv )
(xv min, yv min )
(xmin, ymin )

*FIGURE 6.4 Viewport transformation.*

can be used for hidden-surface removal. We shall use the term screen coordinates to
refer to the two-dimensional system that is the same as window coordinates but lacks
the depth coordinate.

#### 6.2.4 Fragment Processing

In the simplest situations, each fragment is assigned a color by the rasterizer and this
color is placed in the frame buffer at the locations corresponding to the fragment’s
location. However, there are many other possibilities.
The separate pixel pipeline (Chapter 7), supported by architectures such as
OpenGL, merges with the results of the geometric pipeline at the rasterization stage.
Consider what happens when a shaded and texture-mapped polygon is processed.
Vertex lighting is computed as part of the geometric processing. The texture values are
not needed until after rasterization when the renderer has generated fragments that
correspond to locations inside a polygon. At this point, interpolation of per-vertex
colors and texture coordinates takes place, and the texture parameters determine how
to combine texture colors and fragment colors to determine ﬁnal colors in the color
buffer.
As we have noted, objects that are in the view volume will not be visible if they
are blocked by any opaque objects closer to the viewer. The required hidden-surface
removal process is typically carried out on a fragment-by-fragment basis.
Until now, we have assumed that all objects are opaque and thus an object located
behind another object is not visible. We can also assume that objects are translucent
and allow some light to pass through. In this case, fragment colors may have to
be blended with the colors of pixels already in the color buffer. We consider this
possibility in Chapter 7.
In most displays, the process of taking the image from the frame buffer and
displaying it on a monitor happens automatically and is not of concern to the appli-
cation program. However, there are numerous problems with the quality of display,
such as the jaggedness associated with images on raster displays. In Chapter 7, we in-
troduce algorithms for reducing this jaggedness, or aliasing, and we discuss problems
with color reproduction on displays.
310   Chapter 6   From Vertices to Fragments

### 6.3    CLIPPING

We can now turn to clipping, the process of determining which primitives, or parts of
primitives, ﬁt within the clipping or view volume deﬁned by the application program.
Clipping is done before the perspective division that is necessary if the w component
of a clipped vertex is not equal to 1. The portions of all primitives that can possibly
be displayed—we have yet to apply hidden-surface removal—lie within the cube
w ≥ x ≥ −w,
w ≥ y ≥ −w,
w ≥ z ≥ −w.
This coordinate system is called normalized device coordinates because it depends
on neither the original application units nor the particulars of the display device,
although the information to produce the correct image is retained in this coordinate
system. Note also that projection has been carried out only partially. We still must do
the perspective division and the ﬁnal orthographic projection.
We shall concentrate on clipping of line segments and polygons because they
are the most common primitives to pass down the pipeline. Although the OpenGL
pipeline does clipping on three-dimensional objects, there are other systems in which
the objects are ﬁrst projected into the x, y plane. Fortunately, many of the most
efﬁcient algorithms are almost identical in two and three dimensions, and we will
focus on these algorithms.

### 6.4    LINE-SEGMENT CLIPPING

A clipper decides which primitives, or parts of primitives, can possibly appear on
the display and be passed on to the rasterizer. Primitives that ﬁt within the speciﬁed
view volume pass through the clipper, or are accepted. Primitives that cannot appear
on the display are eliminated, or rejected or culled. Primitives that are only partially
within the view volume must be clipped such that any part lying outside the volume
is removed.
Clipping can occur at one or more places in the viewing pipeline. The modeler
may clip to limit the primitives that the hardware must handle. The primitives may
be clipped after they have been projected from three- to two-dimensional objects.
In OpenGL, primitives are clipped against a three-dimensional view volume before
rasterization. We shall develop a sequence of clippers. For both pedagogic and his-
toric reasons, we start with two two-dimensional line-segment clippers. Both extend
directly to three dimensions and to clipping of polygons.

#### 6.4.1 Cohen-Sutherland Clipping

The two-dimensional clipping problem for line segments is shown in Figure 6.5. We
can assume for now that this problem arises after three-dimensional line segments
have been projected onto the projection plane and that the window is part of the
                                                                              6.4 Line-Segment Clipping                311
A                      C

*FIGURE 6.5 Two-dimensional clipping.*

projection plane mapped to the viewport on the display. All values are speciﬁed as
real numbers. We can see that the entire line segment AB appears on the display,
whereas none of CD appears. EF and GH have to be shortened before being displayed.
Although a line segment is completely determined by its endpoints, GH shows that
even if both endpoints lie outside the clipping window, part of the line segment may
still appear on the display.
We could compute the intersections of the lines (of which the segments are parts)
with the sides of the window and could thus determine the necessary information
1001   1000   1010
for clipping. However, we want to avoid intersection calculations, if possible, because                            y = ymax
each intersection requires a ﬂoating-point division. The Cohen-Sutherland algorithm         0001   0000   0010
y = ymin
was the ﬁrst to seek to replace most of the expensive ﬂoating-point multiplications         0101 0100 0110
and divisions with a combination of ﬂoating-point subtractions and bit operations.             x = xmin x = xmax
The algorithm starts by extending the sides of the window to inﬁnity, thus break-
ing up space into the nine regions shown in Figure 6.6. Each region can be assigned a      FIGURE 6.6 Breaking up of
unique 4-bit binary number, or outcode, b0b1b2b3, as follows. Suppose that (x, y) is       space and outcodes.
a point in the region; then

1 if y > ymax ,
b0 =
0 otherwise.
Likewise, b1 is 1 if y < ymin , and b2 and b3 are determined by the relationship between
x and the left and right sides of the window. The resulting codes are indicated in
Figure 6.7. For each endpoint of a line segment, we ﬁrst compute the endpoint’s
outcode, a step that can require eight ﬂoating-point subtractions per line segment.
Consider a line segment whose outcodes are given by o1 = outcode(x1, y1) and
o2 = outcode(x2 , y2). We can now reason on the basis of these outcodes. There are
four cases:
1. (o1 = o2 = 0). Both endpoints are inside the clipping window, as is true for
segment AB in Figure 6.7. The entire line segment is inside, and the segment
can be sent on to be rasterized.
2. (o1 = 0, o2 = 0; or vice versa). One endpoint is inside the clipping window;
one is outside (see segment CD in Figure 6.7). The line segment must be
shortened. The nonzero outcode indicates which edge or edges of the window
are crossed by the segment. One or two intersections must be computed. Note
that after one intersection is computed, we can compute the outcode of the
312   Chapter 6   From Vertices to Fragments
G                       B          D          F
A              C

*FIGURE 6.7 Cases of outcodes in Cohen-Sutherland algorithm.*

point of intersection to determine whether another intersection calculation is
required.
3. (o1 & o2 = 0). By taking the bitwise AND of the outcodes, we determine
whether or not the two endpoints lie on the same outside side of the window.
If so, the line segment can be discarded (see segment EF in Figure 6.7).
4. (o1 & o2 = 0). Both endpoints are outside, but they are on the outside of
different edges of the window. As we can see from segments GH and IJ in
Figure 6.7, we cannot tell from just the outcodes whether the segment can be
discarded or must be shortened. The best we can do is to intersect with one
of the sides of the window and to check the outcode of the resulting point.
All our checking of outcodes requires only Boolean operations. We do intersection
calculations only when they are needed, as in the second case, or where the outcodes
did not contain enough information, as in the fourth case.
The Cohen-Sutherland algorithm works best when there are many line segments
but few are actually displayed. In this case, most of the line segments lie fully outside
one or two of the extended sides of the clipping rectangle and can thus be eliminated
on the basis of their outcodes. The other advantage is that this algorithm can be
extended to three dimensions. The main disadvantage of the algorithm is that it
must be used recursively. Consider line segment GH in Figure 6.7. It must be clipped
against both the left and top sides of the clipping window. Generally, the simplest
way to do so is to use the initial outcodes to determine the ﬁrst side of the clipping
window to clip against. After this ﬁrst shortening of the original line segment, a new
outcode is computed for the new endpoint created by shortening, and the algorithm
is reexecuted.
We have not discussed how to compute any required intersections. The form this
calculation takes depends on how we choose to represent the line segment, although
only a single division should be required in any case. If we use the standard explicit
form of a line,
y = mx + h,
where m is the slope of the line and h is the line’s y intercept, then we can compute
m and h from the endpoints. However, vertical lines cannot be represented in this
form—a critical weakness of the explicit form. If we were interested in only the
                                                                               6.4 Line-Segment Clipping   313
Cohen-Sutherland algorithm, it would be fairly simple to program all cases directly
because the sides of the clipping rectangle are parallel to the axes. However, we are

```cpp
interested in more than just clipping; consequently, other representations of the line
```

and line segment are of importance. In particular, parametric representations are
almost always used in computer graphics. We have already seen the parametric form
of the line in Chapter 4; the parametric representation of other types of curves is
considered in Chapter 10.

#### 6.4.2 Liang-Barsky Clipping

If we use the parametric form for lines, we can approach the clipping of line segments
in a different—and ultimately more efﬁcient—manner. Suppose that we have a line
segment deﬁned by the two endpoints p1 = [x1, y1]T and p2 = [x2 , y2]T . We can use
these endpoints to deﬁne a unique line that we can express parametrically, either in
matrix form,
p(α) = (1 − α)p1 + αp2 ,
or as two scalar equations,
x(α) = (1 − α)x1 + αx2 ,
y(α) = (1 − α)y1 + αy2 .
Note that this form is robust and needs no changes for horizontal or vertical lines.
As the parameter α varies from 0 to 1, we move along the segment from p1 to p2.
Negative values of α yield points on the line on the other side of p1 from p2. Similarly,
values of α > 1 give points on the line past p2 going off to inﬁnity.
Consider a line segment and the line of which it is part, as shown in Figure 6.8(a).
As long as the line is not parallel to a side of the window (if it is, we can handle that
situation with ease), there are four points where the line intersects the extended sides
of the window. These points correspond to the four values of the parameter: α1, α2,
α3, and α4. One of these values corresponds to the line entering the window; another
corresponds to the line leaving the window. Leaving aside, for the moment, how we
4
4
3                      3 2
2
1                      1
(a)                             (b)

*FIGURE 6.8 Two cases of a parametric line and a clipping window.*

314        Chapter 6   From Vertices to Fragments
compute these intersections, we can order them and determine which correspond to

```cpp
intersections that we need for clipping. For the given example,
```

1 > α4 > α3 > α2 > α1 > 0.
Hence, all four intersections are inside the original line segment, with the two in-
nermost (α2 and α3) determining the clipped line segment. We can distinguish this
case from the case in Figure 6.8(b), which also has the four intersections between the
endpoints of the line segment, by noting that the order for this case is
1 > α4 > α2 > α3 > α1 > 0.
The line intersects both the top and the bottom of the window before it intersects
either the left or the right; thus, the entire line segment must be rejected. Other cases
of the ordering of the points of intersection can be argued in a similar way.
Efﬁcient implementation of this strategy requires that we avoid computing inter-
sections until they are needed. Many lines can be rejected before all four intersections
are known. We also want to avoid ﬂoating-point divisions where possible. If we use
the parametric form to determine the intersection with the top of the window, we
ﬁnd the intersection at the value
ymax − y1
α=              .
y2 − y1
Similar equations hold for the other three sides of the window. Rather than com-
puting these intersections, at the cost of a division for each, we instead write the
α(y2 − y1) = α y = ymax − y1 =       ymax .
All the tests required by the algorithm can be restated in terms of ymax , y, and
similar terms can be computed for the other sides of the windows. Thus, all decisions
about clipping can be made without ﬂoating-point division. Only if an intersection is
needed (because a segment has to be shortened) is the division done. The efﬁciency of
this approach, compared to that of the Cohen-Sutherland algorithm, is that we avoid
multiple shortening of line segments and the related reexecutions of the clipping
algorithm. We forgo discussion of other efﬁcient two-dimensional, line-clipping al-
gorithms because, unlike the Cohen-Sutherland and Liang-Barsky algorithms, these
algorithms do not extend to three dimensions.

### 6.5    POLYGON CLIPPING

Polygon clipping arises in a number of ways. Certainly, we want to be able to clip
polygons against rectangular windows for display. However, we may at times want
windows that are not rectangular. Other parts of an implementation, such as shadow

*FIGURE 6.9 Polygon clipping   generation and hidden-surface removal, can require clipping of polygons against*

in shadow generation.
                                                                                       6.5 Polygon Clipping            315
other polygons. For example, Figure 6.9 shows the shadow of a polygon that we create
by clipping a polygon that is closer to the light source against polygons that are farther
away. Many antialiasing methods rely on our ability to clip polygons against other
polygons.
We can generate polygon-clipping algorithms directly from line-clipping algo-
rithms by clipping the edges of the polygon successively. However, we must be careful
to remember that a polygon is a two-dimensional object with an interior, and de-
pending on the form of the polygon, we can generate more than one polygonal object                 (a)               (b)
by clipping. Consider the nonconvex (or concave) polygon in Figure 6.10(a). If we            FIGURE 6.10 Clipping of a
clip it against a rectangular window, we get the result shown in Figure 6.10(b). Most        concave polygon. (a) Before
viewers looking at this ﬁgure would conclude that we have generated three polygons           clipping. (b) After clipping.
by clipping. Unfortunately, implementing a clipper that can increase the number of
objects can be a problem. We could treat the result of the clipper as a single polygon,
as shown in Figure 6.11, with edges that overlap along the sides of the window, but
this choice might cause difﬁculties in other parts of the implementation.
Convex polygons do not present such problems. Clipping a convex polygon
against a rectangular window can leave at most a single convex polygon (see Exer-
cise 6.3). A graphics system might then either forbid the use of concave polygons or
divide (tessellate) a given polygon into a set of convex polygons, as shown in Fig-
ure 6.12. (OpenGL 4.1 includes tessellation functions.)                                      FIGURE 6.11 Creation of a
For rectangular clipping regions, both the Cohen-Sutherland and the Liang-              single polygon.
Barsky algorithms can be applied to polygons on an edge-by-edge basis. There is
another approach, developed by Sutherland and Hodgeman, that ﬁts well with the
pipeline architectures that we have discussed.
A line-segment clipper can be envisioned as a black box whose input is the pair
of vertices from the segment to be tested and clipped and whose output either is a
pair of vertices corresponding to the clipped line segment or is nothing if the input
line segment lies outside the window (Figure 6.13).
Rather than considering the clipping window as four line segments, we can con-
sider it as the object created by the intersection of four inﬁnite lines that determine
the top, bottom, right, and left sides of the window. We can then subdivide our clip-
per into a pipeline of simpler clippers, each of which clips against a single line that is
the extension of an edge of the window. We can use the black-box view on each of the
individual clippers.

*FIGURE 6.12 Tessellation of a concave polygon.*

316            Chapter 6       From Vertices to Fragments
(x2, y2)
(x4, y4)
(x1, y1)                (x3, y3)
(x3, y3)                                   (x2, y2)                (x4, y4)
(x1, y1)
(a)                                       (b)

*FIGURE 6.13 Two views of clipping. (a) Clipping against a rectangle.*

(b) Clipper as a black box.
(x2, y2)
(xi, ymax)
y = ymax                                                    (x1, y1)                 (x1, y1)
(x2, y2)                (xi, ymax)
(x1, y1)
(a)                                      (b)

*FIGURE 6.14 Clipping against top. (a) Graphically. (b) Black-box view.*

(x1, y1)                  Suppose that we consider clipping against only the top of the window. We can
think of this operation as a black box whose input and output are pairs of vertices,
with the value of ymax as a parameter known to the clipper (Figure 6.14). Using the
similar triangles in Figure 6.15, we see that if there is an intersection, it lies at
(x3, y3)          y = ymax
x − x1
x3 = x1 + (ymax − y1) 2      ,
y2 − y1
(x2, y2)                            y3 = ymax .

*FIGURE 6.15 Intersection              Thus, the clipper returns one of three pairs: {(x1, y1), (x2 , y2)}; {(x1, y1), (xi ,*

with the top of the window.
ymax )}; or {(xi , ymax ), (x2 , y2)}. We can clip against the bottom, right, and left lines
independently, using the same equations, with the roles of x and y exchanged as nec-
essary and the values for the sides of the window inserted. The four clippers can now
be arranged in the pipeline of Figure 6.16. If we build this conﬁguration in hardware,
we have a clipper that is working on four vertices concurrently. Figure 6.17 shows a
simple example of the effect of successive clippers on a polygon.
                                                                                               6.6 Clipping of Other Primitives   317
(x1, y1)
(x3, y3)
(x4, y4)
(x5, y5)
(x2, y2)
(a)
(x1, y1)            (x3, y3)                  (x3, y3)              (x3, y3)                    (x3, y3)
(x2, y2)            (x2, y2)                  (x5, y5)              (x5, y5)                    (x4, y4)
Top                     Bottom                   Right                      Left
(b)

*FIGURE 6.16 Pipeline clipping. (a) Clipping problem. (b) Pipeline*

clippers.
Top                                      Bottom
clip                                      clip
Left                                      Right
clip                                       clip

*FIGURE 6.17 Example of pipeline clipping.*


### 6.6    CLIPPING OF OTHER PRIMITIVES

Our emphasis in Chapters 1 through 5 was on writing programs in which the objects
are built from line segments and triangles. We often render the curved objects that
we discuss in Chapter 10 by subdividing them into small, approximately ﬂat poly-
gons. In pipeline architectures, we usually ﬁnd some variant of the clippers that we
have presented. Nevertheless, there are situations in which we want either to clip ob-
jects before they reach the hardware or to employ algorithms optimized for other
primitives.
318        Chapter 6   From Vertices to Fragments
(a)                          (b)

*FIGURE 6.18 Using bounding boxes. (a) Polygon and clipping window.*

(b) Polygon, bounding box, and clipping window.

#### 6.6.1 Bounding Boxes and Volumes

Suppose that we have a many-sided polygon, as shown in Figure 6.18(a). We could
apply one of our clipping algorithms, which would clip the polygon by individually
clipping all that polygon’s edges. However, we can see that the entire polygon lies
outside the clipping window. We can exploit this observation through the use of
the axis-aligned bounding box or the extent of the polygon (Figure 6.18(b)): the
smallest rectangle, aligned with the window, that contains the polygon. Calculating
the bounding box requires merely going through the vertices of the polygon to ﬁnd
the minimum and maximum of both the x and y values.
Once we have the bounding box, we can often avoid detailed clipping. Consider

*FIGURE 6.19 Clipping with    the three cases in Figure 6.19. For the polygon above the window, no clipping is*

bounding boxes.              necessary, because the minimum y for the bounding box is above the top of the
window. For the polygon inside the window, we can determine that it is inside by
comparing the bounding box with the window. Only when we discover that the
bounding box straddles the window do we have to carry out detailed clipping, using
all the edges of the polygon. The use of extents is such a powerful technique—in both
two and three dimensions—that modeling systems often compute a bounding box
for each object automatically and store the bounding box with the object.
Axis-aligned bounding boxes work in both two and three dimensions. In three
dimensions, they can be used in the application to perform clipping to reduce the
burden on the pipeline. Other volumes, such as spheres, can also work well. One
of the other applications of bounding volumes is in collision detection (Chapter 9).
One of the fundamental operations in animating computer games is to determine if
two moving entities have collided. For example, consider two animated characters
moving in a sequence of images. We need to know when they collide so that we can
alter their paths. This problem has many similarities to the clipping problem because
we want to determine when the volume of one intersects the volume of the other. The
complexity of the objects and the need to do these calculations very quickly make this
problem difﬁcult. A common approach is to place each object in a bounding volume,
either an axis-aligned bounding box or a sphere, and to determine if the volumes

```cpp
intersect. If they do, then detailed calculations can be done.
```

                                                                      6.7 Clipping in Three Dimensions   319
(a)                              (b)

*FIGURE 6.20 Curve clipping.*


#### 6.6.2 Curves, Surfaces, and Text

The variety of curves and surfaces that we can deﬁne mathematically makes it difﬁcult
to ﬁnd general algorithms for processing these objects. The potential difﬁculties can
be seen from the two-dimensional curves in Figure 6.20. For a simple curve, such
as a quadric, we can compute intersections, although at a cost higher than that
for lines. For more complex curves, such as the spiral, not only must intersection
calculations be computed with numerical techniques, but even determining how
many intersections we must compute may be difﬁcult. We can avoid such problems
by approximating curves with line segments and surfaces with planar polygons. The
use of bounding boxes can also prove helpful, especially in cases such as quadratics,
where we can compute intersections exactly but would prefer to make sure that the
calculation is necessary before carrying it out.
The handling of text differs from API to API, with many APIs allowing the user to
specify how detailed a rendering of text is required. There are two extremes. On one
end, text is stored as bit patterns and is rendered directly by the hardware without
any geometric processing. Any required clipping is done in the frame buffer. At the
other extreme, text is deﬁned like any other geometric object and is then processed
through the standard viewing pipeline. OpenGL allows both these cases by not having
a separate text primitive. The user can choose which mode she prefers by deﬁning
either bitmapped characters, using pixel operations, or stroke characters, using the
standard primitives.

#### 6.6.3 Clipping in the Frame Buffer

We might also consider delaying clipping until after objects have been projected and
converted into screen coordinates. Clipping can be done in the frame buffer through
a technique called scissoring. However, it is usually better to clip geometric entities
before the vertices reach the frame buffer; thus, clipping within the frame buffer
generally is required only for raster objects, such as blocks of pixels.

### 6.7   CLIPPING IN THREE DIMENSIONS

In three dimensions, we clip against a bounded volume rather than against a bounded
region in the plane. The simplest extension of two-dimensional clipping to three
320   Chapter 6   From Vertices to Fragments
(x2, y2, z2)
(x1, y1, z1)                    (x max, ymax, z max)
(x min, ymin, z min)

*FIGURE 6.21 Three-dimensional clipping against a right parallelepiped.*

dimensions is for the right parallelepiped clipping region (Figure 6.21):
xmin ≤ x ≤ xmax ,
ymin ≤ y ≤ ymax ,
zmin ≤ z ≤ zmax
−w ≤ x ≤ w,
−w ≤ y ≤ w,
−w ≤ z ≤ w.
Our three clipping algorithms (Cohen-Sutherland, Liang-Barsky, and
Sutherland-Hodgeman) and the use of extents can be extended to three dimensions.
For the Cohen–Sutherland algorithm, we replace the 4-bit outcode with a 6-bit out-
code. The additional 2 bits are set if the point lies either in front of or behind the
clipping volume (Figure 6.22). The testing strategy is virtually identical for the two-
and three-dimensional cases.
For the Liang-Barsky algorithm, we add the equation
z(α) = (1 − α)z1 + αz2
to obtain a three-dimensional parametric representation of the line segment. We have
to consider six intersections with the surfaces that form the clipping volume, but we
can use the same logic as we did in the two-dimensional case. Pipeline clippers add
two modules to clip against the front and back of the clipping volume.
The major difference between two- and three-dimensional clippers is that in
three dimensions we are clipping either lines against planes or polygons against
                                                                       6.7 Clipping in Three Dimensions            321
00        01
01        10
01                00
01        01
00                    01        00     01
10                            00     10
01                    01              10
00                              01     01
00      00                    00     00
01        10                          10
00                   00                01
01      00                           01
10                01        00         00               10
10                             00      10
01                 00                   10
10                          01         00
00       10                    00      00
01      10                              10
10               00                    00
01       10                            01
01      00        10                    10
00      10
10                10
01        10
00      00

*FIGURE 6.22 Cohen-Sutherland regions in three dimensions.*

planes instead of clipping lines against lines as we do in two dimensions. Conse-                                   n
quently, our intersection calculations must be changed. A typical intersection calcu-
lation can be posed in terms of a parametric line in three dimensions intersecting a
plane (Figure 6.23). If we write the line and plane equations in matrix form (where n
is the normal to the plane and p0 is a point on the plane), we must solve the equations                p0
p2
p( )
p(α) = (1 − α)p1 + αp2 ,
p1
n . (p(α) − p0) = 0

*FIGURE 6.23 Plane–line inter-*

for the α corresponding to the point of intersection. This value is                        section.
n . (p0 − p1)
α=                    ,
n . (p2 − p1)
and computation of an intersection requires six multiplications and a division. How-
ever, if we look at the standard viewing volumes, we see that simpliﬁcations are
possible. For orthographic viewing (Figure 6.24), the view volume is a right paral-
lelepiped, and each intersection calculation reduces to a single division, as it did for
two-dimensional clipping.
When we consider an oblique view (Figure 6.25), we see that the clipping volume
no longer is a right parallelepiped. Although you might think that we have to compute
dot products to clip against the sides of the volume, here is where the normalization
process that we introduced in Chapter 4 pays dividends. We showed that an oblique
projection is equivalent to a shearing of the data followed by an orthographic projec-
tion. Although the shear transformation distorts objects, they are distorted such that
they project correctly by an orthographic projection. The shear also distorts the clip-
ping volume from a general parallelepiped to a right parallelepiped. Figure 6.26(a)
shows a top view of an oblique volume with a cube inside the volume. Figure 6.26(b)
322   Chapter 6   From Vertices to Fragments
y = ymax
z = zmin
x = xmin
x = xmax
y = ymin    z = zmax

*FIGURE 6.24 Clipping for orthographic viewing.*

z = zmin
z = zmax

*FIGURE 6.25 Clipping for oblique viewing.*

Projection plane                                Projection plane
(a)                              (b)

*FIGURE 6.26 Distortion of view volume by shear. (a) Top view before*

shear. (b) Top view after shear.
shows the volume and object after they have been distorted by the shear. As far as pro-
jection is concerned, carrying out the oblique transformation directly or replacing it
by a shear transformation and an orthographic projection requires the same amount
of computation. When we add in clipping, it is clear that the second approach has
a deﬁnite advantage because we can clip against a right parallelepiped. This example
illustrates the importance of considering the incremental nature of the steps in an im-
plementation. Analysis of either projection or clipping in isolation fails to show the
importance of the normalization process.
For perspective projections, the argument for normalization is just as strong.
By carrying out the perspective-normalization transformation from Chapter 4, but
not the orthographic projection, we again create a rectangular clipping volume and
simplify all subsequent intersection calculations.
                                                                                               6.8 Rasterization   323

### 6.8    RASTERIZATION

We are now ready to take the ﬁnal step in the journey from the speciﬁcation of geo-
metric entities in an application program to the formation of fragments: rasterization
of primitives. In this chapter, we are concerned with only line segments and polygons,
both of which are deﬁned by vertices. We can assume that we have clipped the prim-
itives such that each remaining primitive is inside the view volume.
Fragments are potential pixels. Each fragment has a color attribute and a location
in screen coordinates that corresponds to a location in the color buffer. Fragments
also carry depth information that can be used for hidden-surface removal. To clarify
the discussion, we will ignore hidden-surface removal until Section 6.11 and thus
we can work directly in screen coordinates. Because we are not considering hidden-
surface removal, translucent fragments, or antialiasing, we can develop rasterization
algorithms in terms of the pixels that they color.
We further assume that the color buffer is an n × m array of pixels, with (0, 0)
corresponding to the lower-left corner. Pixels can be set to a given color by a single
function inside the graphics implementation of the form
write_pixel(int ix, int iy, int value);
The argument value can be either an index, in color-index mode, or a pointer to
an RGBA color. On the one hand, a color buffer is inherently discrete; it does not
make sense to talk about pixels located at places other than integer values of ix and
iy. On the other hand, screen coordinates, which range over the same values as do ix
and iy, are real numbers. For example, we can compute a fragment location such as
(63.4, 157.9) in screen coordinates but must realize that the nearest pixel is centered
either at (63, 158) or at (63.5, 157.5), depending on whether pixels are considered to
be centered at whole or half integer values.
Pixels have attributes that are colors in the color buffer. Pixels can be displayed in
multiple shapes and sizes that depend on the characteristics of the display. We address
this matter in Section 6.13. For now, we can assume that a pixel is displayed as a
square, whose center is at the location associated with the pixel and whose side is
equal to the distance between pixels. In OpenGL, the centers of pixels are located
at values halfway between integers. There are some advantages to this choice (see
Exercise 6.19). We also assume that a concurrent process reads the contents of the
color buffer and creates the display at the required rate. This assumption, which holds
in many systems that have dual-ported memory, allows us to treat the rasterization
process independently of the display of the contents of the frame buffer.
The simplest scan-conversion algorithm for line segments has become known as
the DDA algorithm, after the digital differential analyzer, an early electromechanical
device for digital simulation of differential equations. Because a line satisﬁes the
differential equation dy/dx = m, where m is the slope, generating a line segment is
equivalent to solving a simple differential equation numerically.
Suppose that we have a line segment deﬁned by the endpoints (x1, y1) and
(x2 , y2). Because we are working in a color buffer, we assume that these values have
324          Chapter 6      From Vertices to Fragments
(x2, y2)             been rounded to have integer values, so the line segment starts and ends at a known
pixel.1 The slope is given by
y2 − y 1      y
y         m=             =      .
x2 − x 1      x
(x1, y1)                 We assume that
0 ≤ m ≤ 1.

*FIGURE 6.27 Line segment in*

window coordinates.
We can handle other values of m using symmetry. Our algorithm is based on writing a
pixel for each value of ix in write_pixel as x goes from x1 to x2. If we are on the line
segment, as shown in Figure 6.27, for any change in x equal to x, the corresponding
y = m x.
As we move from x1 to x2, we increase x by 1 in each iteration; thus, we must increase

*FIGURE 6.28 Pixels gener-         y by*

ated by DDA algorithm.
y = m.
Although each x is an integer, each y is not, because m is a ﬂoating-point number
and we must round it to ﬁnd the appropriate pixel, as shown in Figure 6.28. Our
algorithm, in pseudocode, is
for (ix=x1; ix <= x2; ix++)

```cpp
{
```

y+=m;
write_pixel(x, round(y), line_color);

```cpp
}
```

where round is a function that rounds a real number to an integer. The reason that

*FIGURE 6.29 Pixels gener-         we limited the maximum slope to 1 can be seen from Figure 6.29. Our algorithm*

ated by high- and low-slope
lines.                            is of this form: For each x, ﬁnd the best y. For large slopes, the separation between
pixels that are colored can be large, generating an unacceptable approximation to the
line segment. If, however, for slopes greater than 1, we swap the roles of x and y, the
algorithm becomes this: For each y, ﬁnd the best x. For the same line segments, we
get the approximations in Figure 6.30. Note that the use of symmetry removes any
potential problems from either vertical or horizontal line segments. You may want to
derive the parts of the algorithm for negative slopes.
Because line segments are determined by vertices, we can use interpolation to
assign a different color to each pixel that we generate. We can also generate various

*FIGURE 6.30 Pixels gener-         1. This assumption is not necessary to derive an algorithm. If we use a ﬁxed-point representation*

ated by revised DDA algo-         for the endpoints and do our calculations using ﬁxed-point arithmetic, then we retain the compu-
rithm.                            tational advantages of the algorithm and produce a more accurate rasterization.
                                                                                     6.9 Bresenham’s Algorithm   325
dash and dot patterns by changing the color that we use as we generate pixels. Neither
of these effects has much to do with the basic rasterization algorithm, as the latter’s
job is to determine only which pixels to color rather than to determine the color that
is used.

### 6.9     BRESENHAM’S ALGORITHM

The DDA algorithm appears efﬁcient. Certainly it can be coded easily, but it re-
quires a ﬂoating-point addition for each pixel generated. Bresenham derived a line-
rasterization algorithm that, remarkably, avoids all ﬂoating-point calculations and
has become the standard algorithm used in hardware and software rasterizers.
We assume, as we did with the DDA algorithm, that the line segment goes
between the integer points (x1, y1) and (x2 , y2) and that the slope satisﬁes
0 ≤ m ≤ 1.
This slope condition is crucial for the algorithm, as we can see with the aid of Fig-
ure 6.31. Suppose that we are somewhere in the middle of the scan conversion of our
line segment and have just placed a pixel at (i + 21 , j + 21 ). We know that the line of
which the segment is part can be represented as
y = mx + h.
At x = i + 21 , this line must pass within one-half the length of the pixel at (i + 21 ,
j + 21 );2 otherwise, the rounding operation would not have generated this pixel. If
we move ahead to x = i + 23 , the slope condition indicates that we must set the color
of one of only two possible pixels: either the pixel at (i + 23 , j + 21 ) or the pixel at
(i + 23 , j + 23 ). Having reduced our choices to two pixels, we can pose the problem
anew in terms of the decision variable d = a − b, where a and b are the distances
between the line and the upper and lower candidate pixels at x = i + 23 , as shown
in Figure 6.32. If d is positive, the line passes closer to the lower pixel, so we choose
the pixel at (i + 23 , j + 21 ); otherwise, we choose the pixel at (i + 23 , j + 23 ). Although
we could compute d by computing y = mx + b, we hesitate to do so because m is a
ﬂoating-point number.
We obtain the computational advantages of Bresenham’s algorithm through two
further steps. First, we replace ﬂoating-point operations with ﬁxed-point operations.
Second, we apply the algorithm incrementally. We start by replacing d with the new
d = (x2 − x1)(a − b) =         x(a − b),
2. We are assuming that the pixels’ centers are located halfway between integers.
326   Chapter 6   From Vertices to Fragments
j +3
y = mx + h
j+1
j1
i+1        i+ 3
2           2

*FIGURE 6.31 Conditions for Bresenham’s algorithm.*

j +3
a              y = mx + h
j+1                        b
j1
i+1        i+ 3
2           2

*FIGURE 6.32 Decision variable for Bresenham’s algorithm.*

a change that cannot affect which pixels are drawn, because it is only the sign of the
decision variable that matters. If we substitute for a and b, using the equation of the
y2 − y1        y
m=              =       ,
x2 − x 1       x
h = y2 − mx2 ,
then we can see that d is an integer. We have eliminated ﬂoating-point calculations,
but the direct computation of d requires a fair amount of ﬁxed-point arithmetic.
Let us take a slightly different approach. Suppose that dk is the value of d at
x = k + 21 . We would like to compute dk+1 incrementally from dk . There are two
situations, depending on whether or not we incremented the y location of the pixel
at the previous step; these situations are shown in Figure 6.33. By observing that a is
the distance between the location of the upper candidate location and the line, we see
that a increases by m only if x was increased by the previous decision; otherwise, it
decreases by m − 1. Likewise, b either decreases by −m or increases by 1 − m when
we increment x. Multiplying by x, we ﬁnd that the possible changes in d are either
−2 y or 2( x − y). We can state this result in the form
                                                                                         6.10 Polygon Rasterization   327
j +3                                          j +3
2                   a                         2                   a
j+1                                           j+1                            b       m
2                                             2
j1                                           j1
2                                             2
i1 i+ 1 i+ 3                                 i1 i+ 1 i+ 3
2    2    2                                   2    2    2

*FIGURE 6.33 Incrementing of the values of a and b.*


2 y          if dk > 0;
dk+1 = dk −
2( y −    x) otherwise.
The calculation of each successive pixel in the color buffer requires only an addition
and a sign test. This algorithm is so efﬁcient that it has been incorporated as a single
instruction on graphics chips. See Exercise 6.14 for calculation of the initial value d0 .

### 6.10     POLYGON RASTERIZATION

One of the major advantages that the ﬁrst raster systems brought to users was the
ability to display ﬁlled polygons. At that time, coloring each point in the interior
of a polygon with a different shade was not possible in real time, and the phrases
rasterizing polygons and polygon scan conversion came to mean ﬁlling a polygon with
a single color. Unlike rasterization of lines, where a single algorithm dominates, there
are many viable methods for rasterizing polygons. The choice depends heavily on the
implementation architecture. We concentrate on methods that ﬁt with our pipeline
approach and can also support shading. In Sections 6.10.4 through 6.10.6, we survey
a number of other approaches.

#### 6.10.1 Inside–Outside Testing

Flat simple polygons have well-deﬁned interiors. If they are also convex, they are
guaranteed to be rendered correctly by OpenGL and by other graphics systems. More
general polygons arise in practice, however, and we can render them in multiple
ways. For nonﬂat polygons,3 we can work with their projections (Section 6.10.2), or
we can use the ﬁrst three vertices to determine a plane to use for the interior. For
ﬂat nonsimple polygons, we must decide how to determine whether a given point is
3. Strictly speaking, there is no such thing as a nonﬂat polygon because the interior is not deﬁned
unless it is ﬂat. However, from a programming perspective, we can deﬁne a polygon by simply giving
a list of vertices, regardless of whether or not they lie in the same plane.
328         Chapter 6    From Vertices to Fragments
inside or outside of the polygon. Conceptually, the process of ﬁlling the inside of a
polygon with a color or pattern is equivalent to deciding which points in the plane of
the polygon are interior (inside) points.
The crossing or odd–even test is the most widely used test for making inside–
outside decisions. Suppose that p is a point inside a polygon. Any ray emanating from
p and going off to inﬁnity must cross an odd number of edges. Any ray emanating
from a point outside the polygon and entering the polygon crosses an even number
of edges before reaching inﬁnity. Hence, a point can be deﬁned as being inside if after

*FIGURE 6.34 Filling with the   drawing a line through it and following this line, starting on the outside, we cross an*

odd–even test.                 odd number of edges before reaching it. For the star-shaped polygon in Figure 6.34,
we obtain the inside coloring shown. Odd–even testing is easy to implement and inte-
grates well with the standard rendering algorithms. Usually, we replace rays through
points with scanlines, and we count the crossing of polygon edges to determine inside
and outside.
However, we might want our ﬁll algorithm to color the star polygon as shown
in Figure 6.35 rather than as shown in Figure 6.34. The winding test allows us to
make that happen. This test considers the polygon as a knot being wrapped around
a point or a line. To implement the test, we consider traversing the edges of the
polygon from any starting vertex and going around the edge in a particular direction
(which direction does not matter) until we reach the starting point. We illustrate
the path by labeling the edges, as shown in Figure 6.35(b). Next we consider an
arbitrary point. The winding number for this point is the number of times it is
encircled by the edges of the polygon. We count clockwise encirclements as positive
and counterclockwise encirclements as negative (or vice versa). Thus, points outside
the star in Figure 6.35 are not encircled and have a winding number of 0, points that
were ﬁlled in Figure 6.34 all have a winding number of 1, and points in the center that
were not ﬁlled by the odd–even test have a winding number of 2. If we change our ﬁll
rule to be that a point is inside the polygon if its winding number is not zero, then we
ﬁll the inside of the polygon as shown in Figure 6.35(a).
Scanline                           Scanline            p
(a)                                 (b)

*FIGURE 6.35 Fill using the winding number test.*

                                                                                    6.10 Polygon Rasterization                329

#### 6.10.2 OpenGL and Concave Polygons                                                                          vi –1

Because OpenGL only renders triangles, which are always ﬂat and convex, we still
have the problem of what to do with more general polygons. One approach is to
work with the application to ensure that they only generate triangles. Another is                      vi
to provide software that can tessellate a given polygon into ﬂat convex polygons,
usually triangles. There are many ways to divide a given polygon into triangles. A                                    vi +1
good tessellation should not produce triangles that are long and thin; it should, if
possible, produce sets of triangles that can use supported features, such as triangle            FIGURE 6.36 Removal of a
strips and triangle fans.                                                                        triangle from a polygon.
Let’s consider one approach to tessellating or triangularizing an arbitrary simple
polygon with n vertices. From the construction, it will be clear that we always obtain
a triangularization using exactly n − 2 triangles. We assume our polygon is speciﬁed                        vi –1
by an ordered list of vertices v0 , v1, . . . , vn−1 . Thus, there is an edge from v0 to v1,
from v1 to v2, and ﬁnally from vn−1 to v0. The ﬁrst step is to ﬁnd the left-most vertex,
vi , a calculation that requires a simple scan of the x components of the vertices. Let                vi
vi−1 and vi+1 be the two neighbors of vi (where the indices are computed modulo n).
These three vertices form the triangle vi−1, vi , vi+1. If the situation is as in Figure 6.36,                       vi +1
then we can proceed recursively by removing vi from the original list and we will have
a triangle and a polygon with n − 1 vertices.                                                    FIGURE 6.37 Vertex inside
However, because the polygon may not be convex, the line segment from vi−1                triangle.
to vi+1 can cross other edges, as shown in Figure 6.37. We can test for this case by
checking if any of the other vertices lie to the left of the line segment and inside the
triangle determined by vi−1, vi , vi+1. If we connect vi to the left-most of these vertices,                vi –1
we split the original triangle into two polygons (as in Figure 6.38), each of which has
at least two vertices fewer than the original triangle. Using the leftmost vertex ensures
that the two polygons are simple. Hence, we can proceed recursively with these two                     vi
triangles, knowing that in the end we will have all triangles.
Note that the worst-case performance of this method occurs when there are no                                 vi +1
vertices in the triangle formed by vi−1, vi , vi+1. We require O(n) tests to make sure that
this is the case and if it is we then remove only one vertex from the original polygon.          FIGURE 6.38 Splitting into
two polygons.
Consequently, the worst-case performance is O(n2). However, if we know in advance
that the polygon is convex, these tests are not needed and the method is O(n). The
best performance in general occurs when the splitting results in two polygons with
an equal number of vertices. If such a split occurs on each step, the method would be
O(n log n). The Suggested Readings at the end of the chapter include methods that
are guaranteed to be O(n log n), but they are more complex than the method outlined
here. In practice, we rarely work with polygons with so many vertices that we need
the more complex methods.

#### 6.10.3 Fill and Sort

A different approach to rasterization of polygons starts with the idea of a polygon
processor: a black box whose inputs are the vertices for a set of two-dimensional
polygons and whose output is a frame buffer with the correct pixels set. Suppose
330          Chapter 6     From Vertices to Fragments
that we consider ﬁlling each polygon with a constant color—a choice we make only
to clarify the discussion. First, consider a single polygon. The basic rule for ﬁlling a
polygon is as follows: If a point is inside the polygon, color it with the inside (ﬁll) color.
This conceptual algorithm indicates that polygon ﬁll is a sorting problem, where we
sort all the pixels in the frame buffer into those that are inside the polygon and those
that are not. From this perspective, we obtain different polygon-ﬁll algorithms using
different ways of sorting the points. We introduce three possibilities:
Flood ﬁll
Scanline ﬁll
Odd–even ﬁll

#### 6.10.4 Flood Fill

We can display an unﬁlled polygon by rasterizing its edges into the frame buffer using
Bresenham’s algorithm. Suppose that we have only two colors: a background color
(white) and a foreground, or drawing, color (black). We can use the foreground color

*FIGURE 6.39 Polygon dis-          to rasterize the edges, resulting in a frame buffer colored as shown in Figure 6.39*

played by edges.                  for a simple polygon. If we can ﬁnd an initial point (x, y) inside the polygon—
a seed point—then we can look at its neighbors recursively, coloring them with
the foreground color if they are not edge points. The ﬂood-ﬁll algorithm can be
expressed in pseudocode, assuming that there is a function read_pixel that returns
the color of a pixel:
flood_fill(int x, int y)

```cpp
{
```

if (read_pixel(x,y) == WHITE)

```cpp
{
```

write_pixel(x,y,BLACK);
flood_fill(x-1,y);
flood_fill(x+1,y);
flood_fill(x,y-1);
flood_fill(x,y+1);

```cpp
}
}
```

(a)
We can obtain a number of variants of ﬂood ﬁll by removing the recursion. One way
to do so is to work one scanline at a time.

#### 6.10.5 Singularities

We can extend most polygon-ﬁll algorithms to other shapes if we use care (see Exer-
(b)                 cise 6.17). Polygons have the distinct advantage that the locations of their edges are
known exactly. Even polygons can present problems, however, when vertices lie on

*FIGURE 6.40 Singularities.*

(a) Zero or two edge crossings.   scanlines. Consider the two cases in Figure 6.40. If we are using an odd–even ﬁll def-
(b) One edge crossing.            inition, we have to treat these two cases differently. For case (a), we can count the
                                                                            6.11 Hidden-Surface Removal   331

```cpp
intersection of the scanline with the vertex as either zero or two edge crossings; for
```

case (b), the vertex–scanline intersection must be counted as one edge crossing.
We can ﬁx our algorithm in one of two ways. We can check to see which of
the two situations we have and then count the edge crossings appropriately. Or we
can prevent the special case of a vertex lying on an edge—a singularity—from ever
arising. We rule it out by ensuring that no vertex has an integer y value. If we ﬁnd one
that does, we perturb its location slightly. Another method—one that is especially
valuable if we are working in the frame buffer—is to consider a virtual frame buffer
of twice the resolution of the real frame buffer. In the virtual frame buffer, pixels are
located only at even values of y, and all vertices are located only at odd values of y.
Placing pixel centers halfway between integers, as OpenGL does, is equivalent to using
this approach.

### 6.11    HIDDEN-SURFACE REMOVAL

Although every fragment generated by rasterization corresponds to a location in a
color buffer, we do not want to display the fragment by coloring the corresponding
pixel if the fragment is from an object behind another opaque object. Hidden-surface
removal (or visible-surface determination) is done to discover what part, if any, of
each object in the view volume is visible to the viewer or is obscured from the viewer
by other objects. We describe a number of techniques for a scene composed purely of
planar polygons. Because most renderers will have subdivided surfaces into polygons
at this point, this choice is appropriate. Line segments can be handled by slight
modiﬁcations (see Exercise 6.7).

#### 6.11.1 Object-Space and Image-Space Approaches

The study of hidden-surface–removal algorithms clearly illustrates the variety of
available algorithms, the differences between working with objects and working with
images, and the importance of evaluating the incremental effects of successive algo-
rithms in the implementation process.
Consider a scene composed of k three-dimensional opaque ﬂat polygons, each
of which we can consider to be an individual object. We can derive a generic object-
space approach by considering the objects pairwise, as seen from the center of projec-
tion. Consider two such polygons, A and B. There are four possibilities (Figure 6.41):
1. A completely obscures B from the camera; we display only A.
2. B obscures A; we display only B.
3. A and B both are completely visible; we display both A and B.
4. A and B partially obscure each other; we must calculate the visible parts of
each polygon.
For complexity considerations, we can regard the determination of which case we
have and any required calculation of the visible part of a polygon as a single opera-
tion. We proceed iteratively. We pick one of the k polygons and compare it pairwise
332   Chapter 6   From Vertices to Fragments
(a)                        (b)                        (c)                           (d)

*FIGURE 6.41 Two polygons. (a) B partially obscures A. (b) A partially*

obscures B. (c) Both A and B are visible. (d) B totally obscures A.
with the remaining k − 1 polygons. After this procedure, we know which part (if any)
of this polygon is visible, and we render the visible part. We are now done with this
polygon, so we repeat the process with any of the other k − 1 polygons. Each step in-
volves comparing one polygon, pairwise, with the other remaining polygons until we
have only two polygons remaining, and we compare these to each other. We can eas-
ily determine that the complexity of this calculation is O(k 2). Thus, without deriving
any of the details of any particular object-space algorithm, we should suspect that the
object-space approach works best for scenes that contain relatively few polygons.
The image-space approach follows our viewing and ray-casting model, as shown
in Figure 6.42. Consider a ray that leaves the center of projection and passes through a
pixel. We can intersect this ray with each of the planes determined by our k polygons,
determine for which planes the ray passes through a polygon, and ﬁnally, for those
rays, ﬁnd the intersection closest to the center of projection. We color this pixel with
the shade of the polygon at the point of intersection. Our fundamental operation is
the intersection of rays with polygons. For an n × m display, we have to carry out this
operation nmk times, giving O(k) complexity.4 Again, without looking at the details
of the operations, we were able to get an upper bound. In general, the O(k) bound
accounts for the dominance of image-space methods. The O(k) bound is a worst-case
bound. In practice, image-space algorithms perform much better (see Exercise 6.9).
However, because image-space approaches work at the fragment or pixel level, their
accuracy is limited by the resolution of the frame buffer.

#### 6.11.2 Sorting and Hidden-Surface Removal

The O(k 2) upper bound for object-oriented hidden-surface removal might remind
you of the poorer sorting algorithms, such as bubble sort. Any method that involves
brute-force comparison of objects by pairs has O(k 2) complexity. But there is a more
direct connection, which we exploited in the object-oriented sorting algorithms in
Section 6.8.5. If we could organize objects by their distances from the camera, we
should be able to come up with a direct method of rendering them.
But if we follow the analogy, we know that the complexity of good sorting al-
gorithms is O(k log k). We should expect the same to be true for object-oriented
4. We can use more than one ray for each pixel to increase the accuracy of the rendering.
                                                                            6.11 Hidden-Surface Removal                 333

*FIGURE 6.42 Image-space hidden-surface removal.*

hidden-surface removal, and, in fact, such is the case. As with sorting, there are
multiple algorithms that meet these bounds. In addition, there are related problems
involving comparison of objects, such as collision detection, that start off looking as
if they are O(k 2) when, in fact, they can be reduced to O(k log k).

*FIGURE 6.43 Polygon with*

spans.

#### 6.11.3 Scanline Algorithms

The attraction of a scanline algorithm is that such a method has the potential to
generate pixels as they are displayed. Consider the polygon in Figure 6.43, with one
6   7
scanline shown. If we use our odd–even rule for deﬁning the inside of the polygon,
5       8
we can see three groups of pixels, or spans, on this scanline that are inside the
4           9
polygon. Note that each span can be processed independently for lighting or depth                                  C
calculations, a strategy that has been employed in some hardware that has parallel                              10
span processors. For our simple example of constant ﬁll, after we have identiﬁed the                       11
spans, we can color the interior pixels of each span with the ﬁll color.                                12
The spans are determined by the set of intersections of polygons with scanlines.
The vertices contain all the information that we need to determine these intersec-          FIGURE 6.44 Polygon gener-
tions, but the method that we use to represent the polygon determines the order in          ated by vertex list.
which these intersections are generated. For example, consider the polygon in Fig-
ure 6.43, which has been represented by an ordered list of vertices. The most obvious                   B
way to generate scanline–edge intersections is to process edges deﬁned by successive                   1   2
vertices. Figure 6.44 shows these intersections, indexed in the order in which this                   3       4
method would generate them. Note that this calculation can be done incrementally                     5            6
(see Exercise 6.18). However, as far as ﬁll is concerned, this order is far from the one            7               C
we want. If we are to ﬁll one scanline at a time, we would like the intersections sorted,          9            8
ﬁrst by scanlines and then by order of x on each scanline, as shown in Figure 6.45.              11        10
A brute-force approach might be to sort all the intersections into the desired order.            A      12
However, a large or jagged polygon might intersect so many edges that the n inter-

*FIGURE 6.45 Desired order*

sections can be large enough that the O(n log n) complexity of the sort makes the           of vertices.
334          Chapter 6   From Vertices to Fragments
j    x1        x2
Scanlines   j+1   x3        x4
j+2   x4        x5         x7   x8

*FIGURE 6.46 Data structure for y–x algorithm.*

calculation too slow for real-time implementations; consider, for example, a polygon
that spans one-half of the scan lines.
A number of methods avoid the general search. One, originally known as the
y–x algorithm, creates a bucket for each scanline. As edges are processed, the in-
tersections with scanlines are placed in the proper buckets. Within each bucket, an
insertion sort orders the x values along each scanline. The data structure is shown in
Figure 6.46. Once again, we see that a properly chosen data structure can speed up
the algorithm. We can go even further by reconsidering how to represent polygons. If
we do so, we arrive at the scanline method that was introduced in Section 6.8.

#### 6.11.4 Back-Face Removal

v                    In Chapter 5, we noted that in OpenGL we can choose to render only front-facing
polygons. For situations where we cannot see back faces, such as scenes composed
of convex polyhedra, we can reduce the work required for hidden-surface removal
by eliminating all back-facing polygons before we apply any other hidden-surface–
removal algorithm. The test for culling a back-facing polygon can be derived from

*FIGURE 6.47 Back-face test.    Figure 6.47. We see the front of a polygon if the normal, which comes out of the front*

face, is pointed toward the viewer. If θ is the angle between the normal and the viewer,
then the polygon is facing forward if and only if
−90 ≤ θ ≤ 90
or, equivalently,
cos θ ≥ 0.
The second condition is much easier to test because, instead of computing the cosine,
we can use the dot product:
n . v ≥ 0.
                                                                              6.11 Hidden-Surface Removal   335
(xi+1, +1)
(xi, yi )

*FIGURE 6.48 Computing the area of a polygon.*

We can simplify this test even further if we note that usually it is applied after transfor-
mation to normalized device coordinates. In this system, all views are orthographic,
with the direction of projection along the z-axis. Hence, in homogeneous coordi-
nates,
⎡ ⎤
⎢0⎥
⎢ ⎥
v=⎢ ⎥.
⎣ 1⎦
Thus, if the polygon is on the surface
ax + by + cz + d = 0
in normalized device coordinates, we need only to check the sign of c to determine
whether we have a front- or back-facing polygon. This test can be implemented easily
in either hardware or software; we must simply be careful to ensure that removing
back-facing polygons is correct for our application.
There is another interesting approach to determining back faces. The algorithm
is based on computing the area of the polygon in screen coordinates. Consider the
polygon in Figure 6.48 with n vertices. Its area a is given by
1
a=      (y + yi )(xi+1 − xi ),
2 i i+1
where the indices are taken modulo n (see Exercise 6.28). A negative area indicates a
back-facing polygon.

#### 6.11.5 The z-Buffer Algorithm

The z-buffer algorithm is the most widely used hidden-surface–removal algorithm.
It has the advantages of being easy to implement, in either hardware or software, and
of being compatible with pipeline architectures, where it can execute at the speed at
336   Chapter 6   From Vertices to Fragments

*FIGURE 6.49 The z-buffer algorithm.*

which fragments are passing through the pipeline. Although the algorithm works in
image space, it loops over the polygons rather than over pixels and can be regarded
as part of the scan-conversion process that we discussed in Section 6.10.
Suppose that we are in the process of rasterizing one of the two polygons shown
in Figure 6.49. We can compute a color for each point of intersection between a ray
from the center of projection and a pixel, using interpolated values of the vertex
shades computed as in Chapter 5. In addition, we must check whether this point is
visible. It will be visible if it is the closest point of intersection along the ray. Hence,
if we are rasterizing B, its shade will appear on the screen if the distance z2 is less
than the distance z1 to polygon A. Conversely, if we are rasterizing A, the pixel that
corresponds to the point of intersection will not appear on the display. Because we
are proceeding polygon by polygon, however, we do not have the information on
all other polygons as we rasterize any given polygon. However, if we keep depth
information with each fragment, then we can store and update depth information
for each location in the frame buffer as fragments are processed.
Suppose that we have a buffer, the z-buffer, with the same resolution as the
frame buffer and with depth consistent with the resolution that we wish to use for
distance. For example, if we have a 1024 × 1280 display and we use standard integers
for the depth calculation, we can use a 1024 × 1280 z-buffer with 32-bit elements.
Initially, each element in the depth buffer is initialized to a depth corresponding to the
maximum distance away from the center of projection.5 The color buffer is initialized
to the background color. At any time during rasterization and fragment processing,
5. If we have already done perspective normalization, we should replace the center of projection with
the direction of projection because all rays are parallel. However, this change does not affect the
z-buffer algorithm, because we can measure distances from any arbitrary plane, such as the plane
z = 0, rather than from the COP.
                                                                               6.11 Hidden-Surface Removal   337
each location in the z-buffer contains the distance along the ray corresponding to the
location of the closest intersection point on any polygon found so far.
The calculation proceeds as follows. We rasterize, polygon by polygon, using one
of the methods from Section 6.10. For each fragment on the polygon correspond-
ing to the intersection of the polygon with a ray through a pixel, we compute the
depth from the center of projection. We compare this depth to the value in the z-
buffer corresponding to this fragment. If this depth is greater than the depth in the
z-buffer, then we have already processed a polygon with a corresponding fragment
closer to the viewer, and this fragment is not visible. If the depth is less than the
depth in the z-buffer,6 then we have found a fragment closer to the viewer. We up-
date the depth in the z-buffer and place the shade computed for this fragment at the
corresponding location in the color buffer. Note that for perspective views, the depth
we are using in the z-buffer algorithm is the distance that has been altered by the
normalization transformation that we discussed in Chapter 4. Although this trans-
formation is nonlinear, it preserves relative distances. However, this nonlinearity can

```cpp
introduce numerical inaccuracies, especially when the distance to the near clipping
```

plane is small.
Unlike other aspects of rendering where the particular implementation algo-
rithms may be unknown to the user, for hidden-surface removal, OpenGL uses the
z-buffer algorithm. This exception arises because the application program must ini-
tialize the z-buffer explicitly every time a new image is to be generated.
The z-buffer algorithm works well with image-oriented approaches to imple-
mentation because the amount of incremental work is small. Suppose that we are
rasterizing a polygon, scanline by scanline—an option we examined in Section 6.9.
The polygon is part of a plane (Figure 6.50) that can be represented as
ax + by + cz + d = 0.
Suppose that (x1, y1, z1) and (x2 , y2 , z2) are two points on the polygon (and the
plane). If
x = x2 − x1 ,
y = y 2 − y1 ,
z = z2 − z 1 ,
then the equation for the plane can be written in differential form as
a x + b y + c z = 0.
6. In OpenGL, we can use the function glDepthFunc to decide what to do when the distances are
equal.
338   Chapter 6   From Vertices to Fragments
(x2, y2, z2)
(x1, y1, z1)
ax + by + cz + d = 0

*FIGURE 6.50 Incremental z-buffer algorithm.*

This equation is in window coordinates, so each scanline corresponds to a line of

```cpp
constant y and y = 0 as we move across a scanline. On a scanline, we increase x in
```

unit steps, corresponding to moving one pixel in the frame buffer, and x is constant.
Thus, as we move from point to point across a scanline,
z=−       x.
This value is a constant that needs to be computed only once for each polygon.
Although the worst-case performance of an image-space algorithm is propor-
tional to the number of primitives, the performance of the z-buffer algorithm is
proportional to the number of fragments generated by rasterization, which depends
on the area of the rasterized polygons.

#### 6.11.6 Scan Conversion with the z-Buffer

We have already presented most of the essentials of polygon rasterization. In Sec-
tion 6.10.1, we discussed the odd–even and winding tests for determining whether a
point is inside a polygon. In Chapter 5, we learned to shade polygons by interpolation.
Here we have only to put together the pieces and to consider efﬁciency.
Suppose that we follow the pipeline once more, concentrating on what happens
to a single polygon. The vertices and normals pass through the geometric transforma-
tions one at a time. The vertices must be assembled into a polygon before the clipping
stage. If our polygon is not clipped out, its vertices and normals can be passed on for
shading and hidden-surface removal. At this point, although projection normaliza-
tion has taken place, we still have depth information. If we wish to use an interpolative
shading method, we can compute the lighting at each vertex.
Three tasks remain: computation of the ﬁnal orthographic projection, hidden-
surface removal, and shading. Careful use of the z-buffer algorithm can accomplish
                                                                           6.11 Hidden-Surface Removal   339
(a)                     (b)

*FIGURE 6.51 Dual representations of a polygon. (a) Normalized device*

coordinates. (b) Screen coordinates.
y=j                 ys = j
(a)                     (b)

*FIGURE 6.52 Dual representations of a scanline. (a) In normalized device*

coordinates. (b) In screen coordinates.
all three tasks simultaneously. Consider the dual representations of a polygon illus-
trated in Figure 6.51. In (a) the polygon is represented in three-dimensional normal-
ized device coordinates; in (b) it is shown after projection in screen coordinates.
The strategy is to process each polygon, one scanline at a time. If we work again
in terms of these dual representations, we can see that a scanline, projected backward
from screen coordinates, corresponds to a line of constant y in normalized device
coordinates (Figure 6.52). Suppose that we simultaneously march across this scan-
line and its back projection. For the scanline in screen coordinates, we move one
pixel width with each step. We use the normalized-device-coordinate line to deter-
mine depths incrementally and to see whether or not the pixel in screen coordinates
corresponds to a visible point on the polygon. Having computed shading for the ver-
tices of the original polygon, we can use interpolation to obtain the correct color for
visible pixels. This process requires little extra effort over the individual steps that
we have already discussed. It is controlled, and thus limited, by the rate at which we
can send polygons through the pipeline. Modiﬁcations such as applying bit patterns,
called stipple patterns, or texture to polygons require only slight changes.
340   Chapter 6   From Vertices to Fragments
(a)                             (b)

*FIGURE 6.53 Painter’s algorithm. (a) Two polygons and a viewer are*

shown. (b) Polygon A partially obscures B when viewed.

#### 6.11.7 Depth Sort and the Painter’s Algorithm

Although image-space methods are dominant in hardware due to the efﬁciency and
ease of implementation of the z-buffer algorithm, often object-space methods are
used within the application to lower the polygon count. Depth sort is a direct imple-
mentation of the object-space approach to hidden-surface removal. We present the
algorithm for a scene composed of planar polygons; extensions to other classes of ob-
jects are possible. Depth sort is a variant of an even simpler algorithm known as the
painter’s algorithm.
Suppose that we have a collection of polygons that is sorted based on how far
from the viewer the polygons are. For the example in Figure 6.53(a), we have two
polygons. To a viewer, they appear as shown in Figure 6.53(b), with the polygon in
front partially obscuring the other. To render the scene correctly, we could ﬁnd the
part of the rear polygon that is visible and render that part into the frame buffer—
a calculation that requires clipping one polygon against the other. Or we could use
an approach analogous to the way a painter might render the scene. She probably
would paint the rear polygon in its entirety and then the front polygon, painting over
that part of the rear polygon not visible to the viewer in the process. Both polygons
would be rendered completely, with the hidden-surface removal being done as a
consequence of the back-to-front rendering of the polygons.7 The two questions
related to this algorithm are how to do the sort and what to do if polygons overlap.
Depth sort addresses both, although in many applications more efﬁciencies can be
found (see, for example, Exercise 6.10).
Suppose we have already computed the extent of each polygon. The next step
of depth sort is to order all the polygons by how far away from the viewer their
maximum z-value is. This step gives the algorithm the name depth sort. Suppose that
the order is as shown in Figure 6.54, which depicts the z-extents of the polygons after
the sort. If the minimum depth—the z-value—of a given polygon is greater than the
maximum depth of the polygon behind the one of interest, we can paint the polygons
back to front and we are done. For example, polygon A in Figure 6.54 is behind all
7. In ray tracing and scientiﬁc visualization, we often use front-to-back rendering of polygons.
Distance from COP                                                                    6.11 Hidden-Surface Removal             341
A                 zmax

*FIGURE 6.54 The z-extents of sorted polygons.*

(a)                           (b)

*FIGURE 6.55 Test for overlap in x- and y-extents. (a) Nonoverlapping*

x-extents. (b) Nonoverlapping y-extents.
the other polygons and can be painted ﬁrst. However, the others cannot be painted
based solely on the z-extents.
If the z-extents of two polygons overlap, we still may be able to ﬁnd an order
to paint (render) the polygons individually and yield the correct image. The depth-
sort algorithm runs a number of increasingly more difﬁcult tests, attempting to ﬁnd
such an ordering. Consider a pair of polygons whose z-extents overlap. The simplest
test is to check their x- and y-extents (Figure 6.55). If either of the x- or y-extents do
not overlap,8 neither polygon can obscure the other and they can be painted in either
order. Even if these tests fail, it may still be possible to ﬁnd an order in which we can
paint the polygons individually. Figure 6.56 shows such a case. All the vertices of one                FIGURE 6.56 Polygons with
polygon lie on the same side of the plane determined by the other. We can process                      overlapping extents.
the vertices (see Exercise 6.12) of the two polygons to determine whether this case
exists.
8. The x- and y-extent tests apply to only a parallel view. Here is another example of the advantage
of working in normalized device coordinates after perspective normalization.
342          Chapter 6    From Vertices to Fragments
Two troublesome situations remain. If three or more polygons overlap cyclically,
as shown in Figure 6.57, there is no correct order for painting. The best we can do
is to divide at least one of the polygons into two parts and attempt to ﬁnd an order
to paint the new set of polygons. The second problematic case arises if a polygon can
pierce another polygon, as shown in Figure 6.58. If we want to continue with depth
sort, we must derive the details of the intersection—a calculation equivalent to clip-
ping one polygon against the other. If the intersecting polygons have many vertices,
we may want to try another algorithm that requires less computation. A performance
analysis of depth sort is difﬁcult because the particulars of the application determine

*FIGURE 6.57 Cyclic overlap.*

how often the more difﬁcult cases arise. For example, if we are working with poly-
gons that describe the surfaces of solid objects, then no two polygons can intersect.
Nevertheless, it should be clear that, because of the initial sort, the complexity must
be at least O(k log k), where k is the number of objects.

### 6.12     ANTIALIASING

Rasterized line segments and edges of polygons look jagged. Even on a display device
that has a resolution as high as 1024 × 1280, we can notice these defects in the display.

*FIGURE 6.58 Piercing poly-       This type of error arises whenever we attempt to go from the continuous representa-*

gons.                            tion of an object, which has inﬁnite resolution, to a sampled approximation, which
has limited resolution. The name aliasing has been given to this effect because of the
tie with aliasing in digital signal processing.
Aliasing errors are caused by three related problems with the discrete nature
of the frame buffer. First, if we have an n × m frame buffer, the number of pixels
is ﬁxed, and we can generate only certain patterns to approximate a line segment.
Many different continuous line segments may be approximated by the same pattern
of pixels. We can say that all these segments are aliased as the same sequence of
pixels. Given the sequence of pixels, we cannot tell which line segment generated the
sequence. Second, pixel locations are ﬁxed on a uniform grid; regardless of where we
would like to place pixels, we cannot place them at other than evenly spaced locations.
Third, pixels have a ﬁxed size and shape.
At ﬁrst glance, it might appear that there is little we can do about such prob-
lems. Algorithms such as Bresenham’s algorithm are optimal in that they choose the
closest set of pixels to approximate lines and polygons. However, if we have a display
that supports more than two colors, there are other possibilities. Although mathe-
matical lines are one-dimensional entities that have length but not width, rasterized
lines must have a width in order to be visible. Suppose that each pixel is displayed as
a square of width 1 unit and can occupy a box of 1-unit height and width on the dis-
play. Our basic frame buffer can work only in multiples of one pixel;9 we can think of
an idealized line segment in the frame buffer as being one pixel wide, as shown in Fig-

*FIGURE 6.59 Ideal raster line.   ure 6.59. Of course, we cannot draw this line, because it does not consist of our square*

9. Some frame buffers permit operations in units of less than one pixel through multisampling
methods.
                                                                                            6.12 Antialiasing      343
(a)                      (b)                       (c)                      (d)

*FIGURE 6.60 Aliased versus antialiased line segments. (a) Aliased line segment.*

(b) Antialiased line segment. (c) Magnified aliased line segment.
(d) Magnified antialiased line segment.
pixels. We can view Bresenham’s algorithm as a method for approximating the ideal
one-pixel-wide line with our real pixels. If we look at the ideal one-pixel-wide line, we
can see that it partially covers many pixel-sized boxes. It is our scan-conversion algo-
rithm that forces us, for lines of slope less than 1, to choose exactly one pixel value
for each value of x. If, instead, we shade each box by the percentage of the ideal line
that crosses it, we get the smoother appearing image shown in Figure 6.60(b). This
technique is known as antialiasing by area averaging. The calculation is similar to
polygon clipping. There are other approaches to antialiasing, as well as antialiasing
algorithms that can be applied to other primitives, such as polygons. Color Plate 8
shows aliased and antialiased versions of a small area of the object in Color Plate 1.
A related problem arises because of the simple way that we are using the z-
buffer algorithm. As we have speciﬁed that algorithm, the color of a given pixel
is determined by the shade of a single primitive. Consider the pixel shared by the
three polygons shown in Figure 6.61. If each polygon has a different color, the color        FIGURE 6.61 Polygons that
assigned to the pixel is the one associated with the polygon closest to the viewer. We       share a pixel.
could obtain a much more accurate image if we could assign a color based on an
area-weighted average of the colors of the three triangles. Such algorithms can be
implemented with fragment shaders on hardware with ﬂoating point frame buffers.
We have discussed only one type of aliasing: spatial-domain aliasing. When we
generate sequences of images, such as for animations, we also must be concerned
with time-domain aliasing. Consider a small object moving in front of the projection
plane that has been ruled into pixel-sized units, as shown in Figure 6.62. If our
rendering process sends a ray through the center of each pixel and determines what
it hits, then sometimes we intersect the object and sometimes, if the projection of the
object is small, we miss the object. The viewer will have the unpleasant experience of
seeing the object ﬂash on and off the display as the animation progresses. There are
several ways to deal with this problem. For example, we can use more than one ray
per pixel—a technique common in ray tracing. What is common to all antialiasing
techniques is that they require considerably more computation than does rendering
without antialiasing. In practice, for high-resolution images, antialiasing is done off-
line and only when a ﬁnal image is needed.
344   Chapter 6   From Vertices to Fragments

*FIGURE 6.62 Time-domain aliasing.*


### 6.13    DISPLAY CONSIDERATIONS

In most interactive applications, the application programmer need not worry about
how the contents of the frame buffer are displayed. From the application program-
mer’s perspective, as long as she uses double buffering, the process of writing into
the frame buffer is decoupled from the process of reading the frame buffer’s contents
for display. The hardware redisplays the present contents of the frame buffer at a rate
sufﬁcient to avoid ﬂicker—usually 60 to 85 Hz—and the application programmer
worries only about whether or not her program can execute and ﬁll the frame buffer
fast enough. As we saw in Chapter 2, the use of double buffering allows the display to
change smoothly, even if we cannot push our primitives through the system as fast as
we would like.
Numerous other problems affect the quality of the display and often cause users
to be unhappy with the output of their programs. For example, the displays of two
monitors may have the same nominal resolution but may display pixels of different
sizes (see Exercises 6.22 and 6.23).
Perhaps the greatest source of problems with displays concerns the basic phys-
ical properties of displays: the range of colors they can display and how they map
software-deﬁned colors to the values of the primaries for the display. The color
gamuts of different displays can differ greatly. In addition, because the primaries
on different systems are different, even when two different monitors can produce the
same visible color, they may require different values of the primaries to be sent to
the displays from the graphics system. In addition, the mapping between brightness
values deﬁned by the program and what is displayed is nonlinear.
OpenGL does not address these issues directly, because colors are speciﬁed as
RGB values that are independent of any display properties. In addition, because RGB
primaries are limited to the range from 0.0 to 1.0, it is often difﬁcult to account for the
full range of color and brightness detectable by the human visual system. However, if
we expand on our discussion of color and the human visual system from Chapter 2,
we can gain some additional control over color in OpenGL.
                                                                            6.13 Display Considerations   345
C = (T1, T2, T3)

*FIGURE 6.63 Color cube.*


#### 6.13.1 Color Systems

Our basic assumption about color, supported by the three-color theory of human
vision, is that the three color values that we determine for each pixel correspond to
the tristimulus values that we introduced in Chapter 2. Thus, a given color is a point
in a color cube, as in Figure 6.63, and can be written symbolically as
C = T1R + T2G + T3B.
However, there are signiﬁcant differences across RGB systems. For example, suppose
that we have a yellow color that OpenGL has represented with the RGB triplet (0.8,
0.6, 0.0). If we use these values to drive both a CRT and a ﬁlm-image recorder, we will
see different colors, even though in both cases the red is 80 percent of maximum, the
green is 60 percent of maximum, and there is no blue. The reason is that the ﬁlm dyes
and the CRT phosphors have different color distributions. Consequently, the range of
displayable colors (or the color gamut) is different for each.
The emphasis in the graphics community has been on device-independent
graphics; consequently, the real differences among display properties are not ad-
dressed by most APIs. Fortunately, the colorimetry literature contains the infor-
mation we need. The standards for many of the common color systems exist. For
example, CRTs are based on the National Television Systems Committee (NTSC)
RGB system. We can look at differences in color systems as being equivalent to differ-
ent coordinate systems for representing our tristimulus values. If C1 = [R1, G1, B1]T
and C2 = [R2 , G2 , B2]T are the representations of the same color in two different
systems, then there is a 3 × 3 color-conversion matrix M such that
C2 = MC1.
Whether we determine this matrix from the literature or by experimentation, it
allows us to produce similar displays on different output devices.
346   Chapter 6   From Vertices to Fragments
There are numerous potential problems even with this approach. The color
gamuts of the two systems may not be the same. Hence, even after the conversion
of tristimulus values, a color may not be producible on one of the systems. Second,
the printing and graphic arts industries use a four-color subtractive system (CMYK)
that adds black (K) as a fourth primary. Conversion between RGB and CMYK often
requires a great deal of human expertise. Third, there are limitations to our linear
color theory. The distance between colors in the color cube is not a measure of how
far apart the colors are perceptually. For example, humans are particularly sensitive
to color shifts in blue. Color systems such as YUV and CIE Lab have been created to
address such issues.
Most RGB color systems are based on the primaries in real systems, such as CRT
phosphors and ﬁlm dyes. None can produce all the colors that we can see. Most
color standards are based on a theoretical three-primary system called the XYZ color
system. Here, the Y primary is the luminance of the color. In the XYZ system, all
colors can be speciﬁed with positive tristimulus values. We use 3 × 3 matrices to
convert from an XYZ color representation to representations in the standard systems.
Color specialists often prefer to work with chromaticity coordinates rather than
tristimulus values. The chromaticity of a color consists of the three fractions of the
color in the three primaries. Thus, if we have the tristimulus values, T1, T2 , and T3,
for a particular RGB color, its chromaticity coordinates are
T1
t1 =                ,
T1 + T2 + T3
T2
t2 =                ,
T1 + T2 + T3
T3
t3 =                  .
T 1 + T2 + T 3
Adding the three equations, we have
t1 + t2 + t3 = 1,
and thus we can work in the two-dimensional t1, t2 space, ﬁnding t3 only when
its value is needed. The information that is missing from chromaticity coordinates,
which was contained in the original tristimulus values, is the sum T1 + T2 + T3 , a
value related to the intensity of the color. When working with color systems, this

```cpp
intensity is often not important to issues related to producing colors or matching
```

colors across different systems.
Because each color fraction must be nonnegative, the chromaticity values are
1 ≥ ti ≥ 0.
                                                                            6.13 Display Considerations   347
1.0
x+y=1
x              1.0

*FIGURE 6.64 Triangle of producible colors in chromaticity coordinates.*

1.0
500 nm
700 nm
400 nm       x                   1.0

*FIGURE 6.65 Visible colors and color gamut of a display.*

All producible colors must lie inside the triangle in Figure 6.64. Figure 6.65 shows
this triangle for the XYZ system and a curve of the representation for each visible
spectral line. For the XYZ system, this curve must lie inside the triangle. Figure 6.65
also shows the range of colors (in x, y chromaticity coordinates) that are producible
on a typical color printer or CRT. If we compare the two ﬁgures, we see that the colors
inside the curve of pure spectral lines but outside the gamut of the physical display
cannot be displayed on the physical device.
One defect of our development of color is that RGB color is based on how color is
produced and measured rather than on how we perceive a color. When we see a given
348   Chapter 6   From Vertices to Fragments
(a)                                 (b)

*FIGURE 6.66 Hue–lightness–saturation color. (a) Using the RGB color*

cube. (b) Using a single cone.
color, we describe it not by three primaries but based on other properties, such as the
name we give the color and how bright a shade we see. The hue–saturation–lightness
(HLS) system is used by artists and some display manufacturers. The hue is the name
we give to a color: red, yellow, gold. The lightness is how bright the color appears.
Saturation is the color attribute that distinguishes a pure shade of a color from a
shade of the same hue that has been mixed with white, forming a pastel shade. We can
relate these attributes to a typical RGB color, as shown in Figure 6.66(a). Given a color
in the color cube, the lightness is a measure of how far the point is from the origin
(black). If we note that all the colors on the principal diagonal of the cube, going from
black to white, are shades of gray and are totally unsaturated, then the saturation is a
measure of how far the given color is from this diagonal. Finally, the hue is a measure
of where the color vector is pointing. HLS colors are usually described in terms of a
color cone, as shown in Figure 6.66(b), or a double cone that also converges at the top.
From our perspective, we can look at the HLS system as providing a representation
of an RGB color in polar coordinates.

#### 6.13.2 The Color Matrix

RGB colors and RGBA colors can be manipulated as any other vector type. In par-
ticular, we can alter their components by multiplying by a matrix we call the color
matrix. For example, if we use an RGBA color representation, the matrix multiplica-
tion converts a color, rgba, to a new color, r g ba, by the matrix multiplication
⎡ ⎤         ⎡ ⎤
⎢ g ⎥       ⎢g ⎥
⎢ ⎥          ⎢ ⎥
⎢ ⎥=C⎢ ⎥.
⎣b ⎦         ⎣b⎦
a          a
Thus, if we are dealing with opaque surfaces for which A = 1, the matrix
                                                                                 6.13 Display Considerations                   349
⎡        ⎤
−1 0 0 1
⎢ 0 −1 0 1 ⎥
⎢           ⎥
C=⎢           ⎥
⎣ 0  0 −1 1 ⎦
0  0 0 1
converts the additive representation of a color to its subtractive representation.

#### 6.13.3 Gamma Correction

In Chapter 2, we deﬁned brightness as perceived intensity and observed that the                                                x
human visual system perceives intensity in a logarithmic manner, as depicted in
Figure 6.67. One consequence of this property is that if we want the brightness steps          FIGURE 6.67 Logarithmic
to appear to be uniformly spaced, the intensities that we assign to pixels should              brightness.
increase exponentially. These steps can be calculated from the measured minimum
and maximum intensities that a display can generate.
In addition, the intensity I of a CRT is related to the voltage V applied by
I ∝ Vγ
log I = c0 + γ log V ,
where the constants γ and c0 are properties of the particular CRT. One implication
of these two results is that two monitors may generate different brightnesses for the
same values in the frame buffer. One way to correct for this problem is to have a
lookup table in the display whose values can be adjusted for the particular character-
istics of the monitor—the gamma correction.
There is an additional problem with CRTs. It is not possible to have a CRT
whose display is totally black when no signal is applied. The minimum displayed

```cpp
intensity is called the dark ﬁeld value and can be problematic, especially when CRT
```

technology is used to project images. The contrast ratio of a display is the ratio of the
maximum to minimum brightness. Newer display technologies have contrast ratios
in the thousands.

#### 6.13.4 Dithering and Halftoning

We have speciﬁed a color buffer by its spatial resolution (the number of pixels)
and by its precision (the number of colors it can display). If we view these separate
numbers as ﬁxed, we say that a high-resolution black-and-white laser printer can
display only 1-bit pixels. This argument also seems to imply that any black-and-white
medium, such as a book, cannot display images with multiple shades. We know from
experience that that is not the case; the trick is to trade spatial resolution for grayscale
or color precision. Halftoning techniques in the printing industry use photographic
means to simulate gray levels by creating patterns of black dots of varying size. The
350         Chapter 6   From Vertices to Fragments
human visual system tends to merge small dots together and sees not the dots, but
rather an intensity proportional to the ratio of white to black in a small area.
Digital halftones differ because the size and location of displayed pixels are ﬁxed.
Consider a 4 × 4 group of 1-bit pixels, as shown in Figure 6.68. If we look at this

*FIGURE 6.68 Digital halftone   pattern from far away, we see not the individual pixels but rather a gray level based*

patterns.
on the number of black pixels. For our 4 × 4 example, although there are 216 different
patterns of black and white pixels, there are only 17 possible shades, corresponding to
0 to 16 black pixels in the array. There are many algorithms for generating halftone, or
dither, patterns. The simplest picks 17 patterns (for our example) and uses them to
create a display with 17 rather than two gray levels, although at the cost of decreasing
the spatial resolution by a factor of 4.
The simple algorithm—always using the same array to simulate a shade—can
generate beat, or moiré, patterns when displaying anything regular. Such patterns
arise whenever we image two regular phenomena, because we see the sum and dif-
ferences of their frequencies. Such effects are closely related to the aliasing problems
we shall discuss in Chapter 7.
Many dithering techniques are based on simply randomizing the least signiﬁcant
bit of the luminance or of each color component. More sophisticated dither algo-
rithms use randomization to create patterns with the correct average properties but
avoid the repetition that can lead to moiré effects (see Exercise 6.26).
Halftoning (or dithering) is often used with color, especially with hard-copy dis-
plays, such as ink-jet printers, that can produce only fully on or off colors. Each pri-
mary can be dithered to produce more visual colors. OpenGL supports such displays
and allows the user to enable dithering (glEnable(GL_DITHER)). Color dithering
allows color monitors to produce smooth color displays, and normally dithering is
enabled. Because dithering is so effective, displays can work well with a limited num-
ber of bits per color, allowing frame buffers to have a limited amount of memory. In
many applications, we need to use the OpenGL query function glGetIntegerv to
ﬁnd out how many bits are being used for each color since this information is impor-
tant when we use some of the techniques from Chapter 7 that read pixels from the
frame buffer. If dithering is enabled and we read pixels out of the frame buffer, pixels
that were written with the same RGB values may return different values when read.
If these small differences are important, dithering should be disabled before reading
from the frame buffer.
We have presented an overview of the implementation process, including a sampling
of the most important algorithms. Regardless of what the particulars of an implemen-
tation are—whether the tasks are done primarily in hardware or in software, whether
we are working with a special-purpose graphics workstation or with a simple graph-
ics terminal, and what the API is—the same tasks must be done. These tasks include
implementation of geometric transformations, clipping, and rasterization. The rela-
tionship among hardware, software, and APIs is an interesting one.
                                                                                      Summary and Notes   351
The Geometry Engine that was the basis of many Silicon Graphics workstations
is a VLSI chip that performed geometric transformations and clipping through a
hardware pipeline. GL, the predecessor of OpenGL, was developed as an API for
users of these workstations. Much of the OpenGL literature also follows the pipeline
approach. We should keep in mind, however, that OpenGL is an API: It does not say
anything about the underlying implementation. In principle, an image deﬁned by an
OpenGL program could be obtained from a ray tracer. We should carry away two
lessons from our emphasis on pipeline architectures. First, this architecture provides
an aid to the applications programmer in understanding the process of creating
images. Second, at present, the pipeline view can lead to efﬁcient hardware and
software implementations.
The example of the z-buffer algorithm is illustrative of the relationship between
hardware and software. Fifteen years ago, many hidden-surface–removal algorithms
were used, of which the z-buffer algorithm was only one. The availability of fast,
dense, inexpensive memory has made the z-buffer algorithm the dominant method
for hidden-surface removal.
A related example is that of workstation architectures, where special-purpose
graphics chips have made remarkable advances in just the past few years. Not only
has graphics performance increased at a rate that exceeds Moore’s law, but many new
features have become available in the graphics processors. The whole approach we
have taken in this book is based on these architectures.
So what does the future hold? Certainly, graphics systems will get faster and less
expensive. More than any other factor, advances in hardware probably will dictate
what future graphics systems will look like. At the present, hardware development is
being driven by the video game industry. For less than $100, we can purchase a graph-
ics card that exceeds the performance of graphics workstations that a few years ago
would have cost more than $100,000. The features and performance of these cards are
optimized for the needs of the computer game industry. Thus, we do not see uniform
speedups in the various graphics functions that we have presented. In addition, new
hardware features are appearing far faster than they can be incorporated into stan-
dard APIs. However, the speed at which these processors operate has challenged both
the graphics and scientiﬁc communities to discover new algorithms to solve problems
that until now had always been solved using conventional architectures.
On the software side, the low cost and speed of recent hardware has enabled soft-
ware developers to produce rendering software that allows users to balance rendering
time and quality of rendering. Hence, a user can add some ray-traced objects to a
scene, the number depending on how long she is willing to wait for the rendering.
The future of standard APIs is much less clear. On one hand, users in the scientiﬁc
community prefer stable APIs so that application codes will have a long lifetime. On
the other hand, users want to exploit new hardware features that are not supported
on all systems. OpenGL has tried to take a middle road. Until OpenGL 3.1, all releases
were backward compatible, so applications developed on earlier versions were guar-
anteed to run on new releases. OpenGL 3.1 and later versions deprecated many core
features of earlier versions, including immediate mode rendering and most of the
default behavior of the ﬁxed-function pipeline. This major change in philosophy has
352   Chapter 6   From Vertices to Fragments
allowed OpenGL to rapidly incorporate new hardware features. For those who need
to run older code, almost all implementations support a compatibility extension with
all the deprecated functions.
Numerous advanced architectures under exploration use massive parallelism.
How parallelism can be exploited most effectively for computer graphics is still
an open issue. Our two approaches to rendering—object-oriented and image-
oriented—lead to two entirely different ways to develop a parallel renderer, which
we shall explore further in Chapter 11.
We have barely scratched the surface of implementation. The literature is rich
with algorithms for every aspect of the implementation process. The references
should help you to explore this topic further.
The books by Rogers [Rog85] and by Foley and colleagues [Fol90] contain many
more algorithms than we can present here. Also see the series Graphic Gems [Gra90,
Gra91, Gra92, Gra94, Gra95] and GPU Gems [Ngu07, Pha05]. Books such as Möller
and Haines [Mol02] and Eberly [Ebe01] cover the inﬂuence of recent advances in
hardware.
The Cohen-Sutherland [Sut63] clipping algorithm goes back to the early years
of computer graphics, as does Bresenham’s algorithm [Bre63, Bre87], which was
originally proposed for pen plotters. See [Lia84] and [Sut74a] for the Liang-Barsky
and Sutherland-Hogman clippers.
Algorithms for triangulation can be found in references on Computational Ge-
ometry. See, for example, de Berg[deB08], which also discusses Delaunay triangula-
tion, which we discuss in Chapter 10.
The z-buffer algorithm was developed by Catmull [Cat75]. See Sutherland
[Sut74b] for a discussion of various approaches to hidden-surface removal.
Our decision to avoid details of the hardware does not imply that the hardware
is either simple or uninteresting. The rate at which a modern graphics processor can
display graphical entities requires sophisticated and clever hardware designs [Cla82,
Ake88, Ake93]. The discussion by Molnar and Fuchs in [Fol90] shows a variety of
approaches.
Pratt [Pra78] provides matrices to convert among various color systems. Half-
tone and dithering are discussed by Jarvis [Jar76] and by Knuth [Knu87].

### 6.1   Consider two line segments represented in parametric form:

p(α) = (1 − α)p1 + αp2 ,
q(β) = (1 − β)q1 + βq2 .
                                                                                               Exercises   353
Find a procedure for determining whether the segments intersect and, if they
do, for ﬁnding the point of intersection.

### 6.2   Extend the argument of Exercise 6.1 to ﬁnd a method for determining whether

two ﬂat polygons intersect.

### 6.3   Prove that clipping a convex object against another convex object results in at

most one convex object.

### 6.4   In what ways can you parallelize the image- and object-oriented approaches to

implementation?

### 6.5   Because both normals and vertices can be represented in homogeneous coor-

dinates, both can be operated on by the model-view transformation. Show that
normals may not be preserved by the transformation.

### 6.6   Derive the viewport transformation. Express it in terms of the three-

dimensional scaling and translation matrices used to represent afﬁne trans-
formations in two dimensions.

### 6.7   Pre–raster-graphics systems were able to display only lines. Programmers pro-

duced three-dimensional images using hidden-line–removal techniques. Many
current APIs allow us to produce wireframe images, composed of only lines, in
which the hidden lines that deﬁne nonvisible surfaces have been removed. How
does this problem differ from that of the polygon hidden-surface removal that
we have considered? Derive a hidden-line–removal algorithm for objects that
consist of the edges of planar polygons.

### 6.8   Often we display functions of the form y = f (x, z) by displaying a rectangular

mesh generated by the set of values {f (xi , zj )} evaluated at regular intervals in x
and z. Hidden-surface removal should be applied because parts of the surface
can be obscured from view by other parts. Derive two algorithms, one using
hidden-surface removal and the other using hidden-line removal, to display
such a mesh.

### 6.9   Although we argued that the complexity of the image-space approach to

hidden-surface removal is proportional to the number of polygons, perfor-
mance studies have shown almost constant performance. Explain this result.

### 6.10 Consider a scene composed of only solid, three-dimensional polyhedra. Can

you devise an object-space, hidden-surface–removal algorithm for this case?
How much does it help if you know that all the polyhedra are convex?

### 6.11 We can look at object-space approaches to hidden-surface removal as analo-

gous to sorting algorithms. However, we argued that the former’s complexity
is O(k 2). We know that only the worst-performing sorting algorithms have
such poor performance, and most are O(k log k). Does it follow that object-
space, hidden-surface–removal algorithms have similar complexity? Explain
your answer.

### 6.12 Devise a method for testing whether one planar polygon is fully on one side of

another planar polygon.
354   Chapter 6   From Vertices to Fragments

### 6.13 What are the differences between our image-space approaches to hidden-

surface removal and to ray tracing? Can we use ray tracing as an alternate
technique to hidden-surface removal? What are the advantages and disadvan-
tages of such an approach?

### 6.14 Write a program to generate the locations of pixels along a rasterized line

segment using Bresenham’s algorithm. Check that your program works for all
slopes and all possible locations of the endpoints. What is the initial value of
the decision variable?

### 6.15 Bresenham’s algorithm can be extended to circles. Convince yourself of this

statement by considering a circle centered at the origin. Which parts of the
circle must be generated by an algorithm and which parts can be found by sym-
metry? Can you ﬁnd a part of the circle such that if we know a point generated
by a scan-conversion algorithm, we can reduce the number of candidates for
the next pixel?

### 6.16 Show how to use ﬂood ﬁll to generate a maze like the one you created in

Exercise 2.7.

### 6.17 Suppose that you try to extend ﬂood ﬁll to arbitrary closed curves by scan-

converting the curve and then applying the same ﬁll algorithm that we used
for polygons. What problems can arise if you use this approach?

### 6.18 Consider the edge of a polygon between vertices at (x1, y1) and (x2 , y2). Derive

an efﬁcient algorithm for computing the intersection of all scan lines with this
edge. Assume that you are working in window coordinates.

### 6.19 Vertical and horizontal edges are potentially problematic for polygon-ﬁll al-

gorithms. How would you handle these cases for the algorithms that we have
presented?

### 6.20 In two-dimensional graphics, if two polygons overlap, we can ensure that they

are rendered in the same order by all implementations by associating a priority
attribute with each polygon. Polygons are rendered in reverse-priority order;
that is, the highest-priority polygon is rendered last. How should we modify
our polygon-ﬁll algorithms to take priority into account?

### 6.21 A standard antialiasing technique used in ray tracing is to cast rays not only

through the center of each pixel but also through the pixel’s four corners. What
is the increase in work compared to casting a single ray through the center?

### 6.22 Although an ideal pixel is a square of 1 unit per side, most CRT systems

generate round pixels that can be approximated as circles of uniform intensity.
If a completely full unit square has intensity 1.0 and an empty square has

```cpp
intensity 0.0, how does the intensity of a displayed pixel vary with the radius
```

of the circle?

### 6.23 Consider a bilevel display with round pixels. Do you think it is wiser to use

small circles or large circles for foreground-colored pixels? Explain your an-
swer.
                                                                                         Exercises   355

### 6.24 Why is defocusing the beam of a CRT sometimes called “the poor person’s

antialiasing”?

### 6.25 Suppose that a monochrome display has a minimum intensity output of Imin —

a CRT display is never completely black—and a maximum output of Imax .
Given that we perceive intensities in a logarithmic manner, how should we
assign k intensity levels such that the steps appear uniform?

### 6.26 Generate a halftone algorithm based on the following idea. Suppose that gray

levels vary from 0.0 to 1.0 and that we have a random-number generator that
produces random numbers that are uniformly distributed over this interval. If
we pick a gray level g, g/100 percent of the random numbers generated will be
less than g.

### 6.27 Images produced on displays that support only a few colors or gray levels tend

to show contour effects because the viewer can detect the differences between
adjacent shades. One technique for avoiding this visual effect is to add a little
noise (jitter) to the pixel values. Why does this technique work? How much
noise should you add? Does it make sense to conclude that the degraded image
created by the addition of noise is of higher quality than that of the original
image?

### 6.28 Show that the area of   a two-dimensional polygon, speciﬁed by the vertices


```cpp
{x1, yi }, is given by 21 i (yi+1 + yi )(xi+1 − xi ). What is the signiﬁcance of
```

a negative area? Hint: Consider the areas of the trapezoids formed by two
successive vertices and corresponding values on the x-axis.
This page intentionally left blank
                                                                     CHA P TE R            7
T    hus far, we have worked directly with geometric objects, such as lines, polygons,
and polyhedra. Although we understood that, if visible, these entities would
eventually be rasterized into pixels in the frame buffer, we did not have to concern
ourselves with working with pixels directly. Over the last 25 years, the major advances
in hardware and software have evolved to allow the application program to access the
frame buffer both directly and indirectly. Many of the most exciting methods that
have evolved over the past two decades rely on interactions between the application
program and various buffers. Texture mapping, antialiasing, compositing, and alpha
blending are only a few of the techniques that become possible when the API allows
us to work with discrete buffers. At the same time, GPUs have evolved to include
a large amount of memory to support discrete techniques. This chapter introduces
these techniques, focusing on those that are supported by OpenGL and by similar
APIs.
We start by looking at the frame buffer in more detail and the basis for working
with arrays of pixels. We then consider mapping methods. These techniques are
applied during the rendering process, and they enable us to give the illusion of a
surface of great complexity, although the surface might be a single polygon. All these
techniques use arrays of pixels to deﬁne how the shading process that we studied in

## Chapter 5 is augmented to create these illusions. We shall then look at some of the

other buffers that are supported by the OpenGL API and how these buffers can be
used for new applications. In particular, we examine techniques for combining or
compositing images. Here we use the fourth “color” in RGBA mode, and we shall see
that we can use this channel to blend images and to create effects such as transparency.
We conclude with a discussion of the aliasing problems that arise whenever we work
with discrete elements.

### 7.1    BUFFERS

We have already used two types of standard buffers: color buffers and depth buffers.
There may be others supported by the hardware and software for special purposes.
What all buffers have in common is that they are inherently discrete: They have
358            Chapter 7   Discrete Techniques
limited resolution, both spatially and in depth. We can deﬁne a (two-dimensional)1
buffer as a block of memory with n × m k-bit elements (Figure 7.1).
We have used the term frame buffer to mean the set of buffers that the graphics
system uses for rendering, including the front and back color buffers, the depth
m                              buffer, and other buffers the hardware may provide. These buffers generally reside on
k          the graphics card. Later in this chapter, we will extend the notion of a frame buffer to
include other buffers that a system might provide for off-screen rendering operations.

*FIGURE 7.1 Buffer.         For now, we will work with just the standard frame buffer.*

At a given spatial location in the frame buffer, the k bits can include 32-bit RGBA
colors, integers representing depths, or bits that can be used for masks. Groups of bits
can store 1-byte representations of color components, integers for depths, or ﬂoating-
point numbers for colors or depths. Figure 7.2 shows the OpenGL frame buffer and
some of its constituent parts. If we consider the entire frame buffer, the values of n
and m match the spatial resolution of the display. The depth of the frame buffer—
the value of k—can exceed a few hundred bits. Even for the simple cases that we have
seen so far, we have 64 bits for the front and back color buffers and 32 bits for the
depth buffer. The numerical accuracy or precision of a given buffer is determined by
its depth. Thus, if a frame buffer has 32 bits each for its front and back color buffers,
each RGBA color component is stored with a precision of 8 bits.
When we work with the frame buffer, we usually work with one constituent
buffer at a time. Thus, we shall use the term buffer in what follows to mean a partic-
ular buffer within the frame buffer. Each of these buffers is n × m and is k bits deep.
However, k can be different for each buffer. For a color buffer, its k is determined
by how many colors the system can display, usually 24 for RGB displays and 32 for
RGBA displays. For the depth buffer, its k is determined by the depth precision that
the system can support, often 32 bits to match the size of a ﬂoating-point number or
an integer. We use the term bitplane to refer to any of the k n × m planes in a buffer,
and pixel to refer to all k of the bits at a particular spatial location. With this deﬁni-
tion, a pixel can be a byte, an integer, or even a ﬂoating-point number, depending on
which buffer is used and how data are stored in the buffer.
The applications programmer generally will not know how information is stored
in the frame buffer, because the frame buffer is inside the implementation which
the programmer sees as a black box. Thus, the application program sends (writes
or draws) information into the frame buffer or obtains (reads) information from the
frame buffer through OpenGL functions. When the application program reads or
writes pixels, not only are data transferred between ordinary processor memory and
graphics memory on the graphics card, but usually these data must be reformatted to
be compatible with the frame buffer. Consequently, what are ordinarily thought of as
digital images, for example JPEG, PNG, or TIFF images, exist only on the application
side of the process. Not only must the application programmer worry how to decode
particular images so they can be sent to the frame buffer through OpenGL functions,
but the programmer also must be aware of the time that is spent in the movement
1. We can also have one-, three-, and four-dimensional buffers.
                                                                                                   7.2 Digital Images   359

*FIGURE 7.2 OpenGL frame buffer.*

of digital data between processor memory and the frame buffer. If the application
programmer also knows the internal format of how data are stored in any of the
buffers, she can often write application programs that execute more efﬁciently.

### 7.2    DIGITAL IMAGES

Before we look at how the graphics system can work with digital images through pixel
and bit operations, let’s ﬁrst examine what we mean by a digital image.2 Within our
programs, we generally work with images that are arrays of pixels. These images can
be of a variety of sizes and data types, depending on the type of image with which we
are working. For example, if we are working with RGB images, we usually represent
each of the color components with 1 byte whose values range from 0 to 255. Thus, we
might declare a 512 × 512 image in our application program as
GLubyte myimage[512][512][3];
or, if we are using a ﬂoating-point representation,
typedef vec3 color3;
color3 myimage[512][512];
If we are working with monochromatic or luminance images, each pixel represents a
gray level from black (0) to white (255), so we would use
GLubyte myimage[512][512];
2. Most references often use the term image instead of digital image. This terminology can be
confused with using the term image to refer to the result of combining geometric objects and a
camera, through the projection process, to obtain what we have called an image. In this chapter,
the context should be clear so that there should not be any confusion.
360   Chapter 7   Discrete Techniques
One way to form digital images is through code in the application program. For
example, suppose that we want to create a 512 × 512 image that consists of an 8 × 8
checkerboard of alternating red and black squares, such as we might use for a game.
The following code will work:
color3 check[512][512];
color3 red = color3(1.0, 0.0, 0.0);
color3 black = color3(0.0, 0.0, 0.0);
for ( int i = 0; i < 512; i++)
for( int j = 0; j < 512; j++)

```cpp
{
```

check[i][j] = ((8*i+j)/64) % 64 ? red : black;

```cpp
}
```

Usually, writing code to form images is limited to those that contain regular
patterns. More often, we obtain images directly from data. For example, if we have
an array of real numbers that we have obtained from an experiment or a simulation,
we can scale them to go over the range 0 to 255 and then convert these data to form
an unsigned-byte luminance image or over 0.0 to 1.0 for a ﬂoating-point image.
There is a third method of obtaining images that has become much more preva-
lent because of the inﬂuence of the Internet. Images are produced by scanning con-
tinuous images, such as photographs, or produced directly using digital cameras.
Each image is in one of many possible “standard” formats. Some of the most popu-
lar formats are GIF, TIFF, PNG, PDF, and JPEG. These formats include direct coding
of the values in some order, compressed but lossless coding, and compressed lossy
coding. Each format arose from the particular needs of a group of applications. For
example, PostScript (PS) images are deﬁned by the PostScript language used to con-
trol printers. These images are an exact encoding of the image data—either RGB or
luminance—into the 7-bit ASCII character set. Consequently, PostScript images can
be understood by a large class of printers and other devices but tend to be very large.
Encapsulated PostScript (EPS) are similar but include additional information that
is useful for previewing images. GIF images are color index images and thus store a
color table and an array of indices for the image.
TIFF images can have two forms. In one form, all the image data are coded di-
rectly. A header describes how the data are arranged. In the second form, the data are
compressed. Compression is possible because most images contain much redundant
data. For example, large areas of most images show very little variation in color or

```cpp
intensity. This redundancy can be removed by algorithms that result in a compressed
```

version of the original image that requires less storage. Compressed TIFF images
are formed by the Lempel-Ziv algorithm that provides optimal lossless compression
allowing the original image to be compressed and recovered exactly. JPEG images
are compressed by an algorithm that allows small errors in the compression and re-

```cpp
construction of the image. Consequently, JPEG images have very high compression
```

ratios, that is, the ratio of the number of bits in the original ﬁle to the number of
bits in the compressed data ﬁle, with little or no visible distortion. Figure 7.3 shows
                                                                                             7.2 Digital Images   361
(a)                                       (b)
(c)
FIGURE 7.3 (a) Original TIFF luminance image. (b) JPEG image com-
pressed by a factor of 18. (c) JPEG image compressed by a factor of 37.
three versions of a single 1200 × 1200 luminance image: uncompressed, as a TIFF
image (Figure 7.3(a)); and as two JPEG images, compressed with different ratios (Fig-
ure 7.3(b) and Figure 7.3(c)). The corresponding ﬁle sizes are 1,440,198; 80,109; and
38,962 bytes, respectively. Thus, the TIFF image has 1 byte for each pixel plus 198
bytes of header and trailer information. For the JPEG images, the compression ra-
tios are approximately 18 and 37. Even with the higher compression ratio, there is
little visible distortion in the image. If we store the original image as a PostScript im-
age, the ﬁle will be approximately twice as large as the TIFF image because each byte
will be converted into two 7-bit ASCII characters, each pair requiring 2 bytes of stor-
age. If we store the image as a compressed TIFF ﬁle, we need only about one-half of
the storage. Using a zip ﬁle—a popular format used for compressing arbitrary ﬁles—
would give about the same result. This amount of compression is image-dependent.
Although this compression method is lossless, the compression ratio is far worse than
362   Chapter 7   Discrete Techniques
is obtainable with lossy JPEG images, which are visibly almost indistinguishable from
the original. This closeness accounts for the popularity of the JPEG format for send-
ing images over the Internet. Most digital cameras produce images in JPEG and RAW
formats. The RAW format gives the unprocessed RGB data plus a large amount of
header information, including the date, the resolution, and the distribution of the
colors.
The large number of image formats poses problems for a graphics API. Although
some image formats are simple, others are quite complex. The OpenGL API avoids
the problem by supporting only blocks of pixels, as compared to images formatted
for ﬁles. Most of these OpenGL formats correspond to internal formats that differ
in the number of bits for each color component and the order of the components
in memory. There is limited support for compressed texture images but not for the
standard formats such as JPEG. Hence, although OpenGL can work with images
that are arrays of standard data types in memory, it is the application programmer’s
responsibility to read any formatted images into processor memory and write them
out as formatted ﬁles. We will not deal with these issues here, as any discussion
would require us to discuss the details of particular image formats. The necessary
information can be found in the Suggested Readings at the end of the chapter.
We can also obtain digital images directly from our graphics system by forming
images of three-dimensional scenes using the geometric pipeline and then reading
these images back. We will see how to do the required operations later in the chapter.

### 7.3    WRITING INTO BUFFERS

In a modern graphics system, a user program can both write into and read from the
buffers. There are two factors that make these operations different from the usual
reading and writing into computer memory. First, we only occasionally want to read
or write a single pixel or bit. Rather, we tend to read and write rectangular blocks
of pixels (or bits), known as bit blocks. For example, we rasterize an entire scan
line at a time when we ﬁll a polygon; we write a small block of pixels when we
display a raster character; we change the values of all pixels in a buffer when we
do a clear operation. Hence, it is important to have both the hardware and software
support a set of operations that work on rectangular blocks of pixels, known as bit-
block transfer (bitblt) operations, as efﬁciently as possible. These operations are also
known as raster operations (raster-ops).
Suppose that we want to take an n × m block of pixels from one of our buffers,
the source buffer, and to copy it into either the same buffer or another buffer, the
destination buffer. This transfer is shown in Figure 7.4. A typical form for a bitblt
write_block(source, n, m, x, y, destination, u, v);
where source and destination are the buffers. The operation writes an n × m
source block whose lower-left corner is at (x,y) to the destination buffer starting at
                                                                                    7.3 Writing into Buffers   363
(u,v)
(x,y) m

*FIGURE 7.4 Writing of a block.*

a location (u,v). Although there are numerous details that we must consider, such
as what happens if the source block goes over the boundary of the destination block,
the essence of bitblt is that a single function call alters the entire destination block.
Note that, from the hardware perspective, the type of processing involved has none of
the characteristics of the processing of geometric objects. Consequently, the hardware
that optimizes bitblt operations has a completely different architecture from the ge-
ometric pipeline. Thus, the OpenGL architecture contains both a geometry pipeline
and a pixel pipeline, each of which usually is implemented separately.

#### 7.3.1 Writing Modes

A second difference between normal writing into memory and bitblt operations is the
variety of ways we can write into the buffers. OpenGL supports 16 different modes
(writing modes) for putting pixel data into a buffer. To understand the full range of
possibilities, let’s consider how we might write into a buffer.
The usual concept of a write-to memory is replacement. The execution of a
y=x;
results in the value at the location where y is stored being replaced with the value at
the location of x.
There are other possibilities. Suppose that we can work one bit at a time in our
buffers. Consider the writing model in Figure 7.5. The bit that we wish to place in
memory, perhaps in an altered form, is called the source bit, s; the place in memory
where we want to put it is called the destination bit, d. If, as in Chapter 3, we
are allowed to read before writing, as depicted in Figure 7.5, then writing can be
described by a replacement function f such that
d ← f (d, s).
For a 1-bit source and destination, there are only 16 possible ways to deﬁne the
function f —namely, the 16 logical operations between two bits. These operations
are shown in Figure 7.6, where each of the 16 columns on the right corresponds
to one possible f . We can use the binary number represented by each column to
364   Chapter 7   Discrete Techniques
write_pixel
read_pixel

*FIGURE 7.5 Writing model.*

s      d     0   1       2   3   4   5   6   7   8    9 10 11 12 13 14 15
0      0     0   0       0   0   0   0   0   0   1    1    1   1   1   1   1   1
0      1     0   0       0   0   1   1   1   1   0    0    0   0   1   1   1   1
1      0     0   0       1   1   0   0   1   1   0    0    1   1   0   0   1   1
1      1     0   1       0   1   0   1   0   1   0    1    0   1   0   1   0   1

*FIGURE 7.6 Writing modes.*

denote a writing mode; equivalently, we can denote writing modes by the logical
operation deﬁned by the column. Suppose that we think of the logical value “1”
as corresponding to a background color (say, white) and “0” as corresponding to
a foreground color (say, black). We can examine the effects of various choices of f .
Writing modes 0 and 15 are clear operations that change the value of the destination
to either the foreground or the background color. The new value of the destination
bit is independent of both the source and the destination values. Modes 3 and 7 are
the normal writing modes. Mode 3 is the function
d ← s.
It simply replaces the value of the destination bit with the source. Mode 7 is the logical
OR operation:
d ← s + d.
Figure 7.7 shows that these two writing modes can have different effects on the
contents of the frame buffer. In this example, we write a dashed line into a frame
buffer that already had a black (foreground colored) rectangle rendered into it. Both
modes write the foreground color over the background color, but they differ if we
try to write the background color over the foreground color. Which mode should be
used depends on what effect the application programmer wishes to create.
                                                                                     7.3 Writing into Buffers   365
Mode 3              Mode 7

*FIGURE 7.7 Writing in modes 3 and 7.*


#### 7.3.2 Writing with XOR

Mode 6 is the exclusive-or operation XOR, denoted by ⊕; it is the most interesting of
the writing modes. Unlike modes 3 and 7, mode 6 cannot be implemented without
a read of the destination bit. The power of the XOR write mode comes from the
property that, if s and d are binary variables, then
d = (d ⊕ s) ⊕ s.
Thus, if we apply XOR twice to a bit, we return that bit to its original state.
The most important applications of this mode involve interaction. Consider
what happens when we use menus in an interactive application, such as in a painting
program. In response to a mouse click, a menu appears, covering a portion of the
screen. After the user indicates an action from the menu, the menu disappears, and
the area of the screen that it covered is returned to that area’s original state. What has
transpired involves the use of off-screen memory, known as backing store. Suppose
that the menu has been stored off-screen as an array of bits, M, and that the area of
the screen where the menu appears is an array of bits, S. Consider the sequence of
S ← S ⊕ M,
M ← S ⊕ M,
S ← S ⊕ M,
where we assume that the XOR operation is applied to corresponding bits in S and
M. If we substitute the result of the ﬁrst equation in the second, and the result of the
second in the third, we ﬁnd that, at the end of the three operations, the menu appears
on the screen. The original contents of the screen, where the menu is now located,
are now off-screen, where the menu was originally. We have swapped the menu
with an area of the screen using three bitblt operations. This method of swapping is
considerably different from the normal mode of swapping, which uses replacement-
mode writing, but requires temporary storage to effect the swap.
366   Chapter 7   Discrete Techniques
There are numerous variants of this technique. One is to move a cursor around
the screen without affecting the area under it. Another is ﬁlling polygons with a
solid color as part of scan conversion. Note that, because many APIs trace their
backgrounds to the days before raster displays became the norm, the XOR-write
mode was not always available. In OpenGL, the standard mode of writing into the
frame buffer is mode 3. Fragments are copied into the frame buffer. We can change
glLogicOp(mode);
glEnable(GL_COLOR_LOGIC_OP);
where mode can be any of the 16 modes. The usual ones are the GL_COPY (the default)
and GL_XOR.

### 7.4    MAPPING METHODS

One of the most powerful uses of discrete data is for surface rendering. The process
of modeling an object by a set of geometric primitives and then rendering these
primitives has its limitations. Consider, for example, the task of creating a virtual
orange by computer. Our ﬁrst attempt might be to start with a sphere. From our
discussion in Chapter 5, we know that we can build an approximation to a sphere
out of triangles, and can render these triangles using material properties that match
those of a real orange. Unfortunately, such a rendering would be far too regular to
look much like an orange. We could instead follow the path that we shall explore in

## Chapter 10: We could try to model the orange with some sort of curved surface, and

then render the surface. This procedure would give us more control over the shape
of our virtual orange, but the image that we would produce still would not look
right. Although it might have the correct overall properties, such as shape and color,
it would lack the ﬁne surface detail of the real orange. If we attempt to add this detail
by adding more polygons to our model, even with hardware capable of rendering tens
of millions of polygons per second, we can still overwhelm the pipeline.
An alternative is not to attempt to build increasingly more complex models, but
rather to build a simple model and to add detail as part of the rendering process. As
we saw in Chapter 6, as the implementation renders a surface—be it a polygon or a
curved surface—it generates sets of fragments, each of which corresponds to a pixel
in the frame buffer. Fragments carry color, depth, and other information that can
be used to determine how they contribute to the pixels to which they correspond. As
part of the rasterization process, we must assign a shade or color to each fragment. We
started in Chapter 5 by using the modiﬁed-Phong model to determine vertex colors
that could be interpolated across surfaces. However, these colors can be modiﬁed dur-
ing fragment processing after rasterization. The mapping algorithms can be thought
of as either modifying the shading algorithm based on a two-dimensional array, the
map, or as modifying the shading by using the map to alter surface parameters, such
as material properties and normals. There are three major techniques:
                                                                                      7.4 Mapping Methods   367

*FIGURE 7.8 Texture mapping a pattern to a surface.*

Texture mapping uses an image (or texture) to inﬂuence the color of a fragment.
Textures can be speciﬁed using a ﬁxed pattern, such as the regular patterns often used
to ﬁll polygons; by a procedural texture-generation method; or through a digitized
image. In all cases, we can characterize the resulting image as the mapping of a texture
to a surface, as shown in Figure 7.8, as part of the rendering of the surface.
Whereas texture maps give detail by painting patterns onto smooth surfaces,
bump maps distort the normal vectors during the shading process to make the sur-
face appear to have small variations in shape, such as the bumps on a real orange.
Reﬂection maps, or environment maps, allow us to create images that have the ap-
pearance of reﬂected materials without our having to trace reﬂected rays. In this
technique, an image of the environment is painted onto the surface as that surface
is being rendered.
The three methods have much in common. All three alter the shading of indi-
vidual fragments as part of fragment processing. All rely on the map being stored as
a one-, two-, or three-dimensional digital image. All keep the geometric complexity
low while creating the illusion of complex geometry. However, all are also subject to
aliasing errors.
There are various examples of two-dimensional mappings in the color plates.
Color Plate 7 was created using an OpenGL environment map and shows how a single
texture map can create the illusion of a highly reﬂective surface while avoiding global
calculations. Color Plate 15 uses a texture map for the surface of the table; Color
Plate 10 uses texture mapping to create a brick pattern. In virtual reality, visualization
simulations, and interactive games, real-time performance is required. Hardware
support for texture mapping in modern systems allows the detail to be added, without
signiﬁcantly degrading the rendering time.
However, in terms of the standard pipeline, there are signiﬁcant differences
among the three techniques. Standard texture mapping is supported by the basic
OpenGL pipeline and makes use of both the geometric and pixel pipelines. Environ-
ment maps are a special case of standard texture mapping but can be altered to create
368   Chapter 7   Discrete Techniques
a variety of new effects if we can alter fragment processing. Bump mapping requires
us to process each fragment independently, something we can do with a fragment
shader.

### 7.5     TEXTURE MAPPING

Textures are patterns. They can range from regular patterns, such as stripes and
checkerboards, to the complex patterns that characterize natural materials. In the real
world, we can distinguish among objects of similar size and shape by their textures.
Thus, if we want to create detailed virtual objects, we can extend our present capabil-
ities by mapping a texture to the objects that we create.
Textures can be one, two, three, or four dimensional. For example, a one-
dimensional texture might be used to create a pattern for coloring a curve. A three-
dimensional texture might describe a solid block of material from which we could
sculpt an object. Because the use of surfaces is so important in computer graphics,
mapping two-dimensional textures to surfaces is by far the most common use of tex-
ture mapping and will be the only form of texture mapping that we shall consider
in detail. However, the processes by which we map these entities is much the same
regardless of the dimensionality of the texture, and we lose little by concentrating on
two-dimensional texture mapping.

#### 7.5.1 Two-Dimensional Texture Mapping

Although there are multiple approaches to texture mapping, all require a sequence of
steps that involve mappings among three or four different coordinate systems. At var-
ious stages in the process, we shall be working with screen coordinates, where the ﬁnal
image is produced; object coordinates, where we describe the objects upon which the
textures will be mapped; texture coordinates, which we use to locate positions in the
texture; and parametric coordinates, which we use to help us deﬁne curved surfaces.
Methods differ according to the types of surfaces we are using and the type of render-
ing architecture we have. Our approach will be to start with a fairly general discussion
of texture, introducing the various mappings, and then to show how texture mapping
is handled by a real-time pipeline architecture, such as that employed by OpenGL.
In most applications, textures start out as two-dimensional images of the sorts
we introduced in Section 7.2. Thus, they might be formed by application programs
or scanned in from a photograph, but, regardless of their origin, they are eventu-
ally brought into processor memory as arrays. We call the elements of these arrays
texels, or texture elements, rather than pixels to emphasize how they will be used.
However, at this point, we prefer to think of this array as a continuous rectangu-
lar two-dimensional texture pattern T(s, t). The independent variables s and t are
known as texture coordinates.3 With no loss of generality, we can scale our texture
coordinates to vary over the interval [0,1].
3. In four dimensions, the coordinates are in (s, t , r , q) space.
                                                                                     7.5 Texture Mapping   369
A texture map associates a texel with each point on a geometric object that
is itself mapped to screen coordinates for display. If the object is represented in
homogeneous or (x, y, z, w) coordinates, then there are functions such that
x = x(s, t),
y = y(s, t),
z = z(s, t),
w = w(s, t).
One of the difﬁculties we must confront is that although these functions exist con-
ceptually, ﬁnding them may not be possible in practice. In addition, we are worried
about the inverse problem: Having been given a point (x, y, z) or (x, y, z, w) on an
object, how do we ﬁnd the corresponding texture coordinates, or equivalently, how
do we ﬁnd the “inverse” functions
s = s(x, y, z , w),
t = t(x, y, z, w)
to use to ﬁnd the texel T(s, t)?
If we deﬁne the geometric object using parametric (u, v) surfaces, such as we did
for the sphere in Section 5.6, there is an additional mapping function that gives object
coordinate values, (x, y, z) or (x, y, z, w) in terms of u and v. Although this mapping
is known for simple surfaces, such as spheres and triangles, and for the surfaces
that we shall discuss in Chapter 10, we also need the mapping from parametric
coordinates (u, v) to texture coordinates and sometimes the inverse mapping from
texture coordinates to parametric coordinates.
We also have to consider the projection process that take us from object coor-
dinates to screen coordinates, going through eye coordinates, clip coordinates, and
window coordinates along the way. We can abstract this process through a function
that takes a texture coordinate pair (s, t) and tells us where in the color buffer the
corresponding value of T(s, t) will make its contribution to the ﬁnal image. Thus,
xs = xs (s, t),
ys = ys (s, t)

```cpp
into coordinates, where (xs , ys ) is a location in the color buffer.
```

Depending on the algorithm and the rendering architecture, we might also want
the function that takes us from a pixel in the color buffer to the texel that makes a
contribution to the color of that pixel.
One way to think about texture mapping is in terms of two concurrent map-
pings: the ﬁrst from texture coordinates to object coordinates, and the second from
parametric coordinates to object coordinates, as shown in Figure 7.9. A third map-
ping takes us from object coordinates to screen coordinates.
370   Chapter 7   Discrete Techniques
s                                    x                           ys

*FIGURE 7.9 Texture maps for a parametric surface.*

Conceptually, the texture-mapping process is simple. A small area of the texture
pattern maps to the area of the geometric surface, corresponding to a pixel in the
ﬁnal image. If we assume that the values of T are RGB color values, we can use these
values either to modify the color of the surface that might have been determined by
a lighting model or to assign a color to the surface based on only the texture value.
This color assignment is carried out as part of the assignment of fragment colors.
On closer examination, we face a number of difﬁculties. First, we must determine
the map from texture coordinates to object coordinates. A two-dimensional texture
usually is deﬁned over a rectangular region in texture space. The mapping from this
rectangle to an arbitrary region in three-dimensional space may be a complex func-
tion or may have undesirable properties. For example, if we wish to map a rectangle to
a sphere, we cannot do so without distortion of shapes and distances. Second, owing
to the nature of the rendering process, which works on a pixel-by-pixel basis, we are
more interested in the inverse map from screen coordinates to texture coordinates. It
is when we are determining the shade of a pixel that we must determine what point in
the texture image to use—a calculation that requires us to go from screen coordinates
to texture coordinates. Third, because each pixel corresponds to a small rectangle on
the display, we are interested in mapping not points to points, but rather areas to ar-
eas. Here again is a potential aliasing problem that we must treat carefully if we are to
avoid artifacts, such as wavy sinusoidal or moiré patterns.
Figure 7.10 shows several of the difﬁculties. Suppose that we are computing a
color for the square pixel centered at screen coordinates (xs , ys ). The center (xs , ys )
corresponds to a point (x, y, z) in object space, but, if the object is curved, the
projection of the corners of the pixel backward into object space yields a curved
                                                                                      7.5 Texture Mapping   371
s                              x                         ys

*FIGURE 7.10 Preimages of a pixel.*


*FIGURE 7.11 Aliasing in texture generation.*

preimage of the pixel. In terms of the texture image T(s, t), projecting the pixel back
yields a preimage in texture space that is the area of the texture that ideally should
contribute to the shading of the pixel.
Let’s put aside for a moment the problem of how we ﬁnd the inverse map, and
let us look at the determination of colors. One possibility is to use the location that
we get by back projection of the pixel center to ﬁnd a texture value. Although this
technique is simple, it is subject to serious aliasing problems, which are especially
visible if the texture is periodic. Figure 7.11 illustrates the aliasing problem. Here,
we have a repeated texture and a ﬂat surface. The back projection of the center of
each pixel happens to fall in between the dark lines, and the texture value is always
the lighter color. More generally, not taking into account the ﬁnite size of a pixel
can lead to moiré patterns in the image. A better strategy—but one more difﬁcult to
implement—is to assign a texture value based on averaging of the texture map over
the preimage. Note that this method is imperfect, too. For the example in Figure 7.11,
we would assign an average shade, but we would still not get the striped pattern of the
texture. Ultimately, we still have aliasing defects due to the limited resolution of both
the frame buffer and the texture map. These problems are most visible when there
are regular high-frequency components in the texture.
Now we can turn to the mapping problem. In computer graphics, most curved
surfaces are represented parametrically. A point p on the surface is a function of two
372   Chapter 7   Discrete Techniques
(smax, tmax )
(umax, vmax )
(smin, tmin)         (umin, vmin)

*FIGURE 7.12 Linear texture mapping.*

parameters u and v. For each pair of values, we generate the point
⎡         ⎤
x(u, v)
p(u, v) = ⎣ y(u, v) ⎦ .
z(u, v)
In Chapter 10, we study in detail the derivation of such surfaces. Given a parametric
surface, we can often map a point in the texture map T(s, t) to a point on the surface
p(u, v) by a linear map of the form
u = as + bt + c,
v = ds + et + f .
As long as ae = bd, this mapping is invertible. Linear mapping makes it easy to
map a texture to a group of parametric surface patches. For example, if, as shown
in Figure 7.12, the patch determined by the corners (smin , tmin ) and (smax , tmax )
corresponds to the surface patch with corners (umin , vmin ) and (umax , vmax ), then the
s − smin
u = umin +               (u   − umin ),
smax − smin max
t − tmin
v = vmin +              (v   − vmin ).
tmax − tmin max
This mapping is easy to apply, but it does not take into account the curvature of the
surface. Equal-sized texture patches must be stretched to ﬁt over the surface patch.
Another approach to the mapping problem is to use a two-part mapping. The
ﬁrst step maps the texture to a simple three-dimensional intermediate surface, such
as a sphere, cylinder, or cube. In the second step, the intermediate surface containing
the mapped texture is mapped to the surface being rendered. This two-step mapping
process can be applied to surfaces deﬁned in either geometric or parametric coordi-
nates. The following example is essentially the same in either system.
Suppose that our texture coordinates vary over the unit square, and that we use
the surface of a cylinder of height h and radius r as our intermediate object, as shown
                                                                                     7.5 Texture Mapping   373

*FIGURE 7.13 Texture mapping with a cylinder.*

in Figure 7.13. Points on the cylinder are given by the parametric equations
x = r cos(2πu),
y = r sin(2π u),
z = v/h,
as u and v vary over (0,1). Hence, we can use the mapping
s = u,
t = v.
By using only the curved part of the cylinder, and not the top and bottom, we were
able to map the texture without distorting its shape. However, if we map to a closed
object, such as a sphere, we must introduce shape distortion. This problem is similar
to the problem of creating a two-dimensional image of the earth for a map. If you
look at the various maps of the earth in an atlas, all distort shapes and distances.
Both texture-mapping and map-design techniques must choose among a variety of
representations, based on where we wish to place the distortion. For example, the
familiar Mercator projection puts the most distortion at the poles. If we use a sphere
of radius r as the intermediate surface, a possible mapping is
x = r cos(2πu),
y = r sin(2πu) cos(2π v),
z = r sin(2πu) sin(2πv).
We can also use a rectangular box, as shown in Figure 7.14. Here, we map the texture
to a box that can be unraveled, like a cardboard packing box. This mapping often is
used with environment maps (Section 7.8).
The second step is to map the texture values on the intermediate object to the
desired surface. Figure 7.15 shows three possible strategies. In Figure 7.15(a), we take
the texture value at a point on the intermediate object, go from this point in the
direction of the normal until we intersect the object, and then place the texture value
at the point of intersection. We could also reverse this method, starting at a point on
the surface of the object and going in the direction of the normal at this point until
374   Chapter 7   Discrete Techniques

*FIGURE 7.14 Texture mapping with a box.*

(a)                (b)                       (c)

*FIGURE 7.15 Second mapping. (a) Using the normal from the interme-*

diate surface. (b) Using the normal from the object surface. (c) Using the
center of the object.
we intersect the intermediate object, where we obtain the texture value, as shown in
Figure 7.15(b). A third option, if we know the center of the object, is to draw a line
from the center through a point on the object, and to calculate the intersection of
this line with the intermediate surface, as shown in Figure 7.15(c). The texture at the
point of intersection with the intermediate object is assigned to the corresponding
point on the desired object.

### 7.6     TEXTURE MAPPING IN OPENGL

OpenGL supports a variety of texture-mapping options. The ﬁrst version of OpenGL
contained the functionality to map one- and two-dimensional textures to one-
through four-dimensional graphical objects. Mapping of three-dimensional textures
now is part of OpenGL and is supported by most hardware. We will focus on mapping
two-dimensional textures to surfaces.
OpenGL’s texture maps rely on its pipeline architecture. We have seen that there
are actually two parallel pipelines: the geometric pipeline and the pixel pipeline. For
texture mapping, the pixel pipeline merges with fragment processing after raster-
ization, as shown in Figure 7.16. This architecture determines the type of texture
mapping that is supported. In particular, texture mapping is done as part of frag-
ment processing. Each fragment that is generated is then tested for visibility with the
z-buffer. We can think of texture mapping as a part of the shading process, but a part
                                                                        7.6 Texture Mapping in OpenGL   375
Geometry                                            Fragment             Frame
Vertices                                   Rasterization
processing                                         processing            buffer

*FIGURE 7.16 Pixel and geometry pipelines.*

that is done on a fragment-by-fragment basis. Texture coordinates are handled much
like normals and colors. They are associated with vertices through the OpenGL state,
and the required texture values can then be obtained by interpolating the texture co-
ordinates at the vertices across polygons.

#### 7.6.1 Two-Dimensional Texture Mapping

Texture mapping requires interaction among the application program, the vertex
shader, and the fragment shader. There are three basic steps. First, we must form a
texture image and place it in texture memory on the GPU. Second, we must assign
texture coordinates to each fragment. Finally, we must apply the texture to each
fragment. Each of these steps can be accomplished in multiple ways, and there are
many parameters that we can use to control the process. As texture mapping has
become more important and GPUs have evolved to support more texture-mapping
options, APIs have added more and more texture-mapping functions.

#### 7.6.2 Texture Objects

In early versions of OpenGL, there was only a single texture, the current texture, that
existed at any time. Each time that a different texture was needed—for example, if we
wanted to apply different textures to different surfaces in the same scene—we had to
set up a new texture map. This process was very inefﬁcient. Each time another texture
image was needed, it had to be loaded into texture memory, replacing the texels that
were already there.
In a manner analogous to having multiple program objects, texture objects allow
the application program to deﬁne objects that consist of the texture array and the
various texture parameters that control its application to surfaces. As long as there is
sufﬁcient memory to retain them, these objects reside in texture memory.
We create a two-dimensional texture object by ﬁrst getting some unused iden-
tiﬁers by calling glGenTextures. For a single texture, we could use the following
code:
GLuint mytex[1];
glGenTextures(1, mytex);
376   Chapter 7   Discrete Techniques
We start forming a new texture object with the function glBindTexture, as in the
following code:
glBindTexture(GL_TEXTURE_2D, mytex[0]);
Subsequent texture functions specify the texture image and its parameters, which
become part of the texture object. Another call to glBindTexture with a new name
starts a new texture object. A later execution of glBindTexture with an existing
name makes that texture object the current texture object. We can delete unused
texture objects by glDeleteTextures.

#### 7.6.3 The Texture Array

Two-dimensional texture mapping starts with an array of texels, which is a two-
dimensional pixel rectangle. Suppose that we have a 512 × 512 image my_texels
that was generated by our program, or perhaps was read in from a ﬁle into an array:
GLubyte my_texels[512][512][3];
We specify that this array is to be used as a two-dimensional texture after a call to
glTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, 512, 512, 0,
GL_RGB, GL_UNSIGNED_BYTE, my_texels);
More generally, two-dimensional textures are speciﬁed through the functions
glTexImage2D(GLenum target, GLint level, GLint iformat,
GLsizei width, GLsizei height, GLint border, GLenum format,
GLenum type, GLvoid *tarray)
The target parameter lets us choose a single image, as in our example, set up a cube
map (Section 7.9), or test if there is sufﬁcient texture memory for a texture image.
The level parameter is used for mipmapping (Section 7.6.5), where 0 denotes the
highest level (or resolution) or that we are not using mipmapping. The third param-
eter speciﬁes how we would like the texture stored in texture memory. The fourth
and ﬁfth parameters (width and height) specify the size of the image in memory. The
border parameter is no longer used and should be set to 0. The format and type
parameters describe how the pixels in the image in processor memory are stored, so
that OpenGL can read those pixels and store them in texture memory.

#### 7.6.4 Texture Coordinates and Samplers

The key element in applying a texture in the fragment shader is the mapping between
the location of a fragment and the corresponding location within the texture image
where we will get the texture color for that fragment. Because each fragment has a
                                                                          7.6 Texture Mapping in OpenGL   377
(511, 511)
1         (0, 0)

*FIGURE 7.17 Mapping to texture coordinates.*

location in the frame buffer that is one of its attributes, we need not refer to this
position explicitly in the fragment shader. The potential difﬁculty is identifying the
desired location in the texture image. In many applications, we could compute this
location from a mathematical model of the objects. In others, we might use some
sort of approximation. OpenGL does not have any preferred method and simply
requires that we provide the location to the fragment shader or compute it within
the shader.
Rather than having to use integer texel locations that depend on the dimensions
of the texture image, we use two ﬂoating-point texture coordinates, s and t, both
of which range over the interval (0.0, 1.0) as we traverse the texture image. For
our example of a 512 × 512 two-dimensional texture image, the value (0.0, 0.0)
corresponds to the texel my_texels[0][0], and (1.0, 1.0) corresponds to the texel
my_texels[511][511], as shown in Figure 7.17. Any values of s and t in the unit

```cpp
interval correspond to a unique texel.
```

It is up to the application and the shaders to determine the appropriate texture
coordinates for a fragment. The most common method is to treat texture coordinates
as a vertex attribute. Thus, we could provide texture coordinates just as we provide
vertex colors in the application. We then would pass these coordinates to the vertex
shader and let the rasterizer interpolate the vertex texture coordinates to fragment
texture coordinates.
Let’s consider a simple example of using our checkerboard texture image for
each side of the cube example. The example is particularly simple because we have
an obvious mapping between each face of the cube and the texture coordinates for
each vertex; namely, we assign texture coordinates (0.0, 0.0), (0.0, 1.0), (1.0, 1.0), and
(1.0, 0.0) to the four corners of each face.
Recall that we form 36 triangles for the six faces. We add an array to hold the
texture coordinates:

```cpp
#define N 36
```

GLfloat tex_coord[N][2];
Here is the code of the quad function:
378   Chapter 7   Discrete Techniques
typedef vec2 point2;

```cpp
void quad(int a, int b, int c, int d)
{
static int i =0; /* vertex and color index */
```

quad_color[i] = colors[a];
points[i] = vertices[a];
tex_coord[i] = point2(0.0, 0.0);
i++;
quad_color[i] = colors[a];
points[i] = vertices[b];
tex_coord[i] = point2(0.0, 1.0);
i++;
quad_color[i] = colors[a];
points[i] = vertices[c];
tex_coord[i] = point2(1.0, 1.0);
i++;
quad_color[i] = colors[a];
points[i] = vertices[a];
tex_coord[i] = point2(0.0, 0.0);
i++;
quad_color[i] = colors[a];
points[i] = vertices[c];
tex_coord[i] = point2(1.0, 1.0);
i++;
quad_color[i] = colors[a];
points[i] = vertices[d];
tex_coord[i] = point2(1.0, 0.0);
i++;

```cpp
}
```

We also need to do our initialization so we can pass the texture coordinates as a vertex
attribute with the identiﬁer texcoord in the vertex shader:
GLuint loc3;
loc3 = glGetAttribLocation(program, "texcoord");
glEnableVertexAttribArray(loc3);
glVertexAttribPointer(loc3, 2, GL_FLOAT, GL_FALSE, 0, tex_coord);
Next, we need to initialize the buffer object to store all of the data. Since we have three
separate arrays of data, we will need to load them into an appropriately sized buffer
in three operations using glBufferSubData:
GLintptr offset;
GLsizeiptr size =
sizeof(points) + sizeof(quad_color) + sizeof(tex_coord);
                                                                      7.6 Texture Mapping in OpenGL   379
glGenBuffers(1, buffer);
glBindBuffer(GL_ARRAY_BUFFER, buffers[0]);
glBufferData(GL_ARRAY_BUFFER, size, NULL, GL_STATIC_DRAW);
offset = 0;
glBufferData(GL_ARRAY_BUFFER, offset, sizeof(points), points);
offset += sizeof(points);
glBufferData(GL_ARRAY_BUFFER, offset, sizeof(quad_color), quad_color);
offset += sizeof(quad_color);
glBufferData(GL_ARRAY_BUFFER, offset, sizeof(tex_coord), tex_coord);
Turning to the vertex shader, we add the texture coordinate attribute and output
the texture coordinates. Here is the vertex shader for the rotating cube with texture
coordinates:
in vec2 texcoord;
in vec4 vPosition;
in vec4 vColor;
out vec4 color;
out vec2 st;
uniform vec3 theta;

```cpp
void main()
{
```

mat4 rx, ry, rz;
vec3 c = cos(theta);
vec3 s = sin(theta);
rz = mat4(c.z, -s.z, 0.0, 0.0,
s.z, c.z, 0.0, 0.0,
0.0, 0.0, 1.0, 0.0,
0.0, 0.0, 0.0, 1.0);
ry = mat4(c.y, 0.0, s.y, 0.0,
0.0, 1.0, 0.0, 0.0,
-s.y, 0.0, c.y, 0.0,
0.0, 0.0, 0.0, 1.0);
rx = mat4(1.0, 0.0, 0.0, 0.0,
0.0, c.x, -s.x, 0.0,
0.0, s.x, c.x, 0.0,
0.0, 0.0, 0.0, 1.0);
gl_Position = rz*ry*rx*vPosition;
color = vColor;
st = texcoord;

```cpp
}
```

The output texture coordinates st are interpolated by the rasterizer and can be inputs
to the fragment shader.
380   Chapter 7   Discrete Techniques
Note that the vertex shader is only concerned with the texture coordinates and
has nothing to do with the texture object we created earlier. We should not be sur-
prised because the texture itself is not needed until we are ready to assign a color to a
fragment, that is in the fragment shader. Note also that many of the complexities of
how we can apply the texture, many of which we have yet to discuss, are inside the
texture object and thus will allow us to use very simple fragment shaders.
The key to putting everything together is a new type of variable called a sampler
which most often appears only in a fragment shader. A sampler variable provides
access to a texture object, including all its parameters. There are sampler variables
for the types of textures supported by OpenGL, including one-dimensional (sam-
pler1D), two-dimensional (sampler2D), and three-dimensional (sampler3D) tex-
tures and special types such as cube maps (samplerCube).
We link the texture object mytex we created in the application with the shader
GLuint tex_loc;
tex_loc = glGetUniformLocation(program, "texMap");
glUniform1i(tex_loc, 0);
where texMap is the name of the sampler in the fragment shader and the second
parameter in glUniform1i refers to the default texture unit. We will discuss mutliple
texture units in Section 7.6.7.
The fragment shader is almost trivial. The interpolated vertex colors and the
texture coordinates are input variables. If we want the texture values to multiply the
colors as if we were using the checkerboard texture to simulate glass that alternates
between clear and opaque, we could multiply the colors from the application by the
values in the texture image as in the following fragment shader:
in vec2 st;
in vec4 color;
uniform sampler2D texMap;

```cpp
void main()
{
```

gl_FragColor = color * texture2D(texMap, st);

```cpp
}
```

In the example shown in Figure 7.18(a), we use the whole texture on a rectangle.
If we used only part of the range of s and t—for example, (0.0, 0.5)—we would
use only part of my_texels for the texture map, and would get an image like that
in Figure 7.18(b). OpenGL interpolates s and t across the quadrilateral, then maps
these values back to the appropriate texel in my_texels. The quadrilateral example
is simple because there is an obvious mapping of texture coordinates to vertices. For
general polygons, the application programmer must decide how to assign the texture
                                                                        7.6 Texture Mapping in OpenGL   381
(a)                                (b)

*FIGURE 7.18 Mapping of a checkerboard texture to a quadrilateral.*

(a) Using the entire texel array. (b) Using part of the texel array.
(a)                                  (b)                                   (c)

*FIGURE 7.19 Mapping of texture to polygons. (a and b) Mapping of a checkerboard texture*

to a triangle. (c) Mapping of a checkerboard texture to a trapezoid.
coordinates. Figure 7.19 shows a few of the possibilities with the same texture map.
Figures 7.19(a) and (b) use the same triangle but different texture coordinates. Note
the artifacts of the interpolation and how quadrilaterals are treated as two triangles
as they are rendered in Figure 7.19(c).
The basics of OpenGL texture mapping are simple: Specify an array of colors
for the texture values, then assign texture coordinates. Unfortunately, there are a
few nasty details that we must discuss before we can use texture effectively. Solving
the resulting problems involves making trade-offs between quality of the images and
efﬁciency.
One problem is how to interpret a value of s or t outside of the range (0.0, 1.0).
Generally, we want the texture either to repeat if we specify values outside this range
or to clamp the values to 0.0 or 1.0—that is, we want to use the values at 0.0 and 1.0
for values below and above the interval (0.0, 1.0), respectively. For repeated textures,
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT);
382         Chapter 7    Discrete Techniques
For t, we use GL_TEXTURE_WRAP_T; for clamping, we use GL_CLAMP_TO_EDGE. By
executing these functions after the glBindTexture, the parameters become part of
the texture object.

#### 7.6.5 Texture Sampling

Aliasing of textures is a major problem. When we map texture coordinates to the array
of texels, we rarely get a point that corresponds to the center of a texel. One option
is to use the value of the texel that is closest to the texture coordinate output by the
rasterizer. This option is known as point sampling, but it is the one most subject
to visible aliasing errors. A better strategy, although one that requires more work,
is to use a weighted average of a group of texels in the neighborhood of the texel
determined by point sampling. This option is known as linear ﬁltering. Thus, in

*FIGURE 7.20 Texels used with    Figure 7.20 we see the location within a texel that is given by bilinear interpolation*

linear filtering.               from the texture coordinates at the vertices and the four texels that would be used
to obtain a smoother value. If we are using linear ﬁltering, there is a problem at the
edges of the texel array as we need additional texel values outside the array.
There is a further complication, however, in deciding how to use the texel values
to obtain a texture value. The size of the pixel that we are trying to color on the screen
may be smaller or larger than one texel, as shown in Figure 7.21.
In the ﬁrst case, the texel is larger than one pixel (magniﬁcation); in the second,
it is smaller (miniﬁcation). In both cases, the fastest strategy is to use the value of
the nearest point sampling. We can specify this option for both magniﬁcation and
miniﬁcation of textures as follows:
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);
Alternatively, we can use ﬁltering to obtain a smoother, less aliased image if we specify
GL_LINEAR instead of GL_NEAREST.
OpenGL has another way to deal with the miniﬁcation problem; it is called
mipmapping. For objects that project to an area of screen space that is small com-
pared with the size of the texel array, we do not need the resolution of the original
t                          xs                         t                             xs
Texel                     Pixel                          Texel                  Pixel
s                         ys                             s                          ys
(a)                                                      (b)

*FIGURE 7.21 Mapping texels to pixels. (a) Minification. (b) Magnification.*

                                                                          7.6 Texture Mapping in OpenGL   383
texel array. OpenGL allows us to create a series of texture arrays at reduced sizes; it
will then automatically use the appropriate size. For a 64 × 64 original array, we can
set up 32 × 32, 16 × 16, 8 × 8, 4 × 4, 2 × 2, and 1 × 1 arrays for the current texture
glGenerateMipmap(GL_TEXTURE_2D);
We can also set up the maps directly using the level parameter in glTexIm-
age2D. This parameter is the level in the mipmap hierarchy for the speciﬁed texture
array. Thus, level 0 refers to the original image, level 1 to the image at half resolution,
and so on. However, we can give a pointer to any image in different calls to glTex-
Image2D and thus can have entirely different images used at different levels of the
mipmap hierarchy. These mipmaps are invoked automatically if we specify
glTexParameteri(GL_TEXTURE_2D,
GL_TEXTURE_MIN_FILTER,GL_NEAREST_MIPMAP_NEAREST);
This option asks OpenGL to use point sampling with the best mipmap. We can
also do ﬁltering within the best mipmap (GL_NEAREST_MIPMAP_LINEAR), point
sampling using linear ﬁltering between mipmaps (GL_LINEAR_MIPMAP_NEAREST),
or both ﬁlters (GL_LINEAR_MIPMAP_LINEAR). Figure 7.22 shows the differences in
mapping a texture using the nearest texel, linear ﬁltering, and mipmapping, both
with using the nearest texel and with linear ﬁltering. The object is a quadrilateral that
appears almost as a triangle when shown in perspective. The texture is a series of
black and white lines that is applied so that the lines converge at the far side of the
quadrilateral. Note that this texture map, because of its regularity, shows dramatic
aliasing effects. The use of the nearest texel shows moiré patterns and jaggedness
in the lines. Using linear ﬁltering makes the lines smoother, but there are still clear
moiré patterns. The texels between the black and white stripes are gray because of
the ﬁltering. Mipmapping also replaces many of the blacks and whites of the two-
color patterns with grays that are the average of the two color values. For the parts
of the object that are farthest from the viewer, the texels are gray and blend with the
background. The mipmapped texture using the nearest texel in the proper mipmap
still shows the jaggedness that is smoothed out when we use linear ﬁltering with the
mipmap. Advances in the speed of graphics processors (GPUs) and the inclusion
of large amounts of texture memory in these GPUs often allows applications to use
ﬁltering and mipmapping without a performance penalty.
A ﬁnal issue in using textures in OpenGL is the interaction between texture and
shading. For RGB colors, there are multiple options. The texture can modulate the
shade that we would have assigned without texture mapping by multiplying the color
components of the texture by the color components from the shader. We could let
the color of the texture totally determine the color of a fragment—a technique called
decaling. These and other options are easily implemented in the fragment shader.
384   Chapter 7   Discrete Techniques
(a)                                        (b)
(c)                                        (d)

*FIGURE 7.22 Texture mapping to a quadrilateral. (a) Point sampling.*

(b) Linear filtering. (c) Mipmapping point sampling. (d) Mipmapping
linear filtering.

#### 7.6.6 Working with Texture Coordinates

Our examples so far have assumed implicitly that we know how to assign texture
coordinates. If we work with rectangular polygons of the same size, then it is fairly
easy to assign coordinates. We can also use the fact that texture coordinates can be
stored as one-, two-, three-, or four-dimensional arrays, just as are vertices. Thus,
texture coordinates can be transformed by matrices and manipulated in the same
manner as we transformed positions with the model-view and projection matrices.
We can use a texture matrix to scale and orient textures and to create effects in which
the texture moves with the object, the camera, or the lights.
However, if the set of polygons is an approximation to a curved object, then as-
signing texture coordinates is far more difﬁcult. Consider the polygonal approxima-
                                                                        7.6 Texture Mapping in OpenGL   385

*FIGURE 7.23 Polygonal model of Utah teapot.*


*FIGURE 7.24 Texture-mapped Utah teapot.*

tion of the Utah teapot4 in Figure 7.23. Although the model uses only quadrilaterals,
these quadrilaterals differ in size, with smaller quadrilaterals in areas where the ob-
ject has higher curvature and larger quadrilaterals in ﬂatter areas. Figure 7.24 shows
our checkerboard texture mapped to the teapot without making any adjustment for
the different sizes of the polygons. As we can see, by assigning the same set of texture
coordinates to each polygon, the texture mapping process adjusts to the individual
sizes of the polygons by scaling the texture map as needed. Hence, in areas such as
the handle, where many small polygons are needed to give a good approximation to
the curved surface, the black and white boxes are small compared to those on the
body of the teapot. In some applications, these patterns are acceptable. However, if
all surfaces of the teapot were made from the same material, we would expect to
see the same pattern on all its parts. In principle, we could use the texture matrix
to scale texture coordinates to achieve the desired display. However, in practice, it is
almost impossible to determine the necessary information from the model to form
the matrix.
One solution to this problem is to generate texture coordinates for each vertex
in terms of the distance from a plane in either eye coordinates or object coordinates.
Mathematically, each texture coordinate is given as a linear combination of the ho-
mogeneous coordinate values. Thus, for s and t,
4. We shall discuss the Utah teapot in detail in Chapter 10.
386   Chapter 7   Discrete Techniques
(a)
(b)

*FIGURE 7.25 Teapot using texture coordinate generation. (a) In object*

coordinates. (b) In eye coordinates.
s = as x + bs y + cs z + ds w,
t = at x + bt y + ct z + dt w.
Figure 7.25(a) shows the teapot with texture coordinate generation in object
space. Figure 7.25(b) uses the same equations but with the calculations in eye space.
By doing the calculation in object space, the texture is ﬁxed to the object and thus
will rotate with the object. Using eye space, the texture pattern changes as we apply
transformations to the object and give the illusion of the object moving through
a texture ﬁeld. One of the important applications of this technique is in terrain
generation and mapping. We can map surface features as textures directly onto a
three-dimensional mesh.

#### 7.6.7 Multitexturing

Thus far, we have looked at applying a single texture to an object. However, there are
many surface rendering effects that can best be implemented by more than a single
application of a texture. For example, suppose that we want to apply a shadow to an
object whose surface shades are themselves determined by a texture map. We could
use a texture map for the shadow, but if there were only a single texture application,
this method would not work.
                                                                                     7.7 Texture Generation   387
Fragment            Texture unit 0          Texture unit 1          Texture unit 2           Frame buffer

*FIGURE 7.26 Sequence of texture units.*

If, instead, we have multiple texture units as in Figure 7.26, then we can accom-
plish this task. Each unit acts as an independent texturing stage starting with the
results of the previous stage. This facility is supported in recent versions of OpenGL.
Suppose that we want to use two texture units. We can deﬁne two texture objects
as part of our initialization. We then activate each in turn and decide how its texture
should be applied. The usual code is of the form
glActiveTexture(GL_TEXTURE0); /* unit 0 */
glBindTexture(GL_TEXTURE_2D, object0);
/* how to apply texture 0 */
glActiveTexture(GL_TEXTURE1); /* unit 1 */
glBindTexture(GL_TEXTURE_2D, object1);
/* how to apply texture 1 */
Each texture unit can use different texture coordinates, and the application needs to
provide those texture coordinates for each unit.

### 7.7    TEXTURE GENERATION

One of the most powerful uses of texture mapping is to provide detail without
generating numerous geometric objects. High-end graphics systems can do two-
dimensional texture mapping in real time; for every frame, the texture is mapped
to objects as part of the rendering process at almost the same rate as non-texture-
mapped objects are processed. Graphics boards for personal computers now contain
a signiﬁcant amount of texture memory and allow game developers to use texture
mapping to create complex animated environments.
If, for example, we want to simulate grass in a scene, we can texture map an
image of grass that we might have obtained by, say, scanning a photograph, faster than
we can generate two- or three-dimensional objects that look like grass. In mapping
applications, rather than generating realistic surface detail for terrain, we can digitize
a real map and paint it on a three-dimensional surface model by texture mapping.
We can also look for procedural methods for determining texture patterns. Of
particular interest are patterns that we see in nature, such as the textures of sand,
grass, or minerals. These textures show both structure (regular patterns) and consid-
erable randomness. Most approaches to generating such textures algorithmically start
with a random-number generator and process its output, as shown in Figure 7.27. We
shall study procedural noise in detail in Chapter 9.
388   Chapter 7   Discrete Techniques

*FIGURE 7.27 Texture generation.*

The generation of a three-dimensional texture ﬁeld T(s, t , r) is a direct extension
of two-dimensional texture-generation techniques. There are some practical advan-
tages to using three-dimensional textures. Most important is that by associating each
(s, t , r) value directly with an (x, y, z) point, we can avoid the mapping problem en-
tirely. The user needs only to deﬁne a function T(s, t , r) with the desired properties.
Conceptually, this process is similar to sculpting the three-dimensional object from a
solid block whose volume is colored by the speciﬁed texture. This technique has been
used to generate objects that look as if they have been carved from solid rock. The
texture-generation process deﬁnes a function T(s, t , r) that displays the graininess
we associate with materials such as marble and granite.
There are other advantages to using three-dimensional textures. Suppose that
we have a two-dimensional texture that we obtained by photographing or modeling
some natural material, such as stone. Now suppose that we want to create a cube that
looks like it was formed from this stone. If we use two-dimensional texture mapping,
we have to map the same pattern to the six sides of the cube. To make the cube
look real, we must try to make the texture map appear continuous at the edges of
the cube, where two texture maps meet. When we work with natural patterns, it is
virtually impossible to ensure that we can do this matching. Note that the problem
is even more serious at the vertices of the cube, where three texture maps meet
(see Exercise 7.26). Often we can use ﬁltering and texture borders to give visually
acceptable results. However, if we use three-dimensional textures, this problem does
not arise.

### 7.8    ENVIRONMENT MAPS

Highly reﬂective surfaces are characterized by specular reﬂections that mirror the en-
vironment. Consider, for example, a shiny metal ball in the middle of a room. We can
see the contents of the room, in a distorted form, on the surface of the ball. Obviously,
this effect requires global information, as we cannot shade the ball correctly without
knowing about the rest of the scene. A physically based rendering method, such as a
ray tracer, can produce this kind of image, although in practice ray-tracing calcula-
tions usually are too time-consuming to be practical for real-time applications. We
can, however, use variants of texture mapping that can give approximate results that
are visually acceptable through environment maps or reﬂection maps.
The basic idea is simple. Consider the mirror in Figure 7.28, which we can look
at as a polygon whose surface is a highly specular material. From the point of view
of a renderer, the position of the viewer and the normal to the polygon are known,
so that the angle of reﬂection is determined as in Chapter 5. If we follow along this
                                                                                   7.8 Environment Maps   389

*FIGURE 7.28 Scene with a mirror.*

angle until we intersect the environment, we obtain the shade that is reﬂected in the
mirror. Of course, this shade is the result of a shading process that involves the light
sources and materials in the scene. We can obtain an approximately correct value of
this shade as part of a two-step rendering pass, similar in some respects to the two-
step texture-mapping process that we outlined in Section 7.5. In the ﬁrst pass, we
render the scene without the mirror polygon, with the camera placed at the center of
the mirror pointed in the direction of the normal of the mirror. Thus, we obtain an
image of the objects in the environment as “seen” by the mirror. This image is not
quite correct (Exercise 7.3), but is usually good enough. We can then use this image
to obtain the shades (texture values) to place on the mirror polygon for the second
normal rendering with the mirror placed back in the scene.
There are two difﬁculties with this approach. First, the images that we obtain in
the ﬁrst pass are not quite correct, because they have been formed without one of the
objects—the mirror—in the environment. Second, we must confront the mapping
issue. Onto what surface should we project the scene in the ﬁrst pass, and where
should we place the camera? Potentially, we want all the information in the scene
as we may want to do something like have our mirror move so that we should see
different parts of the environment on successive frames, and thus a simple projection
will not sufﬁce.
There have been a variety of approaches to this projection problem. The clas-
sic approach is to project the environment onto a sphere centered at the center of
projection. In Figure 7.29, we see some polygons that are outside the sphere and
their projections on the sphere. Note that a viewer located at the center of the sphere
cannot tell whether she is seeing the polygons in their original positions or their pro-
jections on the sphere. This illusion is similar to what we see in a planetarium. The
“stars” that appear to be an inﬁnite distance away are actually the projection of lights
onto the hemisphere which encloses the audience.
In the original version of environment mapping, the surface of the sphere was
then converted to a rectangle using lines of longitude and latitude for the mapping.
Although conceptually simple, there are problems at the poles where the shape dis-
tortion becomes inﬁnite. Computationally, this mapping does not preserve areas very
well and requires evaluating a large number of trigonometric functions.
390   Chapter 7   Discrete Techniques

*FIGURE 7.29 Mapping of the environment.*

(s,t, – d )   z

*FIGURE 7.30 Reflection map.*

OpenGL supports a variation of this method called sphere mapping. The appli-
cation program supplies a circular image that is the orthographic projection of the
sphere onto which the environment has been mapped. The advantage of this method
is that the mapping from the reﬂection vector to two-dimensional texture coordi-
nates on this circle is simple and can be implemented in either hardware or software.
The difﬁcult part is obtaining the required circular image. It can be approximated
by taking a perspective projection with a very wide-angle lens or by remapping some
other type of projection, such as the cube projection that we discuss next. We load
the texture image in texture memory through glTexImage2D.
The equations for generating the texture coordinates can be understood with the
help of Figure 7.30. It is probably easiest if we work backward from the viewer to the
image. Suppose that the texture map is in the plane z = −d, where d is positive and we
                                                                                    7.8 Environment Maps   391
project backward orthogonally toward a unit sphere centered at the origin. Thus, if
√ coordinates in the plane are (s, t), then the projector intersects the sphere
at (s, t , (1.0 − s2 − t 2)). For the unit sphere centered at the origin, the coordinates
of any point on the sphere are also the components of the unit normal at that point.
We can then compute the direction of reﬂection, as in Chapter 5, by
r = 2(n . v)n − v,
⎡ ⎤
v=⎣t ⎦,
⎡               ⎤
n=⎣        t         ⎦.
√
1.0 − s2 − t 2
The vector r points into the environment. Thus, any object that r intersects has
texture coordinates (s, t). However, this argument is backward because we start with
an object deﬁned by vertices. Given r, we can solve for s and t and ﬁnd that if
⎡   ⎤
r = ⎣ ry ⎦ ,
r   1
s= x + ,
f  2
ry   1
t=       + ,
f  2
f = 2 rx2 + ry2 + (rz + 1)2 .
If we put everything into eye coordinates, we compute r using the unit vector from
the origin to the vertex for v and the vertex normal for n.
This process reveals some issues that show that this method is only approximate.
The reﬂection map is only correct for the vertex at the origin. In principle, each vertex
should have its own reﬂection map. Actually each point on the object should have
its own map and not an approximate value computed by interpolation. The errors
are most signiﬁcant the farther the object is from the origin. Nevertheless, reﬂection
392   Chapter 7   Discrete Techniques

*FIGURE 7.31 Reflective cube map.*

mapping gives visually acceptable results in most situations, especially when there is
animation as in ﬁlms and games.
If we want to compute an environment map using the graphics system, we prefer
to use the standard projections that are supported by the graphics systems. For an
environment such as a room, the natural intermediate object is a box. We compute six
projections, corresponding to the walls, ﬂoor, and ceiling, using six virtual cameras
located at the center of the box, each pointing in a different direction. At this point, we
can treat the six images as a single environment map and derive the textures from it, as
in Figure 7.31. Color Plate 23 shows one frame from Pixar Animation Studio’s Geri’s
Game. A reﬂection map was computed on a box (Color Plate 24) and then mapped
to Geri’s glasses.
We could also compute the six images in our program and use them to compute
the circular image required by OpenGL’s spherical maps. Note that all these methods
can suffer from geometric distortions and aliasing problems. In addition, unless the
application recomputes the environment maps, they are not correct if the viewer
moves.
Regardless of how the images are computing, once we have them, we can specify
a cube map in OpenGL with six function calls, one for each face of a cube centered
at the origin. Thus, if we have a 512 × 512 RGBA image imagexp for the positive-x
face, we have the following:
glTexImage2D(GL_TEXTURE_CUBE_MAP_POSITIVE_X, 0, GL_RGBA, 512, 512,
0, GL_RGBA, GL_UNSIGNED_BYTE, imagexp);
For reﬂection maps, the calculation of texture coordinates can be done auto-
matically. However, cube maps are fundamentally different from sphere maps, which
are much like standard two-dimensional texture maps with special coordinate cal-
                                                                            7.9 Reflection Map Example   393
culations. Here, we must use three-dimensional texture coordinates, which are often
computed in the shader.
These techniques are examples of multipass rendering (or multirendering),
where, in order to compute a single image, we compute multiple images, each using
the rendering pipeline. Multipass methods are becoming increasingly more impor-
tant as the power of graphics cards has increased to the point that we can render a
scene multiple times from different perspectives in less time than is needed for rea-
sonable refresh rates. Equivalently, most of these techniques can be done within the
fragment shader.

### 7.9   REFLECTION MAP EXAMPLE

Let’s look at a simple example of a reﬂection map based on our rotating cube example.
In this example, we will use a cube map in which each of the six texture maps is a
single texel. Our rotating cube will be totally reﬂective and placed inside a box, each
of whose sides is one of the six colors: red, green, blue, cyan, magenta, and yellow.
Here’s how we can set up the cube map as part of initialization using texture unit 1:
GLuint tex[1];
GLubyte red[3] = {255, 0, 0};
GLubyte green[3] = {0, 255, 0};
GLubyte blue[3] = {0, 0, 255};
GLubyte cyan[3] = {0, 255, 255};
GLubyte magenta[3] = {255, 0, 255};
GLubyte yellow[3] = {255, 255, 0};
glActiveTexture(GL_TEXTURE1);
glGenTextures(1, tex);
glBindTexture(GL_TEXTURE_CUBE_MAP, tex);
glTexImage2D(GL_TEXTURE_CUBE_MAP_POSITIVE_X ,0,GL_RGB,1,1,0,GL_RGB,
GL_UNSIGNED_BYTE, red);
glTexImage2D(GL_TEXTURE_CUBE_MAP_NEGATIVE_X ,0,GL_RGB,1,1,0,GL_RGB,
GL_UNSIGNED_BYTE, green);
glTexImage2D(GL_TEXTURE_CUBE_MAP_POSITIVE_Y ,0,GL_RGB,1,1,0,GL_RGB,
GL_UNSIGNED_BYTE, blue);
glTexImage2D(GL_TEXTURE_CUBE_MAP_NEGATIVE_Y ,0,GL_RGB,1,1,0,GL_RGB,
GL_UNSIGNED_BYTE, cyan);
glTexImage2D(GL_TEXTURE_CUBE_MAP_POSITIVE_Z ,0,GL_RGB,1,1,0,GL_RGB,
GL_UNSIGNED_BYTE, magenta);
glTexImage2D(GL_TEXTURE_CUBE_MAP_NEGATIVE_Z ,0,GL_RGB,1,1,0,GL_RGB,
GL_UNSIGNED_BYTE, yellow);
glTexParameteri(GL_TEXTURE_CUBE_MAP,GL_TEXTURE_MIN_FILTER,GL_NEAREST);
394   Chapter 7   Discrete Techniques

*FIGURE 7.32 Reflection cube map.*

The texture map will be applied using a sampler in the fragment shader. We set up
the required uniform variables as in our other examples:
GLuint texMapLocation;
texMapLocation = glGetUniformLocation(program, "texMap");
glUniform1i(texMapLocation, 1); // corresponding to unit 1
Now that we have set up the cube map, we can turn to the determination of the
texture coordinates. The required computations for a reﬂection or environment map
are shown in Figure 7.32. We assume that the environment has already been mapped
to the cube. The difference between a reﬂection map and a simple cube texture map
is that we use the reﬂection vector to access the texture for a reﬂection map rather
than the view vector. We can compute the reﬂection vector at each vertex in our
vertex program and then let the fragment program interpolate these values over the
primitive.
However, to compute the reﬂection vector, we need the normal to each side of
the rotating cube. We can compute normals in the application and send them to the
vertex shader as a vertex attribute through the quad function
point4 normals[N];
vec4 normal;

```cpp
void quad(int a, int b, int c, int d)
{
static int i =0;
```

normal = normalize(cross(vertices[b]-vertices[a],
vertices[c]-vertices[b]));
normals[i] = normal;
points[i] = vertices[a];
i++;
                                                                              7.9 Reflection Map Example   395
normals[i] = normal;
points[i] = vertices[b];
i++;
normals[i] = normal;
points[i] = vertices[c];
i++;
normals[i] = normal;
points[i] = vertices[a];
i++;
normals[i] = normal;
points[i] = vertices[c];
i++;
normals[i] = normal;
points[i] = vertices[d];
i++;

```cpp
}
```

and combining the normal data in a vertex array
glBindBuffer(GL_ARRAY_BUFFER, buffer);
glBufferData(GL_ARRAY_BUFFER, sizeof(points) + sizeof(normals),
NULL, GL_STATIC_DRAW);
glBufferSubData(GL_ARRAY_BUFFER, 0, sizeof(points), points);
glBufferSubData(GL_ARRAY_BUFFER, sizeof(points),
sizeof(normals), normals)
loc2 = glGetAttribLocation(program, "Normal");
glEnableVertexAttribArray(loc2);
We will assume that the rotation to the cube is applied in the application and its effect
is incorporated in the model-view matrix. We also assume that the camera location
is ﬁxed. The normals must then be rotated in the vertex shader before we can use the
reflect function to compute the direction of reﬂection. Our vertex shader is
in vec4 vPosition;
in vec4 Normal;
out vec3 R;
uniform mat4 ModelView;
uniform mat4 Projection;

```cpp
void main()
{
```

gl_Position = Projection*ModelView*vPosition;
vec3 eyePos = vPosition.xyz;
396   Chapter 7   Discrete Techniques
vec4 NN = ModelView*Normal;
vec3 N = normalize(NN.xyz);
R = reflect(eyePos, N);

```cpp
}
```

It computes the reﬂection vector in eye coordinates as a varying variable. If we want
the color to be totally determined by the texture, the fragment shader is simply as
follows:
in vec3 R;
uniform samplerCube texMap;

```cpp
void main()
{
```

vec4 texColor = textureCube(texMap, R);
gl_FragColor = texColor;

```cpp
}
```

We can create more complex lighting by having the color determined in part by the
specular, diffuse, and ambient lighting as we did for the modiﬁed-Phong lighting
model.
However, we must be careful as to which frame we want to use in our shaders.
The difference between this example and previous ones is that the environment map
usually is computed in world coordinates. Object positions and normals are speciﬁed
in object coordinates and are brought into the world frame by modeling transforma-
tions in the application. We usually never see the object-coordinate representation of
objects because the model-view transformation converts object coordinates directly
to eye coordinates. In many applications, we deﬁne our objects directly without mod-
eling transformations so that model and object coordinates are the same. However,
we want to write our program in a manner that allows for modeling transformations
when we do reﬂection mapping. One way to accomplish this task is to compute the
modeling matrix in the application and pass it to the fragment program as a uni-
form variable. Also note that we need the inverse transpose of the modeling matrix
to transform the normal. However, if we pass in the inverse matrix as another uni-
form variable, we can postmultiply the normal to obtain the desired result. Color
Plate 12 shows the use of a reﬂection map to determine the colors on the teapot. The
teapot is set inside a cube, each of whose sides is one of the colors red, green, blue,
cyan, magenta, or yellow.

### 7.10    BUMP MAPPING

Bump mapping is a texture-mapping technique that can give the appearance of great
complexity in an image without increasing the geometric complexity. Unlike simple
texture mapping, bump mapping will show changes in shading as the light source or
object moves, making the object appear to have variations in surface smoothness.
                                                                                      7.10 Bump Mapping   397
Let’s start by returning to our example of creating an image of an orange. If
we take a photograph of a real orange, we can apply this image as a texture map
to a surface. However, if we move the lights or rotate the object, we immediately
notice that we have the image of a model of an orange rather than the image of a
real orange. The problem is that a real orange is characterized primarily by small
variations in its surface rather than by variations in its color, and the former are not
captured by texture mapping. The technique of bump mapping varies the apparent
shape of the surface by perturbing the normal vectors as the surface is rendered; the
colors that are generated by shading then show a variation in the surface properties.
Unlike techniques such as environment mapping that can be implemented without
programmable shaders, bump mapping cannot be done in real time without them.

#### 7.10.1 Finding Bump Maps

We start with the observation that the normal at any point on a surface characterizes
the orientation of the surface at that point. If we perturb the normal at each point
on the surface by a small amount, then we create a surface with small variations in
its shape. If this perturbation to the normal can be applied only during the shading
process, we can use a smooth model of the surface, which must have a smooth
normal, but we can shade it in a way that gives the appearance of a complex surface.
Because the perturbations are to the normal vectors, the rendering calculations are
correct for the altered surface, even though the more complex surface deﬁned by the
perturbed normals need never be created.
We can perturb the normals in many ways. The following procedure for para-
metric surfaces is an efﬁcient one. Let p(u, v) be a point on a parametric surface. The
⎡ ∂x ⎤                  ⎡ ∂x ⎤
∂u                     ∂v
⎢ ∂y ⎥                 ⎢ ⎥
pu = ⎣ ∂u ⎦,           pv = ⎣ ∂y
∂v ⎦
∂z                     ∂z
∂u                     ∂v
lie in the plane tangent to the surface at the point. Their cross product can be nor-
malized to give the unit normal at that point:
pu × pv
n=              .
|pu × pv |
Suppose that we displace the surface in the normal direction by a function called
the bump, or displacement, function, d(u, v), which we can assume is known and
small (|d(u, v)| 1). The displaced surface is given by
p = p + d(u, v)n.
We would prefer not to create the displaced surface because such a surface would
have a higher geometric complexity than the undisplaced surface and would thus
slow down the rendering process. We just want to make it look as though we have
398   Chapter 7   Discrete Techniques
displaced the original surface. We can achieve the desired look by altering the normal
n, instead of p, and using the perturbed normal in our shading calculations.
The normal at the perturbed point p is given by the cross product
n = pu × pv .
We can compute the two partial derivatives by differentiating the equation for p,
∂d
pu = pu +         n + d(u, v)nu ,
∂u
∂d
pv = pv +         n + d(u, v)nv .
∂v
If d is small, we can neglect the term on the right of these two equations and take their
cross product, noting that n × n = 0, to obtain the approximate perturbed normal:
∂d          ∂d
n ≈ n +          n × pv +    n × pu .
∂u          ∂v
The two terms on the right are the displacement, the difference between the original
and perturbed normals. The cross product of two vectors is orthogonal to both of
them. Consequently, both cross products yield vectors that lie in the tangent plane at
p, and their sum must also be in the tangent plane.
Although pu and pv lie in the tangent plane perpendicular to n, they are
not necessarily orthogonal to each other. We can obtain an orthogonal basis and a
corresponding rotation matrix using the cross product. First, we normalize n and
pu, obtaining the vectors
n
m=         ,
|n|
pu
t=         .
|pu|
We obtain the third orthogonal vector, b, by
b = m × t.
The vector t is called the tangent vector at p, and b is called the binormal vector at
p. The matrix
M =[t          b m ]T
is the rotation matrix that will convert representations in the original space to repre-
sentations in terms of the three vectors. The new space is sometimes called tangent
                                                                                     7.10 Bump Mapping   399
space. Because the tangent and binormal vectors can change for each point on the
surface, tangent space is a local coordinate system.
To better understand the implication of having introduced another frame, one
that is local to the point on the surface, let’s consider a bump from the plane z = 0.
The surface can be written in implicit form as
f (x, y) = ax + by + c = 0.
If we let u = x and v = y, then, if a = 0, we have
⎡           ⎤
⎢ b       c ⎥
p(u, v) = ⎣ − v − ⎦ .
The vectors ∂p/∂u and ∂p/∂v can be normalized to give the orthogonal vectors
pu
= [ 1 0 0 ]T ,
|pu|
pv
= [ 0 1 0 ]T .
|pv |
Because these vectors turned out to be orthogonal, they serve as the tangent binormal
vectors. The unit normal is
n = [ 0 0 1 ]T .
For this case, the displacement function is a function d(x, y). To specify the
bump map, we need two functions that give the values of ∂d/∂x and ∂d/∂y. If these
functions are known analytically, we can evaluate them either in the application
or in the shader. More often, however, we have a sampled version of d(x, y) as an
array of pixels D = [dij ]. The required partial derivatives can be approximated by the
difference between adjacent elements in the following array:
∂d
∝ dij − di−1, j ,
∂x
∂d
∝ dij − di, j−1.
∂y
These arrays can be precomputed in the application and stored as a texture called a
normal map. The fragment shader can obtain the values using a sampler.
Before we develop the necessary shaders, consider what is different for the gen-
eral case when the surface is not described by the plane z = 0. In our simple case,
the tangent space axes aligned with the object or world axes. In general, the normal
from a surface will not point in the z-direction nor along any particular axis. In ad-
dition, the tangent and binormal vectors, although orthogonal to each other and the
400   Chapter 7   Discrete Techniques
normal, will have no particular orientation with respect to the world or object axes.
The displacement function is measured along the normal so its partial derivatives
are in an arbitrary plane. However, in tangent space this displacement is along the
z-coordinate axis. Hence, the importance of the matrix M composed of the normal,
tangent, and binormal vectors is that it allows us to go to the local coordinate system
in which the bump map calculations match what we just did. The usual implemen-
tation of bump mapping is to ﬁnd this matrix and transform object-space vectors

```cpp
into vectors in a tangent space local coordinate system. Because tangent space is lo-
```

cal, the change in representation can be different for every fragment. With polygonal
meshes, the calculation can be simpler if we use the same normal across each polygon
and the application can send the tangent and binormal to the vertex shader once for
each polygon.
We are almost ready to write vertex and fragment shaders for bump mapping.
The entities that we need for lighting—the surface normal, the light vector(s), the
half-angle vector, the vertex location—are usually in eye or object coordinates at the
point in the process when we do lighting. Whether we use a normal map or compute
the perturbation of the normal procedurally in a fragment shader, the displacements
are in texture-space coordinates. For correct shading, we have to convert either the
normal map to object-space coordinates or the object-space coordinates to texture-
space coordinates. In general, the latter requires less work because it can be carried
out on a per-vertex basis in the vertex shader rather than on a per-fragment basis.
As we have seen, the matrix needed to convert from object space to texture space is
precisely the matrix composed of the normal, tangent, and binormal.
We can send the normal to the vertex shader as a vertex attribute if it changes
at each vertex, or, if we are working with a single ﬂat polygon at a time, we can
use a uniform variable. The application can also provide tangent vectors in a similar
manner. The binormal can then be computed in the shader using the cross-product
function. These computations are done in the vertex shader, which produces a light
vector and view vector in tangent coordinates for use in the fragment shader. Because
the normal vector in tangent coordinates always points in the positive z-direction, the
view and light vectors are sufﬁcient for doing lighting in tangent space.

#### 7.10.2 Bump Map Example

Our example is a single square in the plane y = 0 with a light source above the plane
that rotates in the plane y = 10.0. We will include only diffuse lighting to minimize
the amount of code we need. Our displacement is a small square in the center of the
original square. Before developing the code, the output is shown in Figure 7.33. The
image on the left is with the light source in its original position; the image on the left
is with the light source rotated 45 degrees in the x − z plane at the same height above
the surface.
First, let’s look at the application program. We will use two triangles for the
square polygon, and each of the six vertices will have a texture. So that part of the
code will be much as in previous examples:
                                                                                   7.10 Bump Mapping   401

*FIGURE 7.33 Bump mapping of a square displacement.*

point4 points[6];
point2 tex_coord[6];

```cpp
void mesh()
{
```

point4 vertices[4] = {
point4(0.0, 0.0, 0.0, 1.0),
point4(1.0, 0.0, 0.0, 1.0),
point4(1.0, 0.0, 1.0, 1.0),
point4(0.0, 0.0, 1.0, 1.0)

```cpp
};
```

points[0] = vertices[0];
tex_coord[0] = point2(0.0, 0.0);
points[1] = vertices[1];
tex_coord[1] = point2(1.0, 0.0);
points[2] = vertices[2];
tex_coord[2] = point2(1.0, 1.0);
points[3] = vertices[2];
tex_coord[3] = point2(1.0, 1.0);
points[4] = vertices[3];
tex_coord[4] = point2(0.0, 1.0);
points[5] = vertices[0];
tex_coord[5] = point2(0.0, 0.0);

```cpp
}
```

We send these data to the GPU as vertex attributes.
The displacement map is generated as an array in the application. The displace-
ment data are in the array data. The normal map is computing by taking the differ-
ences to approximate the partial derivatives for two of the components and using 1.0
for the third to form the array normals. Because these values are stored as colors in
a texture image, the components are scaled to the interval (0.0, 1.0).
402   Chapter 7   Discrete Techniques

```cpp
const int N = 256;
float data[N+1][N+1];
```

vec3 normals[N][N];
for(int i = 0; i < N+1; i++)
for(int j = 0; j < N+1; j++)
data[i][j]=0.0;
for(int i = N/4; i < 3*N/4; i++)
for(int j = N/4; j < 3*N/4; j++)
data[i][j] = 1.0;
for(int i = 0;i < N; i++)
for(int j = 0;j < N; j++)

```cpp
{
```

vec4 n = vec3(data[i][j] - data[i+1][j], 0.0, data[i][j] -
data[i][j+1]);
normals[i][j] = 0.5*normalize(n) + 0.5;

```cpp
}
```

The array normals is then sent to the GPU by building a texture object. We send
a projection matrix, a model-view matrix, the light position, and the diffuse lighting
parameters to the shaders as uniform variables. Because the surface is ﬂat, the normal
is constant and can be sent to the shader as a uniform variable. Likewise, the tangent
vector is constant and can be any vector in the same plane as the polygon and can also
be sent to the vertex shader as a uniform variable.
We now turn to the vertex shader. In this simple example, we want to do the
calculations in texture space. Hence, we must transform both the light vector and eye
vector to this space. The required transformation matrix is composed of the normal,
tangent, and binormal vectors.
The normal and tangent are speciﬁed in object coordinates and must ﬁrst be
converted to eye coordinates. The required transformation matrix is the normal
matrix, which is the inverse transpose of the upper-left 3 × 3 submatrix of the model-
view matrix (see Exercise 7.30). We assume this matrix is computed in the application
and sent to the shader as another uniform variable. We can then use the transformed
normal and tangent to give the binormal in eye coordinates. Finally, we use these
three vectors to transform the view vector and light vector to texture space. Here is
the vertex shader:
/* bump map vertex shader */
out vec3 L; /* light vector in texture-space coordinates */
out vec3 V; /* view vector in texture-space coordinates */
out vec2 st; /* texture coordinates */
in vec2 texcoord;
in vec4 vPosition;
                                                                                7.10 Bump Mapping   403
uniform vec3 Normal;
uniform vec4 LightPosition;
uniform mat4 ModelView;
uniform mat4 Projection;
uniform mat4 NormalMatrix;
uniform vec3 objTangent;

```cpp
void main()
{
```

gl_Position = Projection*ModelView*vPosition;
st = texcoord;
vec3 eyePosition = vec3(ModelView*vPosition);
vec3 eyeLightPos = LightPosition.xyz;
/* normal, tangent, and binormal in eye coordinates */
vec3 N = normalize(NormalMatrix*Normal);
vec3 T = normalize(NormalMatrix*objTangent);
vec3 B = cross(N, T);
/* light vector in texture space */
L.x = dot(T, eyeLightPos-eyePosition);
L.y = dot(B, eyeLightPos-eyePosition);
L.z = dot(N, eyeLightPos-eyePosition);
L = normalize(L);
/* view vector in texture space */
V.x = dot(T, -eyePosition);
V.y = dot(B, -eyePosition);
V.z = dot(N, -eyePosition);
V = normalize(V);

```cpp
}
```

Our strategy for the fragment shader is to provide the normalized perturbed
normals as a texture map from the application as a normal map. The fragment shader
is given by the following code:
in vec3 L;
in vec3 V;
in vec2 st;
uniform vec4 DiffuseProduct;
uniform sampler2D texMap;
404   Chapter 7   Discrete Techniques

```cpp
void main()
{
```

vec4 N = texture2D(texMap, st);
vec3 NN = normalize(2.0*N.xyz-1.0);
vec3 LL = normalize(L);

```cpp
float Kd = max(dot(NN.xyz, LL), 0.0);
```

gl_FragColor = Kd*DiffuseProduct;

```cpp
}
```

The values in the texture map are scaled back to the interval (−1.0, 1.0). The diffuse
product is a vector computed in the application, each of whose components is the
product of a diffuse light component and a diffuse material component.
Note that this example does not use the texture-space view vectors computed in
the vertex shader. These vectors would be necessary if we wanted to add a specular
term. We have only touched the surface (so to speak) of bump mapping. Many
of its most powerful applications are when it is combined with procedural texture
generation, which we explore further in Chapter 8.

### 7.11    COMPOSITING TECHNIQUES

Thus far, we have assumed that we want to form a single image and that the ob-
jects that form this image have surfaces that are opaque. OpenGL provides a mecha-
nism, through alpha (α) blending, that can, among other effects, create images with
translucent objects. The alpha channel is the fourth color in RGBA (or RGBα) color
mode. Like the other colors, the application program can control the value of A (or
α) for each pixel. However, in RGBA mode, if blending is enabled, the value of α con-
trols how the RGB values are written into the frame buffer. Because fragments from
multiple objects can contribute to the color of the same pixel, we say that these ob-
jects are blended or composited together. We can use a similar mechanism to blend
together images.

#### 7.11.1 Opacity and Blending

The opacity of a surface is a measure of how much light penetrates through that
surface. An opacity of 1 (α = 1) corresponds to a completely opaque surface that
blocks all light incident on it. A surface with an opacity of 0 is transparent; all light
passes through it. The transparency or translucency of a surface with opacity α is
given by 1 − α.
Consider the three uniformly lit polygons shown in Figure 7.34. Assume that
the middle polygon is opaque, and the front polygon, nearest to the viewer, is trans-
parent. If the front polygon were perfectly transparent, the viewer would see only
the middle polygon. However, if the front polygon is only partially opaque (partially
transparent), similar to colored glass, the color that viewer sees is a blending of the
colors of the front and middle polygon. Because the middle polygon is opaque, the
                                                                            7.11 Compositing Techniques   405

*FIGURE 7.34 Translucent and opaque polygons.*

viewer does not see the back polygon. If the front polygon is red and the middle is
blue, she sees magenta, due to the blending of the colors. If we let the middle polygon
be only partially opaque, she sees the blending of the colors of all three polygons.
In computer graphics, we usually render polygons one at a time into the frame
buffer. Consequently, if we want to use blending or compositing, we need a way to
apply opacity as part of fragment processing. We can use the notion of source and
destination pixels, just as we used source and destination bits in Section 7.3. As a
polygon is processed, pixel-sized fragments are computed and, if they are visible, are
assigned colors based on the shading model in use. Until now, we have used the color
of a fragment—as computed by the shading model and by any mapping techniques—
to determine the color of the pixel in the frame buffer at the location in screen
coordinates of the fragment. If we regard the fragment as the source pixel and the
frame-buffer pixel as the destination, we can combine these values in various ways.
Using α values is one way of controlling the blending on a fragment-by-fragment
basis. Combining the colors of polygons is similar to joining two pieces of colored
glass into a single piece of glass that has a higher opacity and a color different from
either of the original pieces.
If we represent the source and destination pixels with the four-element (RGBα)
s = [ sr   sg    sb    sa ] ,
d = [ dr    dg    db     da ] ,
then a compositing operation replaces d with
d = [ br sr + cr dr    bg sg + cg dg   bb sb + cb db   ba sa + ca da ] .
The arrays of constants b = [ br bg bb ba ] and c = [ cr cg cb ca ] are the
source and destination blending factors, respectively. As occurs with RGB colors,
a value of α over 1.0 is limited (or clamped) to the maximum of 1.0, and negative
values are clamped to 0.0. We can choose both the values of α and the method of
combining source and destination values to achieve a variety of effects.
406   Chapter 7   Discrete Techniques

#### 7.11.2 Image Compositing

The most straightforward use of α blending is to combine and display several images
that exist as pixel maps or equivalently, as sets of data that have been rendered inde-
pendently. In this case, we can regard each image as a radiant object that contributes
equally to the ﬁnal image. Usually, we wish to keep our RGB colors between 0 and
1 in the ﬁnal image, without having to clamp those values greater than 1. Hence, we
can either scale the values of each image or use the source and destination blending
factors.
Suppose that we have n images that should contribute equally to the ﬁnal display.
At a given pixel, image i has components Ci αi . Here, we are using Ci to denote the
color triplet (Ri , Gi , Bi ). If we replace Ci by n1 Ci and αi by n1 , then we can simply add
each image into the frame buffer (assuming the frame buffer is initialized to black
with an α = 0). Alternately, we can use a source blending factor of n1 by setting the α
value for each pixel in each image to be n1 , and using 1 for the destination blending
factor and α for the source blending factor. Although these two methods produce the
same image, if the hardware supports compositing, the second may be more efﬁcient.
Note that if n is large, blending factors of the form n1 can lead to signiﬁcant loss of
color resolution. Recent frame buffers support ﬂoating point arithmetic and thus can
avoid this problem.

#### 7.11.3 Blending and Compositing in OpenGL

The mechanics of blending in OpenGL are straightforward. We enable blending by
glEnable(GL_BLEND);
Then we set up the desired source and destination factors by
glBlendFunc(source_factor, destination_factor);
OpenGL has a number of blending factors deﬁned, including the values 1 (GL_ONE)
and 0 (GL_ZERO), the source α and 1 − α (GL_SRC_ALPHA and GL_ONE_MINUS_
SRC_ALPHA), and the destination α and 1 − α (GL_DST_ALPHA and
GL_ONE_MINUS_DST_ALPHA). The application program speciﬁes the desired op-
tions and then uses RGBA color.
The major difﬁculty with compositing is that for most choices of the blending
factors the order in which we render the polygons affects the ﬁnal image. For example,
many applications use the source α as the source blending factor and 1 − α for the
destination factor. The resulting color and opacity are
(Rd  , Gd  , Bd  , αd  ) = (αs Rs + (1 − αs )Rd , αs G + (1 − αs )Gd , αs Bs
+ (1 − αs )Bd , αs αd + (1 − αs )αd ).
This formula ensures that neither colors nor opacities can saturate. However, the
resulting color and α values depend on the order in which the polygons are rendered.
                                                                             7.11 Compositing Techniques           407
Consequently, unlike in most OpenGL programs, where the user does not have to
worry about the order in which polygons are rasterized, to get a desired effect we
must now control this order within the application.
A more subtle but visibly apparent problem occurs when we combine opaque
and translucent objects in a scene. Normally, when we use blending, we do not en-
able hidden-surface removal, because polygons behind any polygon already rendered
would not be rasterized and thus would not contribute to the ﬁnal image. In a scene
with both opaque and transparent polygons, any polygon behind an opaque polygon
should not be rendered, but translucent polygons in front of opaque polygons should
be composited. There is a simple solution to this problem that does not require the
application program to order the polygons. We can enable hidden-surface removal as
usual and can make the z-buffer read-only for any polygon that is translucent. We do
glDepthMask(GL_FALSE);
When the depth buffer is read-only, a translucent polygon that lies behind any opaque
polygon already rendered is discarded. A translucent polygon that lies in front of any
polygon that has already been rendered is blended with the color of the polygons it
is in front of. However, because the z-buffer is read-only for this polygon, the depth
values in the buffer are unchanged. Opaque polygons set the depth mask to true and
are rendered normally. Note that because the result of compositing depends on the
order in which we composite individual elements, we may notice defects in images
in which we render translucent polygons in an arbitrary order. If we are willing to
sort the translucent polygons, then we can render all the opaque polygons ﬁrst and
then render the translucent polygons in a back-to-front order with the z-buffer in a
read-only mode.

#### 7.11.4 Antialiasing Revisited

One of the major uses of the α channel is for antialiasing. Because a line must have
a ﬁnite width to be visible, the default width of a line that is rendered should be
one pixel wide. We cannot produce a thinner line. Unless the line is horizontal or
vertical, such a line partially covers a number of pixels in the frame buffer, as shown in
Figure 7.35. Suppose that, as part of the geometric-processing stage of the rendering        FIGURE 7.35 Raster line.
process, as we process a fragment, we set the α value for the corresponding pixel to be
a number between 0 and 1 that is the amount of that pixel covered by the fragment.
We can then use this α value to modulate the color as we render the fragment to
the frame buffer. We can use a destination blending factor of 1 − α and a source
destination factor of α. However, if there is overlap of fragments within a pixel, then
there are numerous possibilities, as we can see from Figure 7.35. In Figure 7.36(a), the
fragments do not overlap; in Figure 7.36(b), they do overlap. Consider the problem
from the perspective of a renderer that works one polygon a time. For our simple
example, suppose that we start with an opaque background and that the frame buffer
starts with the background color C0. We can set α0 = 0, because no part of the pixel
408   Chapter 7   Discrete Techniques
(b)
(a)

*FIGURE 7.36 Fragments. (a) Nonoverlapping. (b) Overlapping.*

has yet been covered with fragments from polygons. The ﬁrst polygon is rendered.
The color of the destination pixel is set to
Cd = (1 − α1)C0 + α1C1,
and its α value is set to
αd = α 1 .
Thus, a fragment that covers the entire pixel (α1 = 1) will have its color assigned to
the destination pixel, and the destination pixel will be opaque. If the background is
black, the destination color will be α1C1. Now consider the fragment from the second
polygon that subtends the same pixel. How we add in its color and α value depends
on how we wish to interpret the overlap. If there is no overlap, we can assign the new
color by blending the color of the destination with the color of the fragment, resulting
in the color and α:
Cd = (1 − α2)((1 − α1)C0 + α1C1) + α2C2 ,
αd = α1 + α2 .
This color is a blending of the two colors and does not need to be clamped. The
resulting value of α represents the new fraction of the pixel that is covered. However,
the resulting color is affected by the order in which the polygons are rendered. The
more difﬁcult questions are what to do if the fragments overlap and how to tell
whether there is an overlap. One tactic is to take a probabilistic view. If fragment 1
occupies a fraction α1 of the pixel, fragment 2 occupies a fraction α2 of the same
pixel, and we have no other information about the location of the fragments within
the pixel, then the average area of overlap is α1α2. We can represent the average case
as shown in Figure 7.37. Hence, the new destination α should be
αd = α1 + α2 − α1α2 .
                                                                          7.11 Compositing Techniques   409
ez
1 0.5z
ez

*FIGURE 7.37 Average overlap.*

How we should assign the color is a more complex problem, because we have
to decide whether the second fragment is in front of the ﬁrst or the ﬁrst is in front
of the second, or even whether the two should be blended. We can deﬁne an appro-
priate blending for whichever assumption we wish to make. Note that, in a pipeline
renderer, polygons can be generated in an order that has nothing to do with their
distances from the viewer. However, if we couple α blending with hidden-surface re-
moval, we can use the depth information to make front-versus-back decisions.
In OpenGL, we can invoke antialiasing without having the user program com-
bine α values explicitly if we enable blending and smoothing for lines or polygons;
for example, we can use
glEnable(GL_LINE_SMOOTH);
glEnable(GL_POLYGON_SMOOTH);
glEnable(GL_BLEND);
glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA);
to enable antialiasing. There may be a considerable performance penalty associated
with antialiasing. Color Plate 8 shows OpenGL’s antialiasing of polygons.

#### 7.11.5 Back-to-Front and Front-to-Back Rendering

Although using the α channel gives us a way of creating the appearance of translu-
cency, it is difﬁcult to handle transparency in a physically correct manner without
taking into account how an object is lit and what happens to rays and projectors that
pass through translucent objects. In Figure 7.38, we can see several of the difﬁcul-
ties. We ignore refraction of light through translucent surfaces—an effect than cannot
be handled easily with a pipeline polygon renderer. Suppose that the rear polygon is
opaque, but reﬂective, and that the two polygons closer to the viewer are translucent.
By following various rays from the light source, we can see a number of possibili-
ties. Some rays strike the rear polygon, and the corresponding pixels can be colored
with the shade at the intersection of the projector and the polygon. For these rays, we
410   Chapter 7   Discrete Techniques

*FIGURE 7.38 Scene with translucent objects.*

should also distinguish between points illuminated directly by the light source and
points for which the incident light passes through one or both translucent polygons.
For rays that pass through only one translucent surface, we have to adjust the color
based on the color and opacity of the polygon. We should also add a term that ac-
counts for the light striking the front polygon that is reﬂected toward the viewer. For
rays passing through both translucent polygons, we have to consider their combined
effect.
For a pipeline renderer, the task is even more difﬁcult—if not impossible—
because we have to determine the contribution that each polygon makes as it is passed
through the pipeline, rather than considering the contributions of all polygons to a
given pixel at the same time. In applications where handling of translucency must
be done in a consistent and realistic manner, we often must sort the polygons from
front to back within the application. Then depending on the application, we can do a
front-to-back or back-to-front rendering using OpenGL’s blending functionality (see
Exercise 7.27).

#### 7.11.6 Scene Antialiasing and Multisampling

Rather than antialiasing individual lines and polygons, as we discussed in Sec-
tion 7.11.4, we can antialias the entire scene using a technique called multisampling.
In this mode, every pixel in the frame buffer contains a number of samples. Each
sample is capable of storing a color, depth, and other values. When a scene is ren-
dered, it is as if the scene is rendered at an enhanced resolution. However, when the
image must be displayed in the frame buffer, all of the samples for each pixel are
combined to produce the ﬁnal pixel color.
In OpenGL, the number of samples per pixel is a function of how the frame
buffer is created when the application initializes. In our programs, since we use GLUT,
you would add the additional option GLUT_MULTISAMPLE to the glutInitDis-
playMode. This will request that the pixels in the frame buffer have multiple samples.
Just as line and polygon antialiasing can be enabled and disabled during the ren-
dering of a frame, so too with multisampling. To turn on multisampling and begin
antialiasing all of the primitives rendered in the frame, simply call glEnable(GL_
MULTISAMPLE). Likewise, calling glDisable(GL_MULTISAMPLE) will stop the
multisampled rendering. Generally speaking, an application will almost always either
multisample all the time, or never.
                                                                        7.11 Compositing Techniques   411

*FIGURE 7.39 Filtering and convolution.*


#### 7.11.7 Image Processing

We can use pixel mapping to perform various image-processing operations. Suppose
that we start with a discrete image. Perhaps this image was generated by a rendering,
or perhaps we obtained it by digitizing a continuous image using a scanner. We can
represent the image with an N × M matrix,
A = [ aij ] ,
of scalar levels. If we process each color component of a color image independently,
we can regard the entries in A as either individual color components or gray (lumi-
nance) levels. A linear ﬁlter produces a ﬁltered matrix B whose elements are

m 
bij =                  hkl ai+k, j+l .
k=−m l=−n
We say that B is the result of convolving A with a ﬁlter matrix H. In general, the
values of m and n are small, and we can represent H by a small (2m + 1 × 2n + 1)
convolution matrix.
We can view the ﬁltering operation as shown in Figure 7.39 for m = n = 1. For
each pixel in A, we place the convolution matrix over aij and take a weighted average
of the surrounding points. The values in the matrix are the weights. For example, for
n = m = 1, we can average each pixel with its four surrounding neighbors using the
3 × 3 matrix
⎡          ⎤
0 1 0
H= ⎣ 1 1 1⎦ .
0 1 0
This ﬁlter can be used for antialiasing. We can use more points and can weight the
⎡       ⎤
1 2 1
1 ⎣
H=      2 4 2⎦.
1 2 1
412   Chapter 7   Discrete Techniques
Note that we must deﬁne a border around A if we want B to have the same dimen-
sions. Other operations are possible with small matrices. For example, we can use the
⎡                ⎤
0 −1 0
H = ⎣ −1 4 −1 ⎦
0      −1   0
to detect changes in value or edges in the image. If the matrix H is k × k, we can
implement a ﬁlter by accumulating k 2 images in the frame buffer, each time adding
in a shifted version of A.

#### 7.11.8 Other Multipass Methods

We can also use blending for ﬁltering in time and depth. For example, if we jitter
an object and render it multiple times, leaving the positions of the other objects
unchanged, we get dimmer copies of the jittered object in the ﬁnal image. If the object
is moved along a path, rather than randomly jittered, we see the trail of the object.
This motion-blur effect is similar to the result of taking a photograph of a moving
object using a long exposure time. We can adjust the object’s α value so as to render
the ﬁnal position of the object with greater opacity or to create the impression of
speed differences.
We can use ﬁltering in depth to create focusing effects. A real camera cannot
produce an image with all objects in focus. Objects within a certain distance from
the camera, the camera’s depth of ﬁeld, are in focus; objects outside it are out of fo-
cus and appear blurred. Computer graphics produces images with an inﬁnite depth
of ﬁeld because we do not have to worry about the limitations of real lenses. Occa-
sionally, however, we want to create an image that looks as though it were produced
by a real camera, or to defocus part of a scene so as to emphasize the objects within
a desired depth of ﬁeld. This time, the trick is to move the viewer in a manner that
leaves a particular plane ﬁxed, as shown in Figure 7.40. Suppose that we wish to keep
the plane at z = zf in focus and to leave the near (z = zmin ) and far (z = zmax ) clip-
ping distances unchanged. If we use Frustum, we specify the near clipping rectangle
(xmin , xmax , ymin , ymax ). If we move the viewer from the origin in the x-direction by
z = z max
(x min, ymin, z min)                         z = zf
z = z min

*FIGURE 7.40 Depth-of-field jitter.*

                                                                               7.12 Sampling and Aliasing   413
x, we must change xmin to
                   x
xmin = xmin +            (zf − znear ).
Similar equations hold for xmax , ymin , and ymax . As we increase   x and   y, we create
a narrower depth of ﬁeld.

### 7.12       SAMPLING AND ALIASING

We have seen a variety of applications in which the conversion from a continuous
representation of an entity to a discrete approximation of that entity leads to visible
errors in the display. We have used the term aliasing to characterize these errors.
When we work with buffers, we are always working with digital images, and, if we
are not careful, these errors can be extreme. In this section, we examine the nature of
digital images and gather facts that will help us to understand where aliasing errors
arise and how the effects of these errors can be mitigated.
We start with a continuous two-dimensional image f (x, y). We can regard the
value of f as either a gray level in a monochromatic image or the value of one of the
primaries in a color image. In the computer, we work with a digital image that is an
array of nm pixels arranged as n rows of m pixels. Each pixel has k bits. There are two
processes involved in going from a continuous image to a discrete image. First, we
must sample the continuous image at nm points on some grid to obtain a set of values

```cpp
{fij }. Each of these samples of the continuous image is the value of f measured over
```

a small area in the continuous image. Then, we must convert each of these samples

```cpp
into a k-bit pixel by a process known as quantization.
```


#### 7.12.1 Sampling Theory

Suppose that we have a rectangular grid of locations where we wish to obtain our
samples of f , as in Figure 7.41. If we assume that the grid is equally spaced, then an
fij = f (x0 + ihx , y0 + jhy ),
f (x0, y0) = f00

*FIGURE 7.41 Sampling grid.*

414        Chapter 7   Discrete Techniques
where hx and hy are the distances between the grid points in the x- and y-directions,
respectively. Leaving aside for now the fact that no real sampler can make such a
precise measurement, there are two important questions. First, what errors have we
made in this idealized sampling process? That is, how much of the information in the
original image is included in the sampled image? Second, can we go back from the
digital image to a continuous image without incurring additional errors? This latter
(a)                step is called reconstruction and describes display processes such as are required in
displaying the contents of a frame buffer on a monitor.
The mathematical analysis of these issues uses Fourier analysis, a branch of ap-
plied mathematics particularly well suited for explaining problems of digital signal
processing. The essence of Fourier theory is that a function, of either space or time,
can be decomposed into a set of sinusoids, at possibly an inﬁnite number of fre-
quencies. This concept is most familiar with sound, where we routinely think of a
particular sound in terms of its frequency components, or spectrum. For a two-
(b)                dimensional image, we can think of it as being composed of sinusoidal patterns in two

*FIGURE 7.42 One-dimensional    spatial frequencies that when added together produce the image. Figure 7.42(a) shows*

decomposition. (a) Function.   a one-dimensional function; Figure 7.42(b) shows the two sinusoids that form it. Fig-
(b) Components.                ure 7.43 shows two-dimensional periodic functions. Thus, every two-dimensional
spatial function f (x, y) has two equivalent representations. One is its spatial form
f (x, y); the other is a representation in terms of its spectrum—the frequency-domain
representation g(ξ , η). The value of g is the contribution to f at the two-dimensional
spatial frequency (ξ , η). By using these alternate representations of functions, we ﬁnd
that many phenomena, including sampling, can be explained much more easily in the
frequency domain.
We can explain the consequences of sampling, without being overwhelmed by
the mathematics, if we accept, without proof, the fundamental theorem known as the
Nyquist sampling theorem. There are two parts to the theorem: The ﬁrst allows us to
discuss sampling errors, whereas the second governs reconstruction. We examine the
second in Section 7.12.2.
(a)                                        (b)

*FIGURE 7.43 Two-dimensional periodic functions.*

                                                                                          7.12 Sampling and Aliasing   415
Nyquist sampling theorem (part 1): The ideal samples of a continuous function
contain all the information in the original function if and only if the continuous
function is sampled at a frequency greater than twice the highest frequency in the
function.
Thus, if we are to have any chance of not losing information, we must restrict
ourselves to functions that are zero in the frequency domain except in a window
of width less than the sampling frequency, centered at the origin. The lowest fre-
quency that cannot be in the data so as to avoid aliasing—one-half of the sampling
frequency—is called the Nyquist frequency. Functions whose spectra are zero out-
side of some window are known as band-limited functions. For a two-dimensional
image, the sampling frequencies are determined by the spacing of a two-dimensional
grid with x and y spacing of 1/hx and 1/hy , respectively. The theorem assumes an
ideal sampling process that gathers an inﬁnite number of samples, each of which is
the exact value at the grid point. In practice, we can take only a ﬁnite number of
samples—the number matching the resolution of our buffer. Consequently, we can-
not produce a truly band-limited function. Although this result is a mathematical
consequence of Fourier theory, we can observe that there will always be some am-
biguity inherent in a ﬁnite collection of sampled points, simply because we do not
know the function outside the region from which we obtained the samples.5
The consequences of violating the Nyquist criteria are aliasing errors. We can
see from where the name aliasing comes by considering an ideal sampling process.
Both the original function and its set of samples have frequency-domain representa-
tions. The spectral components of the sampled function are replicas of the spectrum
of the original function, with their centers separated by the sampling frequency. Con-
sider the one-dimensional function in Figure 7.44(a), with the samples indicated.
Figure 7.44(b) shows its spectrum; in Figure 7.44(c), we have the spectrum of the
sampled function, showing the replications of the spectrum in Figure 7.44(b).6 Be-
cause we have sampled at a rate higher than the Nyquist frequency, there is a separa-
tion between the replicas.
Now consider the case in Figure 7.45. Here, we have violated the Nyquist crite-
rion, and the replicas overlap. Consider the central part of the plot, which is magni-
ﬁed in Figure 7.46 and shows only the central replica, centered at the origin, and the
replica to its right, centered at ξs . The frequency ξ0 is above the Nyquist frequency
ξs /2. There is, however, a replica of ξ0, generated by the sampling process from the
replica on the right, at ξs − ξ0, a frequency less than the Nyquist frequency. The en-
ergy at this frequency can be heard, if we are dealing with digital sound, or seen, if we
are considering two-dimensional images. We say that the frequency ξ0 has an alias at
5. This statement assumes no knowledge of the underlying function f , other than a set of its samples.
If we have additional information, such as knowledge that the function is periodic, knowledge of the
function over a ﬁnite interval can be sufﬁcient to determine the entire function.
6. We show the magnitude of the spectrum because the Fourier transform produces complex num-
bers for the frequency-domain components.
416   Chapter 7   Discrete Techniques
f (x )                   |g( )|                    |gs( )|
(a)                          (b)                      (c)

*FIGURE 7.44 Band-limited function. (a) Function and its samples in the*

spatial domain. (b) Spectrum of the function. (c) Spectrum of the samples.
|g( )|

*FIGURE 7.45 Overlapping replicas.*

|g( )|
0        s 0   s

*FIGURE 7.46 Aliasing.*

ξs − ξ0. Note that once aliasing has occurred, we cannot distinguish between infor-
mation that was at a frequency in the original data and information that was placed
at this frequency by the sampling process.
We can demonstrate aliasing and ambiguity without using Fourier analysis by
looking at a single sinusoid, as shown in Figure 7.47. If we sample this sinusoid at
twice its frequency, we can recover it from two samples. However, these same two
samples are samples of a sinusoid of twice this frequency, and they can also be samples
of sinusoids of other multiples of the basic frequency. All these frequencies are aliases
of the same original frequency. If we know that the data were band limited, however,
then the samples can describe only the original sinusoid.
If we were to do an analysis of the frequency content of real-world images, we
would ﬁnd that the spectral components of most images are concentrated in the
lower frequencies. Consequently, although it is impossible to construct a ﬁnite-sized
image that is band limited, the aliasing errors often are minimal because there is little
content in frequencies above the Nyquist frequency, and little content is aliased into
                                                                             7.12 Sampling and Aliasing   417

*FIGURE 7.47 Aliasing of sinusoid.*

f (x, y )
(a)                                       (b)

*FIGURE 7.48 Scanning of an image.*

(a) Point sampling. (b) Area averaging.
frequencies below the Nyquist frequency. The exceptions to this statement arise when
there is regular (periodic) information in the continuous image. In the frequency
representation, regularity places most of the information at a few frequencies. If any
of these frequencies is above the Nyquist limit, the aliasing effect is noticeable as
beat or moiré patterns. Examples that you might have noticed include the patterns
that appear on video displays when people in the images wear striped shirts or plaid
ties, and wavy patterns that arise both in printed (halftoned) ﬁgures derived from
computer displays and in digital images of farmland with plowed ﬁelds.
Often, we can minimize aliasing by preﬁltering before we scan an image or by
controlling the area of the data that the scanner uses to measure a sample. Figure 7.48
shows two possible ways to scan an image. In Figure 7.48(a), we see an ideal scanner.
It measures the value of a continuous image at a point, so the samples are given by
fij = f (xi , yi ).
In Figure 7.48(b), we have a more realistic scanner that obtains samples by taking a
weighted average over a small interval to produce samples of the form
 x +s/2  y +s/2
fij =                            f (x, y)w(x, y)dydx.
xi −s/2      yi −s/2
418   Chapter 7   Discrete Techniques
5        3         1              1        3         5
4        2                        2         4

*FIGURE 7.49 Sinc function.*

By selecting the size of the window s and the weighting function w, we can attenuate
high-frequency components in the image, and thus we can reduce aliasing. Fortu-
nately, real scanners must take measurements over a ﬁnite region, called the sampling
aperture; thus, some antialiasing takes place even if the user has no understanding of
the aliasing problem.

#### 7.12.2 Reconstruction

Suppose that we have an (inﬁnite) set of samples, the members of which have been
sampled at a rate greater than the Nyquist frequency. The reconstruction of a contin-
uous function from the samples is based on part 2 of the Nyquist sampling theorem.
Nyquist sampling theorem (part 2): We can reconstruct a continuous function f (x)
from its samples {fi } by the formula
∞

f (x) =          fi sinc(x − xi ).
i=−∞
The function sinc(x) (see Figure 7.49) is deﬁned as
sin πx
sinc(x) =           .
πx
The two-dimensional version of the reconstruction formula for a function f (x, y)
with ideal samples {fij } is
∞
 ∞

f (x, y) =                 fij sinc(x − xi ) sinc(y − yj ).
i=−∞ j=−∞
These formulas follow from the fact that we can recover an unaliased func-
tion in the frequency domain by using a ﬁlter that is zero except in the interval
(−ξs /2, ξs /2)—a low-pass ﬁlter—to obtain a single replica from the inﬁnite number
                                                                             7.12 Sampling and Aliasing         419
f0
f 1
f1
f (x )

*FIGURE 7.50 One-dimensional reconstruction.*


*FIGURE 7.51 Two-dimensional sinc function.*

of replicas generated by the sampling process shown in Figure 7.44. The reconstruc-
tion of a one-dimensional function is shown in Figure 7.50. In two dimensions, the
reconstruction involves use of a two-dimensional sinc, as shown in Figure 7.51. Un-
fortunately, the sinc function cannot be produced in a physical display, because of its
negative side lobes. Consider the display problem for a CRT display. We start with a
digital image that is a set of samples. For each sample, we can place a spot of light
centered at a grid point on the display surface, as shown in Figure 7.52. The value of
the sample controls the intensity of the spot, or modulates the beam. We can control      FIGURE 7.52 Display of a
the shape of the spot by using techniques such as focusing the beam. The reconstruc-      point on CRT.
tion formula tells us that the beam should have the shape of a two-dimensional sinc,
but because the beam puts out energy, the spot must be nonnegative at all points.
Consequently, the display process must make errors. We can evaluate a real display
by considering how well its spot approximates the desired sinc. Figure 7.53 shows the
sinc and several one-dimensional approximations. The Gaussian-shaped spot corre-
sponds to the shape of many CRT spots, whereas the rectangular spot might corre-
420   Chapter 7   Discrete Techniques
(a)             (b)          (c)            (d)

*FIGURE 7.53 Display spots. (a) Ideal spot. (b) Rectangular approxima-*

tion. (c) Piecewise-linear approximation. (d) Gaussian approximation.
q(g )
q4
q3
q2
q1
q0                             g
g0 g1     g2   g3   g4

*FIGURE 7.54 Quantizer.*

spond to an LCD display with square pixels. Note that we can make either approxi-
mation wider or narrower. If we analyze the spot proﬁles in the frequency domain, we
ﬁnd that the wider spots are more accurate at low frequencies but are less accurate at
higher frequencies. In practice, the spot size that we choose is a compromise. Visible
differences across monitors often can be traced to different spot proﬁles.

#### 7.12.3 Quantization

The mathematical analysis of sampling explains a number of important effects. How-
ever, we have not included the effect of each sample being quantized into k discrete
levels. Given a scalar function g with values in the range
gmin ≤ g ≤ gmax ,
a quantizer is a function q such that, if gi ≤ g ≤ gi+1,
q(g) = qi .
Thus, for each value of g, we assign it one of k values, as shown in Figure 7.54. In
general, designing a quantizer involves choosing the {qi }, the quantization levels, and
the {gi }, the threshold values. If we know the probability distribution for g, p(g), we
can solve for the values that minimize the mean square error:

e = (g − q(g))2p(g)dg .
                                                                                      Summary and Notes   421
However, we often design quantizers based on the perceptual issues that we discussed
in Chapter 1. A simple rule of thumb is that we should not be able to detect one-level
changes, but should be able to detect all two-level changes. Given the threshold for
the visual system to detect a change in luminance, we usually need at least 7 or 8 bits
(or 128 to 256 levels). We should also consider the logarithmic intensity-brightness
response of humans. To do so, we usually distribute the levels exponentially, to give
approximately equal perceptual errors as we go from one level to the next.
In the early days of computer graphics, practitioners worked with only two- and
three-dimensional geometric objects, whereas those practitioners who were involved
with only two-dimensional images were considered to be working in image process-
ing. Advances in hardware have made graphics and image-processing systems practi-
cally indistinguishable. For those practitioners involved with synthesizing images—
certainly a major part of computer graphics—this merging of ﬁelds has brought forth
a multitude of new techniques. The idea that a two-dimensional image or texture can
be mapped to a three-dimensional surface in no more time than it takes to render the
surface with constant shading would have been unthinkable 15 years ago. Now, these
techniques are routine.
Techniques such as texture mapping have had an enormous effect on real-time
graphics. In ﬁelds such as animation, virtual reality, and scientiﬁc visualization, we
use hardware texture mapping to add detail to images without burdening the geo-
metric pipeline. The use of compositing techniques through the alpha channel allows
the application programmer to perform tasks, such as antialiasing, and to create ef-
fects, such as fog and depth of ﬁeld, that until recently were done on different types
of architectures, after the graphics had been created.
Mapping methods provide some of the best examples of the interactions among
graphics hardware, software, and applications. Consider texture mapping. Although
it was ﬁrst described and implemented purely as a software algorithm, once people
saw its ability to create scenes with great visual complexity, hardware developers
started putting large amounts of texture memory in graphics systems. Once texture
mapping was implemented in hardware, it could be done in real time, a development
that led to the redesign of many applications, notably computer games.
Recent advances in GPUs provide many new possibilities. One is that the pipeline
is now programmable. The programmability of the fragment processor makes possi-
ble many new texture-manipulation techniques while preserving interactive speeds.
Second, the inclusion of large amounts of memory on the GPU removes one of the
major bottlenecks in discrete methods, namely, many of the transfers of image data
between processor memory and the GPU. Third, GPU architectures are designed for
rapid processing of discrete data by incorporating a high degree of parallelism for
fragment processing. Finally, the availability of ﬂoating-point frame buffers elimi-
nates many of the precision issues that plagued techniques that manipulated image
data.
422   Chapter 7   Discrete Techniques
In this chapter, we have concentrated on techniques that are supported by re-
cently available hardware and APIs. Many of the techniques introduced here are
recent. Many more have appeared in the recent literature and are available only for
programmable processors.
Environment mapping was developed by Blinn and Newell [Bli76]. Texture mapping
was ﬁrst used by Catmull; see the review by Heckbert [Hec86]. Hardware support for
texture mapping came with the SGI Reality Engine; see Akeley [Ake93]. Perlin and
Hoffert [Per89] designed a noise function to generate two- and three-dimensional
texture maps. Many texture synthesis techniques are discussed in Ebert et al. [Ebe02].
The aliasing problem in computer graphics has been of importance since the
advent of raster graphics; see Crow [Cro81]. The ﬁrst concerns were with rasteri-
zation of lines, but later other forms of aliasing arose with animations [Mag85] and
ray tracing [Gla89]. The image-processing books [Pra78, Gon08, Cas96] provide an

```cpp
introduction to signal processing and aliasing in two dimensions. The books by Glass-
```

ner [Gla95] and Watt and Policarpo [Wat98] are aimed at practitioners of computer
graphics.
Many of the compositing techniques, including use of the α channel, were sug-
gested by Porter and Duff [Por84]. The OpenGL Programming Guide [Shr10] contains
many examples of how buffers can be used. The recent literature includes many new
examples of the use of buffers. See the recent issues of the journals Computer Graphics
and IEEE Computer Graphics and Applications.
Technical details on most of the standard image formats can be found in [Mia99,
Mur94].

### 7.1   Show how you can use the XOR writing mode to implement an odd–even ﬁll

algorithm.

### 7.2   What are the visual effects of using XOR to move a cursor around on the

screen?

### 7.3   How is an image produced with an environment map different from a ray-

traced image of the same scene?

### 7.4   In the movies and television, the wheels of cars and wagons often appear to be

spinning in the wrong direction. What causes the effect? Can anything be done
to ﬁx this problem? Explain your answer.

### 7.5   We can attempt to display sampled data by simply plotting the points and

letting the human visual system merge the points into shapes. Why is this
technique dangerous if the data are close to the Nyquist limit?
                                                                                        Exercises   423

### 7.6   Why do the patterns of striped shirts and ties change as an actor moves across

the screen of your television?

### 7.7   Why should we do antialiasing by preprocessing the data, rather than by post-

processing them?

### 7.8   Suppose that we have two translucent surfaces characterized by opacities α1

and α2. What is the opacity of the translucent material that we create by using
the two in series? Give an expression for the transparency of the combined
material.

### 7.9   Assume that we view translucent surfaces as ﬁlters of the light passing through

them. Develop a blending model based on the complementary colors CMY.

### 7.10 In Section 7.11 we used 1 − α and α for the destination and source blending

factors, respectively. What would be the visual difference if we used 1 for the
destination factor and kept α for the source factor?

### 7.11 Create interactive paintbrushes that add color gradually to image. Also use

blending to add erasers that gradually remove images from the screen.

### 7.12 Devise a method of using texture mapping for the display of arrays of three-

dimensional pixels (voxels).

### 7.13 Show how to use the luminance histogram of an image to derive a lookup table

that will make the altered image have a ﬂat histogram.

### 7.14 When we supersample a scene using jitter, why should we use a random jitter

pattern?

### 7.15 Suppose that a set of objects is texture mapped with regular patterns such as

stripes and checkerboards. What is the difference in aliasing patterns that we
would see when we switch from parallel to perspective views?

### 7.16 Consider a scene composed of simple objects, such as parallelepipeds, that are

instanced at different sizes. Suppose that you have a single texture map and
you are asked to map this texture to all the objects. How would you map the
texture so that the pattern would be the same size on each face of each object?

### 7.17 Write a program using mipmaps in which each mipmap is constructed from a

different image. Is there a practical application for such a program?

### 7.18 Using either your own image-processing code for convolution or the imaging

extensions of OpenGL, implement a general 3 × 3 ﬁltering program for lumi-
nance images.

### 7.19 Take an image from a digital camera or from some other source and apply

3 × 3 smoothing and sharpening ﬁlters, repetitively. Pay special attention to
what happens at the edges of the ﬁltered images.

### 7.20 Repeat Exercise 7.19 but ﬁrst add a small amount of random noise to the

image. Describe the differences between the results of the two exercises.

### 7.21 If your system supports the imaging extensions, compare the performance

of ﬁltering using the extensions with ﬁltering done by your own code using
processor memory.
424   Chapter 7   Discrete Techniques

### 7.22 One of the most effective methods of altering the contrast of an image is to

allow the user to design a lookup interactively. Consider a graph in which a
curve is approximated with three connected line segments. Write a program
that displays an image, allows the user to specify the line segments interactively,
and shows the image after it has been altered by the curve.

### 7.23 In a similar vein to Exercise 7.22, write an interactive program that allows users

to design pseudocolor maps.

### 7.24 Devise a method to convert the values obtained from a cube map to values for

a spherical map.

### 7.25 Write an interactive program that will return the colors of pixels on the display.


### 7.26 Suppose we want to create a cube that has a black and white checkerboard

pattern texture mapped to its faces. Can we texture map the cube so that the
colors alternate as we traverse the cube from face to face?

### 7.27 In what types of applications might you prefer a front-to-back rendering in-

stead of a back-to-front rendering?

### 7.28 The color gamut in chromaticity coordinates is equivalent to the triangle in

RGB space that is deﬁned by the primaries. Write a program that will display
this triangle and the edges of the cube in which it lies. Each point on the
triangle should have the color determined by its coordinates in RGB space.
This triangle is called the Maxwell Triangle.

### 7.29 Find the matrix that converts NTSC RGB and use it to redisplay the color

gamut of your display in xy chromaticity coordinates.

### 7.30 Show that the normal matrix is the inverse transpose of the upper-left 3 × 3

submatrix of the model-view matrix.
                                                                   CHA P TE R           8
M      odels are abstractions of the world—both of the real world in which we live
and of virtual worlds that we create with computers. We are all familiar with
mathematical models that are used in all areas of science and engineering. These
models use equations to model the physical phenomena that we wish to study. In
computer science, we use abstract data types to model organizations of objects; in
computer graphics, we model our worlds with geometric objects. When we build a
mathematical model, we must choose carefully which type of mathematics ﬁts the
phenomena that we wish to model. Although ordinary differential equations may be
appropriate for modeling the dynamic behavior of a system of springs and masses, we
would probably use partial differential equations to model turbulent ﬂuid ﬂow. We
go through analogous processes in computer graphics, choosing which primitives to
use in our models and how to show relationships among them. Often, as is true of
choosing a mathematical model, there are multiple approaches, so we seek models
that can take advantage of the capabilities of our graphics systems.
In this chapter, we explore multiple approaches to developing and working with
models of geometric objects. We consider models that use as building blocks a set of
simple geometric objects: either the primitives supported by our graphics systems or
a set of user-deﬁned objects built from these primitives. We extend the use of trans-
formations from Chapter 3 to include hierarchical relationships among the objects.
The techniques that we develop are appropriate for applications, such as robotics and
ﬁgure animation, where the dynamic behavior of the objects is characterized by rela-
tionships among the parts of the model.
The notion of hierarchy is a powerful one and is an integral part of object-
oriented methodologies. We extend our hierarchical models of objects to hierarchical
models of whole scenes, including cameras, lights, and material properties. Such
models allow us to extend our graphics APIs to more object-oriented systems and
also give us insight into using graphics over networks and distributed environments,
such as the World Wide Web.
426           Chapter 8       Modeling and Hierarchy

### 8.1   SYMBOLS AND INSTANCES

Our ﬁrst concern is how we can store a model that may include many sophisticated
objects. There are two immediate issues: how we deﬁne an object more complex than
the ones we have dealt with until now and how we represent a collection of these
y                         objects. Most APIs take a minimalist attitude toward primitives: They contain only
a few primitives, leaving it to the user to construct more complex objects from these
primitives. Sometimes additional libraries provide objects built on top of the basic
primitives. We assume that we have available a collection of basic three-dimensional
objects provided by these options.
x              We can take a nonhierarchical approach to modeling by regarding these objects
as symbols and by modeling our world as a collection of symbols. Symbols can
include geometric objects, fonts, and application-speciﬁc sets of graphical objects.
Symbols are usually represented at a convenient size and orientation. For example, a

*FIGURE 8.1 Cylinder symbol.         cylinder is usually oriented parallel to one of the axes, as shown in Figure 8.1, often*

with a unit height, a unit radius, and its bottom centered at the origin.
Most APIs, including OpenGL, make a distinction between the frame in which
the symbol is deﬁned, sometimes called the model frame, and the object or world
frame. This distinction can be helpful when the symbols are purely shapes, such as
the symbols that we might use for circuit elements in a CAD application, and have no
physical units associated with them. In OpenGL, we have to set up the transformation
from the frame of the symbol to the object coordinate frame within the application.
Thus, the model-view matrix for a given symbol is the concatenation of an instance
transformation that brings the symbol into object coordinates and a matrix that
brings the symbol into the eye frame.
The instance transformation that we introduced in Chapter 3 allows us to place
instances of each symbol in the model, at the desired size, orientation, and location.
Thus, the instance transformation
M = TRS
is a concatenation of a translation, a rotation, and a scale (and possibly a shear), as
shown in Figure 8.2. Consequently, OpenGL programs often contain repetitions of
code in the following form:
mat4 instance;
mat4 model_view;
instance = Translate(dx, dy, dz)*RotateZ(rz)*
RotateY(ry)*RotateX(rx)*Scale(sx, sy, sz);
model_view = model_view*instance;
cylinder(); /* or some other symbol */
In this example, the instance matrix is computed and alters the model-view matrix.
The resulting model-view matrix is sent to the vertex shader using glUniform. The
                                                                                                  8.2 Hierarchical Models         427
y                                 y                              y               y
S                                     R               T
x                                     x               x               x
z                                   z                                     z               z

*FIGURE 8.2 Instance transformation.*

Symbol         Scale            Rotate         Translate
1          sx , sy , sz   x , y , z       dx , dy , dz
·
·

*FIGURE 8.3 Symbol–instance transformation table.*

code for cylinder generates vertices and can send them to the vertex shader using
glDrawArrays. Alternately, we can apply the model-view matrix in the application
as we generate the vertices.
We can also think of such a model in the form of a table, as shown in Figure 8.3.
Here, each symbol is assumed to have a unique numerical identiﬁer. The table stores
this identiﬁer and the parameters necessary to build the instance transformation
matrix. The table shows that this modeling technique contains no information about
relationships among objects. However, the table contains all the information that we
require to draw the objects and is thus a simple data structure or model for a group
of geometric objects. We could search the table for an object, change the instance
transformation for an object, and add or delete objects. However, the ﬂatness of the
representation limits us.

### 8.2       HIERARCHICAL MODELS

Suppose that we wish to build a model of an automobile that we can animate. We can
compose the model from ﬁve parts—the chassis and the four wheels (Figure 8.4)—                            FIGURE 8.4 Automobile
each of which we can describe by using our standard graphics primitives. Two frames                       model.
of a simple animation of the model are shown in Figure 8.5. We could write a program
428   Chapter 8   Modeling and Hierarchy

*FIGURE 8.5 Two frames of animation.*

to generate this animation by noting that if each wheel has a radius r, then a 360-
degree rotation of a wheel must correspond to the car moving forward (or backward)
a distance of 2πr. The program could then contain one function to generate each
wheel and another to generate the chassis. All these functions could use the same
input, such as the desired speed and direction of the automobile. In pseudocode, our
program might look like this:

```cpp
{
float s; /* speed */
float d[3]; /* direction */
float t; /* time */
```

/* determine speed and direction at time t*/
draw_right_front_wheel(s,d);
draw_left_front_wheel(s,d);
draw_right_rear_wheel(s,d);
draw_left_rear_wheel(s,d);
draw_chassis(s,d);

```cpp
}
```

This program is just the kind that we do not want to write. It is linear and shows none
of the relationships among the components of the automobile. There are two types of
relationships that we would like to exploit. First, we cannot separate the movement of
the car from the movement of the wheels. If the car moves forward, the wheels must
turn.1 Second, we would like to use the fact that all the wheels of the automobile are
identical; they are merely located in different places, with different orientations.
We can represent the relationships among parts of the models, both abstractly
and visually, with graphs. Mathematically, a graph consists of a set of nodes (or
vertices) and a set of edges. Edges connect pairs of nodes or possibly connect a node
to itself. Edges can have a direction associated with them; the graphs we use here
are all directed graphs, which are graphs that have their edges leaving one node and
entering another.
1. It is not clear whether we should say the wheels move the chassis or the chassis moves the wheels.
From a graphics perspective, the latter view is probably more useful.
                                                                                         8.3 A Robot Arm                                            429
Right-front    Left-front         Right-rear   Left-rear
wheel         wheel               wheel       wheel

*FIGURE 8.6 Tree structure for an automobile.*

The most important type of graph we use is a tree. A (connected) tree is a
directed graph without closed paths or loops. In addition, each node but one—
the root node—has one edge entering it. Thus, every node except the root has a
parent node, the node from which an edge enters, and can have one or more child
nodes, nodes to which edges are connected. A node without children is called a
terminal node, or leaf. Figure 8.6 shows a tree that represents the relationships in
our car model. The chassis is the root node, and all four wheels are its children.
Although the mathematical graph is a collection of set elements, in practice, both
the edges and nodes can contain additional information. For our car example, each                                    Chassis
node can contain information deﬁning the geometric objects associated with it. The
information about the location and orientation of the wheels can be stored either in
their nodes or in the edges connecting them with their parent.
In most cars the four wheels are identical, so storing the same information on
how to draw each one at four nodes is inefﬁcient. We can use the ideas behind the
instance transformation to allow us to use a single prototype wheel in our model.                                     Wheel
If we do so, we can replace the tree structure by the directed acyclic graph (DAG)
in Figure 8.7. In a DAG, although there are loops, we cannot follow directed edges        FIGURE 8.7 Directed-acyclic-
around any loop. Thus, if we follow any path of directed edges from a node, the path      graph (DAG) model of an auto-
terminates at another node, and in practice, working with DAGs is no more difﬁcult        mobile.
than working with trees. For our car, we can store the information that positions each
instance of the single prototype wheel in the chassis node, in the wheel node, or with
the edges.
Both forms—trees and DAGs—are hierarchical methods of expressing the re-
lationships in the physical model. In each form, various elements of a model can be
related to other parts—their parents and their children. We will explore how to ex-
press these hierarchies in a graphics program.

### 8.3    A ROBOT ARM

Robotics provides many opportunities for developing hierarchical models. Consider
the simple robot arm illustrated in Figure 8.8(a). We can model it with three simple
objects, or symbols, perhaps using only two parallelepipeds and a cylinder. Each of
the symbols can be built up from our basic primitives.
430         Chapter 8   Modeling and Hierarchy
y                    y                    y

x                    x                    x

z                       z                    z
(a)                                                 (b)

*FIGURE 8.8 Robot arm. (a) Total model. (b) Components.*

The robot arm consists of the three parts shown in Figure 8.8(b). The mechanism
has three degrees of freedom, two of which can be described by joint angles between
components and the third by the angle the base makes with respect to a ﬁxed point on
the ground. In our model, each joint angle determines how to position a component
with respect to the component to which it is attached, or in the case of the base, the
joint angle positions it relative to the surrounding environment. Each joint angle is
measured in each component’s own frame. We can rotate the base about its vertical
axis by an angle θ. This angle is measured from the x-axis to some ﬁxed point on
the bottom of the base. The lower arm of the robot is attached to the base by a joint
that allows the arm to rotate in the plane z = 0 in the arm’s frame. This rotation is
speciﬁed by an angle φ that is measured from the x-axis to the arm. The upper arm is
attached to the lower arm by a similar joint, and it can rotate by an angle ψ, measured
like that for the lower arm, in its own frame. As the angles vary, we can think of the
frames of the upper and lower arms as moving relative to the base. By controlling the
three angles, we can position the tip of the upper arm in three dimensions.
Suppose that we wish to write a program to render our simple robot model.
Rather than specifying each part of the robot and its motion independently, we take
an incremental approach. The base of the robot can rotate about the y-axis in its
frame by the angle θ. Thus, we can describe the motion of any point p on the base by
applying a rotation matrix Ry (θ ) to it.
The lower arm is rotated about the z-axis in its own frame, but this frame must
be shifted to the top of the base by a translation matrix T(0, h1, 0), where h1 is
the height above the base to the point where the joint between the base and the
lower arm is located. However, if the base has rotated, then we must also rotate
the lower arm, using Ry (θ ). We can accomplish the positioning of the lower arm
by applying Ry (θ )T(0, h1, 0)Rz (φ) to the arm’s vertices. We can interpret the matrix
Ry (θ )T(0, h1, 0) as the matrix that positions the lower arm relative to the object or
world frame and Rz (φ) as the matrix that positions the lower arm relative to the base.

*FIGURE 8.9 Movement of         Equivalently, we can interpret these matrices as positioning the frames of the lower*

robot components and frames.   arm and base relative to some world frame, as shown in Figure 8.9.
                                                                                           8.3 A Robot Arm             431
When we apply similar reasoning to the upper arm, we ﬁnd that this arm has
to be translated by a matrix T(0, h2 , 0) relative to the lower arm and then rotated
by Rz (ψ). The matrix that controls the upper arm is thus Ry (θ )T(0, h1, 0)Rz (φ)T(0,
h2 , 0)Rz (ψ). The form of the display function for an OpenGL program to display the
robot as a function of the joint angles (using the array theta[3] for θ , φ, and ψ)
shows how we can alter the model-view matrix incrementally to display the various
parts of the model efﬁciently:

```cpp
void display()
{
```

glClear(GL_COLOR_BUFFER_BIT);
model_view = RotateY(theta[0]);
base();
model_view = model_view*Translate(0.0, BASE_HEIGHT, 0.0)
*RotateZ(theta[1]);
lower_arm();
model_view = model_view*Translate(0.0, LOWER_ARM_HEIGHT, 0.0)                                        Base
*RotateZ(theta[2]);
upper_arm();
glutSwapBuffers();

```cpp
}                                                                                                     Lower arm
```

Note that we have described the positioning of the arm independently of the details of
the individual parts. As long as the positions of the joints do not change, we can alter
the form of the robot by changing only the functions that draw the three parts. This                  Upper arm
separation makes it possible to write separate functions to describe the components
and to animate the robot. Figure 8.10 shows the relationships among the parts of the        FIGURE 8.10 Tree structure
robot arm as a tree. The complete program implements the structure and uses the             for the robot arm in Figure 8.8.
mouse to animate the robot through a menu. It uses three parallelepipeds for the base
and arms. If we use our cube code from Chapter 3 with an instance transformation
for the parts, then the robot is rendered by the code:
mat4 instance;
mat4 model_view;

```cpp
void base()
{
```

instance = Translate(0.0, 0.5*BASE_HEIGHT, 0.0)
*Scale(BASE_WIDTH, BASE_HEIGHT, BASE_WIDTH);
glUniformMatrix4fv(model_view_loc, 16, GL_TRUE, model_view*instance);
glDrawArrays(GL_TRIANGLES, 0, N);

```cpp
}
void upper_arm()
{
```

instance = Translate(0.0, 0.5*UPPER_ARM_HEIGHT, 0.0)
*Scale(UPPER_ARM_WIDTH, UPPER_ARM_HEIGHT, UPPER_ARM_WIDTH);
432       Chapter 8   Modeling and Hierarchy
glUniformMatrix4fv(model_view_loc, 16, GL_TRUE, model_view*instance);
glDrawArrays(GL_TRIANGLES, 0, N);

```cpp
}
void lower_arm()
{
```

instance = Translate(0.0, 0.5*LOWER_ARM_HEIGHT, 0.0)
*Scale(LOWER_ARM_WIDTH, LOWER_ARM_HEIGHT, LOWER_ARM_WIDTH);
glUniformMatrix4fv(model_view_loc, 16, GL_TRUE, model_view*instance);
glDrawArrays(GL_TRIANGLES, 0, N);

```cpp
}
```

In each case, the instance transformation must scale the cube to the desired size,
and because the cube vertices are centered at the origin, each cube must be raised
to have its bottom in the place y = 0. The product of the model-view and instance
transformations is sent to the vertex shader followed by the vertices (and colors if
desired) for each part of the robot. Because the model-view matrix is different for
each part of the robot, we render each part once its data have been sent to the GPU.
Draw                      Note that in this example, because we are using cubes for all the parts, we need to send
the points to the GPU only once. However, if the parts were using different symbols,
M                     then we would need to use glDrawArrays in each drawing function.
Child       Child              Returning to the tree in Figure 8.10, we can look at it as a tree data structure of
nodes and edges—as a graph. If we store all the necessary information in the nodes,

*FIGURE 8.11 Node            rather than in the edges, then each node (Figure 8.11) must store at least three items:*

representation.                   1. A pointer to a function that draws the object represented by the node
2. A homogeneous-coordinate matrix that positions, scales, and orients this
node (and its children) relative to the node’s parent
3. Pointers to children of the node
Certainly, we can include other information in a node, such as a set of attributes
(color, texture, material properties) that applies to the node. Drawing an object
described by such a tree requires performing a tree traversal. That is, we must visit
every node; at each node, we must compute the matrix that applies to the primitives
pointed to by the node and must display these primitives. Our OpenGL program
shows an incremental approach to this traversal.
This example is a simple one: There is only a single child for each of the parent
nodes in the tree. The next example shows how we handle more complex models.

### 8.4     TREES AND TRAVERSAL

Figure 8.12 shows a boxlike representation of a humanoid that might be used for
a robot model or in a virtual reality application. If we take the torso as the root
element, we can represent this ﬁgure with the tree shown in Figure 8.13. Once we
have positioned the torso, the position and orientation of the other parts of the model

*FIGURE 8.12 A humanoid      are determined by the set of joint angles. We can animate the ﬁgure by deﬁning the*

figure.                     motion of its joints. In a basic model, the knee and elbow joints might each have only
                                                                                      8.4 Trees and Traversal   433
Left-upper    Right-upper     Left-upper    Right-upper
arm           arm              leg           leg
Left-lower    Right-lower     Left-lower    Right-lower
arm           arm              leg           leg

*FIGURE 8.13 Tree representation of Figure 10.12.*

a single degree of freedom, like the robot arm, whereas the joint at the neck might
have two or three degrees of freedom.
Let’s assume that we have functions, such as head and left_upper_arm, that
draw the individual parts (symbols) in their own frames. We can now build a set of
nodes for our tree by deﬁning matrices that position each part relative to its parent,
exactly as we did for the robot arm. If we assume that each body part has been deﬁned
at the desired size, each of these matrices is the concatenation of a translation matrix
with a rotation matrix. We can show these matrices, as we do in Figure 8.14, by using
the matrices to label the edges of the tree. Remember that each matrix represents the
incremental change when we go from the parent to the child.
The interesting part of this example is how we do the traversal of the tree to draw
the ﬁgure. In principle, we could use any tree-traversal algorithm, such as a depth-
ﬁrst or breadth-ﬁrst search. Although in many applications it is insigniﬁcant which
traversal algorithm is used, we will see that there are good reasons for always using
the same algorithm for traversing our graphs. We will always traverse our trees left
to right, depth ﬁrst. That is, we start with the left branch, follow it to the left as deep
Mh            Mlua                Mrua          Mlul           Mrul
Left-upper    Right-upper     Left-upper    Right-upper
arm           arm              leg           leg
Mlla         Mrla            Mlll          Mrll
Left-lower    Right-lower     Left-lower    Right-lower
arm           arm              leg           leg

*FIGURE 8.14 Tree with matrices.*

434   Chapter 8   Modeling and Hierarchy
as we can go, then go back up to the ﬁrst right branch, and proceed recursively. This
order of traversal is called a pre-order traversal.
We can write a tree-traversal function in one of two ways. We can do the traversal
explicitly in the application code, using stacks to store the required matrices and
attributes as we move through the tree. We can also do the traversal recursively. In this
second approach, the code is simpler because the storage of matrices and attributes
is done implicitly. We develop both approaches because both are useful and because
their development yields further insights into how we can build graphics systems.

#### 8.4.1 A Stack-Based Traversal

Consider the drawing of the ﬁgure by a function figure. This function might be
called from the display callback or from a mouse callback in an animation that uses
the mouse to control the joint angles. The model-view matrix, M, in effect when this
function is invoked, determines the position of the ﬁgure relative to the rest of the
scene (and to the camera). The ﬁrst node that we encounter results in the torso being
drawn with M applied to all the torso’s primitives. We then trace the leftmost branch
of the tree to the node for the head. There we invoke the function head with the
model-view matrix updated to MMh . Next, we back up to the torso node, then go
down the subtree deﬁning the left arm. This part looks just like the code for the robot
arm; we draw the left-upper arm with the matrix MMlua and the left-lower arm with
matrix MMlua Mlla . Then we move on to the right arm, left leg, and right leg. Each
time we switch limbs, we must back up to the root and recover M.
It is probably easiest to think in terms of the current transformation matrix of

## Chapter 3—the model-view matrix C that is applied to the primitives deﬁned at a

node.2 The matrix C starts out as M, is updated to MMh for the head, and later to
MMlul Mlll , and so on. The application program must manipulate C before each call
to a function deﬁning a part of the ﬁgure. Note that as we back up the tree to start the
right upper arm, we need M again. Rather than reforming it (or any other matrix we
might need to reuse in a more complex model), we can store (push) it on a stack and
recover it with pop. Here is a simple stack class with a capacity of 50 matrices:

```cpp
class matrix_stack
{
public:
static const int MAX = 50;
```

matrix_stack() {index = 0;}

```cpp
void push(const mat4& matrix);
```

mat4 pop();

```cpp
private:
```

mat4 matrices[MAX];

```cpp
int index;
};
```

2. We can ignore the projection matrix for now.
                                                                             8.4 Trees and Traversal   435

```cpp
void matrix_stack::push(const mat4& matrix)
{
```

matrices[index] = matrix;
index++;

```cpp
}
```

mat4 matrix_stack::pop()

```cpp
{
```

index--;
return matrices[index];

```cpp
}
```

Our traversal code will have translations and rotations intermixed with pushes and
pops of the model-view matrix. Consider the code (without parameter values) for
the beginning of the function figure:
mat4 model_view;
matrix_stack mvstack;
figure()

```cpp
{
```

mvstack.push(model_view);
torso();
model_view = model_view*Translate()*Rotate();
head();
model_view = mvstack.pop();
mvstack.push(model_view);
model_view = model_view*Translate()*Rotate();
left_upper_arm();
model_view = mvstack.pop();
mvstack.push(model_view);
model_view = Translate()*Rotate();
left_lower_arm();
model_view = mvstack.pop();
mvstack.push(model_view);
model_view = Translate()*Rotate();
right_upper_arm();
model_view = mvstack.pop();
mvstack.push(model_view);
.
.
.

```cpp
}
```

436   Chapter 8   Modeling and Hierarchy
The ﬁrst push duplicates the current model-view matrix putting the copy on the top
of the model-view–matrix stack. This method of pushing allows us to work immedi-
ately with the other transformations that alter the model-view matrix, knowing that
we have preserved a copy on the stack. The following calls to Translate and Ro-
tate determine Mh and concatenate it with the initial model-view matrix. We can
then generate the primitives for the head. The subsequent pop recovers the original
model-view matrix. Note that we must do another push to leave a copy of the original
model-view matrix that we can recover when we come back to draw the right leg.
The functions for the individual parts are similar to the previous example. Here
is the torso function:

```cpp
void torso()
{
```

mvstack.push(model_view);
instance = Translate(0.0, 0.5*TORSO_HEIGHT,0.0)
*Scale(TORSO_WIDTH, TORSO_HEIGHT, TORSO_WIDTH);
glUniformMatrix4fv(model_view_loc, 16, GL_TRUE, model_view*instance);
colorcube();
glDrawArrays(GL_TRIANGLES, 0, N);
model_view = mvstack.pop();

```cpp
}
```

Note the use of a push at the beginning and a pop at the end of the function. These
serve to isolate the code and protect other parts of the program from being affected
by state changes in this function. You should be able to complete this function by
continuing in a similar manner.
Appendix A contains a complete program that implements this ﬁgure with a
menu that will allow you to change the various joint angles. The individual parts
are implemented using parallelepipeds, and the entire model can be shaded as we
discussed in Chapter 5.
We have not considered how attributes such as color and material properties are
handled by our traversal of a hierarchical model. Attributes are state variables: Once
set, they remain in place until changed again. Hence, we must be careful as we traverse
our tree. For example, suppose that within the code for torso we set the color to red
and then within the code for head set the color to blue. If there are no other color
changes, the color will still be blue as we traverse the rest of the tree and may remain
blue after we leave the code for figure. Here is an example in which the particular
traversal algorithm can make a difference, because the current state can be affected
differently depending on the order in which the nodes are visited.
This situation may be disconcerting, but there is a solution. We can create other
stacks that allow us to deal with attributes in a manner similar to our use of the
model-view matrix. If we push the attributes on the attribute stack on entrance to the
function figure, and pop on exit, we have restored the attributes to their original
state. Moreover, we can use additional pushes and pops within figure to control
how attributes are handled in greater detail.
                                                                          8.5 Use of Tree Data Structures                      437
In a more complex model, we can apply these ideas recursively. If, for example,
we want to use a more detailed model of the head—one incorporating eyes, ears, a
nose, and a mouth—then we could model these parts separately. The head would
then itself be modeled hierarchically, and its code would include the pushing and
popping of matrices and attributes.
Although we have discussed only trees, if two or more nodes call the same
function, we really have a DAG, but DAGs present no additional difﬁculties.
Color Plates 22 and 27 show hierarchical models of robots and ﬁgures used in
simulations. These objects were created with high-level interactive software that relies
on our ability to traverse hierarchical structures to render the models.
The approach that we used to describe hierarchical objects is workable but has
limitations. The code is explicit and relies on the application programmer to push and
pop the required matrices and attributes. In reality, we implemented a stack-based
representation of a tree. The code was hardwired for the particular example and thus
would be difﬁcult to extend or use dynamically. The code also does not make a clear
distinction between building a model and rendering it. Although many application
programmers write code in this form, we prefer to use it primarily to illustrate the
ﬂow of an OpenGL program that implements tree hierarchies. We now turn to a more
general and powerful approach to working with tree-structured hierarchies.

### 8.5    USE OF TREE DATA STRUCTURES

Our second approach is to use a standard tree data structure to represent our hierar-
chy and then to render it with a traversal algorithm that is independent of the model.
We use a left-child, right-sibling structure.
Consider the alternate representation of a tree in Figure 8.15. It is arranged                           (a)
such that all the elements at the same level are linked left to right. The children of
a given node are represented as a second list arranged from the leftmost child to                      Root
the rightmost. This second list points downward in Figure 8.15. This representation
describes the structure of our hierarchical ﬁgure, but the structure still lacks the
graphical information.
At each node, we must store the information necessary to draw the object: a func-
tion that deﬁnes the object and the homogeneous coordinate matrix that positions
the object relative to its parent. Consider the following node structure:
(b)

```cpp
{
```

mat m;                                                                                  FIGURE 8.15 (a) Tree.

```cpp
void (*f)();                                                                            (b) Left-child, right-sibling
struct treenode *sibling;                                                               representation.
struct treenode *child;
} treenode;
```

438   Chapter 8   Modeling and Hierarchy
The array m stores a 4 × 4 homogeneous coordinate matrix. When we render the
node, this matrix must ﬁrst multiply the current model-view matrix; then the func-
tion f, which includes the graphics primitives, is executed. We also store a pointer to
the sibling node on the right and a pointer to the leftmost child. If one or the other
does not exist, then we store the null pointer (NULL). For our ﬁgure, we specify 10
nodes corresponding to the 10 parts of our model:
treenode torso_node, head_node, lua_node, rua_node, lll_node,
rll_node, lla_node, rla_node, rul_node, lul_node;
We can specify the nodes either in the main function or in myinit. For example,
consider the root of the ﬁgure tree—the torso node. It can be oriented by a rotation
about the y-axis. We can form the required rotation matrix using our matrix func-
tions, and the function to be executed after forming the matrix is torso. The torso
node has no siblings, and its leftmost child is the head node, so the torso node is given
as follows:
torso_node.m = RotateY(theta[0]);
torso_node.f = torso;
torso_node.sibling = NULL;
torso_node.child = &head_node;
If we use a cube as the basis for the torso, the drawing function might look as follows:

```cpp
void torso()
{
```

mvstack.push(model_view);
instance = Translate(0.0, 0.5*TORSO_HEIGHT, 0.0)
*Scale(TORSO_WIDTH, TORSO_HEIGHT, TORSO_WIDTH);
glUniformMatrix4fv(model_view_loc, 16, GL_TRUE, model_view*instance);
colorcube();
glDrawArrays(GL_TRIANGLES, 0, N);
model_view = mvstack.pop();

```cpp
}
```

The instance transformation ﬁrst scales the cube to the desired size and then trans-
lates it so its bottom lies in the plane y = 0.
The torso is the root node of the ﬁgure, so its code is a little different from the
other nodes. Consider the speciﬁcation for the left-upper arm node:
lua_node.m = Translate(-(TORSO_WIDTH+UPPER_ARM_WIDTH),
0.9*TORSO_HEIGHT, 0.0)*RotateX(theta[3]);
lua_node.f = left_upper_arm;
lua_node.sibling = &rua_node;
lua_node.child = &lla_node;
and the left_upper_arm function
                                                                          8.5 Use of Tree Data Structures   439

```cpp
void left_upper_arm()
{
```

mvstack.push(model_view);
instance = Translate(0.0, 0.5*UPPER_ARM_HEIGHT, 0.0)
*Scale(UPPER_ARM_WIDTH, UPPER_ARM_HEIGHT, UPPER_ARM_WIDTH);
glUniformMatrix4fv(model_view_loc, 16, GL_TRUE, model_view*instance);
colorcube();
glDrawArrays(GL_TRIANGLES, 0, N);
model_view = mvstack.pop();

```cpp
}
```

The upper arm must be translated relative to the torso and its own width to get
the center of rotation in the correct place. The node for the upper arm has both a
sibling (the upper right arm) and a child (the lower left arm). To render the left upper
arm, we ﬁrst compute an instance transformation that gives it the desired size and
positions so its bottom is also on the plane y = 0. This instance matrix is concatenated
with the current model-view matrix to position the upper left arm correctly in object
coordinates. The other nodes are speciﬁed in a similar manner.
Traversing the tree in the same order (preorder traversal) as in Section 8.4 can be
accomplished by the recursive code as follows:

```cpp
void traverse(treenode* root)
{
```

if (root == NULL) return;
mvstack.push(model_view);
model_view = model_view*root->m;
root->f();
if (root->child != NULL) traverse(root->child);
model_view = mvstack.pop();
if (root->sibling != NULL) traverse(root->sibling);

```cpp
}
```

To render a nonnull node, we ﬁrst save the graphics state with mvpush(model_
view). We then use the matrix at the node to modify the model-view matrix. We
then draw the objects at the node with the function pointed to by f. Finally, we
traverse all the children recursively. Note that because we have multiplied the model-
view matrix by the local matrix, we are passing this altered matrix to the children.
For the siblings, however, we do not want to use this matrix, because each has its own
local matrix. Hence, we must return to the original state (mvstack.pop()) before
traversing the children. If we are changing attributes within nodes, either we can push
and pop attributes within the rendering functions, or we can push the attributes when
we push the model-view matrix.
One of the nice aspects of this traversal method is that it is completely indepen-
dent of the particular tree; thus, we can use a generic display callback such as the
following:
440   Chapter 8   Modeling and Hierarchy

```cpp
void display(void)
{
```

glClear(GL_COLOR_BUFFER_BIT|GL_DEPTH_BUFFER_BIT);
traverse(&torso_node);
glutSwapBuffers();

```cpp
}
```

We again animate the ﬁgure by controlling individual joint angles, which are
selected from a menu and are incremented and decremented through the mouse
buttons. Thus, the dynamics of the program are in the mouse callback, which changes
an angle, recomputes the appropriate node matrix, and then posts a redisplay:
mymouse(int button, int state, int x, int y)

```cpp
{
```

if (button == GLUT_LEFT_BUTTON && state == GLUT_DOWN)

```cpp
{
```

theta[angle] += 5.0;
if (theta[angle] > 360.0 ) theta[angle] -= 360.0;

```cpp
}
```

if (button == GLUT_RIGHT_BUTTON && state == GLUT_DOWN)

```cpp
{
```

theta[angle] -= 5.0;
if (theta[angle] < 360.0 ) theta[angle] += 360.0;

```cpp
}
```

mvstack.push(model_view);
switch (angle)

```cpp
{
```

case 0 :
torso_node.m = RotateY(theta[0]);
break;
case 1 : case 2 :
head_node.m = Translate(0.0, TORSO_HEIGHT+0.5*HEAD_HEIGHT, 0.0)
*RotateX(theta[1])*RotateY(theta[2])
*Translate(0.0, -0.5*HEAD_HEIGHT, 0.0);
break;

```cpp
// rest of cases
}
}
```

There is one more feature that we can add to show the ﬂexibility of this approach.
As the program executes, we can add or remove dynamic nodes rather than static
nodes. We can create dynamic nodes with the following code:
typedef treenode* tree_ptr;
tree_ptr torso_ptr = new treenode;
Nodes are deﬁned as before. For example,
                                                                                             8.6 Animation   441
lua_node->m = Translate(-(TORSO_WIDTH+UPPER_ARM_WIDTH),
0.9*TORSO_HEIGHT, 0.0)*RotateX(theta[3]);
lua_node->f = left_upper_arm;
lua_node->sibling = &rua_node;
lua_node->child = &lla_node;
traverse(torso_ptr);
For our ﬁgure example, there is no particular advantage to the dynamic ap-
proach. In a more general setting, however, we can use the dynamic approach to
create structures that change interactively. For example, we can use this form to write
an application that will let us edit ﬁgures, adding and removing parts as desired. This
type of implementation is the basis for the scene trees that we discuss in Section 8.8.
Color Plate 27 shows one frame of an animation with the ﬁgure using cylinders for
most of the parts and adding lighting.
Note that as we have coded our examples, there is a ﬁxed traversal order for the
graph. If we had applied some other traversal algorithm, we could have produced
a different image if we made any state changes within the graph, such as changing
transformations or attributes. We can avoid some of these potential problems if we
are careful to isolate parts of our code by pushing and popping attributes and matrices
in each node (although there is a performance penalty for doing so too often).

### 8.6    ANIMATION

The models that we developed for our two examples—the robot arm and the ﬁgure—
are articulated: The models consist of rigid parts connected by joints. We can make
such models change their positions in time—animate them—by altering the values
of a small set of parameters. Hierarchical models allow us to model the compound
motions incorporating the physical relationships among the parts of the model. What
we have not discussed is how to alter the parameters over time so as to achieve the
desired motion.
Of the many approaches to animation, a few basic techniques are of particular
importance when we work with articulated ﬁgures. These techniques arise both from
traditional hand animation and from robotics.
In the case of our robot model, consider the problem of moving the tip of the
upper arm from one position to another. The model has three degrees of freedom—
the three angles that we can specify. Although each set of angles has a unique position
for the tip, the converse is not true. Given a desired position of the tip of the arm,
there may be no set of angles that place the tip as desired, a single set of angles that
yields the speciﬁed position, or multiple sets of angles that place the tip at the desired
position.
Studying kinematics involves describing the position of the parts of the model
based on only the joint angles. We can use our hierarchical-modeling methods either
442   Chapter 8   Modeling and Hierarchy
to determine positions numerically or to ﬁnd explicit equations that give the position
of any desired set of points in the model in terms of the joint angles. Thus, if θ is
an array of the joint angles and p is an array whose elements are the vertices in our
model, a kinematic model is of the form
p = f (θ ).
Likewise, if we specify the rates of change of the joint angles—the joint velocities—
then we can obtain velocities of points on the model.
The kinematic model neglects matters such as the effects of inertia and friction.
We could derive more complex differential equations that describe the dynamic be-
havior of the model in terms of applied forces—a topic that is studied in robotics.
Whereas both kinematics and dynamics are ways of describing the forward be-
havior of the model, in animation we are more concerned with inverse kinematics
and inverse dynamics: Given a desired state of the model, how can we adjust the joint
angles so as to achieve this position? There are two major concerns. First, given an en-
vironment including the robot and other objects, we must determine whether there
exists a sequence of angles that achieves the desired state. There may be no single-
θ = f −1(p).
For a given p, in general we cannot tell if there is any θ that corresponds to this
position or if there are multiple values of θ that satisfy the equation. Even if we can
ﬁnd a sequence of joint angles, we must ensure that as we go through this sequence
our model does not hit any obstacles or violate any physical constraints. Although,
for a model as simple as our robot, we might be able to ﬁnd equations that give the
joint angles in terms of the position, we cannot do so in general, because the forward
equations do not have unique inverses. The ﬁgure model, which has 11 degrees of
freedom, should give you an idea of how difﬁcult it is to solve this problem.
A basic approach to overcoming these difﬁculties comes from traditional hand-
animation techniques. In key-frame animation, the animator positions the objects
at a set of times—the key frames. In hand animation, animators then can ﬁll in the
remaining frames, a process called in-betweening. In computer graphics, we can
automate in-betweening by interpolating the joint angles between the key frames or,
equivalently, using simple approximations to obtain the required dynamic equations
between key frames. Using GPUs, much of the work required for in-betweening
can now be automated as part of the pipeline, often using the programmability of
recent GPUs. We can also use the spline curves that we develop in Chapter 10 to give
smooth methods of going between key frames. Although we can develop code for
the interpolation, both a skillful (human) animator and good interactive methods
are crucial if we are to choose the key frames and the positions of objects in these
frames.
                                                                                       8.7 Graphical Objects   443

### 8.7    GRAPHICAL OBJECTS

Although we have introduced multiple graphics paradigms, our development has
been heavily based on a pipeline implementation of the synthetic-camera model.
We made this choice because we want to support interactive three-dimensional ap-
plications with currently available hardware and software. Consequently, we have
emphasized a mode of graphics in which geometric data are placed on the GPU and
rendered to the screen almost immediately afterward. We have not made full use of
the fact that once data are on the GPU, they can be reused without regenerating them
in the application.
In addition, our desire to present the basics of implementation has led us to de-
velop graphics in a manner that was never far from the details of the implementation.
For all its beneﬁts, this approach has not let us exploit many high-level alternatives to
developing graphical applications.
Now we move to a higher level of abstraction and introduce two major concepts.
First, we expand our notion of objects from geometric objects, such as polygons
and vectors, to include most of the elements within a graphics program, such as
viewers, lights, and material properties. Second, we focus on objects that exist even
after their images have been drawn and even if we never display them. We investigate
other approaches, such as the use of classes in C++ or structures in C. Although the
OpenGL API does not support this approach directly, we do not have to abandon
OpenGL. We still use OpenGL for rendering, and we regard what we develop as a
software layer on top of OpenGL.

#### 8.7.1 Methods, Attributes, and Messages

Our programs manipulate data. The data may be in many forms, ranging from
numbers to strings to the geometric entities that we build in our applications. In
traditional imperative programming, the programmer writes code to manipulate
the data, usually through functions. The data are passed to a function through the
function’s parameters. Data are returned in a similar manner. To manipulate the data
sent to it, the function must be aware of how those data are organized. Consider,
for example, the cube that we have used in many of our previous examples. We
have seen that we can model it in various ways, including with vertex pointers, edge
lists, and lists of polygon vertices. The application programmer may care little about
which model is used and may prefer to regard the cube as an atomic entity or an
object. In addition, she may care little about the details of how the cube is rendered
to the screen: which shading model or which polygon-ﬁll algorithm is used. She
can assume that the cube “knows how to render itself ” and that conceptually the
rendering algorithm is tied to the object itself. In some ways, OpenGL supports this
view by using the state of the graphics system to control rendering. For example, the
color of the cube, its orientation, and the lights that are applied to its surfaces can all
be part of the state of the graphics system and may not depend on how the cube is
modeled.
444   Chapter 8   Modeling and Hierarchy

*FIGURE 8.16 Imperative programming paradigm.*


*FIGURE 8.17 Object-oriented paradigm.*

However, if we are working with a physical cube, we might ﬁnd this view a
bit strange. The location of a physical cube is tied to the physical object, as are its
color, size, and orientation. Although we could use OpenGL to tie some properties
to a virtual cube—through pushing and popping various attributes and matrices—
the underlying programming model does not support these ideas well. For example,
a function that transforms the cube would have to know exactly how the cube is
represented and would work as shown in Figure 8.16.
The application programmer would write a function that would take as its inputs
a pointer to the cube’s data and the parameters of the transformation. It would then
manipulate the data for the cube and return control to the application program
(perhaps also returning some values).
Object-oriented design and object-oriented programming look at manipulation
of objects in a fundamentally different manner. Even in the early days of object-
oriented programming, languages such as Smalltalk recognized that computer graph-
ics provides excellent examples of the power of the object-oriented approach. Recent
trends within the software community indicate that we can combine our pipeline
orientation with an object orientation to build even more expressive and high-
performance graphics systems.
Object-oriented programming languages deﬁne objects as modules with which
we build programs. These modules include the data that deﬁne the module, such as
the vertices for our cube, properties of the module (attributes), and the functions
(methods) that manipulate the module and its attributes. We send messages to
objects to invoke a method. This model is shown in Figure 8.17.
The advantage to the writer of the application program is that she now does not
need to know how the cube is represented; she needs to know only what functionality
the cube object supports—what messages she can send to it.
Although the C struct has some of the properties of objects, the C language
does not support the full power of an object-oriented approach. In C++, the struct
is replaced with the class. C++ classes have two important properties that we can
                                                                                   8.7 Graphical Objects   445
exploit to get the ﬂavor of the object-oriented approach. C programmers should have
no trouble understanding these concepts.

#### 8.7.2 A Cube Object

Suppose that we wish to create a cube object in C that has a color attribute and a
homogeneous coordinate transformation associated with it. In C, we could use a

```cpp
struct of the following form:
struct cube
{
float color[3];
float matrix[4][4];
```

/* implementation goes here */

```cpp
}
```

The implementation part of the structure contains the information on how a cube
is actually represented. Typically, an application programmer does not need this
information and needs to change only the color or matrix associated with the
cube.
Once the struct has been deﬁned, instances of the cube object can be created
as are other basic data types:
cube a, b;
Attributes that are part of the class deﬁnition can be changed for each instance of the
cube. Thus, we can set the color of cube a to red as follows:
a.color[0] = 1.0;
a.color[1] = a.color[2] = 0.0;
It should be clear how such a struct can be implemented within an OpenGL system.
Although we have created a retained cube object, we are limited in how we can
manipulate it or render it. We could write a function that would render the cube
render_cube(a);
Or we could rotate the cube by invoking the method
rotate_cube(a, theta, d);
where d is the vector about which we wish to rotate.
446   Chapter 8   Modeling and Hierarchy
This approach is workable but has limitations. One is that we need separate
rendering and rotation functions for each type of object. A second is that the imple-
mentation part of the code is accessible to application programs. C++ classes solve
both of these problems. A C++ class can have public, private, and protected mem-
bers. The public members are similar to the members of C struct and can be altered
by any function. The private members neither are visible to nor can be altered by a
function that uses the class. The protected members are visible to classes within the
same hierarchy. A programmer can also declare classes to be friends to give access to
speciﬁc classes. Thus, in C++, we deﬁne the cube as

```cpp
class cube
{
public:
```

vec4 color;
mat4 model;

```cpp
private:
```

/* implementation goes here */

```cpp
}
```

thereby protecting and hiding the details of the implementation. Furthermore, C++

```cpp
classes allow us to have members that are functions or methods. Once we add such
```

functions, the object-orientation becomes clearer. Suppose that we add member
functions to the public part of the cube class as follows:

```cpp
void render();
void translate(float x, float y, float z);
void rotate(float theta, float axis_x, float axis_y,
float axis_z);
```

Now an application program could create, translate, rotate, and render a cube
through the following code:
cube a;
a.rotate(45.0, 1.0, 0.0, 0.0);
a.translate(1.0, 2.0, 3.0);
a.render();
Conceptually, this code assumes that an instance of the cube “knows” how to rotate
itself and that by executing the code a.rotate, we are sending a message to a cube
object that we would like it to carry out such a rotation. We could easily write an
implementation of the rotation and translation methods that would use our rotation
and translation functions to change the matrix member of the cube object.
It is less clear what a function call such as a.render really does. Each instance of
an object persists and can be altered by further code in the application program. What
is most important is that we have created an object that continues to exist somewhere
                                                                                      8.7 Graphical Objects   447
in our system in a form that is not visible to the application program. The attributes
of the object are also in the system and can be altered by functions such as rotate.
The render function causes the object to be redrawn using the object’s state rather
than the system’s current state. Hence, the render step involves sending data to the
GPU with the necessary vertex attribute data coming from the implementation part
of the object and using a function such as glDrawArrays to display the object.

#### 8.7.3 Implementing the Cube Object

As an example of the choices that go into developing the private part of an object,
let’s consider the cube. One basic implementation would be similar to what we did
for our rotating cube examples; we look at the cube as comprised of six faces, each
of which consists of two triangles. Thus, the private part of the cube might be of the

```cpp
private:
```

vec4 points[36];
vec4 colors[36];
Note that we are allowing for different colors at each vertex. The constructor for a
cube would set values for the points with a default of a unit cube.
We can do far better if we include more information in the implementation. Of
particular interest is information that might help in the rendering. Thus, whereas
OpenGL will do hidden-surface removal correctly through the z-buffer algorithm, we
can often do much better by eliminating objects earlier through a separate visibility
test, as we will discuss in Section 8.11. To support this functionality, we might want to
include information that determines a bounding volume for the object. For example,
we can include the axis-aligned bounding box for objects within the private part of
the code. For polygonal objects, we need simply save the minimum and maximum of
the x, y, and z of the vertices (after they have been transformed by any transformation
matrix stored with the object).

#### 8.7.4 Objects and Hierarchy

One of the major advantages of object-oriented design is the ability to reuse code
and to build more sophisticated objects from a small set of simple objects. As in
Section 8.4, we can build a ﬁgure object from cubes and have multiple instances of
this new object, each with its own color, size, location, and orientation. A class for the
humanoid ﬁgure could refer to the classes for arms and legs; the class for a car could
refer to classes for wheels and a chassis. Thus, we would once more have tree-like
representations similar to those that we developed in Section 8.5.
Often in object-oriented design, we want the representations to show relation-
ships more complex than the parent–child relationship that characterizes trees. As we
have used trees, the structure is such that the highest level of complexity is at the root
and the relationship between a parent and child is a “has-a” relationship. Thus, the
stick ﬁgure has two arms and two legs, whereas the car has four wheels and a chassis.
448   Chapter 8   Modeling and Hierarchy
We can look at hierarchy in a different manner, with the simplest objects being
the top of the hierarchy and the relationship between parents and children being an
“is-a” relationship. This type of hierarchy is typical of taxonomies. A mammal is an
animal. A human is a mammal. We used this relationship in describing projections. A
parallel projection is a planar geometric projection; an oblique projection is a parallel
projection. “Has-a” relationships allow us to deﬁne multiple complex objects from
simpler objects and also allow the more complex object to inherit properties from
the simpler object. Thus, if we write the code for a parallel projection, the oblique-
projection code can use this code and reﬁne only the parts that are necessary to
convert the general parallel projection to an oblique one. For geometric objects, we
can deﬁne base objects with a default set of properties such as their color and material
properties. An application programmer could then use these properties or change
them in subobjects. These concepts are supported by languages such as C++ that
allow for subclasses and inheritance.

#### 8.7.5 Geometric Objects

Suppose that we now want to build an object-oriented graphics system. What objects
should we include? Although it is clear that we want to have objects such as points,
vectors, polygons, rectangles, and triangles (possibly using subclasses), it is less clear
how we should deal with attributes, light sources, and viewers. For example, should
a material property be associated with an object such as a cube, or is it a separate
object? The answer can be either or both. We could create a cube class in which there
is a member for each of the ambient, diffuse, and specular material properties that
we introduced with the Phong model in Chapter 5. We could also deﬁne a material

```cpp
class using code such as the following:
class material
{
public:
```

vec4 specular;

```cpp
float shininess;
```

vec4 diffuse;
vec4 ambient;

```cpp
}
```

We could then assign the material to a geometric object through a member function
of the cube class as follows:
cube a;
material b;
a.setMaterial(b);
Light sources are geometric objects—they have position and orientation among their
features—and we can easily add a light source object:
                                                                                            8.8 Scene Graphs   449

```cpp
class light
{
public:
```

boolean type;
boolean near;
vec4 position;
vec4 orientation;
vec4 specular;
vec4 diffuse;
vec4 ambient;

```cpp
}
```

Once we have built up a collection of geometric objects, we can use it to describe a
scene. To take advantage of the hierarchical relationships that we have introduced, we
develop a new tree structure called a scene graph.

### 8.8    SCENE GRAPHS

If we think about what goes into describing a scene, we can see that in addition to
our graphical primitives and geometric objects derived from these primitives, we
have other objects, such as lights and a camera. These objects may also be deﬁned
by vertices and vectors and may have attributes, such as color, that are similar to the
attributes associated with geometric primitives. It is the totality of these objects that
describes a scene, and there may be hierarchical relationships among these objects.
For example, when a primitive is deﬁned in a program, the camera parameters that
exist at that time are used to form the image. If we alter the camera lens between the
deﬁnition of two geometric objects, we may produce an image in which each object
is viewed differently. Although we cannot create such an image with a real camera,
the example points out the power of our graphics systems. We can extend our use of
tree data structures to describe these relationships among geometric objects, cameras,
lights, and attributes.
Knowing that we can write a graphical application program to traverse a graph,
we can expand our notion of the contents of a graph to describe an entire scene.
One possibility is to use a tree data structure and to include various attributes at
each node—in addition to the instance matrix and a pointer to the drawing function.
Another possibility is to allow new types of nodes, such as attribute-deﬁnition nodes
and matrix-transformation nodes. Consider the tree in Figure 8.18. Here we have set
up individual nodes for the colors and for the model-view matrices. The place where
there are branches at the top can be considered a special type of node, a group node
whose function is to isolate the two children. The group node allows us to preserve
the state that exists at the time that we enter a node and thus isolates the state of
the subtree beginning at a group node from the rest of the tree. Using our preorder
traversal algorithm, the corresponding application code is of the following form:
450   Chapter 8   Modeling and Hierarchy
Group                                    Group
Color        Translate      Object 1        Translate           Object 3
Object 2

*FIGURE 8.18 Scene tree.*

object1
object2
object3
The group nodes correspond to the OpenGL push and pop functions. This code
preserves and restores both the attributes and the model-view matrix before exiting.
It sets a drawing color that applies to the rest of the tree and traverses the tree in a
manner similar to the ﬁgure example.
We can go further and note that we can use the attribute and matrix stacks to
store the viewing conditions; thus, we can create a camera node in the tree. Although
we probably do not want a scene in which individual objects are viewed with different
cameras, we may want to view the same set of objects with multiple cameras, produc-
ing, for example, the multiview orthographic projections and isometric view that are
used by architects and engineers. Such images can be created with a scene graph that
has multiple cameras.
The scene graph we have just described is equivalent to an OpenGL program in
the sense that we can use the tree to generate the program in a totally mechanical
fashion. This approach was taken by Open Inventor and later by Open Scene Graph
(OSG), both object-oriented APIs that were built on top of OpenGL. Open Inventor
                                                                                  8.9 Open Scene Graph             451
and OSG programs build, manipulate, and render a scene graph. Execution of a pro-
gram causes traversal of the scene graph, which in turn executes graphics functions
that are implemented in OpenGL.
The notion of scene graphs couples nicely with the object-oriented paradigm

```cpp
introduced in Section 8.7. We can regard all primitives, attributes, and transforma-
```

High-level API
tions as software objects, and we can deﬁne classes to manipulate these entities. From
this perspective, we can make use of concepts such as data encapsulation to build up
scenes of great complexity with simple programs that use predeﬁned software ob-
jects. We can even support animations through software objects that appear as nodes                 OpenGL
in the scene graph but cause parameters to change and the scene to be redisplayed.
Although, in Open Inventor, the software objects are rendered using OpenGL, the
scene graph itself is a database that includes all the elements of the scene. OpenGL is
the rendering engine that allows the database to be converted to an image, but it is
not used in the speciﬁcation of the scene. Game engines employ a similar strategy in
which the game play modiﬁes a scene graph that can be traversed and rendered at an

```cpp
interactive rate.                                                                                   Hardware
```

Graphics software systems are evolving to the conﬁguration shown in Fig-
ure 8.19. OpenGL is the rendering engine. It usually sits on top of another layer         FIGURE 8.19 Modern
known as the hardware abstraction layer (HAL), which is a virtual machine that            graphics architecture.
communicates with the physical hardware. Above OpenGL is an object-oriented layer
that supports scene graphs and a storage mechanism. User programs can be written
for any of the layers, depending on what facilities are required by the application.

### 8.9   OPEN SCENE GRAPH

Open Scene Graph is probably the most popular of the full scene graph APIs and
provides much of the functionality lacking in our example. In addition to supporting
a wider variety of nodes, there are two additional concepts that are key to OSG.
First, one of the beneﬁts of a higher level of software than OpenGL is that such
software can balance the workload between the CPU and the graphics processor.
Consider how we process geometry in OpenGL. An application produces primitives
that are speciﬁed through sets of vertices. As we have seen, OpenGL’s main concern is
rendering. All geometric primitives pass down at least part of the pipeline. It is only
at the end of vertex processing that primitives that lie outside the view volume are
clipped out. If a primitive is blocked from the viewer by another opaque geometric
object and cannot appear in the ﬁnal image, it nevertheless passes through most of
the pipeline and only during hidden-surface removal will it be eliminated. Although
present GPUs can process millions of vertices per second, many applications have
such complex geometry that even these GPUs cannot render the geometry at a sufﬁ-
ciently high frame rate. OSG uses two strategies, occlusion culling and level of detail
rendering, to lower the rendering load.
Occlusion culling seeks to eliminate objects that cannot be visible because they
are blocked by other objects before they enter the rendering pipeline. In Figure 8.20,
452   Chapter 8   Modeling and Hierarchy

*FIGURE 8.20 Occlusion.*

we see that the square lies in the view volume and blocks the triangle from view.
Although the z-buffer algorithm would yield a correct rendering, because OpenGL
processes each object independently it cannot discover the occlusion. However, all
the geometry is stored in the OSG scene graph, as is the information on the viewer.
Hence, OSG can use one of many algorithms to go through the scene graph and cull
objects. We will examine an approach to occlusion culling that uses binary spatial
partitioning in Section 8.11.
The second strategy is based on an argument similar to the one that we used
to justify mipmaps for texture mapping. If we can tell that a geometric object will
render to a small area of the display, we do not need to render all the detail that might
be its geometry. Once more, the necessary information can be placed in the scene
graph. OSG has a level-of-detail node whose children are the models of an object
with different levels of geometric complexity. The application program sets up these
nodes. During the traversal of the scene graph, OSG determines which level of detail
to use.
Level-of-detail rendering is important not only to OSG but also to real-time ap-
plications such as interactive games that are built using proprietary game engines.
Game engines are very large complex software objects that may comprise millions
of lines of code. Although a game engine may use OpenGL or DirectX to render the
graphics and make extensive use of programmable shaders, a game engine also has
to handle the game play and manage complex interactions that might involve multi-
ple players. Game engines use scene graphs to maintain all the needed information,
including the geometry and texture maps, and use level of detail extensively in pro-
cessing their scene graphs. In the next section, we will examine some related issues
involving graphics over the Internet.
The second major concept is how the scene graph is processed for each frame.
OSG uses three traversals rather than the single traversal in our simple scene graph.
The goal of the traversal process is to create a list of the geometry that can be rendered.
This list contains the geometry at the best level of detail and only the geometry that
has survived occlusion culling. In addition, the geometry has been sorted so that
translucent surfaces will be rendered correctly.
                                                                         8.10 Graphics and the Internet   453
The ﬁrst traversal deals with updates to the scene graph that might be generated
by callbacks that handle interaction or changes to the geometry from the application
program. The second traversal builds a list of the geometry that has to be rendered.
This traversal uses occlusion culling, translucency, level of detail, and bounding vol-
umes. The ﬁnal traversal goes through the geometry list and issues the necessary
OpenGL calls to render the geometry.

### 8.10    GRAPHICS AND THE INTERNET

Before leaving the subject of scene graphs, we consider some of the issues that govern
how graphics can be conveyed over the Internet. Of particular interest are multiplayer
games that can involve thousands of concurrent participants, each of whom poten-
tially can affect the scene graph of any other participant, and applications with large
models that may be distributed over multiple sites.
The Internet has had an enormous effect on virtually all communications and
computer applications. It allows us to communicate information in a multitude of
forms and makes possible new methods of interaction. In order to use the Internet
to its full potential, we need to move graphical information efﬁciently, to build ap-
plications that are viewable from many locations, and to access resources distributed
over many sites. OpenGL and its extensions have had a major inﬂuence on the de-
velopment of net-based three-dimensional applications and standards. We will take
a graphics-oriented approach and see what extensions we need to develop Internet
applications. Some of the concepts will be familiar. We use the client–server model to
allow efﬁcient rendering. We also look at how we can implement graphical applica-
tions that are independent of the API.

#### 8.10.1 Hypermedia and HTML

As the Internet evolved, a series of standard high-level protocols became widely
accepted for transferring mail, ﬁles, and other types of information. Systems such
as the X Window system allowed users to open windows on remote systems and
to transfer basic graphical information. As the Internet grew, however, more and
more information became publicly available, and users needed more sophisticated
methods to share information that was stored in a distributed way in diverse formats.
There are three key elements necessary for sharing such information: (1) an
addressing scheme that allows users to identify resources over the network, (2) a
method of encoding information in addition to simple text, such as pictures and
references (or links) to other resources, and (3) a method of searching for resources

```cpp
interactively.
```

The ﬁrst two needs were addressed by researchers at the European Particle Phys-
ics Center (CERN), who created the World Wide Web, which is essentially a net-
worked hypertext system. Resources—ﬁles—are identiﬁed by a unique Uniform Re-
source Locator (URL) that consists of three parts: the protocol for transferring the
document, the server where the document is located, and the location on the server
where the document is to be found. For example, the URL for support for this text
454   Chapter 8   Modeling and Hierarchy
can be found at http://www.cs.unm.edu/∼angel/BOOK. The ﬁrst part (http) indi-
cates that the information will be transferred using the hypertext transfer protocol
http. The second part (www.cs.unm.edu) identiﬁes the server as the World Wide Web
site of the Computer Science Department at the University of New Mexico. The ﬁnal
part indicates that the information is stored in user account angel under the directory

```cpp
public_html/BOOK. Because no document is indicated, a default document—the
```

home page—will be displayed.
The second contribution of CERN was the Hypertext Markup Language
(HTML), which provided a simple way to describe a document consisting of text,
references to other documents (links), and images. HTML documents are text docu-
ments, typically in ASCII code or one of the standard extended character sets.
The combination of URL addressing and HTML documents provided a way
of making resources available. But until the National Center for SuperComputer
Applications (NCSA) came up with its browser, Mosaic, it was not easy for a user
to ﬁnd resources and to search the Web. Browsers are interactive programs that
allow the user to search for and download documents on the Web. Mosaic, and later
Netscape Navigator, opened the door to “surﬁng” the Web.

#### 8.10.2 Java and Applets

One issue with interactive computer graphics on the Internet is the heterogenous
collection of computer hardware and operating systems. An application programmer
cannot create an application that will execute on any machine connected to the Web,
even if they use an industry standard API like OpenGL.
Java solves a portion of this problem by creating a machine in software that
can be implemented on any computer. Java programs are compiled into byte code
that can be run on any Java machine, regardless of what the underlying hardware is.
Thus, the client and server exchange byte code over the Web. Small programs in byte
code, called applets, are understood by the standard Web browsers and have added a
tremendous amount of dynamic behavior to the Web.

#### 8.10.3 Interactive Graphics and the Web

While HTML is useful for structuring information for layout on a rendered Web page,
until HTML version 5 it lacked any rendering capabilities. Most often, any interactive
rendering that was done in a browser was originally done using a Java applet; however,
the requirement of downloading an applet combined with support of Java virtual ma-
chines dimished Java’s promise of application portability, particularly when related to
three-dimensional computer graphics.
As Web browsers became more capable of supporting various formats, other
technologies evolved. Adobe’s Flash technology uses a plugin to a Web browser to
provide interactivity, but it lacks (at the time of this writing) a comprehensive solu-
tion for doing three-dimensional graphics, with most of its focus on rendering video
or simple user interfaces.
Another technology that evolved was JavaScript. JavaScript is a derivative lan-
guage of Java that is interpreted and executed by the Web browser, as compared to
                                                                               8.11 Other Tree Structures          455
a virtual machine plugin. This allows tighter integration of normal Web page ren-
dering, as interactive, user-controlled rendering. Once again, however, JavaScript’s
rendering capabilities were mostly focused on two-dimensional operations, and most
three-dimensional renderings were done through software renderers implemented
in JavaScript. Until recently, there wasn’t a widely adopted standard for interactive
three-dimensional graphics.

#### 8.10.4 WebGL

WebGL is a derivative of OpenGL (or more speciﬁcally, OpenGL ES version 2.0, the
embedded system version of OpenGL). It provides JavaScript bindings for OpenGL
functions and allows an HTML page using WebGL to render using any GPU resources
available in the system where the Web browser is running.
WebGL is currently under development by the Khronos Group (the same indus-
try consortium that develops OpenGL) at the time of this writing. It integrates the
rendering capabilities of HTML5’s Canvas element. As with modern OpenGL appli-
cations, all rendering is controlled by vertex and fragment shaders.

### 8.11    OTHER TREE STRUCTURES

Tree and DAG structures provide powerful tools to describe scenes; trees are also used
in a variety of other ways in computer graphics, of which we consider three. The ﬁrst
is the use of expression trees to describe an object hierarchy for solid objects; the
other two describe spatial hierarchies that we can use to increase the efﬁciency of
many rendering algorithms.

#### 8.11.1 CSG Trees

The polygonal representation of objects that we have used has many strengths and a
few weaknesses. The most serious weakness is that polygons describe only the surfaces
that enclose the interior of a three-dimensional object, such as a polyhedron. In CAD
applications, this limitation causes difﬁculties whenever we must employ any volu-
metric properties of the graphical object, such as its weight or its moment of inertia.
In addition, because we display an object by drawing its edges or surfaces, there can
be ambiguities in the display. For example, the wireframe shown in Figure 8.21 can         FIGURE 8.21 Wireframe that
be interpreted either as a cube with a hole through it created by removal of a cylinder    has two possible interpreta-
tions.
or as a solid cube composed of two different materials.
Constructive solid geometry (CSG) addresses these difﬁculties. Assume that we
start with a set of atomic solid geometric entities, such as parallelepipeds, cylinders,
and spheres. The attributes of these objects can include surface properties, such
as color or reﬂectivity, but also volumetric properties, such as size and density. In
describing scenes of such objects, we consider those points in space that constitute
each object. Equivalently, each object is a set of points, and we can use set algebra to
form new objects from these solid primitives.
456   Chapter 8   Modeling and Hierarchy
A                   B               A∪B
A∩B                  A–B

*FIGURE 8.22 Set operations.*

A                      B            C             D       (A – B) ∩ (C ∪ D)

*FIGURE 8.23 CSG object.*

CSG modeling uses three set operations: union, intersection, and set difference.
The union of two sets A and B, written A ∪ B, consists of all points that are either in
A or in B. The intersection of A and B, A ∩ B, is the set of all points that are in both
A and B. The set difference, A − B, is the set of points that are in A and are not in B.
Figure 8.22 shows two objects and possible objects created by the three set operations.
Objects are described by algebraic expressions. The expression (A − B) ∩ (C ∪
D) might describe an object such as the one illustrated in Figure 8.23.
Typically, we store and parse algebraic expressions using expression trees, where

```cpp
internal nodes store operations and terminal nodes store operands. For example, the
```

tree in Figure 8.24 is a CSG tree that represents the object (A − B) ∩ (C ∪ D) in
Figure 8.23. We can evaluate or render the CSG tree by a postorder traversal; that
is, we recursively evaluate the tree to the left of a node and the tree on the right of the
node, and ﬁnally use these values to evaluate the node itself. Rendering of objects in
CSG often is done with a variant of ray tracing; see Exercise 8.10 and Chapter 11.
                                                                                8.11 Other Tree Structures   457
A                 B        C                D

*FIGURE 8.24 CSG tree.*

B                                                                       F

*FIGURE 8.25 Collection of polygons and a viewer.*


#### 8.11.2 BSP Trees

Scene graphs and CSG trees describe hierarchical relationships among the parts of
an object. We can also use trees to describe the world object space and encapsulate
the spatial relationships among groups of objects. These relationships can lead to fast
methods of visibility testing to determine which objects might be seen by a camera,
thus avoiding processing all objects with tests such as the z-buffer algorithm. These
techniques have become very important in real-time animations for computer games.
One approach to spatial hierarchy starts with the observation that a plane divides
or partitions three-dimensional space into two parts (half spaces). Successive planes
subdivide space into increasingly smaller partitions. In two dimensions, we can use
lines to partition space.
Consider the polygons shown in Figure 8.25, with the viewer located as indicated.
Arguing as we did in Chapter 7, there is an order in which to paint these polygons
so that the image will be correct. Rather than using a method such as depth sort
each time we want to render these polygons, we can store the relative-positioning
information in a tree. We start the construction of the tree using the plane of one
polygon to separate groups of polygons that are in front of it from those that are
behind it. For example, consider a simple world in which all the polygons are parallel
and are oriented with their normals parallel to the z-axis. This assumption makes it
easier to illustrate the algorithm but does not affect the algorithm as long as the plane
458   Chapter 8   Modeling and Hierarchy
B                                        E

*FIGURE 8.26 Top view of polygons.*

B                       E                F

*FIGURE 8.27 Binary space partitioning (BSP) tree.*

of any polygon separates the other polygons into two groups. In this world, the view
from the z-direction is as shown in Figure 8.26.
Plane A separates the polygons into two groups, one containing B and C, which
are in front of A, and the second containing D, E, and F, which are behind A. We
use this plane to start a binary space partitioning tree (BSP tree) that stores the
separating planes and the order in which they are applied. Thus, in the BSP tree in
Figure 8.27, A is at the root, B and C are in the left subtree, and D, E, and F are
in the right subtree. Proceeding recursively, C is behind the plane of B, so we can
complete the left subtree. The plane of D separates E and F, thus completing the right
subtree. Note that for a given set of polygons, there are multiple possible BSP trees
corresponding to the order in which we choose to make our partitions. In the general
case, if a separating plane intersects a polygon, then we can break up the polygon into
two polygons, one in front of the plane and one behind it, similar to what we did with
overlapping polygons in the depth-sort algorithm in Chapter 6.
                                                                                8.11 Other Tree Structures            459
B                                                                             F

*FIGURE 8.28 Movement of the viewer to back.*

We can use this tree to paint the polygons by doing a backward in-order traver-
sal. That is, we traverse the tree recursively, drawing the right subtree ﬁrst, followed
by the root, and ﬁnally by the left subtree. One of the advantages of BSP trees is that
we can use the same tree even if the viewer moves by changing the traversal algo-
rithm. If the viewer moves to the back, as shown in Figure 8.28, then we can paint the
polygons using a standard in-order traversal—left subtree, root, right subtree. Also
note that we can use the algorithm recursively wherever planes separate sets of poly-
gons or other objects into groups, called clusters. Thus, we might group polygons

```cpp
into polyhedral objects, then group these polyhedra into clusters. We can then ap-
```

ply the algorithm within each cluster. In applications such as ﬂight simulators, where
the world model does not change but the viewer’s position does, the use of BSP trees
can be efﬁcient for doing visible surface determination during rendering. The tree
contains all the required information to paint the polygons; the viewer’s position de-
termines the traversal algorithm.
BSP trees are but one form of hierarchy to divide space. Another is the use of
bounding volumes, such as spheres. The root of a tree of bounding spheres would be
the sphere that contains all the objects in a scene. Subtrees would then correspond to
groups of objects within the larger sphere, and the root nodes would be the bounding
spheres for each object. We could use the same idea with other types of bounding
volumes, such as the bounding boxes that we discussed in Chapter 6. Spheres are
particularly good for interactive games because we can quickly determine if an object
is potentially visible or whether two objects might collide.

#### 8.11.3 Quadtrees and Octrees

One limitation of BSP trees is that the planes that separate polygons can have an
arbitrary orientation so that construction of the tree can be costly, involving ordering
and often splitting of polygons. Octrees and quadtrees avoid this problem by using
separating planes and lines parallel to the coordinate axes.
Consider the two-dimensional picture in Figure 8.29. We assume that this pic-
ture is composed of black and white pixels, perhaps formed by the rendering of a
three-dimensional scene. If we wish to store the scene, we can save it as a binary array.   FIGURE 8.29 Two-
But notice the great deal of coherence in the picture. Pixels of each color are clustered   dimensional space of pixels.
460         Chapter 8    Modeling and Hierarchy
together. We can draw two lines, as in Figure 8.30, dividing the region into quadrants.
Noting that one quadrant is all white, we can assign a single color to it. For the other
three, we can subdivide again and continue subdividing any quadrant that contains
pixels of more than a single color. This information can be stored in a tree called a
quadtree, in which each level corresponds to a subdivision and each node has four
children. Thus, the quadtree for our original simple picture is as shown in Figure 8.31.
Because we construct the quadtree by subdividing space with lines parallel to
the coordinate axes, formation and traversal of the tree are simpler than are the
corresponding operations for a BSP tree. One of the most important advantages of
quadtrees is that they can reduce the amount of memory needed to store images.
Quadtrees partition two-dimensional space. They can also be used to partition

*FIGURE 8.30 First subdivision   object space in a manner similar to BSP trees and thus can be traversed in an order*

of space.
depending on the position of the viewer so as to render correctly the objects in each
region. In three dimensions, quadtrees extend to octrees. The partitioning is done
by planes parallel to the coordinate axes, and each step of the partitioning subdivides
space into eight octants, as shown in Figure 8.32.

*FIGURE 8.31 Quadtree.*


*FIGURE 8.32 Octree.*

                                                                                      Suggested Readings        461
Octrees are used for representing volume data sets that consist of volume ele-
ments called voxels, as shown in Figure 8.33. The arguments that have been made for
quadtrees and octrees can also be applied to the spatial partitioning of objects, rather
than pixels or voxels. For example, we can use recursive subdivision of two- or three-
dimensional space for clipping. After each subdivison, we compare the bounding box
of each object with each subdivided rectangle or cube to determine if the object lies
in that region of the subdivided space.

*FIGURE 8.33 Volume data*

set.
The speed at which modern hardware can render geometric objects has opened up
the possibilities of a variety of modeling systems. As users of computer graphics, we
need a large arsenal of techniques if we are to make full use of our graphics systems.
We have introduced hierarchical modeling. Not only are there the many other forms
that we investigate in this chapter and the next, but we can combine these techniques
to generate new ones. The Suggested Readings will help you to explore modeling
methods.
We have presented basic themes that apply to most approaches to modeling. One
is the use of hierarchy to incorporate relationships among objects in a scene. We have
seen that we can use fundamental data structures, such as trees and DAGs, to repre-
sent such relationships; traversing these data structures becomes part of the rendering
process. The use of scene graphs in Open Scene Graph, VRML, and Java3D allows
the application programmer to build complex animated scenes from a combination
of predeﬁned and user-deﬁned software modules. Tree-structured models are also
used to describe complex shaders that involve the interaction of light sources, ma-
terial properties, atmospheric effects, and a variety of local reﬂection models. These
could be implemented with RenderMan, Cg, or GLSL.
Object-oriented approaches are standard for complex applications and for appli-
cations that are distributed over networks. Unfortunately, there has not been agree-
ment on a single object-oriented API. However, the actual rendering in most high-
end systems is done at the OpenGL level because the closeness of this API to the
hardware makes for efﬁcient use of the hardware. Consequently, both application
programmers and system developers need to be familiar with multiple levels of APIs.

## Chapter 9 presents an entirely different, but complementary, approach to mod-

eling based on procedural methods.
Hierarchical transformations through the use of a matrix stack were described in the
graphics literature more than 30 years ago [New73]. The PHIGS API [ANSI88] was
the ﬁrst to incorporate them as part of a standard package. See Watt [Wat92] for
an introduction to the use of articulated ﬁgures in animation. The paper by Lassiter
462   Chapter 8   Modeling and Hierarchy
[Las87] shows the relationship between traditional animation techniques as practiced
in the movie industry and animation in computer graphics.
BSP trees were ﬁrst proposed by Fuchs, Kedem, and Naylor [Fuc80] for use in
visibility testing and were later used in many other applications, such as CSG. See
[Mol02] for additional applications.
Scene graphs are the heart of Open Inventor [Wer94]. The Open Inventor data-
base format was the basis of VRML [Har96]. Most recent APIs, such as Java3D
[Swo00] and DirectX [Kov97], are object oriented. For a discussion of Java and ap-
plets, see [Cha98] and [Arn96]. Trees are integral to the RenderMan Shading Lan-
guage [Ups89], where they are used to construct shaders. Modeling systems, such as
Maya, allow the user to specify different shaders and rendering algorithms for differ-
ent objects. See [Ma07] for an introduction to Open Scene Graph.
Many applications of visibility testing can be found in [Mol02].
The use of scene graphs in game engine design is discussed in [Ebe01]. How the
engine looks for a game programmer is described for the Torque engine in [Mau06].

### 8.1   For our simple robot model, describe the set of points that can be reached by

the tip of the upper arm.

### 8.2   Find equations for the position of any point on the simple robot in terms of

the joint angles. Can you determine the joint angles from the position of the
tip of the upper arm? Explain your answer.

### 8.3   Given two points in space that are reachable by the robot, describe a path

between them in terms of the joint angles.

### 8.4   Write a simple circuit-layout program in terms of a symbol–instance trans-

formation table. Your symbols should include the shapes for circuit
elements—such as resistors, capacitors, and inductors for electrical circuits—
or the shapes for various gates (and, or, not) for logical circuits.

### 8.5   We can write a description of a binary tree, such as we might use for a search,

as a list of nodes with pointers to its children. Write an OpenGL program that
will take such a description and display the tree graphically.

### 8.6   Robotics is only one example in which the parts of the scene show compound

motion, where the movement of some objects depends on the movement of
other objects. Other examples include bicycles (with wheels), airplanes (with
propellers), and merry-go-rounds (with horses). Pick an example of com-
pound motion. Write a graphics program to simulate your selection.

### 8.7   Given two polygons with the same number of vertices, write a program that

will generate a sequence of images that converts one polygon into the other.

### 8.8   Starting with the tree node in Section 8.5, add an attribute to the node and

make any required changes to the traversal algorithm.
                                                                                         Exercises   463

### 8.9   Build a simple scene graph system that includes polygons, materials, a viewer,

and light sources.

### 8.10 Why is ray tracing or ray casting a good strategy for rendering a scene described

by a CSG tree?

### 8.11 Show how quadtrees can be used to draw an image at different resolutions.


### 8.12 Write a program that will allow the user to construct simple articulated ﬁgures

from a small collection of basic shapes. Your program should allow the user to
place the joints, and it should animate the resulting ﬁgures.

### 8.13 Is it possible to design a scene graph structure that is independent of the

traversal algorithm?

### 8.14 Using the scene graph we developed in this chapter, add the ability to store

scene graphs in text format and to read them in from ﬁles.

### 8.15 Add the ability to animate objects to our scene graph.


### 8.16 Starting with the robot in Section 8.3, add a hand or “gripper” to the end of

the arm.

### 8.17 Add wheels to the robot of Section 8.3 and thus the ability to have it move over

a ﬂat surface.

### 8.18 BSP trees can be made more efﬁcient if they are used hierarchically with ob-

jects grouped in clusters. Visibility checking is then done using the bounding
volumes of the clusters. Implement such an algorithm and use it with a scene
graph renderer.
This page intentionally left blank
                                                                    CHA P TE R           9
T    hus far, we have assumed that the geometric objects that we wish to create
can be described by their surfaces, and that these surfaces can be modeled (or
approximated) by convex planar polygons. Our use of polygonal objects was dictated
by the ease with which we could describe these objects and our ability to render them
on existing systems. The success of computer graphics attests to the importance of
such models.
Nevertheless, even as these models were being used in large CAD applications for
ﬂight simulators, in computer animations, in interactive video games, and to create
special effects in ﬁlms, both users and developers recognized the limitations of these
techniques. Physical objects such as clouds, smoke, and water did not ﬁt this style of
modeling. Adding physical constraints and modeling complex behaviors of objects
were not part of polygonal modeling. In response to such problems, researchers have
developed procedural models, which use algorithmic methods to build representa-
tions of the underlying phenomena, generating polygons only as needed during the
rendering process.

### 9.1   ALGORITHMIC MODELS

When we review the history of computer graphics, we see that the desire to create
increasingly more realistic graphics has always outstripped advances in hardware.
Although we can render more than 50 million polygons per second on existing com-
modity hardware, applications such as ﬂight simulation, virtual reality, and computer
games can demand rendering speeds greater than 500 million polygons per second.
Furthermore, as rendering speeds have increased, database sizes also have increased
dramatically. A single data set may contain more than 1 billion polygons.
Often, however, applications have such needs because they use existing software
and modeling paradigms. Astute researchers and application programmers have sug-
gested that we would not require as many polygons if we could render a model gen-
erating only those polygons that both are visible and project to an area at least the
size of one pixel. We have seen examples of this idea in previous chapters, for exam-
ple, when we considered culling polygons before they reached the rendering pipeline.
466   Chapter 9   Procedural Methods
Nevertheless, a more productive approach has been to reexamine the way in which we
do our modeling and seek techniques, known as procedural methods, that generate
geometrical objects in a different manner from what we have seen thus far. Procedu-
ral methods span a wide range of techniques. What they have in common is that they
describe objects in an algorithmic manner and produce polygons only when needed
as part of the rendering process.
In many ways, procedural models can be understood by an analogy with meth-
ods that we use to represent irrational numbers—such as square roots, sines,√ and
cosines—in a computer. Consider, for example, three ways of representing 2. We
√
2 = 1.414 . . . ,
√
ﬁlling in as many digits as we like; or, more abstractly, we can deﬁne the 2 as the
x 2 = 2.
√
However, within the computer, 2 might be the result of executing an algorithm. For
example, consider Newton’s method. Starting with an initial approximation x0 = 1,
xk  1
xk+1 =        + .
2   xk
√
√ successive value of xk is a better approximation to the 2. From this perspective,
2 is deﬁned by an algorithm; equivalently, it is deﬁned through a program. For
objects we deal with in computer graphics, we can take a similar approach. For
example, a sphere centered at the origin can be deﬁned as the mathematical object
that satisﬁes the equation
x2 + y2 + z2 = r 2.
It also is the result of the tetrahedron subdivision process that we developed in Chap-
ter 5 and of our program for doing that subdivision. A potential beneﬁt of the second
view is that when we render spheres, we can render small spheres (in screen space)
with fewer triangles than we would need for large spheres.
A second type of problem with polygonal modeling has been the difﬁculty of
combining computer graphics with physical laws. Although we can build and animate
polygonal models of real-world objects, it is far more difﬁcult to make these graphical
objects act as solids and not penetrate one another.
We introduce four of many possible approaches to procedural modeling. In
the ﬁrst, we work with particles that obey Newton’s laws. We then design systems
of particles that are capable of complex behaviors that arise from solving sets of
differential equations—a routine numerical task for up to thousands of particles. The
                                                     9.2 Physically Based Models and Particle Systems   467
positions of the particles yield the locations at which to place our standard geometric
objects in a world model.
The second approach—language-based models—enables us to control complex-
ity by replacing polygonal models with models similar to those used for both natural
and computer languages. With these models we can approximate many natural ob-
jects with a few rules that generate the required graphical entities. Combined with
fractal geometry, these models allow us to generate images using only the number of
polygons required for display.
The third approach—fractal geometry—is based on the self-similarity that we
see in many natural phenomena. Fractal geometry gives us a way of generating mod-
els at any desired level of detail. Finally, we introduce procedural noise as a method of

```cpp
introducing a controlled amount of randomness into our models. Procedural noise
```

has been used to create texture maps, turbulent behavior in ﬂuid models, realistic
motion in animations, and fuzzy objects such as clouds.

### 9.2    PHYSICALLY BASED MODELS AND PARTICLE SYSTEMS

One of the great strengths—and weaknesses—of modeling in computer graphics is
that we can build models based on any principles we choose. The graphical objects
that we create may have little connection with physical reality. Historically, the at-
titude was that if something looked right, that was sufﬁcient for most purposes.
Not being constrained by physical models, which were often either not known or
too complex to simulate in real time, allows the creation of the special effects that
we see in computer games and movies. In ﬁelds such as scientiﬁc visualization, this
ﬂexibility allows mathematicians to “see” shapes that do not exist in the usual three-
dimensional space and to display information in new ways. Researchers and engineers
can construct prototypes of objects that are not limited by our ability to construct
them with present materials and equipment.
However, when we wish to simulate objects in the real world and to see the results
of this simulation on our display, we can get into trouble. Often, it is easy to make a
model for a group of objects moving through space, but it is far more difﬁcult to keep
track of when two objects collide and to have the graphics system react in a physically
correct manner. Indeed, it is far easier in computer graphics to let a ball go directly
through a wall than to model the ball bouncing off the surface, incorporating the
correct elastic rebound.
Recently, researchers have become interested in physically based modeling, a
style of modeling in which the graphical objects obey physical laws. Such mod-
eling can follow either of two related paths. In one, we model the physics of the
underlying process and use the physics to drive the graphics. For example, if we
want a solid object to appear to tumble in space and to bounce from various sur-
faces, we can, at least in principle, use our knowledge of dynamics and contin-
uum mechanics to derive the required equations. This approach is beyond the scope
468   Chapter 9   Procedural Methods
of a ﬁrst course in computer graphics, and we shall not pursue it. The other ap-
proach is to use a combination of basic physics and mathematical constraints to
control the dynamic behavior of our objects. We follow this approach for a group
of particles.
Particle systems are collections of particles, typically point masses, in which the
dynamic behavior of the particles can be determined by the solution of sets of coupled
differential equations. Particle systems have been used to generate a wide variety of
behaviors in a number of ﬁelds. In ﬂuid dynamics, people use particle systems to
model turbulent behavior. Rather than solving partial differential equations, we can
simulate the behavior of the system by following a group of particles that is subject to
a variety of forces and constraints. We can also use particles to model solid objects.
For example, a deformable solid can be modeled as a three-dimensional array of
particles that are held together by springs. When the object is subjected to external
forces, the particles move and their positions approximate the shape of the solid
object.
Computer graphics practitioners have used particles to model such diverse phe-
nomena as ﬁreworks, the ﬂocking behavior of birds, and wave action. In these appli-
cations, the dynamics of the particle system gives the positions of the particles, but at
each location we can place a graphical object, rather than a point.
In all these cases, we work with a group of particles, each member of which we
can regard as a point mass. We use physical laws to write equations that we can solve
numerically to obtain the state of these particles at each time step. As a ﬁnal step,
we can render each particle as a graphical object—perhaps as a colored point for a
ﬁreworks application or a cartoon character in an animation.

### 9.3    NEWTONIAN PARTICLES

We consider a set of particles that is subject to Newton’s laws. Although there is
no reason that we could not use other physical laws or construct a set of our own
(virtual) physical laws, the advantage of starting with Newtonian particles is that
we can obtain a wide range of behaviors using simple, well-understood physics. A
Newtonian particle must obey Newton’s second law, which states that the mass of the
particle (m) times that particle’s acceleration (a) is equal to the sum of the forces (f )
acting on the particle, or symbolically,
ma = f .
Note that both the acceleration and force are vectors, usually in three dimensions.
One consequence of Newton’s laws is that for an ideal point-mass particle—one
whose total mass is concentrated at a single point—its state is completely determined
by its position and velocity. Thus, in three-dimensional space, an ideal particle has 6
degrees of freedom, and a system of n particles has 6n state variables—the positions
                                                                                               9.3 Newtonian Particles   469
and velocities of all the particles. Within some reference frame, the state of the ith
particle is given by two three-element column matrices,1 a position matrix
⎡ ⎤
⎣
pi = yi ⎦
⎡ dx ⎤
⎡ ⎤
ẋi       dt
⎢ dy ⎥
vi = ⎣ ẏi ⎦ = ⎢    ⎥
⎣ dt ⎦ .
żi       dz
Knowing that acceleration is the derivative of velocity and that velocity is the deriva-
tive of position, we can write Newton’s second law for a particle as the six coupled,
ﬁrst-order differential equations
ṗi = vi ,
v̇i =     f (t).
Hence, the dynamics of a system of n particles is governed by a set of 6n coupled,
ordinary differential equations.
In addition to its state, each particle may have a number of attributes, including
its mass (mi ), and a set of properties that can alter what its behavior is and how it
is displayed. For example, some attributes govern how we render the particle and
determine its color, shape, and surface properties. Note that although the dynamics
of a simple particle system is based on each particle being treated as a point mass, the
user can specify how each particle should be rendered. For example, each particle
may represent a person in a crowd scene, or a molecule in a chemical-synthesis
application, or a piece of cloth in the simulation of a ﬂag blowing in the wind. In
each case, the underlying particle system governs the location and the velocity of the
center of mass of the particle. Once we have the location of a particle, we can place
the desired graphical object at this location.
The set of forces on the particles, {fi }, determines the behavior of the system.
These forces are based on the state of the particle system and can change with time.
We can base these forces on simple physical principles, such as spring forces, or on
physical constraints that we wish to impose on the system, or we can base them on
external forces, such as gravity, that we wish to apply to the system. By designing the
forces carefully, we can obtain the desired system behavior.
1. We have chosen to use three-dimensional arrays here, rather than homogeneous coordinate
representations, both to be consistent with the way these equations are usually written in the physics
literature and to simplify the resulting differential equations.
470   Chapter 9   Procedural Methods
The dynamic state of the system is obtained by numerical methods that in-
volve stepping through approximations to the set of differential equations. A typical
time step is based on computing the forces that apply to the n particles through a
user-deﬁned function, using these forces to update the state through a numerical
differential-equation solver, and ﬁnally using the new positions of the particles and
their attributes to render whatever graphical objects we wish to place at the particles’
locations. Thus, in pseudocode, we have a loop of the form

```cpp
float time, delta; float state[6n], force[3n];
```

state=get_initial_state();
for(time=t0; time<time_final; time+=delta)

```cpp
{
```

/* compute forces */
force=force_function(state, time);
/* apply standard differential equation solver */
state=ode(force, state, time, delta);
/* display result */
render(state, time);

```cpp
}
```

The main component that we must design in a given application is the function that
computes the forces on each particle.

#### 9.3.1 Independent Particles

There are numerous simple ways that particles can interact and determine the forces
that act on each particle. If the forces that act on a given particle are independent of
other particles, the force on the ith particle can be described by the equation
fi = fi (pi , vi ).
A simple case occurs where each particle is subject only to a constant gravitational
fi /mi = g.
If this force points down, then
⎡      ⎤
g = ⎣ −g ⎦ ,
where g is positive, and each particle will trace out a parabolic arc. If we add a term
proportional to the velocity, we can have the particle subject to frictional forces, such
as drag. If some of the attributes, such as color, change with time and if we give each
particle a (random) lifetime, then we can simulate phenomena such as ﬁreworks.
                                                                                    9.3 Newtonian Particles         471
pi, j +1
pi 1, j      pi,j              pi +1, j
pi, j 1

*FIGURE 9.1 Mesh of particles.*

More generally, external forces are applied independently to each point. If we allow
the particles to drift randomly and render each as a large object, rather than as a point,
we can model clouds or ﬂows with independent particles.

#### 9.3.2 Spring Forces

If in a system of n particles all particles are independent, then the force calculation
is O(n). In the most general case, the computation of the forces on a given particle
may involve contributions due to pairwise interactions with all the other particles,
an O(n2) computation. For large particle systems, an O(n2) computation can be too
slow to be useful. Often, we can reduce this complexity by having a particle interact
with only those particles that are close to it.
Consider the example of using particles to create a surface whose shape varies
over time, such as a curtain or a ﬂag blowing in the wind. We can use the location
of each particle as a vertex for a rectangular mesh, as shown in Figure 9.1. The
shape of the mesh changes over time, as a result both of external forces that act on
each particle, such as gravity or wind, and of forces between particles that hold the
mesh together, giving it the appearance of a continuous surface. We can approximate
this second type of force by considering the forces between a particle and its closest
neighbors. Thus, if pij is the location of the particle at row i, column j of the mesh,
the force calculation for pij needs to consider only the forces between pi j and pi+1, j ,
pi−1, j , pi, j+1, and pi, j−1—an O(n) calculation.
One method to model the forces among particles is to consider adjacent parti-           q                           p
cles as connected by a spring. Consider two adjacent particles, located at p and q,

*FIGURE 9.2 Particles con-*

connected by a spring, as shown in Figure 9.2. Let f denote the force acting on p from       nected by a spring.
q. A force −f acts on q from p. The spring has a resting length s, which is the distance
between particles if the system is not subject to external forces and is allowed to come
to rest. When the spring is stretched, the force acts in the direction d = p − q; that is,
it acts along the line between the points. This force obeys Hooke’s law:
f = −ks (|d| − s)       ,
|d|
472        Chapter 9      Procedural Methods
where ks is the spring constant and s is the length of the spring when it is at rest. This
law shows that the farther apart the two particles are stretched, the stronger is the
force attracting them back to the resting position. Conversely, when the ends of the
spring are pushed together, the force pulls them back such that their positions move
to a separation by the resting length. As we have stated Hooke’s law, however, there
is no damping (or friction) in the system. A system of masses and springs deﬁned
p–q
q      in such a manner will oscillate forever when perturbed. We can include a drag, or
damping term, in Hooke’s law. The damping force operates in the same direction
as the spring force, but depends on the velocity between the particles. The fraction
p         (p – q) (p – q)     of the velocity that contributes to damping is proportional to the projection of the

*FIGURE 9.3 Computation of       velocity vector onto the vector deﬁned by the two points as shown in Figure 9.3.*

the spring damping force.       Mathematically, Hooke’s law with the damping term is given by
ḋ . d    d
f = − ks (|d| − s) + kd                .
|d|     |d|
Here, kd is the damping constant, and
ḋ = ṗ − q̇.
A system of masses and springs with damping that is not subjected to external forces
will eventually come to rest.
The four images in Color Plate 31 show a mesh that is generated from the
locations of a set of particles. Each interior particle is connected to its four neighbors
by springs. The particles are also subject to external forces—the wind. At each time
step, once the positions of the particles are determined, we can render the mesh using
techniques such as texture mapping (Section 7.5) to create the detailed appearance of
the surface.

#### 9.3.3 Attractive and Repulsive Forces

Whereas spring forces are used to keep a group of particles together, repulsive forces
push particles away from one another and attractive forces pull particles toward
one another. We could use repulsive forces to distribute particles over a surface,
or if the particles represent locations of objects, to keep objects from hitting one
another. We could use attractive forces to build a model of the solar system or to
create applications that model satellites revolving about the earth. The equations for
attraction and repulsion are essentially the same except for a sign. Physical models of
particle behavior may include both attractive and repulsive forces. See Exercise 9.14.
For a pair of particles, located at p and q, the repulsive force acts in the direction
d = p − q and is inversely proportional to the particles’ distance from each other. For
example, we could use the expression
f = −kr
|d|3
                                                                               9.4 Solving Particle Systems         473
for an inverse-square-law term. Changing the minus sign to a positive sign and re-
placing kr by (ma mb )/g gives us the attractive force between two particles of mass ma
and mb , where g is the gravitational constant.
In the general case, where each particle is subject to forces from all other par-
ticles, the computation of attractive and repulsive forces is O(n2). Unlike meshes
of particles connected by springs, where we attempt to keep the particles in the
same topological arrangement, particles subject to attractive and repulsive forces usu-
ally change their positions among themselves. Hence, strategies to avoid the O(n2)
force calculation are more complex. One approach is to divide space into three-
dimensional cells, each of which can contain multiple particles or even no particles,
as shown in Figure 9.4.                                                                      FIGURE 9.4 Division of space
For forces that are inversely proportional to distance, we can choose a cell size       into cells.
such that the forces on a particle from particles in other than its own or adjacent cells
are negligible. If this partitioning is possible, then the O(n2) calculation is reduced to
O(n). However, there is a cost to partitioning, and particles can move from one cell
to another. The difﬁculty of the ﬁrst problem depends on the particular application,
because we can often obtain an initial particle distribution with little effort, but at
other times we might have to do a sort. We can solve the second problem by looking at
the particle positions after each time step and redistributing the particles or, perhaps,
changing the cells. We can use various data structures to store the particles and cell
information.
One other approach that is often used is to replace interactions among particles
by interactions between particles and a force ﬁeld. For example, when we compute
the gravitational force on a point mass on the surface of the earth, we use the value of
the gravitational ﬁeld rather than the point-to-point force between the point mass of
the particle and a second large point mass at the center of the earth. If we were
concerned with only the mass of the earth and our point on the surface, the two
approaches would require about the same amount of work. However, if we were to
also include the force from the moon, the situation would be more complex. If we
used point masses, we would have two point-to-point forces to compute for our
mass on the surface, but if we knew the gravitational ﬁeld, the calculation would
be the same as before. Of course, the calculation of the ﬁeld is more complex when
we consider the moon; for particle systems, however, we can often neglect distant
particles, so the particle-ﬁeld method may be more efﬁcient. We can often compute
the approximate ﬁeld on a grid, then use the value at the nearest grid points to give
the forces on each particle. After the new state of each particle has been computed, we
can update the ﬁeld. Both of these strategies can often reduce the O(n2) calculation
of forces to O(n log n).

### 9.4    SOLVING PARTICLE SYSTEMS

Consider a particle system of n particles. If we restrict ourselves to the simple forces
that we just described, the entire particle system can be described by 6n ordinary
474   Chapter 9   Procedural Methods
u̇ = g(u, t),
where u is an array of the 6n position and velocity components of our n particles, and
g includes any external forces applied to the particles. Thus, if we have two particles
a and b connected by a spring without damping, we might have
u T = [ u0     u1 u2     u3    u4    u5        u6   u7   u8    u9    u10      u11 ]
= [ ax    ay   az   ȧx   ȧy   ȧz       bx   by   bz   ḃx   ḃy    ḃz ] .
Given the external forces and the state of the particle system u at any time t, we can
                                                                  
g T = u3 u4 u5 − kdx − kdy − kdz u9 u10 u11 kdx kdy kdz .
Here k is the spring constant and dx , dy , and dz are the components of the normalized
vector d between a and b. Thus, we must ﬁrst compute
⎡           ⎤
u 0 − u6
1                       ⎣ u1 − u7 ⎦ .
d= 
(u0 − u5)2 + (u1 − u6)2 + (u2 − u7)2
u2 − u8
Numerical ordinary differential equation solvers rely on our ability to evaluate
g to approximate u at future times. We can develop a family of differential equation
solvers based upon Taylor’s theorem. The simplest is known as Euler’s method. Sup-
u̇ = g(u, t)
over a short time h:
 t+h                           t+h
u̇dτ = u(t + h) − u(t) =       g(u, τ )dτ .
t                                         t
If h is small, we can approximate the value of g over the interval [t , t + h]by the value
of g at t; thus,
u(t + h) ≈ u(t) + hg(u(t), t).
This expression shows that we can use the value of the derivative at t to get us to an
approximate value of u(t + h), as shown in Figure 9.5.
This expression matches the ﬁrst two terms of the Taylor expansion; we can write
u(t + h) = u(t) + hu̇(t) + O(h2) = u(t) + hg(u(t), t) + O(h2),
showing that the error we incur in making the approximation is proportional to the
square of the step size.
                                                                             9.4 Solving Particle Systems   475
u(t ) + hu(t )
u(t )
u(t )
t           t+h

*FIGURE 9.5 Approximation of the solution of a differential equation.*

This method is particularly easy to implement. We evaluate the forces (both
external and among particles) at time t, compute g, multiply it by h, and add it to
the present state. We can apply this method iteratively to compute further values at
t + 2h, t + 3h, . . . . The work involved is one calculation of the forces for each time
step.
There are two potential problems with Euler’s method: accuracy and stability.
Both are affected by the step size. The accuracy of Euler’s method is proportional to
the square of the step size. Consequently, to increase the accuracy we must cut the step
size, thus increasing the time it takes to solve the system. A potentially more serious
problem concerns stability. As we go from step to step, the per-step errors that we
make come from two sources: the approximation error that we made by using the
Taylor series approximation and the numerical errors that we make in computing
the functions. These errors can either cancel themselves out as we compute further
states, or they can accumulate and give us unacceptably large errors that mask the true
solution. Such behavior is called numerical instability. Fortunately, for the standard
types of forces that we have used, if we make the step size small enough, we can
guarantee stability. Unfortunately, the required step size for stability may be so small
that we cannot solve the equations numerically in a reasonable amount of time. This
unstable behavior is most pronounced for spring-mass systems, where the spring

```cpp
constant determines the stiffness of the system and leads to what are called stiff sets
```

of differential equations.
There are two general approaches to this problem. One is to seek another type
of ordinary differential equation solver, called a stiff equation solver—a topic that is
beyond the scope of this book. Another is to ﬁnd other differential equation solvers
similar in philosophy to Euler’s method but with a higher per-step accuracy. We
derive one such method because it gives us insight into the family of such methods.
References to both approaches are given at the end of the chapter.
Suppose that we start as before by integrating the differential equations over a
476   Chapter 9   Procedural Methods
 t+h
u(t + h) = u(t) +           g(u, τ )dτ .
This time, we approximate the integral by an average value over the interval [t , t + h]:
 t+h
g(u, τ )dτ ≈ (g(u(t), t) + g(u(t + h), t + h)).
t                  2
The problem now is we do not have g(u(t + h), t + h); we have only g(u(t), t). We
can use Euler’s method to approximate g(u(t + h), t + h); that is, we can use
g(u(t + h), t + h) ≈ g(u(t) + hg(u(t), t), t + h).
This method is known as the improved Euler method or the Runge–Kutta method
of order 2. Note that to go from t to t + h, we must evaluate g twice. However, if
we were to use Taylor’s theorem to evaluate the per-step error, we would ﬁnd that
it is now O(h3). Thus, even though we are doing more work per step, we can use
larger step sizes, and the method is stable for step sizes larger than those for which
Euler’s method was stable. In general, we can use increasingly more accurate per-
step formulas and derive a set of methods called the Runge–Kutta formulas. The
most popular is the fourth-order method that has a per-step error of O(h4) and
requires four function evaluations per step. In practice, we can do even better with
this number of function evaluations, achieving errors of O(h5). More important,
good solvers adjust their own step size so as to ensure stability.

### 9.5     CONSTRAINTS

Simply allowing a group of particles to change state according to a set of differential
equations often is insufﬁcient to model real-world behavior such as collisions. Al-
though the impact of an object hitting a wall is subject to Newton’s laws, if we were to
model the object as a collection of particles, the resulting system would be too com-
plex for most purposes. Instead, we regard conditions such as the one in which two
solid objects cannot penetrate each other as constraints that can be stated separately
from the laws governing individual particle behavior.
There are two types of constraints that we can impose on particles. Hard con-
straints are those that must be adhered to exactly. For example, a ball must bounce
off a wall; it cannot penetrate the wall and emerge from the other side. Nor can we
allow the ball just to come close and then go off in another direction. Soft constraints
are those that we need only come close to satisfying. For example, we might want two
particles to be separated approximately by a speciﬁed distance, as in a particle mesh.

#### 9.5.1 Collisions

Although, in general, hard constraints can be difﬁcult to impose, there are a few sit-
uations that can be dealt with directly for ideal point particles. Consider the problem
of collisions. We can separate the problem into two parts: detection and reaction.
                                                                                             9.5 Constraints   477

*FIGURE 9.6 Particle penetrating a polygon.*

Suppose that we have a collection of particles and other geometric objects and the
particles repel one another. We therefore need to consider only collisions between
each particle and the other objects. If there are n particles and m polygons that deﬁne
the geometric objects, at each time step we can check whether any particle has gone
through any of the polygons.
Suppose that one of the particles has penetrated a polygon, as shown in Fig-
ure 9.6. We can detect this collision by inserting the position of the particle into the
equation of the plane of the polygon. If the time step of our differential equation
solver is small, we can assume that the velocity between time steps is constant, and
we can use linear interpolation to ﬁnd the time at which the particle actually hit the
polygon.
What happens to the particle after a collision is similar to what happens when
light reﬂects from a surface. If there is an inelastic collision, the particle loses none
of its energy, so its speed is unchanged. However, its direction after the collision is in
the direction of a perfect reﬂection. Thus, given the normal at the point of collision
Pc and the previous position of the particle P0, we can compute the direction of a
perfect reﬂection, as we did in Chapter 5, using the vector from the particle to the
surface and the normal at the surface, as shown in Figure 9.7:
r = 2(p0 − pc ) . n n − (P0 − Pc ).
The particle will be a distance along this reﬂector equal to the distance it would have
penetrated the polygon in the absence of collision detection, as shown in Figure 9.8.
The velocity is changed to be along the direction of reﬂection, with the same
magnitude. Equivalently, the tangential component of the velocity—the part in the
plane of the polygon—is unchanged, whereas the direction of the normal component
is reversed.
A slightly more complex calculation is required for an elastic collision, in which
the particle loses some of its energy when it collides with another object. The coefﬁ-
cient of restitution of a particle is the fraction of the normal velocity retained after
the collision. Thus, the angle of reﬂection is computed as for the inelastic collision,
478   Chapter 9   Procedural Methods
P0                                 r

*FIGURE 9.7 Particle reflection.*

P0

*FIGURE 9.8 Position after collision.*

and the normal component of the velocity is reduced by the coefﬁcient of restitution.
See Section 9.6.4 for an example.
The major cost of dealing with collisions is the complexity of detection. In appli-
cations such as games, approximate detection often is sufﬁcient. In this case, we can
replace complex objects consisting of many polygons by simple objects, such as their
bounding volumes, for which collision detection is simpler.
Note that use of particles avoids the complex calculations necessary for objects
with ﬁnite sizes. In many ways, solving the collision problem is similar to clipping
arbitrary objects against each other; the calculation is conceptually simple but is in
practice time-consuming and messy. In addition, if we have objects with ﬁnite size,
we have to consider inertial forces, thereby increasing the dimension of the system
of equations that we must solve. Consequently, in computer graphics, we are usually
willing to accept an approximate solution using ideal point particles, and we obtain
an acceptable rendering by placing objects at the location of the particle.
                                                                                             9.5 Constraints   479

*FIGURE 9.9 Contact force.*

There is another case of hard constraints that arises often and can be handled
correctly: contact forces. Suppose that we have a particle that is subject to a force
pushing it along a surface, as shown in Figure 9.9. The particle cannot penetrate the
surface, and it cannot bounce from the surface because of the force being applied
to it. We can argue that the particle is subject to the tangential component of the
applied force—that is, the part of the applied force along the surface. We can also
apply frictional terms in this direction.
Note that collision detection, as opposed to how we deal with a collision once it
has been detected, is often another O(n2) calculation. Consider, for example, a game
such as pool, in which balls are moving around a table, or a simulation of molecules
moving within a bounded volume. As any pair of particles can collide, a brute-force
approach would be to check all pairs of particles at each time step. Faster approaches
involve bounding-box methods and hardware support for collision detection.

#### 9.5.2 Soft Constraints

Most hard constraints are difﬁcult to enforce. For example, if we want to ensure that
a particle’s velocity is less than a maximum velocity or that all the particles have a

```cpp
constant amount of energy, then the resulting mathematics is far more difﬁcult than
```

what we have already seen, and such constraints do not always lead to a simple set of
ordinary differential equations.
In many situations, we can work with soft constraints: constraints that we need
to come only close to satisfying. For example, if we want a particle whose location is p
to remain near the position p0, we can consider the penalty function |p − p0|2 . The
smaller this function is, the closer we are to obeying the constraint. This function is
one example of an energy function whose value represents the amount of some type
of energy stored in the system. In physics, such functions can represent quantities,
such as the potential or kinetic energy in the system. Physical laws can be written
either as differential equations, like those we used for our particles, or in terms of the
minimization of expressions involving energy terms. One advantage of the latter form
is that we can express constraints or desired behavior of a system directly in terms
of potential or energy functions. Conversion of these expressions to force laws is a
mechanical process, but its mathematical details are beyond the scope of this book.
480   Chapter 9   Procedural Methods

### 9.6   A SIMPLE PARTICLE SYSTEM

We conclude this discussion by building a simple particle system that can be ex-
panded to more complex behaviors. Our particles are all Newtonian so their state
is described by their positions and velocities. In addition, each particle can have its
own color index and mass. We start with the following structure:
typedef point4 vec4;

```cpp
{
int color;
```

point4 position;
vec4 velocity;

```cpp
float mass;
} particle;
```

Here we are using four-dimensional homogeneous coordinates for our positions and
velocities. A particle system is an array of particles
particle particles[MAX_NUM_PARTICLES];
We can initialize the system with the particles in random locations inside a centered
cube with side length 2.0 and with random velocities as follows:

```cpp
int num_particles;
```

for(int i=0; i<num_particles; i++)

```cpp
{
```

particles[i].mass = 1.0;
particles[i].color = i%NUM_COLORS;
for(int j=0; j<3; j++)

```cpp
{
```

particles[i].position[j] = 2.0*((float) rand()/RAND_MAX)-1.0;
particles[i].velocity[j] = speed*2.0*((float)
rand()/RAND_MAX)-1.0;

```cpp
}
```

particles[i].position[3] = 1.0;
particles[i].velocity[3] = 0.0;

```cpp
}
```


#### 9.6.1 Displaying the Particles

Given the position of a particle, we can display it using any set of primitives that we
like. A simple starting point is to display each particle as a point. Here is a display
callback that loops through the array of particles in which we have num_particles,
                                                                         9.6 A Simple Particle System   481

```cpp
void display(void)
{
```

glClear(GL_COLOR_BUFFER_BIT);
for(i=0; i<num_particles; i++)

```cpp
{
```

point_colors[i+24] = colors[particles[i].color];
points[i+24] = particles[i].position;

```cpp
}
```

glBindBuffer(GL_ARRAY_BUFFER, buffers[0]);
glBufferData(GL_ARRAY_BUFFER, sizeof(points), +sizeof(colors)NULL,
GL_DYNAMIC_DRAW);
glBufferSubData(GL_ARRAY_BUFFER, 0, sizeofpoints,points);
glBufferSubData(GL_ARRAY_BUFFER, sizeof(points), sizeof(colors),colors);
glDrawArrays(GL_POINTS, 24, num_particles);
glutSwapBuffers();

```cpp
}
```

where the colors are stored in an array colors
typedef color4 vec4;
color4 colors[8] = {color4(0.0,0.0,0.0, 1.0), color4(1.0,0.0,0.0, 1.0),
color4(1.0,1.0,0.0, 1.0), color4(0.0,1.0,0.0, 1.0),
color4(0.0,0.0,1.0, 1.0), color4(1.0,0.0,1.0, 1.0),
color4(0.0,1.0,1.0, 1.0), color4(1.0,1.0,1.0, 1.0)};

#### 9.6.2 Updating Particle Positions

We use the idle callback to update the particle positions using the elapsed time and
Euler integration:

```cpp
float last_time, present_time;
void idle(void)
{
int i, j;
float dt;
```

present_time = glutGet(GLUT_ELAPSED_TIME); /* in milliseconds */
dt = 0.001*(present_time - last_time); /* in seconds */
for(i=0; i<num_particles; i++)

```cpp
{
```

for(j=0; j<3; j++)

```cpp
{
```

particles[i].position[j]+=dt*particles[i].velocity[j];
particles[i].velocity[j]+=dt*forces(i,j)/particles[i].mass;

```cpp
}
```

482   Chapter 9   Procedural Methods
collision(i);

```cpp
}
```

last_time = present_time;
glutPostRedisplay();

```cpp
}
```

The positions are updated using the velocity, and the velocity is updated by comput-
ing the forces on that particle. We have assumed that the time interval is short enough
that we can compute the forces on each particle as we update its state. A more robust
strategy would be to compute all the forces on all the particles ﬁrst and put the results

```cpp
into an array that can be used to update the state.
```

We shall use the collision function to keep the particles inside a box. It could also
be used to deal with collisions between particles.

#### 9.6.3 Collisions

We use the collision function to keep the particles inside the initial axis-aligned box.
Our strategy is to increment the position of each particle and then check if the particle
has crossed one of the sides of the box. If it has crossed a side, then we can treat
the bounce as a reﬂection. Thus, we need only change the sign of the velocity in the
normal direction. If the coefﬁcient of restitution is less than 1.0, the particles will slow
down when they hit a side of the box.

```cpp
float coef; /* coefficient of restitution */
void collision(int n)
{
int i;
```

for (i=0; i<3; i++)

```cpp
{
```

if(particles[n].position[i]>=1.0)

```cpp
{
```

particles[n].velocity[i] = -coef*particles[n].velocity[i];
particles[n].position[i] = 1.0-coef*
(particles[n].position[i]-1.0);

```cpp
}
```

if(particles[n].position[i]<=-1.0)

```cpp
{
```

particles[n].velocity[i] = -coef*particles[n].velocity[i];
particles[n].position[i] = -1.0-coef*
(particles[n].position[i]+1.0);

```cpp
}
}
}
```

                                                                              9.6 A Simple Particle System   483

#### 9.6.4 Forces

If the forces are set to zero, the particles will bounce around the box on linear paths
continuously. If the coefﬁcient is less than 1.0, eventually the particles will slow to a
halt. The easiest force to add is gravity. For example, if all the particles have the same
mass, we can add a gravitational term in the y-direction by
bool gravity =      TRUE;

```cpp
float forces(int i, int j)
{
```

if(!gravity) return(0.0);
else if(j==1) return(-1.0);
else return(0.0);

```cpp
}
```

We can add repulsive or attractive forces (see Exercise 9.14) by computing the
distances between all pairs of particles at the beginning of each iteration and then
using any inverse square term. The exercises at the end of the chapter suggest some
extensions to the system.

#### 9.6.5 Flocking

Some of the most interesting applications of a particle system are for simulating
complex behaviors among the particles. Perhaps a more accurate statement is that we
can produce what appears to be complex behavior using some simple rules for how
particles interact. A classic example is simulating the ﬂocking behavior of birds. How
does a large group of birds maintain a ﬂock without each bird knowing the positions
of all the other birds? We can investigate some possibilities by making some minor
modiﬁcations to our particle system.
One possibility is to change the direction of each particle so the particle steers
toward the center of the system. Thus, each time that we update the system, we
compute the average position:

```cpp
float cm[3];
```

for (k=0; k<3; k++)

```cpp
{
```

cm[k]=0;
for(i=0;i<num_particles;i++) cm[k]+=particles[i].position[k];
cm[k]/=num_particles;

```cpp
}
```

We can now compute a new velocity direction that lies between the updated velocity
vector particles[i].velocity and the vector from particles[i].position
to the average position, as in Figure 9.10. See Exercise 9.20.
484   Chapter 9   Procedural Methods

*FIGURE 9.10 Changing a particle’s direction.*


### 9.7   LANGUAGE-BASED MODELS

Graphs, such as the trees and DAGs we introduced in Chapter 8, offer but one way
of representing hierarchical relationships among objects. In this section, we look at
language-based models for representing relationships. Not only do these methods
provide an alternate way of showing relationships, but they also lead to procedural
methods for describing objects, such as plants and terrain.
If we look at natural objects, such as plants, we see that although no two trees
are identical, we may have no difﬁculty telling the difference between two species of
trees. Various methods have been proposed that give different realizations each time
the program is run but which have clear rules for deﬁning the structure. We look at
the use of tree data structures for generating objects that look like plants.
In computer science, tree data structures are used for describing the parsing
of sentences into constituent parts, in both computer and natural languages. For
computer programs, doing this parsing is part of compiling the statements in a
computer program. For natural languages, we parse sentences to determine whether
they are grammatically correct. The tree that results from the parsing of a correct
sentence gives the structure or syntax of that sentence. The interpretation of the
individual elements in the tree—the words—give the meaning or semantics of the
sentence.
If we look at only the syntax of a language, there is a direct correlation between
the rules of the language and the form of the trees that represent the sentences. We
can extend this idea to hierarchical objects in graphics, relating a set of rules and a
tree-structured model. These systems are known as tree grammars. A grammar can
be deﬁned by a set of symbols and a set of symbol-replacement rules, or productions,
that specify how to replace a symbol by one or more symbols. Typical rules are
A → BC ,
B → ABA.
Given a set of productions, we can generate an inﬁnite number of strings. In general,
there is more than one rule that we can apply to a given symbol at any time, and, if
we select randomly which rule to apply, we can generate a different string each time
the program is executed. Programs can be written that not only generate such strings
                                                                                9.7 Language-Based Models              485

*FIGURE 9.11 The Koch curve rule.*

but also take strings as input and test whether the strings are valid members of the
set of strings generated by a given set of rules. Thus, we might have a set of rules for
generating a certain type of object, such as a tree or a bush, and a separate program
that can identify objects in a scene based on which grammar generates the shape.
The interpretation of the symbols in a string converts the string to a graphical ob-
ject. There are numerous ways to generate rules and to interpret the resulting strings
as graphical objects. One approach starts with the turtle graphics (Exercise 2.4) sys-
tem. In turtle graphics, we have three basic ways of manipulating a graphics cursor, or
turtle. The turtle can move forward 1 unit, turn right, or turn left. Suppose that the
angle by which the turtle can turn is ﬁxed. We can then denote our three operations
(a)
as F, R, and L. Any string of these operations has a simple graphical interpretation.
For example, if the angle is 120 degrees, the string FRFRFR generates an equilateral
triangle. We use the special symbols [ and ] to denote pushing and popping the state
of the turtle (its position and orientation) onto a stack (an operation equivalent to
using parentheses). Consider the production rule
F → FLFRRFLF ,
with an angle of 60 degrees. The graphical interpretation of this rule is shown in
Figure 9.11. If we apply the rule again, in parallel to all instances of F, we get the curve                (b)
in Figure 9.12(a); if we apply it to a triangle, we get the closed curve in Figure 9.12(b).    FIGURE 9.12 Koch curves. (a)
These curves are known as the Koch curve and Koch snowﬂake, respectively. If                   Curve. (b) Snowflake.
we scale the geometric interpretation of the curve each time that we execute the
algorithm, so as to leave the original vertices in their original locations, we ﬁnd we are
generating a longer curve at each iteration, but this curve always ﬁts inside the same
box. In the limit, we have a curve that has inﬁnite length, never crosses itself, but ﬁts
in a ﬁnite box. It also is continuous but has a discontinuous derivative everywhere.
Another classic example is the Hilbert curve. Hilbert curves are formed from
four simple primitives we can call A0 , B0 , C0 , and D0, shown in Figure 9.13. Each is
a ﬁrst-order Hilbert curve. The arrows are there to indicate we start drawing each
in one corner. There are four Hilbert curves of each order N, which we can call
AN , BN , CN , and DN . We form each from the order N − 1 curves by combining the
four types according to the following rules:
AN = BN−1 ↑ AN −1 → AN −1 ↓ CN −1,
BN = AN−1 → BN −1 ↑ BN −1 ← DN −1,
CN = DN−1 ← CN−1 ↓ CN −1 → AN−1,
DN = CN−1 ↓ DN −1 ← DN −1 ↑ BN−1.
486           Chapter 9      Procedural Methods
A0             B0            C0            D0

*FIGURE 9.13 The zero order Hilbert patterns.*

The interpretation of these rules is that the N th pattern is formed by combining four
patterns of order N − 1 in speciﬁed directions. We can see from Figure 9.14 for the
ﬁrst-order curve A1 that links corresponding to the arrows in the formulas must be
added to connect the patterns. Note also that each pattern starts in a different corner.
When the curves are drawn, the arrows and links are left out, and we obtain curves
such as in Figure 9.15.
A1                       If we scale the lengths of links as we go to higher-order curves, we can verify

*FIGURE 9.14 Hilbert rule for       that, like the Koch curves, the Hilbert curves get longer and longer, never crossing*

type A.                            themselves, but always ﬁt in the same box. In the limit, the Hilbert curves ﬁll every
point in the box and are known as space-ﬁlling curves.
The push and pop operators allow us to develop side branches. Consider the rule
F → F[RF]F[LF]F ,
where the angle is 27 degrees (Figure 9.16). Note that we start at the bottom and the

*FIGURE 9.15 Second-order*

angle is measured as a right or left deviation from pointing forward.
Hilbert curve.                          If we start with a single line segment, the resulting object is that shown in Fig-
ure 9.16. We can proceed in a number of ways. One method is to apply the rule again
to each F in the sequence, resulting in the object in Figure 9.17. We can also adjust
the length corresponding to a forward movement of the turtle so that branches get
smaller on successive iterations. The object resembles a bush and will look more like
a bush if we iterate a few more times. However, having only one rule and applying it
in parallel results in every bush looking the same.
A more interesting strategy is to apply the rule randomly to occurrences of F. If
we do so, our single rule can generate both of the objects in Figure 9.18. Adding a few

*FIGURE 9.16 The rule F →           more productions and controlling the probability function that determines which*

F[RF]F[LF]F.
rule is to be applied next allows the user to generate a variety of types of trees. With
only slight modiﬁcations, we can also draw leaves at the ends of the branches.
One of the attractions of this strategy is that we have deﬁned a class of objects
based on only a handful of rules and a few parameters. Suppose that we wish to
create a group of trees. The direct approach is to generate as many objects as needed,
representing each one as a collection of geometric objects (lines, polygons, curves).
In a complex scene, we might then be overwhelmed with the number of primitives
generated. Depending on the viewing conditions, most of these primitives might not
appear in the image, because they would be clipped out or would be too far from the
viewer to be rendered at a visible size. In contrast, using our procedural method, we

*FIGURE 9.17 Second itera-*

describe objects by simple algorithms and generate the geometric objects only when
tion of the rule in Figure 9.16.   we need them and to only the level of detail that we need.
                                                                    9.8 Recursive Methods and Fractals   487

*FIGURE 9.18 Results of random application of the rule from Figure 9.16.*


*FIGURE 9.19 Three rules for the Sierpinski gasket.*

We can also describe a grammar directly in terms of shapes and afﬁne transfor-
mations, creating a shape grammar. Consider our old friend the Sierpinski gasket.
We can deﬁne a subdivision step in terms of three afﬁne transformations, each of
which scales the original triangle to one-half of the size and places the small copy in
a different position, as shown in Figure 9.19. We can apply these rules randomly, or
we can apply all three in parallel. In either case, in the limit, we derive the gasket.
We now have three related procedural methods that can generate either models
of natural objects or models of interesting mathematical objects. The examples of
the Koch curve and the Sierpinski gasket introduce a new aspect to the generation
process—a method that can be applied recursively and that, each time it is executed,
generates detail similar in shape to the original object. Such phenomena can be
explored through fractal geometry.

### 9.8   RECURSIVE METHODS AND FRACTALS

The language-based procedural models offer but one approach to generating com-
plex objects with simple programs. Another approach, based on fractal geometry,
uses the self-similarity of many real-world objects. Fractal geometry was developed
by Mandelbrot, who was able to create a branch of mathematics that enables us to
488   Chapter 9   Procedural Methods
work with interesting phenomena with which we cannot deal using the tools of ordi-
nary geometry. Workers in computer graphics have used the ideas of fractal geometry
not only to create beautiful and complex objects but also to model many real-world
entities that are not modeled easily by other methods. Graphical objects generated by
fractals have been called graftals.

#### 9.8.1 Rulers and Length

There are two pillars to fractal geometry: the dependence of geometry on scale and
self-similarity. We can examine both through the exploration of one of the questions
that led to fractal geometry: What is the length of a coastline? Say that we have a
map of a coastline. Because a typical stretch of coastline is wavy and irregular, we
can take a string, lay it over the image of the coastline, and then measure the length
of the string, using the scale of the map to convert distances. However, if we get
a second map that shows a closer view of the coastline, we see more detail. The
added detail looks much like the view of the ﬁrst map, but with additional inlets and
protrusions visible. If we take our string and measure the length on the second map,
taking into account the difference in scale between the two maps, we will measure a
greater distance. We can continue this experiment by going to the coast and trying
to measure with even greater precision. We ﬁnd new detail, perhaps even to the level
of measuring individual pebbles along the shore. In principle, we could continue this
process down to the molecular level, each time seeing a similar picture with more
detail and measuring a greater length.
If we want to get any useful information, or at least a measurement on which
two people might agree, we must either limit the resolution of the map or, equiva-
lently, pick the minimum unit that we can measure. In computer graphics, if we use
perspective views, we have a similar problem, because what detail we see depends on
how far we are from the object.
We can approach these problems mathematically by considering our recursion
for the Koch snowﬂake in Section 9.7. Here, each line segment of length 1 was re-
placed by four line segments of length 1/3 (Figure 9.20). Hence, each time that we
replace a segment, we span the distance between the same two endpoints with a
curve four-thirds of the length of the original. If we consider the limit as we iterate
an inﬁnite number of times, the issue of dimension arises. The curve cannot be an
ordinary one-dimensional curve, because, in the limit, it has inﬁnite length and its
ﬁrst derivative is discontinuous everywhere. It is not a two-dimensional object, how-
ever, because it does not ﬁll a two-dimensional region of the plane. We can resolve
this problem by deﬁning a fractional dimension.
1     1
3     3
1             1
1                        3             3

*FIGURE 9.20 Lengthening of the Koch curve.*

                                                                    9.8 Recursive Methods and Fractals   489
1            1
1              1

*FIGURE 9.21 Line segment, square, and cube.*


*FIGURE 9.22 Subdivision of the objects for h = 31 .*


#### 9.8.2 Fractal Dimension

Consider a line segment of length 1, a unit square, and a unit cube, as shown in
Figure 9.21. Under any reasonable deﬁnition of dimension, the line segment, square,
and cube are one-, two-, and three-dimensional objects, respectively. Suppose that
we have a ruler whose resolution is h, where h = n1 is the smallest unit that we can
measure. We assume that n is an integer. We can divide each of these objects into
similar units in terms of h, as shown in Figure 9.22. We divide the line segment

```cpp
into k = n identical segments, the square into k = n2 small squares, and the cube
into k = n3 small cubes. In each case, we can say that we have created new objects
```

by scaling the original object by a factor of h and replicating it k times. Suppose that
d is the dimension of any one of these objects. What has remained constant in the
subdivision is that the whole is the sum of the parts. Mathematically, for any of the
objects, we have the equality
= kn−d = 1.
Solving for d, we can deﬁne the fractal dimension as
d=        .
In other words, the fractal dimension of an object is determined by how many similar
objects we create by subdivision. Consider the Koch curve. We create four similar
objects by the subdivision (scaling) of the original by a factor of 3. The corresponding
ln 4
d=        = 1.26186.
ln 3
490         Chapter 9   Procedural Methods
Now consider the Sierpinski gasket. A scaling step is shown in Figure 9.23. Each time
that we subdivide a side by a factor of 2, we keep three of the four triangles created,
ln 3
d=        = 1.58496.
ln 2

*FIGURE 9.23 Subdivision of*

the Sierpinski gasket.         In both examples, we can view the object created by the subdivision as occupying
more space than a curve but less space than a ﬁlled area. We can create a solid
version of the gasket in a three-dimensional space by starting with a tetrahedron and
subdividing each of the faces, as shown in Figure 9.24. We keep the four tetrahedrons
at the original vertices, discarding the region in the middle. The object that we create
ln 4
d=        = 2,

*FIGURE 9.24 Solid gasket.          ln 2*

even though it does not lie in the plane. Also, note that although the volume is
reduced by each subdivision, the surface area is increased. Suppose that we start with
a cube and divide it into thirds, as shown in Figure 9.25. Next, we remove the center
by pushing out the pieces in the middle of each face and the center, thus leaving 20 of
the original 27 subcubes. This object has a fractal dimension of
ln 20
d=         = 2.72683.
ln 3

*FIGURE 9.25 Subdivision of a   Although these constructions are interesting and are easy to generate graphically at*

cube.
any level of recursion, they are by themselves not useful for modeling the world.
However, if we add randomness, we get a powerful modeling technique.

#### 9.8.3 Midpoint Division and Brownian Motion

A fractal curve has dimension 1 ≤ d < 2. Curves with lower fractal dimension appear
smoother than curves with higher fractal dimension. A similar statement holds for
surfaces that have fractal dimension 2 ≤ d < 3. In computer graphics, there are many
situations where we would like to create a curve or surface that appears random
but that has a measurable amount of roughness. For example, the silhouette of a
mountain range forms a curve that is rougher (has higher fractal dimension) than
the skyline of the desert. Likewise, a surface model of mountain terrain should have a
higher fractal dimension than the surface of farmland. We also often want to generate
these objects in a resolution-dependent manner. For example, the detail that we
generate for a terrain used in a speed-critical application, such as in a ﬂight simulator,
should be generated at high resolution for only those areas near the aircraft.
The random movement of particles in ﬂuids is known as Brownian motion.
Simulating such motion provides an interesting approach to generating natural
curves and surfaces. Physicists have modeled Brownian motion by forming polylines
in which each successive point on the polyline is displaced by a random distance and
                                                                      9.8 Recursive Methods and Fractals   491
p+q
(a)                         (b)

*FIGURE 9.26 Midpoint displacement. (a) Original line segment. (b) Line*

segment after subdivision.

*FIGURE 9.27 Fractal curves with 1, 2, 4, 8, and 16 segments.*

in a random direction from its predecessor. True Brownian motion is based on a par-
ticular random-number distribution that generates paths that match physical particle
paths. In computer graphics, we are more concerned with rapid computation, and
with the ability to generate curves with a controllable amount of roughness; thus, we
use the term Brownian motion in this broader sense.
Although we could attempt to generate Brownian motion through the direct
generation of a polyline, a more efﬁcient method is to use a simple recursive process.
Consider the line segment in Figure 9.26(a). We ﬁnd its midpoint; then, we displace
the midpoint in the normal direction, by a random distance, as in Figure 9.26(b).
We can repeat this process any number of times to produce curves like that in Fig-
ure 9.27. The variance of the random-number generator, or the average displacement,
should be scaled by a factor, usually of 1/2, each time, because the line segments are
shortened at each stage. We can also allow the midpoint to be displaced in a ran-
dom direction, rather than only along the normal. If the random numbers are always
positive, we can create skylines. If we use a zero-mean Gaussian random-number gen-
erator, with variance proportional to l 2(2−d), where l is the length of the segment to be
subdivided, then d is the fractal dimension of the resulting curve. The value d = 1.5
corresponds to true Brownian motion.
492   Chapter 9   Procedural Methods

*FIGURE 9.28 Midpoint subdivision of a tetrahedron facet.*

(a)                                      (b)

*FIGURE 9.29 Fractal terrain. (a) Mesh. (b) Subdivided mesh with*

displaced vertices.

#### 9.8.4 Fractal Mountains

The best-known uses of fractals in computer graphics have been to generate moun-
tains and terrain. We can generate a mountain with our tetrahedron-subdivision
process by adding in a midpoint displacement. Consider one facet of the tetrahedron,
as shown in Figure 9.28. First, we ﬁnd the midpoints of the sides; then, we displace
each midpoint,2 creating four new triangles. Once more, by controlling the variance
of the random-number generator, we can control the roughness of the resulting ob-
ject. Note that we must take great care in how the random numbers are generated if
we are to create objects that are topologically correct and do not fold into themselves;
see the suggested readings at the end of this chapter.
This algorithm can be applied equally well to any mesh. We can start with a ﬂat
mesh of rectangles in the x, z-plane, and subdivide each rectangle into four smaller
rectangles, displacing all vertices upward (in the y-direction). Figure 9.29 shows one
example of this process. Section 9.9 presents another approach that can be used for
generating terrain.
2. We can also displace the original vertices.
                                                                      9.8 Recursive Methods and Fractals   493
z = x + iy

*FIGURE 9.30 Complex plane.*


#### 9.8.5 The Mandelbrot Set

The famous Mandelbrot set is an interesting example of fractal geometry that can be
generated easily with OpenGL’s pixel drawing functionality. Although the Mandel-
brot set is easy to generate, it shows inﬁnite complexity in the patterns it generates. It
also provides a good example of generating images and using color lookup tables. In
this discussion, we assume that you have a basic familiarity with complex arithmetic.
We denote a point in the complex plane as
z = x + iy,
where x is the real part and y is the imaginary part of z (Figure 9.30). If z1 = x1 + iy1
and z2 = x2 + iy2 are two complex numbers, complex addition and multiplication
are deﬁned by
z1 + z2 = x1 + x2 + i(y1 + y2),
z1z2 = x1x2 − y1y2 + i(x1y2 + x2y1).
The pure imaginary number i has the property that i2 = −1. A complex number z
|z|2 = x 2 + y 2 .
In the complex plane, a function
w = F(z)
maps complex points into complex points. We can use such a function to deﬁne a
zk+1 = F(zk ),
where z0 = c is a given initial point. If we plot the locations of zk for particular
starting points, we can see several of the possibilities in Figure 9.31. For a particular
494   Chapter 9   Procedural Methods
z2 = F(z1)
z3 = F(z2)
z0                    z1 = F(z0 )

*FIGURE 9.31 Paths from complex recurrence.*

function F, some initial values generate sequences that go off to inﬁnity. Others may
repeat periodically, and still other sequences converge to points called attractors. For
example, consider the function
zk+1 = zk2 ,
where z0 = c. If c lies outside a unit circle, the sequence {zk } diverges; if c is inside the
unit circle, {zk } converges to an attractor at the origin; if |c| = 1, each zk is on the unit
circle. If we consider the points for which |c| = 1, we can see that, depending on the
value of c, we can generate either a ﬁnite number of points or all the points on the
unit circle.
A more interesting example is the function
zk+1 = zk2 + c,
with z0 = 0 + i0. The point c is in the Mandelbrot set if and only if the points
generated by this recurrence remain ﬁnite. Thus, we can break the complex plane

```cpp
into two groups of points: those that belong to the Mandelbrot set and those that do
```

not. Graphically, we can take a rectangular region of the plane and color points black
if they are in the set and white if they are not (Figure 9.32a). However, it is the regions
on the edges of the set that show the most complexity, so we often want to magnify
these regions.
The computation of the Mandelbrot set can be time-consuming; there are a few
tricks to speed it up. The area centered at c = −0.5 + i0.0 is of the most interest,
although we probably want to be able to change both the size and the center of our
window.
We can usually tell after a few iterations whether a point will go off to inﬁnity. For
example, if |zk | > 4, successive values will be larger, and we can stop the iteration. It is
more difﬁcult to tell whether a point near the boundary will converge. Consequently,
an approximation to the set is usually generated as follows. We ﬁx a maximum num-
ber of iterations. If, for a given c, we can determine that the point diverges, we color
white the point corresponding to c in our image. If, after the maximum number of
iterations, |zk | is less than some threshold, we decide that it is in the set, and we color
                                                                          9.8 Recursive Methods and Fractals   495
(a)                                         (b)

*FIGURE 9.32 Mandelbrot set. (a) Black and white coloring. (b) Detail*

along edges.
it black. For other values of |zk |, we assign a unique color to the point correspond-
ing to c. These colors are usually based on the value of |zk | after the last iteration or,
alternately, how rapidly the points converge or diverge.
The book’s Web site contains a program that generates an approximation to the
set. The user can set the size and center of the rectangle and the number of iterations
to be carried out. The magnitudes of the numbers zk are clamped to be in the range
of 0.0 to 1.0. We generate an n × m 1-byte array image by looping over all pixels up
to the maximum number of iterations.
We display the image as a texture mapped onto a square comprised of two
triangles. Hence, the vertices and texture coordinates can be given using a unit cube
point4 points[6] = {point4(0.0, 0.0, 0.0, 1.0), point4(0.0, 1.0, 0.0, 1.0),
point4(1.0, 1.0, 0.0, 1.0), point4(1.0, 1.0, 0.0, 1.0),
point4(1.0, 0.0, 0.0, 1.0), point4(0.0, 0.0, 0.0, 1.0)};
GLfloat tex_coord[6][2] = {{0.0, 0.0}, {0.0, 1.0}, {1.0, 1.0}, {1.0, 1.0},

```cpp
{1.0, 0.0}, {0.0, 0.0}};
```

or if we use two-dimensional vertices, we can use the same array:
GLfloat points[6][2] = {{0.0, 0.0}, {0.0, 1.0}, {1.0, 1.0}, {1.0, 1.0},

```cpp
{1.0, 0.0}, {0.0, 0.0}};
```

We set up a texture map just as in Chapter 7. We can create a texture image in many
ways. The simplest is to construct a luminance image
GLfloat image[N][N];
496   Chapter 9   Procedural Methods
from the values generated by the calculation of the set and then display it as usual:

```cpp
void display()
{
```

glClear(GL_COLOR_BUFFER_BIT);
glDrawArrays(GL_TRIANGLES, 0, 6);
glutSwapBuffers();

```cpp
}
```

The image in Figure 9.32a was obtained in this manner. However, we can map the
luminance values into colors in a manner that enhances the detail. Consider the RGB
GLfloat image[N][M][3];
and the map from luminance values v generated by the iteration
if(v>1.0) v=1.0; /* clamp if > 1 */
image[i][j][0] = v;
image[i][j][1] = 2.0*sin(v)-1.0;
image[i][j][2] = 1.0 - v;
We specify an intensity-to-red map that assigns no red to black (0.0), assigns full red
to white (1.0), and linearly interpolates between these values for the other intensities.
For blue, we go from full blue for zero intensity to no blue for full intensity. We assign
the intensity-to-green values sinusoidally. This green assignment enhances the detail
in regions where there are slow changes in intensity (Figure 9.32b).

### 9.9    PROCEDURAL NOISE

We have used pseudorandom-number generators to generate the Sierpinski gasket
and for fractal subdivision. The use of pseudorandom-number generators has many
other uses in computer graphics ranging from generating textures to generating mod-
els of natural objects such as clouds and ﬂuids. However, there are both practical and
theoretical reasons that the simple random-number generator that we have used is
not a good choice for our applications.
Let’s start with the idea of white noise. White noise is what we get from thermal
activity in electrical circuits or what we see on a television screen when we tune to
a channel that has no signal. Ideal white noise has the property that if we look at
a sequence of samples, we can ﬁnd no correlation among the samples and thus we
cannot predict the next sample from the previous samples. Mathematically, white
noise has the property that its power spectrum—the average spectrum we would see
in the frequency domain—is ﬂat; all frequencies are present with equal strength.
Within the computer, we can generate pseudorandom sequences of numbers.
These random-number generators, such as the function rand that we have used, pro-
duce uncorrelated sequences, but, because the sequences repeat after a long period,
                                                                                     9.9 Procedural Noise   497
they are not truly random even though they work well enough for most applications.
However, for many other applications white noise is not what we need. Often we want
randomness but do not want successive samples to be totally uncorrelated. For ex-
ample, suppose that we want to generate some terrain for an interactive game. We
can use a polygonal mesh whose heights vary randomly. True white noise would give
a very rough surface due to the high-frequency content in the noise. If we want a
more realistic surface that might model a fairly smooth terrain, we would want ad-
jacent vertices to be close to each other. Equivalently, we would like to remove high
frequencies from the noise generator or at least alter or “color” the spectrum.
There is an additional problem with the high frequencies in white noise: aliasing.
As we saw in Chapter 7, sampling will cause aliasing of frequencies about the Nyquist
rate, which can lead to annoying visual artifacts in the image.
There are various possible solutions to these problems. Assume that we want
to generate a sequence of random samples that are bandlimited and for which we
know which frequencies we would like to be present. We could sample the sum
of sinusoidal terms with low frequencies and random amplitudes and phases. This
method of Fourier synthesis works in principle but requires expensive evaluations
of trigonometric functions for each sample. Another class of methods is based on
Figure 9.33. If the process is a digital ﬁltering of the white noise, we can design the
ﬁlter to include the frequencies we want at the desired magnitude and phase. Because
this noise has a nonuniform spectrum, it is often called colored noise.
If we start with what we would like to see in a method, we can design a procedu-
ral approach that is based on Figure 9.33 but which is much more computationally
feasible. Besides wanting to minimize the computation required, we want repeata-
bility and locality. If we used a random method to form a pattern or a texture, we
must be able to repeat it exactly as we regenerate an object. We also want to be able
to generate our pattern or texture using only local data rather than global data.
Suppose that we generate a pseudorandom sequence on a one-, two-, or three-
dimensional grid (or lattice) in which the grid points are integers. We can use the
values at the grid points to generate points for noninteger values, that is, for points
between the cells determined by adjacent grid values. For example, suppose that we
want to generate a two-dimensional texture; we start by forming a rectangular array
of pseudorandom numbers. We can use this array to generate values for any (s, t)
texture coordinates by interpolation. A simple interpolation would be to ﬁnd the cell
corresponding to a given (s, t) pair and use bilinear interpolation on the values at the
corners to get an interior value.
This method generates what is known as value noise. We can control the smooth-
ness by selecting how large a part of the array we use. For example, suppose that we
White noise                Colored noise

*FIGURE 9.33 Generating correlated random numbers.*

498   Chapter 9   Procedural Methods
generate a 256 × 256 array of pseudorandom numbers and use bilinear interpola-
tion to form a 128 × 128 texture. We can use a single cell and interpolate the desired
128 × 128 values using only the four values at the corners of the cell. In this case, we
would get a very smooth texture image. Alternately, we could use a larger part of the
array. If we used a 4 × 4 part of the array, each of the 16 cells would be interpolated
to provide 64 values of the texture. Since we would be using 16 of the pseudorandom
numbers, this texture would show more variation than our ﬁrst example. We could
use a 128 × 128 part of the pseudorandom array. In this case, we would not need any

```cpp
interpolation and would have a completely uncorrelated texture.
```

The problem with this process is that bilinear interpolation over each cell will
result in visible artifacts as we go from cell to cell forming our texture. We can get
around this problem by using an interpolation formula that uses data from adjacent
cells to give a smoother result. The most common methods use cubic polynomials of
the type that we shall study in Chapter 10. Without going into detail on any particular
type, we can note that a cubic polynomial has four coefﬁcients, and thus we need four
data points to specify it. For a two-dimensional process, we need the data at the eight
adjacent cells, or 16 (4 × 4) data points, to determine values within that cell. In three
dimensions, we need data at the 26 adjacent cells, or 64 data points. Although we
would get a smoother result, in two or three dimensions the amount of computation
and the required data manipulation make this method problematic.
The solution to this problem is to use gradient noise. Suppose that we model
noise in three dimensions as a continuous function n(x, y, z). Near a grid point
(i, j, k) where i, j, and k are integers, we can approximate n(x, y, z) by the ﬁrst terms
∂n           ∂n          ∂n
n(x, y, z) ≈ n(i, j, k) + (x − i)      + (y − j)    + (z − k) .
∂x           ∂y          ∂z
The vector
⎡ ⎤ ⎡ ∂n ⎤
gx       ∂x
⎢ ⎥
g = ⎣ gy ⎦ = ⎣ ∂n ⎦
∂y
gz        ∂n
∂z
is the gradient at (i, j, k). Note that x − i, y − j, and z − k are the fractional parts
of the position within a cell. To generate gradient noise, we ﬁrst compute normal-
ized pseudorandom gradient vectors at each grid point. We can get these vectors by
generating a set of uniformly distributed random points on a unit sphere. We ﬁx
the values at the grid points to be zero (n(i, j, k) = 0). In three dimensions, within
each cell, we have eight gradient vectors, one from each corner of the cell that we can
use to approximate n(x, y, z). The standard technique is to use a ﬁltered (smoothed)

```cpp
interpolation of these gradients to generate noise at points inside the cell. Noise gen-
```

erated in this manner is often called Perlin noise after its original creator or just
noise. This noise function is built into RenderMan and GLSL. The actual implemen-
tation of noise uses a hash table so that rather than ﬁnding random gradients for the
entire grid only 256 or 512 pseudorandom numbers are needed. Figure 9.34 shows
                                                                                9.9 Procedural Noise   499
(a)                                                     (b)
(c)

*FIGURE 9.34 Gradient noise images (a) using noise, (b) using 50*noise, (c) using 100*noise.*

500   Chapter 9   Procedural Methods
two-dimensional gradient noise at three different frequencies. Each is a 256 × 256
luminance image that starts with the same array of pseudorandom numbers.
Procedural noise has been used in many ways. For example, adding a noise to the
joint positions in our ﬁgure model can give a sense of realism to the model. Adding
a nonlinearity such as taking the absolute value in the noise generator generates
sequences that have been used to model turbulence in ﬂows and to generate textures.
Procedural noise is also used for modeling “fuzzy” objects such as simulated clouds.
Procedural methods have advantages in that we can control how many primitives we
produce and at which point in the process these primitives are generated. Equally im-
portant is that procedural graphics provides an object-oriented approach to building
models—an approach that should be of increasing importance in the future.
Combining physics with computer graphics provides a set of techniques that
has the promise of generating physically correct animations and of providing new
modeling techniques. Recent examples, such as the use of physical modeling for the
motion of 1000 balloons in Pixar Animation Studio’s Up, show how the solution of
complex systems of equations can provide the foundations of an animation.
Particle systems are but one example of physically based modeling, but they
represent a technique that has wide applicability. One of the most interesting and
informative exercises that you can undertake at this point is to build a particle system.
Particle methods are used routinely in commercial animations, both for sim-
ulation of physical phenomena, such as ﬁre, clouds, and moving water, and in de-
termining the positions of animated characters. Particle systems have also become a
standard approach to simulating physical phenomena, often replace complex partial
differential equation models, and are used even if a graphical result is not needed.
In interactive games and simulations, each particle can be given complex behavioral
rules. This merging of graphics and artiﬁcial intelligence is known as agent-based
modeling.
Fractals provide another method for generating objects with simple algorithms
and programs that produce images that appear to have great complexity. Procedural
noise has been at the heart of almost all procedural-modeling methods, and its true
power is often best demonstrated when it is combined with one or more of the other
methods that we have presented.
As we look ahead, we see a further convergence of graphics methods with meth-
ods from physics, mathematics, and other sciences. Historically, given the available
computing power, we were content to accept visuals that “looked OK” but were not
especially close to the correct physics in applications such as simulation and inter-
active games. Even in applications in which we might spend days rendering a single
frame, the true physics was still too complex to simulate well. However, with the con-
tinued advances in available computing power and the lowered cost of accessing such
power, we expect to see more and more physically correct modeling in all applications
of computer graphics.
                                                                                         Exercises   501
Particle systems were introduced in computer graphics by Reeves [Ree83]. Since then,
they have been used for a variety of phenomena, including ﬂocking of birds [Rey87],
ﬂuid ﬂow, ﬁre, modeling of grass, and display of surfaces [Wit94a]. Particles are also
used extensively in physics and mathematics and provide an alternative to solving
complex systems of partial differential equations that characterize ﬂuid ﬂow and solid
mechanics. See, for example, [Gre88]. Our approach follows Witkin [Wit94b]. Many
examples of procedural modeling are in [Ebe02].
There is a wealth of literature on fractals and related methods. The paper by
Fournier [Fou82] was the ﬁrst to show the fractal mountain. For a deeper treatment
of fractal mathematics, see the books by Mandelbrot [Man82] and Peitgen [Pei88].
The use of graph grammars has appeared in a number of forms [Pru90, Smi84,
Lin68]. Both Hill [Hil07] and Prusinkiewicz [Pru90] present interesting space-ﬁlling
curves and surfaces. Barnsley’s iterated-function systems [Bar93] provide another
approach to use of self-similarity; they have application in such areas as image com-
pression.
Gradient noise is due to Perlin [Per85, Per89, Per02]. Many applications to tex-
ture and object generation are in [Ebe02], as is a discussion of value noise.

### 9.1   Find a set of productions to generate the Sierpinski gasket by starting with a

single equilateral triangle.

### 9.2   How could you determine the fractal dimension of a coastline? How would you

verify that the shape of a coastline is indeed a fractal?

### 9.3   Start with the tetrahedron-subdivision program that we used in Chapter 5

to approximate a sphere. Convert this program into one that will generate a
fractal mountain.

### 9.4   We can write a description of a binary tree, such as we might use for search,

as a list of nodes with pointers to its children. Write an OpenGL program that
will take such a description and display the tree graphically.

### 9.5   Write a program for a simple particle system of masses and springs. Render the

particle system as a mesh of quadrilaterals. Include a form of interaction that
allows a user to put particles in their initial positions.

### 9.6   Extend Exercise 9.5 by adding external forces to the particle system. Create an

image of a ﬂag blowing in the wind.

### 9.7   Write a program to fractalize a mesh. Try to use real elevation data for the

initial positions of the mesh.

### 9.8   Write a program that, given two polygons with the same number of vertices,

will generate a sequence of images that converts one polygon into the other.

### 9.9   If we use the basic formula that we used for the Mandlebrot set but this time ﬁx

the value of the complex number c and ﬁnd the set of initial points for which
502   Chapter 9   Procedural Methods
we get convergence, we have the Julia set for that c. Write a program to display
Julia sets. Hint: Use values of c near the edges of the Mandelbrot set.

### 9.10 Write a particle system that simulates the sparks that are generated by welding

or by ﬁreworks.

### 9.11 Extend Exercise 9.10 to simulate the explosion of a polyhedron.


### 9.12 Combine alpha blending (Chapter 7), sphere generation (Chapter 5), and

fractals to create clouds.

### 9.13 Use fractals to generate the surface of a virtual planet. Your output should show

continents and oceans.

### 9.14 In the Lennard-Jones particle system, particles are attracted to each other by

a force proportional to the inverse of the distance between them raised to the
12th power but are repelled by another force proportional to the inverse of the
same distance raised to the 24th power. Simulate such a system in a box. To
make the simulation easier, you can assume that a particle that leaves the box
reenters the box from the opposite side.

### 9.15 Create a particle system in which the region of interest is subdivided into cubes

of the same size. A particle can only interact with particles in its own cube and
the cubes adjacent to it.

### 9.16 In animations, particle systems are used to give the positions of the characters.

Once the positions are determined, two-dimensional images of the characters
can be texture-mapped onto polygons at these positions. Build such a system
for moving characters subject to both external forces that move them in the
desired direction and repulsive forces that keep them from colliding. How can
you keep the polygons facing the camera?

### 9.17 Add particle lifetimes to the particle system. Reimplement the particle system

using a linked list of particles rather than an array so that particles can be added
or eliminated more easily.

### 9.18 Render particles in the example particle system with shaded approximations

to spheres.

### 9.19 Use a spring-mass system to simulate a hair or a blade of grass by connecting

four points in a chain.

### 9.20 Experiment with various ﬂocking algorithms. For example, particles may up-

date their velocities to move toward the center of mass of all particles. Another
approach is to have each particle move toward a “friend” particle.

### 9.21 Add procedural noise to the ﬁgure model so that at rest there is a slight move-

ment of each joint.

### 9.22 Implement a fractal landscape using procedural noise. Include the capability

to move the viewer and to zoom in and out.
                                                         CHA P TE R                        10
T    he world around us is full of objects of remarkable shapes. Nevertheless, in
computer graphics, we continue to populate our virtual worlds with ﬂat ob-
jects. We have a good reason for such persistence. Graphics systems can render ﬂat
three-dimensional polygons at high rates, including doing hidden-surface removal,
shading, and texture mapping. We could take the approach that we took with our
sphere model and deﬁne curved objects that are, in (virtual) reality, collections of ﬂat
polygons. Alternatively, and as we will do here, we can provide the application pro-
grammer with the means to work with curved objects in her program, leaving the
eventual rendering of these objects to the implementation.
We introduce three ways to model curves and surfaces, paying most attention to
the parametric polynomial forms. We also discuss how curves and surfaces can be
rendered on current graphics systems, a process that usually involves subdividing the
curved objects into collections of ﬂat primitives. From the application programmer’s
perspective, this process is transparent because it is part of the implementation. It is
important to understand the work involved, however, so that we can appreciate the
practical limitations we face in using curves and surfaces.

### 10.1    REPRESENTATION OF CURVES AND SURFACES

Before proceeding to our development of parametric polynomial curves and sur-
faces, we pause to summarize our knowledge of the three major types of object
representation—explicit, implicit, and parametric—and to observe the advantages
and disadvantages of each form. We can illustrate the salient points using only lines,
circles, planes, and spheres.

#### 10.1.1 Explicit Representation

The explicit form of a curve in two dimensions gives the value of one variable, the
dependent variable, in terms of the other, the independent variable. In x, y space,
y = f (x),
504   Chapter 10   Curves and Surfaces
or if we are fortunate, we might be able to invert the relationship and express x as a
function of y:
x = g(y).
There is no guarantee that either form exists for a given curve. For the line, we usually
y = mx + h,
in terms of its slope m and y-intercept h, even though we know that this equation
does not hold for vertical lines. This problem is one of many coordinate-system–
dependent effects that cause problems for graphics systems and, more generally, for
all ﬁelds where we work with design and manipulation of curves and surfaces. Lines
and circles exist independently of any representation, and any representation that fails
for certain orientations, such as vertical lines, has serious deﬁciencies.
Circles provide an even more illustrative example. Consider a circle of radius r
centered at the origin. A circle has constant curvature—a measure of how rapidly a
curve is bending at a point. No closed two-dimensional curve can be more symmetric
than the circle. However, the best we can do, using an explicit representation, is to
write one equation for half of it,

y=       r 2 − x2 ,
and a second equation,

y = − r 2 − x2 ,
for the other half. In addition, we must also specify that these equations hold only if
0 ≤ |x| ≤ r.
In three dimensions, the explicit representation of a curve requires two equa-
tions. For example, if x is again the independent variable, we have two dependent
variables:
y = f (x),
z = g(x).
A surface requires two independent variables, and a representation might take the
z = f (x, y).
As is true in two dimensions, a curve or surface may not have an explicit representa-
tion. For example, the equations
                                                          10.1 Representation of Curves and Surfaces   505
y = ax + b,
z = cx + d
describe a line in three dimensions, but these equations cannot represent a line in
a plane of constant x. Likewise, a surface represented by an equation of the form
z = f (x, y) cannot represent a sphere, because a given x and y can generate zero, one,
or two points on the sphere.

#### 10.1.2 Implicit Representations

Most of the curves and surfaces with which we work have implicit representations. In
two dimensions, an implicit curve can be represented by the equation
f (x, y) = 0.
Our two examples—the line and the circle centered at the origin—have the respective
ax + by + c = 0,
x 2 + y 2 − r 2 = 0.
The function f , however, is really a testing, or membership, function that divides
space into those points that belong to the curve and those that do not. It allows us to
take an x, y pair and to evaluate f to determine whether this point lies on the curve.
In general, however, it gives us no analytic way to ﬁnd a value y on the curve that
corresponds to a given x, or vice versa. The implicit form is less coordinate-system–
dependent than is the explicit form, however, in that it does represent all lines and
circles.
In three dimensions, the implicit form
f (x, y, z) = 0
describes a surface. For example, any plane can be written as
ax + by + cz + d = 0
for constants a, b, c, and d, and a sphere of radius r centered at the origin can be
x 2 + y 2 + z 2 − r 2 = 0.
Curves in three dimensions are not as easily represented in implicit form. We can
represent a curve as the intersection, if it exists, of the two surfaces:
f (x, y, z) = 0,
g(x, y, z) = 0.
506        Chapter 10    Curves and Surfaces
Thus, if we test a point (x, y, z) and it is on both surfaces, then it must lie on
their intersection curve. In general, most of the curves and surfaces that arise in real
applications have implicit representations. Their use is limited by the difﬁculty in
obtaining points on them.
Algebraic surfaces are those for which the function f (x, y, z) is the sum of poly-
nomials in the three variables. Of particular importance are the quadric surfaces,
where each term in f can have degree up to 2.1 Quadrics are of interest not only be-
cause they include useful objects (such as spheres, disks, and cones) but also because
when we intersect these objects with lines, at most two intersection points are gener-
ated. We will use this characteristic to render quadrics in Section 10.9 and for use in
ray tracing in Chapter 11.

#### 10.1.3 Parametric Form

The parametric form of a curve expresses the value of each spatial variable for points
on the curve in terms of an independent variable, u, the parameter. In three dimen-
sions, we have three explicit functions:
x = x(u),
y = y(u),
p(u)            z = z(u).
One of the advantages of the parametric form is that it is the same in two and
three dimensions. In the former case, we simply drop the equation for z. A use-
z                       ful interpretation of the parametric form is to visualize the locus of points p(u) =

*FIGURE 10.1 Parametric        [ x(u) y(u) z(u) ]T being drawn as u varies, as shown in Figure 10.1. We can think*

curve.                        of the derivative
⎡ dx(u) ⎤
dp(u) ⎢       ⎥
=⎢
⎣
dy(u) ⎥
du ⎦
dz(u)
as the velocity with which the curve is traced out and points in the direction tangent
to the curve.
Parametric surfaces require two parameters. We can describe a surface by three
x = x(u, v),
y = y(u, v),
z = z(u, v),
1. Degree is measured as the sum of the powers of the individual terms, so x, yz, or z 2 can be in a
quadric, but xy 2 cannot.
                                                          10.1 Representation of Curves and Surfaces             507
⎡         ⎤
x(u, v)
p(u, v) = ⎣ y(u, v) ⎦ .
z(u, v)
As u and v vary over some interval, we generate all the points p(u, v) on the surface.
As we saw with our sphere example in Chapter 5, the vectors given by the column
⎡ ∂x(u, v) ⎤
∂u
∂p ⎣ ∂y(u, v) ⎦
=   ∂u
∂u   ∂z(u, v)
∂u
⎡ ∂x(u, v) ⎤
∂v
∂p ⎣ ∂y(u, v) ⎦
=   ∂v
∂v   ∂z(u, v)
∂v                                                                                        p
determine the tangent plane at each point on the surface. In addition, as long as these
vectors are not parallel, their cross product gives the normal (Figure 10.2) at each      FIGURE 10.2 Tangent plane
point; that is:                                                                           and normal at a point on a
parametric surface.
n = ∂p/∂u × ∂p/∂v.
The parametric form of curves and surfaces is the most ﬂexible and robust for
computer graphics. We could still argue that we have not fully removed all depen-
dencies on a particular coordinate system or frame, because we are still using the x,
y, and z for a particular representation. It is possible to develop a system solely on
the basis of p(u) for curves and p(u, v) for surfaces. For example, the Frenet frame
is often used for describing curves in three-dimensional space, and it is deﬁned start-
ing with the tangent and the normal at each point on the curve. As in our discussion
of bump mapping in Chapter 7, we can compute a binormal for the third direction.
However, this frame changes for each point on the curve. For our purposes, the para-
metric form for x, y, z within a particular frame is sufﬁciently robust.

#### 10.1.4 Parametric Polynomial Curves

Parametric forms are not unique. A given curve or surface can be represented in many
ways, but we will ﬁnd that parametric forms in which the functions are polynomials
in u for curves and polynomials in u and v for surfaces are of most use in computer
graphics. Many of the reasons will be summarized in Section 10.2.
508         Chapter 10      Curves and Surfaces
Consider a curve of the form2
⎡      ⎤
x(u)
p(u) = ⎣ y(u) ⎦ .
z(u)
A polynomial parametric curve of degree3 n is of the form

p(u) =          uk ck ,
k=0
where each ck has independent x, y, and z components; that is,
⎡    ⎤
ck = ⎣ cyk ⎦ .
The n + 1 column matrices {ck } are the coefﬁcients of p; they give us 3(n + 1) degrees
of freedom in how we choose the coefﬁcients of a particular p. There is no coupling,
however, among the x, y, and z components, so we can work with three independent
equations, each of the form

p(u) =          u k ck ,
k=0
p(umax)
where p is any one of x, y, or z. There are n + 1 degrees of freedom in p(u). We can
deﬁne our curves for any range interval of u:
p(umin)
x          umin ≤ u ≤ umax ;
however, with no loss of generality (see Exercise 10.3), we can assume that 0 ≤ u ≤ 1.
As the value of u varies over its range, we deﬁne a curve segment, as shown in

*FIGURE 10.3 Curve segment.       Figure 10.3.*


#### 10.1.5 Parametric Polynomial Surfaces

We can deﬁne a parametric polynomial surface as
⎡         ⎤
x(u, v)     n  m
p(u, v) = ⎣ y(u, v) ⎦ =         cij ui v j .
z(u, v)     i=0 j=0
2. At this point there is no need to work in homogeneous coordinates; in Section 10.8 we shall work
in them to derive NURBS curves.
3. OpenGL often uses the term order to mean one greater than the degree.
                                                                                      10.2 Design Criteria                   509
We must specify 3(n + 1)(m + 1) coefﬁcients to determine a particular surface                                  y
p(u, v). We will always take n = m and let u and v vary over the rectangle 0 ≤ u, v ≤ 1,
u=1
deﬁning a surface patch, as shown in Figure 10.4. Note that any surface patch can be                      p(u, v)
viewed as the limit of a collection of curves that we generate by holding either u or v    v=0

```cpp
constant and varying the other. Our strategy will be to deﬁne parametric polynomial                                      v=1
```

curves and to use the curves to generate surfaces with similar characteristics.                          u=0                    x

*FIGURE 10.4 Surface patch.*


### 10.2    DESIGN CRITERIA

The way curves and surfaces are used in computer graphics and computer-aided
design is often different from the way they are used in other ﬁelds and from the
way you may have seen them used previously. There are many considerations that
determine why we prefer to use parametric polynomials of low degree, including:
Ability to evaluate derivatives                                                     FIGURE 10.5 Model airplane.
We can understand these criteria with the aid of a simple example. Suppose that
we want to build a model airplane, using ﬂexible strips of wood for the structure.         FIGURE 10.6 Cross-section
We can build the body of the model by constructing a set of cross sections and then        curve.
connecting them with longer pieces, as shown in Figure 10.5. To design our cross
sections, we might start with a picture of a real airplane or sketch a desired curve.                    Desired
One such cross section might be like that shown in Figure 10.6. We could try to get
a single global description of this cross section, but that description probably would
not be what we want. Each strip of wood can be bent to only a certain shape before
breaking and can bend in only a smooth way. Hence, we can regard the curve in
Figure 10.6 as only an approximation to what we actually build, which might be more                  Approximate
like Figure 10.7. In practice, we probably will make our cross section out of a number
of wood strips, each of which will become a curve segment for the cross section.           FIGURE 10.7 Approximation
of cross-section curve.
Thus, not only will each segment have to be smooth, but we also want a degree of
smoothness where the segments meet at join points.
Note that although we might be able to ensure that a curve segment is smooth,                                 Join point
we have to be particularly careful at the join points. Figure 10.8 shows an example in           p(v )               q(v )
which, although the two curve segments are smooth, at the join point the derivative is
discontinuous. The usual deﬁnition of smoothness is given in terms of the derivatives
along the curve. A curve with discontinuities is of little interest to us. Generally, a
curve with a continuous ﬁrst derivative is smoother than a curve whose ﬁrst derivative     FIGURE 10.8 Derivative dis-
has discontinuities (and so on for the higher derivatives). These notions become more      continuity at join point.
precise in Section 10.3. For now, it should be clear that for a polynomial curve
510          Chapter 10      Curves and Surfaces

p(u) =         ck u k ,
k=0
all derivatives exist and can be computed analytically. Consequently, the only places
where we can encounter continuity difﬁculties are at the join points.
We would like to design each segment individually, rather than designing all the
segments by a single global calculation. One reason for this preference is that we
would like to work interactively with the shape, carefully molding it to meet our
speciﬁcations. When we make a change, this change will affect the shape in only
the area where we are working. This sort of local control is but one aspect of a
more general stability principle: Small changes in the values of input parameters should
cause only small changes in output variables. Another statement of this principle is:
Small changes in independent variables should cause only small changes in dependent
p1
variables.
p3
p0                                    Working with our piece of wood, we might be able to bend it to approximate
p(u)
p2             the desired shape by comparing it to the entire curve. More likely, we would consider
data at a small number of control, or data, points and would use only those data

*FIGURE 10.9 Curve segment         to design our shape. Figure 10.9 shows a possible curve segment and a collection*

and control points.               of control points. Note that the curve passes through, or interpolates, some of the
control points but only comes close to others. As we will see throughout this chapter,
in computer graphics and CAD we are usually satisﬁed if the curve passes close to the
control-point data, as long as it is smooth.
This example shows many of the reasons for working with polynomial paramet-
ric curves. In fact, the spline curves that we discuss in Sections 10.7 and 10.8 derive
their name from a ﬂexible wood or metal device that shipbuilders used to design the
shape of hulls. Each spline was held in place by pegs, and the bending properties of
the material gave the curve segment a polynomial shape.
Returning to computer graphics, remember that we need methods for rendering
curves (and surfaces). A good mathematical representation may be of limited value
if we cannot display the resulting curves and surfaces easily. We would like to display
whatever curves and surfaces we choose with techniques similar to those used for ﬂat
objects, including color, shading, and texture mapping.

### 10.3     PARAMETRIC CUBIC POLYNOMIAL CURVES

Once we have decided to use parametric polynomial curves, we must choose the
degree of the curve. On one hand, if we choose a high degree, we will have many
parameters that we can set to form the desired shape, but evaluation of points on
the curve will be costly. In addition, as the degree of a polynomial curve becomes
higher, there is more danger that the curve will become rougher. On the other hand,
if we pick too low a degree, we may not have enough parameters with which to work.
However, if we design each curve segment over a short interval, we can achieve many
of our purposes with low-degree curves. Although there may be only a few degrees
                                                                                           10.4 Interpolation   511
of freedom, these few may be sufﬁcient to allow us to produce the desired shape
in a small region. For this reason, most designers, at least initially, work with cubic
polynomial curves.
We can write a cubic parametric polynomial using a row and column matrix as

p(u) = c0 + c1u + c2u2 + c3u3 =          ck uk = uT c,
k=0
⎡   ⎤                  ⎡ ⎤
c0                   1                        ⎡     ⎤
⎢c ⎥                 ⎢ u ⎥                        ckx
⎢ ⎥                  ⎢ ⎥
c=⎢ 1⎥,              u=⎢ 2⎥,                 ck = ⎣ cky ⎦ .
⎣ c2 ⎦               ⎣u ⎦
c3                   u3
Thus, c is a column matrix containing the coefﬁcients of the polynomial; it is what
we wish to determine from the control-point data. We will derive a number of types
of cubic curves. The types will differ in how they use the control-point data. We
seek to ﬁnd 12 equations in 12 unknowns for each type, but because x, y, and z
are independent, we can group these equations into three independent sets of four
equations in four unknowns. When we discuss NURBS in Section 10.8.4, we will be
working in homogeneous coordinates, so we will have to use the w coordinate and
thus will have four sets of four equations in four unknowns.
The design of a particular type of cubic will be based on data given at some val-
ues of the parameter u. These data might take the form of interpolating conditions
in which the polynomial must agree with the data at some points. The data may also
require the polynomial to interpolate some derivatives at certain values of the pa-
rameter. We might also have smoothness conditions that enforce various continuity
conditions at the join points that are shared by two curve segments. Finally, we may
have conditions that are not as strict, requiring only that the curve pass close to sev-
eral known data points. Each type of condition will deﬁne a different type of curve,
and depending on how we use some given data, the same data can deﬁne more than
a single curve.

### 10.4    INTERPOLATION

Our ﬁrst example of a cubic parametric polynomial is the cubic interpolating poly-
nomial. Although we rarely use interpolating polynomials in computer graphics, the
derivation of this familiar polynomial illustrates the steps we must follow for our
other types, and the analysis of the interpolating polynomial illustrates many of the
important features by which we evaluate a particular curve or surface.
Suppose that we have four control points in three dimensions: p0, p1, p2, and p3.
512   Chapter 10   Curves and Surfaces
⎡    ⎤
p k = ⎣ yk ⎦ .
We seek the coefﬁcients c such that the polynomial p(u) = uT c passes through, or

```cpp
interpolates, the four control points. The derivation should be easy. We have four
```

three-dimensional interpolating points; hence, we have 12 conditions and 12 un-
knowns. First, however, we have to decide at which values of the parameter u the

```cpp
interpolation takes place. Lacking any other information, we can take these values to
```

be the equally spaced values u = 0, 31 , 23 , 1—remember that we have decided to let u
always vary over the interval [0, 1]. The four conditions are thus
p0 = p(0) = c0 ,
                2        3
1         1      1         1
p1 = p       = c0 + c 1 +      c2 +      c3 ,
3          3      3         3
                2        3
2          2      2         2
p2 = p       = c0 + c1 +       c2 +      c3 ,
3          3      3         3
p3 = p(1) = c0 + c1 + c2 + c3 .
We can write these equations in matrix form as
p = Ac,
⎡   ⎤
p0
⎢p ⎥
⎢ ⎥
p=⎢ 1⎥
⎣ p2 ⎦
p3
⎡                       ⎤
1 0      0      0
⎢                2    3 ⎥
⎢1          1    1      1
⎥
⎢           3    3      3   ⎥
A=⎢                2    3 ⎥ .
⎢           2    2      2   ⎥
⎣1          3    3      3   ⎦
1   1    1      1
The matrix form here has to be interpreted carefully. If we interpret p and c as column
matrices of 12 elements, the rules of matrix multiplication are violated. Instead, we
view p and c each as a four-element column matrix whose elements are three-element
row matrices. Hence, multiplication of an element of A, a scalar, by an element of c,
a three-element column matrix, yields a three-element column matrix, which is the
                                                                                                         10.4 Interpolation   513
p6
p4
p0      p1                p3
p5
p2

*FIGURE 10.10 Joining of interpolating segments.*

same type as an element of p.4 We can show that A is nonsingular, and we can invert
it to obtain the interpolating geometry matrix
⎡                             ⎤
1       0      0      0
⎢ −5.5            −4.5    1 ⎥
⎢           9                 ⎥
MI = A −1 = ⎢                                ⎥
⎣ 9      −22.5     18    −4.5 ⎦
−4.5 13.5 −13.5 4.5
and the desired coefﬁcients
c = MI p.
Suppose that we have a sequence of control points p0 , p1, . . . , pm. Rather than
deriving a single interpolating curve of degree m for all the points—a calculation we
could do by following a similar derivation to the one for cubic polynomials—we can
derive a set of cubic interpolating curves, each speciﬁed by a group of four control
points, and each valid over a short interval in u. We can achieve continuity at the
join points by using the control point that determines the right side of one segment
as the ﬁrst point for the next segment (Figure 10.10). Thus, we use p0 , p1, p2 , p3 to
ﬁnd the ﬁrst segment, we use p3 , p4 , p5 , p6 for the second, and so on. Note that if
each segment is derived for the parameter u varying over the interval (0, 1), then the
matrix MI is the same for each segment. Although we have achieved continuity for
the sequence of segments, derivatives at the join points will not be continuous.

#### 10.4.1 Blending Functions

We can obtain additional insights into the smoothness of the interpolating polyno-
mial curves by rewriting our equations in a slightly different form. We can substitute
the interpolating coefﬁcients into our polynomial; we obtain
p(u) = uT c = uT MI p,
4. We could use row matrices for the elements of p and c: In that case, ordinary matrix multiplica-
tions would work, because we would have a 4 × 4 matrix multiplying a 4 × 3 matrix. However, this
method would fail for surfaces. The real difﬁculty is that we should be using tensors to carry out the
mathematics—a topic beyond the scope of this book.
514   Chapter 10   Curves and Surfaces
p(u) = b(u)T p,
b(u) = MIT u
is a column matrix of the four blending polynomials
⎡       ⎤
b0(u)
⎢ b (u) ⎥
⎢       ⎥
b(u) = ⎢ 1 ⎥ .
⎣ b2(u) ⎦
b3(u)
Each blending polynomial is a cubic. If we express p(u) in terms of these blending

p(u) = b0(u)p0 + b1(u)p1 + b2(u)p2 + b3(u)p3 =             bi (u)pi ,
i=0
then we can see that the polynomials blend together the individual contributions of
each control point and enable us to see the effect of a given control point on the entire
curve. These blending functions for the cubic interpolating polynomial are shown in
Figure 10.11 and are given by the equations
          
9        1          2
b0(u) = −     u−          u−        (u − 1),
2        3          3

27           2
b1(u) = u u −            (u − 1),
2           3

27          1
b2(u) = − u u −            (u − 1),
2          3
          
9          1          2
b3(u) = u u −            u−        .
2          3          3
Because all the zeros of the blending functions lie in the closed interval [0, 1], the
blending functions must vary substantially over this interval and are not particularly
smooth. This lack of smoothness is a consequence of the interpolating requirement
that the curve must pass through the control points, rather than just come close to
them. This characteristic is even more pronounced for interpolating polynomials of
higher degree. This problem and the lack of derivative continuity at the join points
account for limited use of the interpolating polynomial in computer graphics. How-
ever, the same derivation and analysis process will allow us to ﬁnd smoother types of
cubic curves.
                                                                                            10.4 Interpolation   515
b1(u)                          b2(u)
1   b0(u)
b3(u)
1                  2
3                  3

*FIGURE 10.11 Blending polynomials for interpolation.*


#### 10.4.2 The Cubic Interpolating Patch

There is a natural extension of the interpolating curve to an interpolating patch. A
bicubic surface patch can be written in the form

3 
p(u, v) =                   ui v j cij ,
i=0 j=0
where cij is a three-element column matrix of the x, y, and z coefﬁcients for the ijth
term in the polynomial. If we deﬁne a 4 × 4 matrix whose elements are three-element
column matrices,
C = [ cij ] ,
p(u, v) = uT Cv,
⎡   ⎤
⎢ v ⎥
⎢ ⎥
v=⎢ 2⎥.
⎣v ⎦
v3
A particular bicubic polynomial patch is deﬁned by the 48 elements of C—that is, 16
three-element vectors.
Suppose that we have 16 three-dimensional control points pij , i = 0, . . . , 3,
j = 0, . . . , 3. We can use these points to specify an interpolating surface patch, as
shown in Figure 10.12. If we assume that these data are used for interpolation at the
equally spaced values of both u and v of 0, 31 , 23 , and 1, then we get three sets of 16
516   Chapter 10   Curves and Surfaces
p31           p32
p30
p20                                     p33
p21         p22     p23
p10              p11            p12      p13
p00               p01         p02         p03

*FIGURE 10.12 Interpolating surface patch.*

equations in 16 unknowns. For example, for u = v = 0, we get the three independent
⎡ ⎤
⎢0⎥
⎢ ⎥
p00 = [ 1 0 0 0 ] C ⎢ ⎥ = c00 .
⎣0⎦
Rather than writing down and solving all these equations, we can proceed in a more
direct fashion. If we consider v = 0, we get a curve in u that must interpolate p00, p10,
p20, and p30. Using our results on interpolating curves, we write this curve as
⎡     ⎤        ⎡ ⎤
p00             1
⎢p ⎥           ⎢0⎥
⎢     ⎥        ⎢ ⎥
p(u, 0) = uT MI ⎢ 10 ⎥ = uT C ⎢ ⎥ .
⎣ p20 ⎦        ⎣0⎦
p30             0
Likewise, the values of v = 31 , 23 , 1 deﬁne three other interpolating curves, each of
which has a similar form. Putting these curves together, we can write all 16 equations
uT MI P = uT CA T ,
where A is the inverse of MI . We can solve this equation for the desired coefﬁcient
C = MI PMTI ,
and substituting into the equation for the surface, we have
p(u, v) = uT MI PMTI v.
                                                                           10.5 Hermite Curves and Surfaces   517
We can interpret this result in several ways. First, the interpolating surface can be
derived from our understanding of interpolating curves—a technique that will enable
us to extend other types of curves to surfaces. Second, we can extend our use of
blending polynomials to surfaces. By noting that MIT u describes the interpolating
blending functions, we can rewrite our surface patch as

3 
p(u, v) =             bi (u)bj (v)pij .
i=0 j=0
Each term bi (u)bj (v) describes a blending patch. We form a surface by blending
together 16 simple patches, each weighted by the data at a control point. The basic
properties of the blending patches are determined by the same blending polynomials
that arose for interpolating curves; thus, most of the characteristics of surfaces are
similar to those of the curves. In particular, the blending patches are not particularly
smooth, because the zeros of the functions bi (u)bj (v) lie inside the unit square in
u, v space. Surfaces formed from curves using this technique are known as tensor-
product surfaces. Bicubic tensor-product surfaces are a subset of all surface patches
that contain up to cubic terms in both parameters. They are an example of separable
surfaces, which can be written as
p(u, v) = f (u)g(v),
where f and g are suitably chosen row and column matrices, respectively. The ad-
vantage of such surfaces is that they allow us to work with functions in u and v
independently.

### 10.5     HERMITE CURVES AND SURFACES

We can use the techniques that we developed for interpolating curves and surfaces to
generate various other types of curves and surfaces. Each type is distinguished from
the others by the way we use the data at control points.

#### 10.5.1 The Hermite Form

Suppose that we start with only the control points p0 and p3,5 and again, we insist
that our curve interpolate these points at the parameter values u = 0 and u = 1,
respectively. Using our previous notation, we have the two conditions
p(0) = p0 = c0 ,
p(1) = p3 = c0 + c1 + c2 + c3 .
5. We use this numbering to be consistent with our interpolation notation, as well as with the
numbering that we use for Bézier curves in Section 10.6.
518           Chapter 10       Curves and Surfaces
We can get two other conditions if we assume that we know the derivatives of the
y                      function at u = 0 and u = 1. The derivative of the polynomial is simply the parametric
p (0)                                      ⎡ dx ⎤
p (1)                du
p(1)                       ⎢ dy ⎥
p(0)                    x        p(u) = ⎢    ⎥
⎣ du ⎦ = c1 + 2uc2 + 3u c3 .
z                            If we denote the given values of the two derivatives as p0 and p3, then our two

*FIGURE 10.13 Definition of          additional conditions (Figure 10.13) are*

the Hermite cubic.
p0 = p(0) = c1,
p3 = p(1) = c1 + 2c2 + 3c3 .
We can write these equations in matrix form as
⎡ ⎤ ⎡                    ⎤
p0        1 0 0 0
⎢p ⎥ ⎢ 1 1 1 1⎥
⎢ 3⎥ ⎢                   ⎥
⎢  ⎥=⎢                  ⎥ c.
⎣ p0 ⎦ ⎣ 0 1 0 0 ⎦
p3       0 1 2 3
⎡ ⎤
p0
⎢p ⎥
⎢ ⎥
q = ⎢ 3 ⎥ ,
⎣ p0 ⎦
p3
we can solve the equations to ﬁnd
c = MH q,
⎡                    ⎤
1     0   0     0
⎢ 0                0 ⎥
⎢        0   1       ⎥
MH = ⎢                     ⎥.
⎣ −3 3 −2 −1 ⎦
p (1) = q (0)                    2 −2 1           1
p(1) = q(0)        The resulting polynomial is given by
p(0)
q(1)
p(u) = uT MH q.

*FIGURE 10.14 Hermite form           We use this method as shown in Figure 10.14, where both the interpolated value*

at join point.                      and the derivative are shared by the curve segments on the two sides of a join point,
                                                                     10.5 Hermite Curves and Surfaces                 519
and thus both the resulting function and the ﬁrst derivative are continuous over all
segments.
We can get a more accurate idea of the increased smoothness of the Hermite form
p(u) = b(u)T q,
where the new blending functions are given by
⎡ 3             ⎤
2u − 3u2 + 1
⎢ −2u3 + 3u2 ⎥
⎢               ⎥
b(u) = MHT
u=⎢ 3               ⎥.
⎣ u − 2u2 + u ⎦
u3 − u2
These four polynomials have none of their zeros inside the interval (0, 1) and are
much smoother than are the interpolating polynomial blending functions (see Exer-
cise 10.16).
We can go on and deﬁne a bicubic Hermite surface patch through these blending
functions,

3 
p(u, v) =             bi (u)bj (v)qij ,
i=0 j=0
where Q = [qij ] is the extension of q to surface data. At this point, however, this
equation is just a formal expression. It is not clear what the relationship is between
the elements of Q and the derivatives of p(u, v). Four of the elements of Q are chosen
to interpolate the corners of the patch, whereas the others are chosen to match certain
derivatives at the corners of the patch. In most interactive applications, however,
the user enters point data rather than derivative data; consequently, unless we have
analytic formulations for the data, usually we do not have these derivatives. However,
the approach we took with the Hermite curves and surfaces will lead to the Bézier
forms that we introduce in Section 10.6.
p(1) = q(0)
p(0)   p(u)

#### 10.5.2 Geometric and Parametric Continuity                                                                        q(1)

q(u)
Before we discuss the Bézier and spline forms, we examine a few issues concerning
continuity and derivatives. Consider the join point in Figure 10.15. Suppose that the     FIGURE 10.15 Continuity at
polynomial on the left is p(u) and the one on the right is q(u). We enforce various       the join point.
continuity conditions by matching the polynomials and their derivatives at u = 1 for
p(u), with the corresponding values for q(u) at u = 0. If we want the function to be
continuous, we must have
⎡         ⎤          ⎡         ⎤
px (1)               qx (0)
p(1) = ⎣ py (1) ⎦ = q(0) = ⎣ qy (0) ⎦ .
pz (1)               qz (0)
520            Chapter 10      Curves and Surfaces
All three parametric components must be equal at the join point; we call this property
C 0 parametric continuity.
When we consider derivatives, we can require, as we did with the Hermite curve,
⎡         ⎤           ⎡        ⎤
px (1)                qx (0)
p(1) = ⎣ py (1) ⎦ = q (0) = ⎣ qy (0) ⎦ .
pz (1)               qz (0)
If we match all three parametric equations and the ﬁrst derivative, we have C 1 para-
metric continuity.
If we look at the geometry, however, we can take a different approach to continu-
ity. In three dimensions, the derivative at a point on a curve deﬁnes the tangent line at
that point. Suppose that instead of requiring matching of the derivatives for the two
segments at the join point, we require only that their derivatives be proportional:
p(1) = αq (0),
for some positive number α. If the tangents of the two curves are proportional, then
they point in the same direction, but they may have different magnitudes. We call this
type of continuity G1 geometric continuity.6 If the two tangent vectors need only to
be proportional, we have only two conditions to enforce, rather than three, leaving
1 extra degree of freedom that we can potentially use to satisfy some other criterion.
q (0)          q (1)
We can extend this idea to higher derivatives and can talk about both C n and Gn
continuity.
p (0)    q(u)    p (1)
Although two curves that have only G1 continuity at the join points have a con-
tinuous tangent at the join points, the value of the constant of proportionality—or
p(u)                 equivalently, the relative magnitudes of the tangents on the two sides of the join
point—does matter. Curves with the same tangent direction but different magnitudes

*FIGURE 10.16 Change of              differ, as shown in Figure 10.16. The curves p(u) and q(u) share the same endpoints,*

magnitude in G1 continuity.         and the tangents at the endpoints point in the same direction, but the curves are
different. This result is exploited in many painting programs, where the user can in-
teractively change the magnitude, leaving the tangent direction unchanged. However,
in other applications, such as animation, where a sequence of curve segments de-
scribes the path of an object, G1 continuity may be insufﬁcient (see Exercise 10.11).

### 10.6      BÉZIER CURVES AND SURFACES

Comparing the Hermite form to the interpolating form is problematic; we are com-
paring forms with some similarities but with signiﬁcant differences. Both are cubic
polynomial curves, but the forms do not use the same data; thus, they cannot be
compared on equal terms. We can use the same control-point data that we used to
6. G0 continuity is the same as C 0 continuity.
                                                                        10.6 Bézier Curves and Surfaces                  521
derive the interpolating curves to approximate the derivatives in the Hermite curves.
The resulting Bézier curves are excellent approximations to the Hermite curves and
are comparable to the interpolating curves because they have been obtained using the
same data. In addition, because these curves do not need derivative information, they
are well suited for use in graphics and CAD.

#### 10.6.1 Bézier Curves

Consider again the four control points: p0, p1, p2, and p3. Suppose that we still insist
on interpolating known values at the endpoints with a cubic polynomial p(u):
p0 = p(0),
p3 = p(1).
Bézier proposed that rather than using the other two control points, p2 and p3,
for interpolation, we use them to approximate the tangents at u = 0 and u = 1. In
parameter space, we can use the linear approximations
p1
p1 − p0                                                                                            p2

p (0) ≈         1
= 3(p1 − p0),
3                                                                                        p(u)
p3 − p 2
p(1) ≈         1
= 3(p3 − p2),
3                                                                              p0                    p3
as shown in Figure 10.17. Applying these approximations to the derivatives of our          FIGURE 10.17 Approximat-
parametric polynomial, p(u) = uT c, at the two endpoints, we have the two condi-           ing tangents.
3p1 − 3p0 = c1,
3p3 − 3p2 = c1 + 2c2 + 3c3
p0 = c 0 ,
p3 = c 0 + c 1 + c 2 + c 3 .
At this point, we again have three sets of four equations in four unknowns that we
can solve, as before, to ﬁnd
c = MB p,
where MB is the Bézier geometry matrix
522   Chapter 10   Curves and Surfaces
0.8
0.6
0.4
0.2
0          0.2        0.4        0.6     0.8         1

*FIGURE 10.18 Blending polynomials for the Bézier cubic.*

⎡      ⎤
1 0 0 0
⎢ −3 3 0 0⎥
⎢           ⎥
MB = ⎢           ⎥.
⎣ 3 −6 3 0 ⎦
−1 3 −3 1
The cubic Bézier polynomial is thus
p(u) = uT MB p.
We use this formula exactly as we did for the interpolating polynomial. If we have a
set of control points, p0 , . . . , pn, we use p0, p1, p2, and p3 for the ﬁrst curve; p3, p4,
p5, and p6 for the second; and so on. It should be clear that we have C 0 continuity,
but we have given up the C 1 continuity of the Hermite polynomial because we use
different approximations on the left and right of a join point.
We can see important advantages to the Bézier curve by examining the blending
functions in Figure 10.18. We write the curve as
p(u) = b(u)T p,
⎡    ⎤
(1 − u)3
⎢ 3u(1 − u)2 ⎥
⎢            ⎥
b(u) = MBT u = ⎢ 2          ⎥.
⎣ 3u (1 − u) ⎦
u3
These four polynomials are one case of the Bernstein polynomials,
d!
bkd (u) =                uk (1 − u)d−k ,
k!(d − k)!
                                                                        10.6 Bézier Curves and Surfaces                  523
which can be shown to have remarkable properties. First, all the zeros of the polyno-
mials are either at u = 0 or at u = 1. Consequently, for each blending polynomial,
bid (u) > 0,
for 1 > u > 0. Without any zeros in the interval, each blending polynomial must be
smooth. We can also show that, in this interval (see Exercise 10.5),
1 > bid (u),

bid (u) = 1.
i=0
Under these conditions, the representation of our cubic Bézier polynomial in terms                 p1
p2
of its blending polynomials,
p(u)

p(u) =          bi (u)pi ,
i=0
p0                    p3
is a convex sum. Consequently, p(u) must lie in the convex hull of the four control
points, as shown in Figure 10.19. Thus, even though the Bézier polynomial does not       FIGURE 10.19 Convex hull

```cpp
interpolate all the control points, it cannot be far from them. These two properties,     and the Bézier polynomial.
```

combined with the fact that we are using control-point data, make it easy to work

```cpp
interactively with Bézier curves. A user can enter the four control points to deﬁne an
```

initial curve, and then can manipulate the points to control the shape.

#### 10.6.2 Bézier Surface Patches

We can generate the Bézier surface patches through the blending functions. If P is a
4 × 4 array of control points,
 
P = pij ,
then the corresponding Bézier patch is

3 
p(u, v) =               bi (u)bj (v)pij = uT MB PMTB v.
i=0 j=0
The patch is fully contained in the convex hull of the control points (Figure 10.20)
and interpolates p00, p03, p30, and p33. We can interpret the other conditions as
approximations to various derivatives at the corners of the patch.
Consider the corner for u = v = 0. We can evaluate p(u) and the ﬁrst partial
derivatives to ﬁnd
p(0, 0) = p00 ,
524   Chapter 10   Curves and Surfaces
p30
p33
p00                            p03

*FIGURE 10.20 Bézier patch.*

p10                    p11
p01
p00

*FIGURE 10.21 Twist at corner of Bézier patch.*

∂p
(0, 0) = 3(p10 − p00),
∂u
∂p
(0, 0) = 3(p01 − p00),
∂v
∂ 2p
(0, 0) = 9(p00 − p01 + p10 − p11).
∂u∂v
The ﬁrst three conditions are clearly extensions of our results for the Bézier curve. The
fourth can be seen as a measure of the tendency of the patch to divert from being ﬂat,
or to twist, at the corner. If we consider the quadrilateral speciﬁed by these points
(Figure 10.21), the points will lie in the same plane only if the twist is zero. Color
Plate 25 uses Bézier patches to create a smooth surface from elevation data.

### 10.7    CUBIC B-SPLINES

In practice, the cubic Bézier curves and surface patches are widely used. They have
one fundamental limitation: At the join points (or patch edges, for surfaces), we have
only C 0 continuity. If, for example, we were to use these curves to design our model-
airplane cross sections, as shown in Section 10.2, and then were to attempt to build
those cross sections, we might be unhappy with the way that the pieces meet at the
join points.
It might seem that we have reached the limit of what we can do with cubic
parametric polynomials, and that if we need more ﬂexibility, we have to either go to
high-degree polynomials or shorten the interval and use more polynomial segments.
Both of these tactics are possibilities—but there is another: We can use the same
                                                                                        10.7 Cubic B-Splines            525
control-point data but not require the polynomial to interpolate any of these points.
If we can come close to the control points and get more smoothness at the join points,
we may be content with the result.

#### 10.7.1 The Cubic B-Spline Curve

In this section, we illustrate a particular example of a B-spline curve and show how
we can obtain C 2 continuity at the join points with a cubic. In Section 10.8, we give a
short introduction to a more general approach to splines—an approach that is gen-
p2
eral enough to include the Bézier curves as a special case. Consider four control points                          p3
in the middle of a sequence of control points: {pi−2 , pi−1, pi , pi+1}. Our previous ap-
proach was to use these four points to deﬁne a cubic curve such that, as the parameter               p(0)
p0              p(1)
u varied from 0 to 1, the curve spanned the distance from pi−2 to pi+1, interpolating                                   p1
pi−2 and pi+1. Instead, suppose that as u goes from 0 to 1, we span only the dis-
tance between the middle two control points, as shown in Figure 10.22. Likewise, we           FIGURE 10.22 Four points
use {pi−3 , pi−2 , pi−1, pi } between pi−2 and pi−1, and {pi−1, pi , pi+1, pi+2} between pi   that define a curve between
and pi+1. Suppose that p(u) is the curve we use between pi−1 and pi , and q(u) is the         the middle two points.
curve to its left, used between pi−2 and pi−1. We can match conditions at p(0) with
conditions at q(1). Using our standard formulation, we are looking for a matrix M,
such that the desired cubic polynomial is
p(u) = uT Mp,
⎡       ⎤
pi−2
⎢p ⎥
⎢       ⎥
p = ⎢ i−1 ⎥ .
⎣ pi ⎦
pi+1
We can use the same matrix to write q(u) as
q(u) = uT Mq,
⎡       ⎤
pi−3
⎢p ⎥
⎢      ⎥
q = ⎢ i−2 ⎥ .
⎣ pi−1 ⎦
In principle, we could write a set of conditions on p(0) that would match conditions
for q(1), and we could write equivalent conditions matching various derivatives of
p(1) with conditions for another polynomial that starts there. For example, the con-
p(0) = q(1)
526   Chapter 10   Curves and Surfaces
requires continuity at the join point, without requiring interpolation of any data.
Enforcing this condition gives one equation for the coefﬁcients of M. There are clearly
many sets of conditions that we can use; each set can deﬁne a different matrix.
We can take a shortcut to deriving the most popular matrix, by noting that
we must use symmetric approximations at the join point. Hence, any evaluation of
conditions on q(1) cannot use pi−3, because this control point does not appear in
the equation for p(u). Likewise, we cannot use pi+1 in any condition on p(0). Two
conditions that satisfy this symmetry condition are
p(0) = q(1) = (pi−2 + 4pi−1 + pi ),
p(0) = q (1) = (pi − pi−2).
If we write p(u) in terms of the coefﬁcient array c,
p(u) = uT c,
c0 = (pi−2 + 4pi−1 + pi ),
c1 = (pi − pi−2).
We can apply the symmetric conditions at p(1):
p(1) = c0 + c1 + c2 + c3 = (pi−1 + 4pi + pi+1),
p(1) = c1 + 2c2 + 3c3 = (pi+1 − pi−1).
We now have four equations for the coefﬁcients of c, which we can solve for a matrix
MS , the B-spline geometry matrix,
⎡         ⎤
1 4  1 0
1⎢⎢ −3 0 3 0⎥ ⎥
MS = ⎢            ⎥.
6 ⎣ 3 −6 3 0 ⎦
−1 3 −3 1
This particular matrix yields a polynomial that has several important properties.
We can see these properties by again examining the blending polynomials:
                                                                                                   10.7 Cubic B-Splines            527
b1(u)                 b2(u)
b0(u)             b3(u)

*FIGURE 10.23 Spline-blending functions.*

⎡               ⎤
(1 − u)3
1⎢⎢ 4 − 6u + 3u
2     3   ⎥
⎥
b(u) = MST u = ⎢                     ⎥.
6 ⎣ 1 + 3u + 3u2 − 3u3 ⎦
u3
These polynomials are shown in Figure 10.23. We can show, as we did for the Bézier
polynomials, that

bi (u) = 1,
i=0
p2
and, in the interval 1 > u > 0,                                                                                               p3
1 > bi (u) > 0.                                                                                            p0
p1
Thus, the curve must lie in the convex hull of the control points, as shown in Fig-
ure 10.24. Note that the curve is used for only part of the range of the convex hull.                    FIGURE 10.24 Convex hull for
We deﬁned the curve to have C 1 continuity; in fact, however, it has C 2 continuity,7 as                 spline curve.
we can verify by computing p(u) at u = 0 and u = 1 and seeing that the values are
the same for the curves on the right and left. It is for this reason that spline curves
are so important. From a physical point of view, metal will bend such that the second
derivative is continuous. From a visual perspective, a curve made of cubic segments
with C 2 continuity will be seen as smooth, even at the join points.
Although we have used the same control-point data as those we used for the
Bézier cubic to derive a smoother cubic curve, we must be aware that we are doing
three times the work that we would do for Bézier or interpolating cubics. The reason
is that we are using the curve between only control point i − 1 and control point i. A
Bézier curve using the same data would be used from control point i − 2 to control
7. If we are concerned with only G2, rather than with C 2, continuity, we can use the extra degrees of
freedom to give additional ﬂexibility in the design of the curves; see Barsky [Bar83].
528   Chapter 10   Curves and Surfaces
point i + 1. Hence, each time we add a control point, a new spline curve must be
computed, whereas for Bézier curves, we add the control points three at a time.

#### 10.7.2 B-Splines and Basis

Instead of looking at the curve from the perspective of a single interval, we can gain
additional insights by looking at the curve from the perspective of a single control
point. Each control point contributes to the spline in four adjacent intervals. This
property guarantees the locality of the spline; that is, if we change a single control
point, we can affect the resulting curve in only four adjacent intervals. Consider the
control point pi . In the interval between u = 0 and u = 1, it is multiplied by the
blending polynomial b2(u). It also contributes to the interval on the left through q(u).
In this interval, its contribution is b1(u + 1)—we must shift the value of u by 1 to the
left for this interval.
The total contribution of a single control point can be written as Bi (u)pi , where
⎧
⎪  0            u < i − 2,
⎪
⎪
⎪
⎪  b   (u + 2)  i − 2 ≤ u < i − 1,
⎪
⎪    0
⎨ b (u + 1) i − 1 ≤ u < i,
Bi (u) =
⎪
⎪  b2(u)        i ≤ u < i + 1,
⎪
⎪
⎪
⎪  b (u − 1) i + 1 ≤ u < i + 2,
⎪
⎩ 3
0            u ≥ i + 2.
This function is pictured in Figure 10.25. Given a set of control points p0 , . . . , pm,
we can write the entire spline with the single expression8

m−1
p(u) =         Bi (u)pi .
i=1
This expression shows that for the set of functions B(u − i), each member is a shifted
version of a single function, and the set forms a basis for all our cubic B-spline
curves. Given a set of control points, we form a piecewise polynomial curve p(u)
over the whole interval as a linear combination of basis functions. Figure 10.26 shows
the function and the contributions from the individual basis functions. The general
theory of splines that we develop in Section 10.8 expands this view by allowing
higher-degree polynomials in the intervals and by allowing different polynomials in
different intervals.

#### 10.7.3 Spline Surfaces

B-spline surfaces can be deﬁned in a similar way. If we start with the B-spline blending
functions, the surface patch is given by
8. We determine the proper conditions for the beginning and end of the spline in Section 10.8.
                                                                                   10.8 General B-Splines   529
b1(u + 1) b2(u)
b0(u + 2)                                 b3(u – 1)
i–2           i–1          i      i+1              i+2

*FIGURE 10.25 Spline basis function.*

pi – 1   pi   pi + 1
pi – 2                          pi + 2
p(u)
i–2 i–1           i    i+1 i+2

*FIGURE 10.26 Approximating function over interval.*

p30
p33
p00                                    p03

*FIGURE 10.27 Spline surface patch.*


3 
p(u, v) =                 bi (u)bj (v)pij .
i=0 j=0
This expression is of the same form as are those for our other surface patches, but
as we can see from Figure 10.27, we use the patch over only the central area, and
we must do nine times the work that we would do with the Bézier patch. However,
because of inheritance of the convex-hull property and the additional continuity at
the edges from the B-spline curves, the B-spline patch is considerably smoother than
a Bézier patch constructed from the same data would be.

### 10.8        GENERAL B-SPLINES

Suppose that we have a set of control points, p0 , . . . , pm. The general approximation
problem is to ﬁnd a function p(u) = [ x(u) y(u) z(u) ]T , deﬁned over an interval
530   Chapter 10   Curves and Surfaces
umax ≥ u ≥ umin , that is smooth and is close, in some sense, to the control points.
Suppose we have a set of values {uk }, called knots, such that
umin = u0 ≤ u1 ≤ . . . ≤ un = umax .
We call the sequence u0 , u1, . . . , un the knot array.9 In splines, the function p(u) is
a polynomial of degree d between the knots,

p(u) =         cjk uj ,   uk < u < uk+1.
j=0
Thus, to specify a spline of degree d, we must specify the n(d + 1) three-dimensional
coefﬁcients cjk . We get the required conditions by applying various continuity re-
quirements at the knots and interpolation requirements at control points.
For example, if d = 3, then we have a cubic polynomial in each interval, and,
for a given n, we must specify 4n conditions. There are n − 1 internal knots. If we
want C 2 continuity at the knots, we have 3n − 3 conditions. If in addition we want to

```cpp
interpolate the n + 1 control points, we have a total of 4n − 2 conditions. We can pick
```

the other two conditions in various ways, such as by ﬁxing the slope at the ends of the
curve. However, this particular spline is global; we must solve a set of 4n equations
in 4n unknowns, and each coefﬁcient will depend on all the control points. Thus,
although such a spline provides a smooth curve that interpolates the control points,
it is not well suited to computer graphics and CAD.

#### 10.8.1 Recursively Defined B-Splines

The approach taken in B-splines is to deﬁne the spline in terms of a set of basis, or
blending, functions, each of which is nonzero over only the regions spanned by a few
knots. Thus, we write the function p(u) as an expansion:

p(u) =         Bid (u)pi ,
i=0
where each function Bid (u) is a polynomial of degree d, except at the knots, and is
zero outside the interval (ui , ui ). The name B-splines comes from the term basis
splines, in recognition that the set of functions {Bid (u)} forms a basis for the given
knot sequence and degree. Although there are numerous ways to deﬁne basis splines,
of particular importance is the set of splines deﬁned by the Cox-deBoor recursion:10

1, uk ≤ u ≤ uk+1;
Bk0 =
0, otherwise
9. Most researchers call this sequence the knot vector, but that terminology violates our decision to
use vector for only directed line segments.
10. This formula is also known as the deCasteljau recursion.
                                                                                           10.8 General B-Splines   531
uk + 1
uk + 1   uk + 2
uk    uk + 1          uk            uk + 2          uk                     uk + 3

*FIGURE 10.28 First three basis functions.*

u − uk                  uk+d − u
Bkd =              Bk, d−1(u) +               Bk+1, d−1(u).
uk+d − uk              uk+d+1 − uk+1
Each of the ﬁrst set of functions, Bk0, is constant over one interval and is zero every-
where else; each of the second, Bk1, is linear over each of two intervals and is zero
elsewhere; each of the third, Bk2, is quadratic over each of three intervals; and so on
(Figure 10.28). In general, Bkd is nonzero over the d + 1 intervals between uk and
uk+d+1, and it is a polynomial of degree d in each of these intervals. At the knots,
there is C d−1 continuity. The convex-hull property holds because

Bi, d (u) = 1,
i=0
1 ≥ Bid (u) ≥ 0,
in the interval umax ≥ u ≥ umin .
However, because each Bid is nonzero in only d + 1 intervals, each control point
can affect only d + 1 intervals, and each point on the resulting curve is within the
convex hull deﬁned by these d + 1 control points.
Note that careful examination of the Cox-deBoor formula shows that each step
of the recursion is a linear interpolation of functions produced on the previous step.
Linear interpolation of polynomials of degree k produces polynomials of degree
k + 1.
A set of spline basis functions is deﬁned by the desired degree and the knot array.
Note that we need what appears to be d − 1 “extra” knot values to specify our spline
because the recursion requires u0 through un+d to specify splines from u0 to un+1.
These additional values are determined by conditions at the beginning and end of
the whole spline.
Note that we have made no statement about the knot values other than that
uk ≤ uk+1. If we deﬁne any 0/0 term that arises in evaluating the recursion as equal
to 1, then we can have repeated, or multiple, knots. If the knots are equally spaced,
we have a uniform spline. However, we can achieve more ﬂexibility by allowing not
only nonuniform knot spacing but also repeated (uk = uk+1) knots. Let’s examine a
few of the possibilities.
532        Chapter 10    Curves and Surfaces
pk           pk + 1
pk  1
pk + 2
uk  1     uk   uk + 1 uk + 2

*FIGURE 10.29 Uniform B-spline.*


#### 10.8.2 Uniform Splines

Consider the uniform knot sequence {0, 1, 2, . . . , n}. The cubic B-spline we dis-
cussed in Section 10.7 could be derived from the Cox-deBoor formula with equally
spaced knots. We use the numbering that we used there (which is shifted from the
Cox-deBoor indexing); between knots k and k + 1, we use the control points pk−1, pk ,
pk+1, and pk+2. Thus, we have a curve deﬁned for only the interval u = 1 to u = n − 1.
For the data shown in Figure 10.29, we deﬁne a curve that does not span the knots.

*FIGURE 10.30 Periodic uni-    In certain situations, such as that depicted in Figure 10.30, we can use the periodic*

form B-spline.                nature of the control-point data to deﬁne the spline over the entire knot sequence.
These uniform periodic B-splines have the property that each spline basis function
is a shifted version of a single function.

#### 10.8.3 Nonuniform B-Splines

Repeated knots have the effect of pulling the spline closer to the control point associ-
ated with the knot. If a knot at the end has multiplicity d + 1, the B-spline of degree
d must interpolate the point. Hence, one solution to the problem of the spline not
having sufﬁcient data to span the desired interval is to repeat knots at the ends, forc-
ing interpolation at the endpoints, and using uniform knots everywhere else. Such
splines are called open splines.
The knot sequence {0, 0, 0, 0, 1, 2, . . . , n − 1, n, n, n, n} is often used for cubic
B-splines. The sequence {0, 0, 0, 0, 1, 1, 1, 1} is of particular interest, because, in this
case, the cubic B-spline becomes the cubic Bézier curve. In the general case, we can
repeat internal knots, and we can have any desired spacing of knots.

#### 10.8.4 NURBS

In our development of B-splines, we have assumed that p(u) is the array [x(u)
y(u) z(u)]T . In two dimensions, however, we could have replaced it with simply
[x(u) y(u)]T , and all our equations would be unchanged. Indeed, the equations re-
main unchanged if we go to four-dimensional B-splines. Consider a control point in
three dimensions:
pi = [xi    yi      zi ].
                                                                                   10.8 General B-Splines   533
The weighted homogeneous-coordinate representation of this point is
⎡ ⎤
⎢y ⎥
⎢ ⎥
qi = w i ⎢ i ⎥ .
⎣ zi ⎦
The idea is to use the weights wi to increase or decrease the importance of a particular
control point. We can use these weighted points to form a four-dimensional B-
spline. The ﬁrst three components of the resulting spline are simply the B-spline
representation of the weighted points,
⎡       ⎤
x(u)        n
⎣
q(u) = y(u) =   ⎦         Bi, d (u)wi pi .
z(u)       i=0
The w component is the scalar B-spline polynomial derived from the set of weights:

w(u) =         Bi, d (u)wi .
i=0
In homogeneous coordinates, this representation has a w component that may not
be equal to 1; thus, we must do a perspective division to derive the three-dimensional
points:
n
1              i=0 Bi, d (u)wi pi
p(u) =        q(u) = n                     .
w(u)              i=0 Bi, d (u)wi
Each component of p(u) is now a rational function in u, and because we have not
restricted the knots in any way, we have derived a nonuniform rational B-spline
(NURBS) curve.
NURBS curves retain all the properties of our three-dimensional B-splines, such
as the convex-hull and continuity properties. They have two other properties that
make them of particular interest in computer graphics and CAD.
If we apply an afﬁne transformation to a B-spline curve or surface, we get the
same function as the B-spline derived from the transformed control points. Because
perspective transformations are not afﬁne, most splines will not be handled correctly
in perspective viewing. However, the perspective division embedded in the construc-
tion of NURBS curves ensures that NURBS curves are handled correctly in perspec-
tive views.
Quadric surfaces are usually speciﬁed by algebraic implicit forms. If we are using
nonrational splines, we can only approximate these surfaces. However, quadrics can
be shown to be a special case of quadratic NURBS curves; thus, we can use a single
modeling method, NURBS curves, for the most widely used curves and surfaces (see
Exercises 10.14 and 10.15). Color Plate 5 shows the mesh generated by a NURBS
534          Chapter 10         Curves and Surfaces
modeling of the surfaces that make up the object in Color Plate 1. OpenGL ultimately
renders this mesh with polygons.

#### 10.8.5 Catmull-Rom Splines

If we relax the requirement that our curves and surfaces must lie within the convex
hull of the data, we can use our data to form other types of splines. One of the most
popular is the Catmull-Rom spline.
Consider again the four control points, p0, p1, p2, and p3, that we used in our
derivation of the Bézier curve. Suppose that rather than deriving a cubic polynomial
that interpolates p0 and p1, we interpolate the middle points p1 and p2:
p1     p'(1)
p2             p(0) = p1,
p(1)           p(2)
p0
p(1) = p2 .
p'(2)
p3
Thus, like the B-spline, our polynomial will be deﬁned over a shorter interval,

*FIGURE 10.31 Constructing            and each time that we add a new control point, we ﬁnd a new curve.*

the Catmull-Rom spline.                  We use the points p0 and p3 to specify tangents at p0 and p1 (Figure 10.31):
p2 − p 0
p(0) ≈              ,
p 3 − p1
p(1) ≈              .
We now have four conditions on the curve
p(u) = c0 + c1u + c2u2 + c3u3 ,
p1 = c0 ,
p2 = c0 + c1 + c2 + c3 ,
p2 − p0
= c1,
p3 − p 1
= c1 + 2c2 + 3c3 .
Note that because as u goes from 0 to 1, we only go from p1 to p2, so p0 and p2
are separated by 2 units in parameter space, as are p1 and p3 . In addition, these
four conditions ensure that the resulting curves are continuous and have continuous
ﬁrst derivatives at the control points, even though we do not have the convex hull
property.
                                                                  10.9 Rendering Curves and Surfaces   535
p(u) = uT MR p,
where MR is the Catmull-Rom geometry matrix
⎡                   ⎤
−1 3 −3 0
1⎢⎢ 2 −5 4 −1 ⎥
⎥
MR = ⎢                      ⎥.
2 ⎣ −1 0       1    0 ⎦
0     2   0    0

### 10.9    RENDERING CURVES AND SURFACES

Once we have speciﬁed a scene with curves and surfaces, we must ﬁnd a way to
render it. There are several approaches, depending on the type of representation. For
explicit and parametric curves and surfaces, we can evaluate the curve or surface at a
sufﬁcient number of points that we can approximate it with our standard ﬂat objects.
We focus on this approach for parametric polynomial curves and surfaces.
For implicit surfaces, we can compute points on the object that are the intersec-
tion of rays from the center of projection through pixels with the object. We can then
use these points to specify curve sections or meshes that can be rendered directly.
However, except for quadrics (Section 10.11), the intersection calculation requires
the solution of nonlinear equations of too high a degree to be practical for real-time
computation.
b(u) = (1 − u)3p0 + (1 − u)2up1 + (1 − u)u2p0 + u3p3 .
If we want to evaluate it at N equally spaced values of u and put the results into an
array points as in our previous examples, the code for a two-dimensional example

```cpp
float d = 1.0/(N-1.0);
float u, uu;
```

for(int i=0; i<N; i++)

```cpp
{
```

u = i*d;
uu = 1.0 - u;
for(int j=0; j<2; j++) points[i][j] = p[0][j]*uu*uu*uu
+ 3.0*p[1][j]*uu*uu*u
+ 3.0*p[2][j]*uu*u*u
+ p[3][j]*u*u*u;

```cpp
}
```

where the control point data are in the array p.
536   Chapter 10   Curves and Surfaces

#### 10.9.1 Polynomial Evaluation Methods

Suppose that we have a representation over our standard interval

p(u) =              ci u i ,       0 ≤ u ≤ 1.
i=0
We can evaluate p(u) at some set of values {uk }, and we can use a polyline (or GL_
LINE_STRIP) to approximate the curve. Rather than evaluate each term uk indepen-
dently, we can group the terms as
p(u) = c0 + u(c1 + u(c2 + u(. . . + cnu))).
This grouping shows that we need only n multiplications to evaluate each p(uk ); this
algorithm is known as Horner’s method. For our typical cubic p(u), the grouping
p(u) = c0 + u(c1 + u(c2 + uc3)).
If the points {ui } are spaced uniformly, we can use the method of forward dif-
ferences to evaluate p(uk ) using O(n) additions and no multiplications. The forward
differences are deﬁned iteratively by the formulas
(0)
p(uk ) = p(uk ),
(1)
p(uk ) = p(uk+1) − p(uk ),
(m+1)                        (m)                 (m)
p(uk ) =                  p(uk+1) −           p(uk ).
If uk+1 − uk = h is constant, then we can show that if p(u) is a polynomial of degree
n, then (n)p(uk ) is constant for all k. This result suggests the strategy illustrated in
Figure 10.32 for the scalar cubic polynomial
p(u) = 1 + 3u + 2u2 + u3 .
t            0              1      2        3       4         5
p       1              7     23       55      109      191
(1)p         6         16         32       54       82
(2)p       10          16         22       28
(3)p         6              6      6

*FIGURE 10.32 Construction of a forward-difference table.*

                                                                         10.9 Rendering Curves and Surfaces   537
t         0        1          2     3      4          5
p     1        7         23    55    109       191
(1)p      6    16            32    54     82
(2)p     10    16            22    28
(3)p      6        6          6

*FIGURE 10.33 Use of a forward-difference table.*

We need the ﬁrst n + 1 values of p(uk ) to ﬁnd (n)p(u0). But once we have (n)p(u0),
we can copy this value across the table and work upward, as shown in Figure 10.33,
to compute successive values of p(uk ), using the rearranged recurrence
(m−1)                    (m)              (m−1)
(pk+1) =             p(uk ) +           p(uk ).
This method is efﬁcient, but it is not without its faults: It applies only to a uniform
grid, and it is prone to accumulation of numerical errors.

#### 10.9.2 Recursive Subdivision of Bézier Polynomials

The most elegant rendering method performs recursive subdivision of the Bézier
curve. The method is based on the use of the convex hull and never requires explicit
evaluation of the polynomial. Suppose that we have a cubic Bézier polynomial (the
method also applies to higher-degree Bézier curves). We know that the curve must lie
within the convex hull of the control points. We can break the curve into two separate
polynomials, l(u) and r(u), each valid over one-half of the original interval. Because
the original polynomial is a cubic, each of these polynomials also is a cubic. Note
that because each is to be used over one-half of the original interval, we must rescale
the parameter u for l and r so that as u varies over the range (0, 1), l(u) traces the
left half of p(u), and r(u) traces the right half of p. Each of our new polynomials
has four control points that both specify the polynomial and form its convex hull.
We denote these two sets of points by {l0 , l1, l2 , l3} and {r0 , r1, r2 , r3}; the original
control points for p(u) are {p0 , p1, p2 , p3}. These points and the two convex hulls
are shown in Figure 10.34. Note that the convex hulls for l and r must lie inside the
convex hull for p, a result known as the variation-diminishing property of the Bézier
curve.
Consider the left polynomial. We can test the convex hull for ﬂatness by measur-
ing the deviation of l1 and l2 from the line segment connecting l0 and l3. If they are
close, we can draw the line segment instead of the curve. If they are not close, we can
divide l into two halves and test the two new convex hulls for ﬂatness. Thus, we have
a recursion that never requires us to evaluate points on a polynomial, but we have yet
to discuss how to ﬁnd {l0 , l1, l2 , l3} and {r0 , r1, r2 , r3}. We will ﬁnd the hull for l(u);
538   Chapter 10   Curves and Surfaces
p1                           p2
l2                r1
l1               l3 = r0               r2
l(u)             r(u)
p0 = l0                                               p3 = r3

*FIGURE 10.34 Convex hulls and control points.*

the calculation for r(u) is symmetric. We can start with
⎡ ⎤
p0
⎢p ⎥
⎢ ⎥
p(u) = uT MB ⎢ 1 ⎥ ,
⎣ p2 ⎦
p3
⎡       ⎤
1 0 0  0
⎢ −3 3    0 ⎥
⎢      0     ⎥
MB = ⎢            ⎥.
⎣ 3 −6 3  0 ⎦
−1 3 −3 −1
 
The polynomial l(u) must interpolate p(0) and p                  2 ; hence,
l(0) = l0 = p0 ,

1    1
l(1) = l3 = p              = (p0 + 3p1 + 3p2 + p3).
2    8
At u = 0, the slope of l must match the slope of p, but, because the parameter for p
covers only the range (0, 21 ), while u varies over (0, 1), implicitly we have made the
substitution ū = 2u. Consequently, derivatives for l and p are related by d ū = 2du,
l(0) = 3(l1 − l0) = p(0) = (p1 − p0).
Likewise, at the midpoint,

                    1    3
l (1) = (l3 − l2) = p      = (−p0 − p1 + p2 + p3).
2    8
These four equations can be solved algebraically. Alternatively, this solution can be
expressed geometrically, with the aid of Figure 10.35. Here, we construct both the
                                                                    10.9 Rendering Curves and Surfaces   539
p1 + p2
p1                        p2
l2              r1
l1              l3 = r0             r2
p0 = l0                                              p3 = r3

*FIGURE 10.35 Construction of subdivision curves.*

left and right sets of control points concurrently. First, we note that the interpolation
l0 = p0 ,
r3 = p3 .
We can verify by substitution in the four equations that the slopes on the left and
l1 = (p0 + p1),
r2 = (p2 + p3).
The interior points are given by

1        1
l2 =    l1 + (p1 + p2) ,
2        2

1         1
r1 =     r + (p + p2) .
2 2 2 1
Finally, the shared middle point is given by
l3 = r0 = (l2 + r1).
The advantage of this formulation is that we can determine both sets of control
points using only shifts (for the divisions by 2) and additions. However, one of the
advantages of the subdivision approach is that it can be made adaptive, and only one
of the sides may require subdivision at some point in the rendering. Also, note that
because the rendering of the curve need not take place until the rasterization stage of
the pipeline and can be done in screen or window coordinates, the limited resolution
of the display places a limit on how many times the convex hull needs to be subdivided
(Exercise 10.18).
540   Chapter 10   Curves and Surfaces

#### 10.9.3 Rendering Other Polynomial Curves by Subdivision

Just as any polynomial is a Bézier polynomial, it is also an interpolating polynomial,
a B-spline polynomial, and any other type of polynomial for a properly selected set
of control points. The efﬁciency of the Bézier subdivision algorithm is such that we
usually are better off converting another curve form to Bézier form and then using
the subdivision algorithm.11 A conversion algorithm can be obtained directly from
our curve formulations. Consider a cubic Bézier curve. We can write it in terms of
the Bézier matrix MB as
p(u) = uT MB p,
where p is the geometry matrix of control points. The same polynomial can be
p(u) = uT Mq,
where M is the matrix for some other type of polynomial and q is the matrix of
control points for this type. We assume that both polynomials are speciﬁed over the
same interval. The polynomials will be identical if we choose
q = M−1MB p.
For the conversion from interpolation to Bézier, the controlling matrix is
⎡                       ⎤
1     0    0     0
⎢ −5      3 − 23     1 ⎥
⎢                    3 ⎥.
MB−1MI = ⎢ 16                     ⎥
⎣ 3 − 23       3 − 65 ⎦
0     0    0     1
For the conversion between cubic B-splines and cubic Bézier curves, it is
⎡             ⎤
1 4 1 0
1⎢⎢0 4 2 0⎥
⎥
MB−1MS = ⎢                 ⎥.
6 ⎣0 2 4 0⎦
0 1 4 1
Figure 10.36 shows four control points and the cubic Bézier polynomial, interpo-
lating polynomial, and spline polynomial. The interpolating and spline forms were
generated as Bézier curves, from the new control points derived from the matrices
11. Even systems that do not render using subdivision are often optimized for rendering Bézier
curves by other methods. Hence, we still might want to convert any type of polynomial curve or
surface to a Bézier curve or surface.
                                                                       10.9 Rendering Curves and Surfaces   541
(a)                (b)                 (c)

*FIGURE 10.36 Cubic polynomials generated as Bézier curves by*

conversion of control points. (a) Bézier polynomial. (b) Interpolating
polynomial. (c) B-spline polynomial.
p30                    p33
p00                             p03

*FIGURE 10.37 Cubic Bézier surface.*

MB−1MI and MB−1MS . All three curves were generated with using recursive subdivi-
sion of Bézier curves. Note that for the spline case, the resulting curve is generated
between only the second and third of the original control points.

#### 10.9.4 Subdivision of Bézier Surfaces

We can extend our subdivision algorithm to Bézier surfaces. Consider the cubic
surface in Figure 10.37, with the 16 control points shown. Each four points in a row or
column determine a Bézier curve that can be subdivided. However, our subdivision
algorithm should split the patch into four patches, and we have no control points
along the center of the patch. We can proceed in two steps.
First, we apply our curve-subdivision technique to the four curves determined by
the 16 control points in the v direction. Thus, for each of u = 0, 31 , 23 , 1, we create two
groups of four control points, with the middle point shared by each group. There are
then seven different points along each original curve; these points are indicated in
Figure 10.38 by circles. We see that there are three types of points: original control
points that are kept after the subdivision (gray), original control points that are
discarded after the subdivision (white), and new points created by the subdivision
(black). We now subdivide in the u-direction using these points. Consider the rows
of constant v, where v is one of 0, 31 , 23 , 1. There are seven groups of four points. Each
group deﬁnes a Bézier curve for a constant v. We can subdivide in the u-direction,
each time creating two groups of four points, again with the middle point shared.
These points are indicated in Figure 10.39. If we divide these points into four groups
of 16, with points on the edges shared (Figure 10.40), each quadrant contains 16
points that are the control points for a subdivided Bézier surface.
542   Chapter 10   Curves and Surfaces
p30                            p33
p00                                 p03

*FIGURE 10.38 First subdivision of surface.*

p30
p33
p00                                p03

*FIGURE 10.39 Points after second subdivision.*

p30
p33
p00                                p03

*FIGURE 10.40 Subdivided quadrant.*

Compared to the calculation for curves, the test for whether the new convex hull
is ﬂat enough to stop a subdivision process is more difﬁcult. Many renderers use a
ﬁxed number of subdivisions, often letting the user pick the number. If a high-quality
rendering is desired, we can let the subdivision continue until the projected size of the
convex hull is less than the size of one pixel.

### 10.10        THE UTAH TEAPOT

We conclude our discussion of parametric surfaces with an example of recursive
subdivision of a set of cubic Bézier patches. The object that we show has become
known as the Utah teapot. The data for the teapot were created at the University
                                                                                  10.10 The Utah Teapot   543
of Utah by Mike Newell for testing of various rendering algorithms. These data have
been used in the graphics community for more than 30 years. The teapot data consist
of the control points for 32 bicubic Bézier patches. They are given in terms of 306
vertices. The ﬁrst 12 patches deﬁne the body of the teapot; the next four deﬁne the
handle; the next four deﬁne the spout; the following eight deﬁne the lid; and the ﬁnal
four deﬁne the bottom. These data are widely available.
For purposes of illustration, let’s assume that we want to subdivide each patch n
times and, after these subdivisions, we will render the ﬁnal vertices using either line
segments or polygons passing through the four corners of each patch. Thus, our ﬁnal
drawing can be done with the following function (for line segments), which adds
the four corner points (which must interpolate the surface) to the array that will be
rendered either with lines or ﬁlled triangles:

```cpp
void draw_patch(point4 p[4][4])
{
```

points[n] = p[0][0];
n++;
points[n] = p[3][0];
n++;
points[n] = p[3][3];
n++;
points[n] = p[0][3];
n++;

```cpp
}
```

We build our patch subdivider from the curve subdivider for a cubic curve c, using
our point4 type:

```cpp
void divide_curve(point4 c[4], point4 r[4], point4 l[4])
{
```

/* division of convex hull of Bezier curve */

```cpp
int i;
```

point4 t;
for(i=0;i<3;i++)
l[0][i]=c[0][i];
r[3][i]=c[3][i];
l[1][i]=(c[1][i]+c[0][i])/2;
r[2][i]=(c[2][i]+c[3][i])/2;
t[i]=(l[1][i]+r[2][i])/2;
l[2][i]=(t[i]+l[1][i])/2;
r[1][i]=(t[i]+r[2][i])/2;
l[3][i]=r[0][i]=(l[2][i]+r[1][i])/2;
for(i=0; i<4; i++) l[i][3] = r[i][3] = 1.0;

```cpp
}
```

544   Chapter 10   Curves and Surfaces
The patch subdivider is easier to write—but is slightly less efﬁcient—if we assume
that we have a matrix-transpose function transpose4. This code is then:

```cpp
void divide_patch(point4 p[4][4], int n)
{
```

point4 q[4][4], r[4][4], s[4][4], t[4][4];
point4 a[4][4], b[4][4];

```cpp
int k;
```

if(n==0) draw_patch(p); /* draw patch if recursion done */
/* subdivide curves in u direction, transpose results, divide
in u direction again (equivalent to subdivision in v) */

```cpp
{
```

for(k=0; k<4; k++) divide_curve(p[k], a[k], b[k]);
transpose4(a);
transpose4(b);
for(k=0; k<4; k++)

```cpp
{
```

divide_curve(a[k], q[k], r[k]);
divide_curve(b[k], s[k], t[k]);

```cpp
}
```

/* recursive division of 4 resulting patches */
divide_patch(q, n-1);
divide_patch(r, n-1);
divide_patch(s, n-1);
divide_patch(t, n-1);

```cpp
}
}
```

A complete teapot-rendering program using shaded polygons is given in Appendix A.
That program contains the teapot data.
Figure 10.41 shows the teapot as a wireframe and with constant shading. Note
that the various patches have different curvatures and sizes; thus, carrying out all
subdivisions to the same depth can create many unnecessarily small polygons.

*FIGURE 10.41 Rendered teapots.*

                                                                                    10.11 Algebraic Surfaces   545

### 10.11     ALGEBRAIC SURFACES

Although quadrics can be generated as a special case of NURBS curves, this class
of algebraic objects is of such importance that it merits independent discussion.
Quadrics are the most important case of the algebraic surfaces that we introduced
in Section 10.1.

#### 10.11.1 Quadrics

Quadric surfaces are described by implicit algebraic equations in which each term is
a polynomial of the form x i y j z k , with i + j + k ≤ 2. Any quadric can be written in the
q(x, y, z) = a11x 2 + 2a12xy + a22y 2 + a33z 2 + 2a23yz + 2a13xz
+ b1x + b2y + b3z + c = 0.
This class of surfaces includes ellipsoids, parabaloids, and hyperboloids. We can write
the general equation in matrix form in terms of the three-dimensional column matrix
p = [ x y z ]T as the quadratic form
pT Ap + bT p + c = 0,
⎡                  ⎤                ⎡    ⎤
a11 a12       a13                    b1
⎣
A = a12 a22        a23 ⎦ ,          b = ⎣ b2 ⎦ .
a13 a23       a33                    b3
The 10 independent coefﬁcients in A, b, and c determine a given quadric. However,
for the purpose of classiﬁcation, we can apply a sequence of rotations and translations
that reduces a quadric to a standard form without changing the type of surface. In
three dimensions, we can write such a transformation as
p = Mp + d.
This substitution creates another quadratic form with A replaced by the matrix
MT AM. The matrix M can always be chosen to be a rotation matrix such that
D = MT AM is a diagonal matrix. The diagonal elements of D can be used to de-
termine the type of quadric. If, for example, the equation is that of an ellipsoid, the
resulting quadratic form can be put in the form
 2    2
a11x + a22 y + a33z 2 − c  = 0,
where all the coefﬁcients are positive. Note that because we can convert to a standard
form by an afﬁne transformation, quadrics are preserved by afﬁne transformations
and thus ﬁt well with our other standard primitives.
546   Chapter 10   Curves and Surfaces

#### 10.11.2 Rendering of Surfaces by Ray Casting

Quadrics are easy to render because we can ﬁnd the intersection of a quadric with
a ray by solving a scalar quadratic equation. We represent the ray from p0 in the
p = p0 + αd.
Substituting into the equation for the quadric, we obtain the scalar equation for α:
α 2dT Ad + αdT (b + 2Ap0) + pT0 Ap0 + bT d + c = 0.
As for any quadratic equation, we may ﬁnd zero, one, or two real solutions. We can
use this result to render a quadric into the frame buffer or as part of a ray-tracing
calculation. In addition, we can apply our standard shading model at every point on
a quadric because we can compute the normal by taking the derivatives
⎡ ∂q ⎤
∂x
⎢ ∂q ⎥
n=⎢    ⎥
⎣ ∂y ⎦ = 2Ap − b.
∂q
∂z
This method of rendering can be extended to any algebraic surface. Suppose that
q(p) = q(x, y, z) = 0.
As part of the rendering pipeline, we cast a ray from the center of projection through
each pixel. Each of these rays can be written in the parametric form
p(α) = p0 + αd.
Substituting this expression into q yields an implicit polynomial equation in α:
q(p(α)) = 0.
We can ﬁnd the points of intersection by numerical methods, or for quadrics, by
the quadratic formula. If we have terms up to x i y j z k , we can have i + j + k points
of intersection, and the surface may require considerable time to render.

### 10.12        SUBDIVISION CURVES AND SURFACES

Let’s reexamine our subdivision formula from Section 10.9.2 from a slightly different
perspective. We start with four points—p0 , p1, p2 , p3—and end up with seven points.
We can call these new points s0 , . . . , s6. We can view each of these sets of points as
deﬁning a piecewise-linear curve, as illustrated in Figure 10.42.
                                                                10.12 Subdivision Curves and Surfaces   547
p1                        p2                    s2        s3      s4
s1                            s5
s0                                      s6
p0                                    p3
FIGURE 10.42 (a) Piecewise-linear curve determined by four points.
(b) Piecewise-linear curve after one subdivision step.
We can use our subdivision formulas to relate the two sets of points:
s0 = p0
s1 = 21 (p0 + p1),
s2 = 41 (p0 + 2p1 + p2),
s3 = 81 (p0 + 3p1 + 3p2 + p3),
s4 = 41 (p1 + 2p2 + p3),
s5 = 21 (p2 + p3),
s6 = p 3 .
The second curve is said to be a reﬁnement of the ﬁrst. As we saw in Section 10.9.2,
we can continue the process iteratively and in the limit converge to the B-spline.
However, in practice, we want to carry out only enough iterations so that the resulting
piecewise-linear curve connecting the new points looks smooth. How many iterations
we need to carry out depends on the size of the projected convex hull, which can be
determined from the camera speciﬁcations. Thus, we have a method that allows us to
render curves at different levels of detail.
These ideas and their beneﬁts are not limited to B-splines. Over the past few
years, a variety of methods for generating these subdivision curves have appeared.
Some interpolate points—such as p0 and p3—while others do not interpolate any of
the original points. But in all cases, the reﬁned curves converge to a smooth curve.

#### 10.12.1 Mesh Subdivision

The next issue we examine is how can we apply these ideas to surfaces. A theory of
subdivision surfaces has emerged that deals with both the theoretical and practi-
cal aspects of these ideas. Rather than generating a general subdivision scheme, we
will focus on meshes of triangles and meshes of quadrilaterals. In practice, many
modeling programs produce one of these types of meshes or a mesh consisting of
548   Chapter 10   Curves and Surfaces
(a)                      (b)                      (c)

*FIGURE 10.43 Polygon subdivision. (a) Triangle. (b) Rectangle. (c) Star-*

shaped polygon.
only triangles and quadrilaterals. If we start with a more general mesh, we can use tes-
sellation to replace the original mesh with one consisting of only triangles or quadri-
laterals.
We can form a quadrilateral mesh from an arbitrary mesh using the Catmull-
Clark method. We divide each edge in half, creating a new vertex at the midpoint. We
create an additional vertex at the centroid of each polygon, that is, the point that is the
average of the vertices that form the polygon. We then form a quadrilateral mesh by
connecting each original vertex to the new vertices on either side of it and connecting
the two new vertices to the centroid. Figure 10.43 shows the subdivision for some
simple polygons. Note that in each case the subdivision creates a quadrilateral mesh.
Once we have created the quadrilateral mesh, it is clear that successive subdivi-
sions create a ﬁner quadrilateral mesh. However, we have yet to do anything to create
a smoother surface. In particular, we want to ensure as much continuity as possible
at the vertices.
Consider the following procedure. First, we compute the average position of each
polygon, its centroid. Then, we replace each vertex by the average of the centroids of
all the polygons that contain the vertex. At this point, we have a smoother surface but
one for which, at vertices not of valence 4, we can see sharp changes in smoothness.
The Catmull-Clark scheme produces a smoother surface with one additional step.
For each vertex not of valence 4, we replace it by
p = p0 + p1,
where p0 is the vertex position before the averaging step, p1 is its position after the
averaging pass, and k is the valence of the vertex. The valence of a vertex is the
number of polygons that share the vertex. This method tends to move edge vertices
at corners more than other outer vertices. Figure 10.44 shows the sequence for a
single rectangle. In Figure 10.44(a), the original vertices are black and the vertices
at the midpoints of the edges are white. The centroid of the original polygon is the
gray vertex at the center, and the centroids of the subdivided polygons are shown as
colored vertices. Figure 10.44(b) shows the movement of the vertices by averaging,
                                                                       10.12 Subdivision Curves and Surfaces   549
(a)                      (b)                      (c)

*FIGURE 10.44 Catmull-Clark subdivision.*

(a)
(b)

*FIGURE 10.45 Loop subdivision. (a) Triangular mesh. (b) Triangles after*

one subdivision.
and Figure 10.44(c) shows the ﬁnal Catmull-Clark subdivision after the correction
factor has been applied.
This scheme does not work as well for meshes that start with all triangles, because
the interior vertices have high valences that do not change with reﬁnement. For
triangular meshes, there is a simple method called Loop subdivision that we can
describe as a variant of the general scheme. We start by doing a standard subdivision
of each triangle by connecting the bisectors of the sides to create four triangles. We
proceed as before but use a weighted centroid of the vertices with a weight of 1/4
for the vertex that is being moved and 3/8 for the other two vertices that form the
triangle. We can get a smoother surface by taking a weighted average of the vertex
positions before and after the averaging step, as we did for the Catmull-Clark scheme.
Loop’s method uses a weight of 53 − 83 ( 83 + 41 cos( 2π
k )) . Figure 10.45 shows a simple
triangle mesh and the resulting mesh after the subdivision step.
The ﬁgures in Color Plate 29 were developed using subdivision surfaces. Fig-
ure 10.46 shows a sequence of meshes generated by a subdivision-surface algorithm
and the rendered surface from the highest resolution mesh. Note that the original
mesh contains polygons with different numbers of sides and vertices with different
550         Chapter 10    Curves and Surfaces

*FIGURE 10.46 Successive subdivisions of polygonal mesh and rendered surface. (Images courtesy*

Caltech Multi-Res Modeling Group)
valences. Also, note that as the mesh is subdivided, each subdivision step yields a
smoother surface.
We have not covered some tricky issues, such as the data structures needed to
ensure that when we insert vertices we get consistent results for shared edges. The
references in the Suggested Readings should help you get started.

### 10.13     MESH GENERATION FROM DATA

In all our examples, we have assumed that the positions for our data were given either
at the nodes of a rectangular grid or possibly at the nodes of a general graph. In many
circumstances, we are given a set of locations that we know are from a surface, but
otherwise the locations are unstructured. Thus, we have a list of locations but no
notion of which points are close to each other.
                                                                        10.13 Mesh Generation from Data   551

*FIGURE 10.47 Height data.*


*FIGURE 10.48 Height data projected onto plane y = 0.*


#### 10.13.1 Height Fields Revisited

One example of how such data arises is topography, where we might take measure-
ments of the heights of random points on the ground from an airplane or satellite,
such as the height ﬁelds we considered in Chapter 4, where they were based on these
measurements being taken over a regular grid in which y = 0 represents the ground.
Consequently, the height data were of the form yij , all of which could be stored in a
matrix. Here, the data are obtained at random unstructured locations, so the starting
point is a set of values {xi , yi , zi }. The topography example has some structure in that
we know all the points lie on a single surface and that no two points can have the same
xi and zi . Figure 10.47 shows a set of points, all of which are above the plane y = 0.
These points can be projected onto the plane y = 0, as in Figure 10.48. We seek an al-
gorithm that connects these points into a triangular mesh, as shown in Figure 10.49.
The mesh in the plane can then be projected back up to connect the original data
with triangles. These three-dimensional triangles can be rendered to give an approx-
imation to the surface that yielded the data. Figure 10.50 shows this mesh.
In the next section, we examine how we can obtain a triangular mesh from a set
of points in the plane.

#### 10.13.2 Delaunay Triangulation

Given a set of points in a plane, there are many ways to form a triangular mesh that
uses all the points as vertices. Even four vertices that specify a convex quadrilateral
can form a two-triangle mesh in two ways, depending on which way we draw a
552   Chapter 10   Curves and Surfaces

*FIGURE 10.49 Triangular mesh.*


*FIGURE 10.50 Three-dimensional mesh.*

(a)              (b)

*FIGURE 10.51 Two splits of a quadrilateral.*

diagonal. For a mesh of n vertices, there will be n − 2 triangles in the mesh but
many ways to triangulate the mesh. From the graphics perspective, not all the meshes
from a given set of vertices are equivalent. Consider the two ways we can triangulate
the four points in Figure 10.51. Because we always want a mesh in which no edges
cross, the four edges colored in black must be in the mesh. Note that they form the
convex hull of the four points. Hence, we only have a choice as to the diagonal. In
Figure 10.51(a), the diagonal creates two long thin triangles whereas the diagonal in
Figure 10.51(b) creates two more robust triangles. We prefer the second case because
long, thin triangles tend to render badly, showing artifacts from the interpolation of
vertex attributes.
In general, the closer a triangle is to an equilateral triangle, the better it is for
rendering. In more mathematical terms, the best triangles have the largest minimum

```cpp
interior angle. If we compare two triangular meshes derived from the same set of
```

points, we can say the better mesh is the one with the largest minimum interior angle
of all the triangles in the mesh. Although it may appear that determining such a mesh
for a large number of triangles is difﬁcult, we can approach the problem in a manner
that will yield the smallest minimum angle.
                                                                       10.13 Mesh Generation from Data   553

*FIGURE 10.52 Circles determined by possible triangulations.*

Consider some vertices in the plane that will be part of a mesh (Figure 10.52).
Focusing on vertex v, it appears that one of the triangles a, v, c or v, c, b should be
part of the mesh. Recall that three points in the plane determine a unique circle that

```cpp
interpolates them. Note that the circle formed by a, v, c does not include another
```

point, whereas the circle formed by v, c, b does. Moreover, the triangle formed by a,
v, c has a smaller minimum angle than the triangle formed by v, c, b. Because these
two triangles share an edge, we can only use one of them in our mesh.
These observations suggest a strategy known as Delaunay triangulation. Given
a set of n points in the plane, the Delaunay triangulation has the following properties,
any one of which is sufﬁcient to deﬁne the triangulation:
1. For any triangle in Delaunay triangulation, the circle passing through its three
vertices has no other vertices in its interior.
2. For any edge in the Delaunay triangulation, there is no circle passing through
the endpoints (vertices) of this edge that includes another vertex in its inte-
rior.
3. If we consider the set of angles of all the triangles in a triangulation, the
Delaunay triangulation has the greatest minimum angle.
Proofs of these properties are in the Suggested Readings at the end of the chapter. The
third property ensures that the triangulation is a good one for computer graphics.
The ﬁrst two properties follow from how we construct the triangulation.
We start by adding three vertices such that all the points in the set of vertices
lie inside the triangle formed by these three vertices, as in Figure 10.53. These extra
vertices and the edges connecting them to other vertices can be removed at the end.
We next pick a vertex v from our data set at random and connect it to the three added
vertices, thus creating three triangles, as shown in Figure 10.54. Note there is no way
a circle determined by any three of the four vertices can include the other, so we need
not do any testing yet.
We next pick a vertex u randomly from the remaining vertices. From Fig-
ure 10.55, we see that this vertex lies inside the triangle formed by a, v, and c and
that the three triangles it forms do not present a problem. However, the edge be-
tween a and v is a diagonal for the quadrilateral formed by a, u, v, and b, and the
circle that interpolates a, u, and v has b in its interior. Hence, if we use this edge, we
will violate the criteria for a Delaunay triangulation. There is a simple solution to this
554   Chapter 10   Curves and Surfaces

*FIGURE 10.53 Starting a Delaunay triangulation.*


*FIGURE 10.54 Triangulation after adding first data point.*

problem. We can choose the other diagonal of the quadrilateral and replace the edge
between a and v with an edge between u and b, an operation called ﬂipping. The
resulting partial mesh is shown in Figure 10.56. Now the circle passing through u, v,
and b does not include any other vertices, nor do any of the other circles determined
by any of the triangles. Thus, we have a Delaunay triangulation of a subset of the
points.
We continue the process by adding another randomly chosen vertex from the
original set of vertices, ﬂipping as necessary. Note that it is not sufﬁcient in general to
ﬂip only one edge corresponding to a just-added vertex. The act of ﬂipping one edge
can require the ﬂipping of other edges, so the process is best described recursively.
Because each ﬂip gives an improvement, the process terminates for each added vertex,
and once we have added all the vertices, we can remove the three vertices we added
to get started and all the edges connected to them. On the average, the triangulation
has a O(n log n) complexity.
There are some potential problems with our approach that arise because we cre-
ate the mesh in a plane and project it back up to the data points. A triangle that has
                                                                     10.13 Mesh Generation from Data   555

*FIGURE 10.55 Adding a vertex requiring flipping.*


*FIGURE 10.56 Mesh after flipping.*

almost equal angles in the plane may not have such nice angles when the correspond-
ing original vertices are connected to form a triangle in three dimensions. A potential
solution would be to apply a similar strategy directly to the three-dimensional data
rather than projecting them onto a plane. For each four points, we can determine a
unique sphere that they interpolate. Thus, we can deﬁne a three-dimensional Delau-
nay triangulation as one in which no other data point lies in the sphere and we use
the four points to specify a tetrahedron. Unfortunately, determining such a spatial
division requires far more work than the Delaunay triangulation in the plane.

#### 10.13.3 Point Clouds

Delaunay triangulation relied on the assumption that our data were 2 21 dimensional;
that is, we knew they came from a single surface and could be recovered from their
556   Chapter 10   Curves and Surfaces
projections onto a plane. In many situations, we have data that are completely un-

```cpp
structured, and often these data sets are very large. For example, laser scans can
```

output tens of millions of points in a short time. Such data sets are known as point
clouds. Often, such data are displayed directly using point primitives. Because we
can shade points in OpenGL as we would a surface, a high density of shaded points
can show three-dimensional structure, especially if the camera can be moved inter-
actively.
Once again, we have only scratched the surface of a deep and important topic. Also
once again, our focus has been on what we can do on a graphics system using a
standard API such as OpenGL. From this perspective, there are huge advantages to
using parametric Bézier curves and surfaces. The parametric form is robust and is
easy to use interactively because the required data are points that can be entered and
manipulated interactively. The subdivision algorithm for Bézier curves and surfaces
gives us the ability to render the resulting objects to any desired degree of accuracy.
We have seen that although Bézier surfaces are easy to render, splines can provide
additional smoothness and control. The texts in the Suggested Readings discuss many
variants of splines that are used in the CAD community.
Quadric surfaces are used extensively with ray tracers, because solving for the
points of intersection between a ray and a quadric requires the solution of only a
scalar quadratic equation. Deciding whether the point of intersection between a ray
and the plane determined by a ﬂat polygon is inside the polygon can be more difﬁcult
than solving the intersection problem for quadric surfaces. Hence, many ray tracers
allow only inﬁnite planes, quadrics, and, perhaps, convex polygons.
Subdivision surfaces have become increasingly more important for two reasons.
First, because commodity hardware can render polygons at such high rates, we can
often achieve the desired smoothness using a large number of polygons that can be
rendered faster than a smaller number of surface patches. However, future hardware
may change this advantage if rendering of curved surfaces is built into the rasterizer.
Second, because we can render a subdivision surface at any desired level of detail, we
can often use subdivision very effectively by not rendering a highly subdivided surface
when it projects to a small area on the screen.
The book by Farin [Far88] provides an excellent introduction to curves and surfaces.
It also has an interesting preface in which Bézier discusses the almost simultaneous
discovery by him and deCasteljau of the surfaces that bear Bézier’s name. Unfor-
tunately, deCasteljau’s work was described in unpublished technical reports, so de-
Casteljau did not receive the credit he deserved for his work until recently. Books such
as those by Rogers [Rog90], Foley [Fol90], Bartels [Bar87], and Watt [Wat00] discuss
                                                                                            Exercises   557
many other forms of splines. See Rogers [Rog00] for an introduction to NURBS. The
Catmull-Rom splines were proposed in [Cat75].
The book by Faux [Fau80] discusses the coordinate-free approach to curves and
surfaces and the Frenet frame.
Although the book edited by Glassner [Gla89] primarily explores ray tracing,
the section by Haines has considerable material on working with quadrics and other
algebraic surfaces.
There has been much recent activity on subdivision curves and surfaces. For
some of the seminal work in the area, see [Che95], [Deb96], [Gor96], [Lev96],
[Sei96], and [Tor96]. Our development follows [War04]. Catmull-Clark subdivision
was proposed in [Cat78]. See also [War03] and [Sta03]. Delaunay triangulation is
covered in most books on computational geometry. See [deB08].

### 10.1    Consider an algebraic surface f (x, y, z) = 0, where each term in f can have

terms in x, y, and z of powers up to m. How many terms can there be in f ?

### 10.2    Consider the explicit equations y = f (x) and z = g(x). What types of curves

do they describe?


### 10.3    Suppose that you have a polynomial p(u) = nk=0 ck uk . Find a polynomial

n
q(v) = k=0 dk v k such that, for each point of p in the interval (a, b), there is
a point v in the range 0 ≤ v ≤ 1, such that p(u) = q(v).

### 10.4    Show that as long as the four control points for the cubic interpolating curve

are deﬁned at unique values of the parameter u, the interpolating geometry
matrix always exists.

### 10.5    Show that in the interval (0, 1), the Bernstein polynomials must be less

than 1.

### 10.6    Verify the C 2 continuity of the cubic spline.


### 10.7    In Section 10.9, we showed that we can write a cubic polynomial as a cubic

Bézier polynomial by choosing a proper set of control points or, equivalently,
the proper convex hull. Using this fact, show how to render an interpolating
curve using the Bézier renderer provided by OpenGL.

### 10.8    Find a homogeneous-coordinate representation for quadrics.


### 10.9    Suppose that we render Bézier patches by adaptive subdivision so that each

patch can be subdivided a different number of times. Do we maintain conti-
nuity along the edges of the patches? Explain your answer.

### 10.10 Write an OpenGL program that will take as input a set of control points and

that will produce the interpolating, B-spline, and Bézier curves for these data.

### 10.11 Suppose that you use a set of spline curves to describe a path in time that an

object will take as part of an animation. How might you notice the difference
between G1 and C 1 continuity in this situation?
558   Chapter 10   Curves and Surfaces

### 10.12 Write a program to generate a cubic Bézier polynomial from an arbitrary

number of points entered interactively. The user should be able to manipulate
the control points interactively.

### 10.13 Derive a simple test for the ﬂatness of a Bézier surface patch.


### 10.14 Derive the open rational quadratic B-spline with the knots {0, 0, 0, 0,

1, 1, 1, 1} and the weights w0 = w2 = 1 and w1 = w.

### 10.15 Using the result of Exercise 10.14, show that if w = 1−rr

, for 0 ≤ r ≤ 1, you get
all the conic sections. Hint: Consider r < 2 and r > 21 .

### 10.16 Find the zeros of the Hermite blending functions. Why do these zeros imply

that the Hermite curve is smooth in the interval (0, 1)?

### 10.17 What is the relationship between the control-point data for a Hermite patch

and the derivatives at the corners of the patch?

### 10.18 For a 1024 × 1280 display screen, what is the maximum number of subdivi-

sions that are needed to render a cubic polynomial surface?

### 10.19 Suppose you have three points, P0 , P1, and P2. First, connect successive points

with parametric line segments where u ranges from 0 to 1 for each. Next,
linearly interpolate between successive pairs of line segments by connecting
points for the same value of u with a line segment and then using the same
value of u to obtain a value along this new line segment. How can you describe
the curve created by this process?

### 10.20 Extend Exercise 10.19 by considering four points. Linearly interpolate, ﬁrst

between the three curves constructed as in that exercise and, second, between
the two curves thus created. Describe the ﬁnal curve determined by the four
points.

### 10.21 What happens in the cubic Bézier curve if the values of the control points P0

and P1 are the same?

### 10.22 Suppose that we divide a Bézier surface patch, ﬁrst along the u-direction.

Then in v, we only subdivide one of the two patches we have created. Show
how this process can create a gap in the resulting surface. Find a simple
solution to this difﬁculty.

### 10.23 Write a program to carry out subdivision of triangular or quadrilateral

meshes. When the subdivision is working correctly, add the averaging step
to form a smoother surface.

### 10.24 Find the blending polynomials for the cubic Catmull-Rom spline. Find the

zeros for these polynomials.

### 10.25 Find the matrix that converts data for the Catmull-Rom spline to control

point data for a Bézier curve that will generate the same curve.
                                                         CHA P TE R                         11
I  n this ﬁnal chapter, we consider a variety of alternative approaches to the standard
pipeline rendering strategy we have used for interactive applications. We have
multiple motivations for introducing these other approaches. We want to be able to
incorporate effects, such as global illumination, that usually are not possible to render
in real time. We also want to produce high-quality images whose resolution is beyond
that of standard computer displays. For example, a single frame of a digital movie may
contain over 10 million pixels and take hours to render.

### 11.1    GOING BEYOND PIPELINE RENDERING

Almost everything we have done so far has led us to believe that given a scene descrip-
tion containing geometric objects, cameras, light sources, and attributes, we could
render the scene in close to real time using available hardware and software. This view
dictated that we would use a pipeline renderer of the type described by the OpenGL
architecture and supported by graphics hardware. Although we have developed a rea-
sonably large bag of tricks that enable us to handle most applications and get around
many of the consequences of using the local lighting model supported by such render-
ers, there are still limitations on what we can do. For example, there are many global
illumination situations that we cannot approximate well with a pipeline renderer. We
would also like to generate images with higher resolution than on a standard worksta-
tion and that contain fewer aliasing artifacts. In many situations, we are willing either
to render at slower speeds or to use multiple computers to achieve these goals. In this
chapter, we introduce a variety of techniques, all of which are of current interest both
to researchers and practitioners.
First, we examine other rendering strategies that are based on the physics of
image formation. Our original discussion of image formation was based on following
rays of light. That approach is built on a very simple physical model and led to the
ray-tracing paradigm for rendering. We start by exploring this model in greater detail
than in previous chapters and show how to get started writing your own ray tracer.
We can also take approaches to rendering, other than ray tracing, that are also
based on physics. We will examine an approach based on energy conservation and
560   Chapter 11   Advanced Rendering
consider an integral equation, the rendering equation, that describes a closed en-
vironment with light sources and reﬂective surfaces. Although this equation is not
solvable in general, we can develop a rendering approach called radiosity that satis-
ﬁes the rendering equation when all surfaces are perfectly diffuse reﬂectors.
We will also look at two approaches that are somewhere between physically cor-
rect renderers and real-time renderers. One is the approach taken in RenderMan. The
other is an approach to rendering that starts with images. Although these methods are
different from each other, both have become important in the animation industry.
We then turn to the problems of working with large data sets and high-resolution
displays. These problems are related because large data sets contain detail that re-
quires displays with a resolution beyond what we can get with standard commodity
devices such as LCD panels. We will consider solutions that use parallelism making
use of commodity components, both processors and graphics cards.
Finally, we introduce image-based rendering, in which we start with multiple
two-dimensional images of a three-dimensional scene and try to use these images to
obtain an image from another viewpoint.

### 11.2    RAY TRACING

In many ways, ray tracing is a logical extension to rendering with a local lighting
model. It is based on our previous observation that of the light rays leaving a source,
the only ones that contribute to our image are those that enter the lens of our syn-
thetic camera, passing through the center of projection. Figure 11.1 shows several of
the possible interactions with a single point source and perfectly specular surfaces.
Rays can enter the lens of the camera directly from the source, from interactions with

*FIGURE 11.1 Rays leaving source.*

                                                                                            11.2 Ray Tracing   561

*FIGURE 11.2 Ray-casting model.*

a surface visible to the camera, after multiple reﬂections from surfaces, or after trans-
mission through one or more surfaces.
Most of the rays that leave a source do not enter the lens and do not contribute
to our image. Hence, attempting to follow all rays from a light source is a time-
wasting endeavor. However, if we reverse the direction of the rays, and consider
only those rays that start at the center of projection, we know that these cast rays
must contribute to the image. Consequently, we start our ray tracer as shown in
Figure 11.2. Here we have included the image plane, and we have ruled it into pixel-
sized areas. Knowing that we must assign a color to every pixel, we must cast at least
one ray through each pixel. Each cast ray either intersects a surface or a light source,
or goes off to inﬁnity without striking anything. Pixels corresponding to this latter
case can be assigned a background color. Rays that strike surfaces—for now, we can
assume that all surfaces are opaque—require us to calculate a shade for the point
of intersection. If we were simply to compute the shade at the point of intersection,
using the modiﬁed-Phong model, we would produce the same image as would our
local renderer. However, we can do much more.
Note that the process that we have described thus far requires all the same steps
as we use in our pipeline renderer: object modeling, projection, and visible-surface
determination. However, as we saw in Chapter 6, the order in which the calculations
are carried out is different. The pipeline renderer works on a vertex-by-vertex basis;
the ray tracer works on a pixel-by-pixel basis.
In ray tracing, rather than immediately applying our reﬂection model, we ﬁrst
check whether the point of intersection between the cast ray and the surface is illu-
minated. We compute shadow, or feeler, rays from the point on the surface to each
source. If a shadow ray intersects a surface before it meets the source, the light is
blocked from reaching the point under consideration, and this point is in shadow, at
least from this source. No lighting calculation needs to be done for sources that are
blocked from a point on the surface. If all surfaces are opaque and we do not consider
light scattered from surface to surface, we have an image that has shadows added to
562   Chapter 11   Advanced Rendering

*FIGURE 11.3 Shadow rays.*


*FIGURE 11.4 Ray tracing with a mirror.*

what we have already done without ray tracing. The price we pay is the cost of doing
a type of hidden-surface calculation for each point of intersection between a cast ray
and a surface. Figure 11.3 shows the shadow rays (solid lines) for two cast rays (dashed
lines) that hit the cube and sphere. A shadow ray from the cube intersects the cylin-
der. Hence, the point of intersection on the cube from the cast ray is illuminated by
only one of the two sources.
Suppose that some of our surfaces are highly reﬂective, like those shown in
Figure 11.4. We can follow the shadow ray as it bounces from surface to surface, until
it either goes off to inﬁnity or intersects a source. Figure 11.4 shows just two of the
paths. The left cast ray intersects the mirror, and the shadow ray to the light source
on the left is not blocked, so if the mirror is in front of the second source, the point
                                                                                           11.2 Ray Tracing                  563

*FIGURE 11.5 Ray tracing with reflection and transmission.*

of intersection is illuminated by only one source. The cast ray on the right intersects
the sphere, and in this case a shadow can reﬂect from the mirror to the source on the
left and in addition the point of intersection is illuminated directly by the source on
the left.
Such calculations are usually done recursively and take into account any absorp-                                       t2
tion of light at surfaces.                                                                           r
Ray tracing is particularly good at handling surfaces that both reﬂect light and                             t1
transmit light through refraction. Using our basic paradigm, we follow a cast ray to                                    r2
r1
a surface (Figure 11.5) with the property that if a ray from a source strikes a point,                        r3
then the light from the source is partially absorbed and some of this light contributes
t3
to the diffuse reﬂection term. The rest of the incoming light is divided between a
transmitted ray and a reﬂected ray. From the perspective of the cast ray, if a light
source is visible at the intersection point, then we need to do three tasks. First, we
must compute the contribution from the light source at the point, using our standard        FIGURE 11.6 Simple ray-
reﬂection model. Second, we must cast a ray in the direction of a perfect reﬂection.        traced environments.
Third, we must cast a ray in the direction of the transmitted ray. These two cast rays
are treated just like the original cast ray; that is, they may intersect other surfaces,
they can end at a source, or they can go off to inﬁnity. At each surface that these                      r
rays intersect, additional rays may be generated by reﬂection and transmission of                   r1        t1
light. Figure 11.6 shows a single cast ray and the path it can follow through a simple
environment. Figure 11.7 shows the ray tree generated. This tree shows which rays                        r2        t2
must be traced; it is constructed dynamically by the ray-tracing process.
Although our ray tracer uses the Blinn-Phong model to include a diffuse term at               r3        t3
the point of intersection between a ray and a surface, the light that is scattered dif-
fusely at this point is ignored. If we were to attempt to follow such light, we would
have so many rays to deal with that the ray tracer might never complete execution.          FIGURE 11.7 Ray tree corre-
sponding to Figure 11.6.
Thus, ray tracers are best suited for highly reﬂective environments. Color Plate 15
was rendered with a public-domain ray tracer. Although the scene contains only a few
564   Chapter 11   Advanced Rendering
objects, the reﬂective and transparent surfaces could not have been rendered realis-
tically without the ray tracer. Also, note the complexity of the shadows in the scene,
another effect created automatically by ray tracing. The image also demonstrates that
ray tracers can incorporate texture mapping with no more difﬁculty than with our
pipeline renderer.

### 11.3    BUILDING A SIMPLE RAY TRACER

The easiest way to describe a ray tracer is recursively, through a single function that
traces a ray and calls itself for the reﬂected and transmitted rays. Most of the work
in ray tracing goes into the calculation of intersections between rays and surfaces.
One reason that it is difﬁcult to implement a ray tracer that can handle a variety of
objects is that as we add more complex objects, computing intersections becomes
problematic. Consequently, most basic ray tracers support only ﬂat and quadric
surfaces.
We have seen the basic considerations that determine the ray-tracing process.
Building a simple recursive ray tracer that can handle simple objects—quadrics and
polyhedra—is quite easy. In this section, we will examine the basic structure and the
functions that are required. Details can be found in the Suggested Readings at the end
of the chapter.
We need two basic functions. The recursive function raytrace follows a ray,
speciﬁed by a point and a direction, and returns the shade of the ﬁrst surface that
it intersects. It will use the function intersect to ﬁnd the location of the closest
surface that the speciﬁed ray intersects.

#### 11.3.1 Recursive Ray Tracing

Let’s consider the procedure trace in pseudocode. We give it a starting point p and
a direction d, and it returns a color c. In order to stop the ray tracer from recursing
forever, we can specify a maximum number of steps max that it can take. We will
assume, for simplicity, that we have only a single light source whose properties, as
well as the description of the objects and their surface properties, are all available
globally. If there are additional light sources, we can add in their contributions in a
manner similar to the way in which we deal with the single source:
color c = trace(point p, vector d, int step)

```cpp
{
```

color local, reflected, transmitted;
point q;
normal n;
if (step > max) return(background_color);
q = intersect(p, d, status);
                                                                          11.3 Building a Simple Ray Tracer   565
if (status == light_source) return(light_source_color);
if (status == no_intersection) return(background_color);
n = normal(q);
r = reflect(q, n);
t = transmit(q, n);
local = phong(q, n, r);
reflected = trace(q, r, step+1);
transmitted = trace(q, t, step+1);
return(local + reflected + transmitted);

```cpp
}
```

Note that the calculation of reﬂected and transmitted colors must take into ac-
count how much energy is absorbed at the surface before reﬂection and transmission.
If we have exceeded the maximum number of steps, we return a speciﬁed background
color. Otherwise, we use intersect to ﬁnd the intersection of the given ray with the
closest object. This function must have the entire database of objects available to it,
and it must be able to ﬁnd the intersections of rays with all types of objects sup-
ported. Consequently, most of the time spent in the ray tracer and the complexity
of the code is hidden in this function. We examine some of the intersection issues in
Section 11.3.2.
If the ray does not intersect any object, we can return a status variable from in-
tersect and return the background color from trace. Likewise, if the ray intersects
the light source, we return the color of the source. If an intersection is returned, there
are three components to the color at this point: a local color that can be computed
using the modiﬁed Phong (or any other) model, a reﬂected color, and, if the surface
is translucent, a transmitted color. Before computing these colors, we must compute
the normal at the point of intersection, as well as the direction of reﬂected and trans-
mitted rays, as in Chapter 5. The complexity of computing the normal depends on
the class of objects supported by the ray tracer, and this calculation can be part of the
function trace.
The computation of the local color requires a check to see if the light source is
visible from the point of closest intersection. Thus, we cast a feeler or shadow ray
from this point toward the light source and check whether it intersects any objects.
We can note that this process can also be recursive because the shadow ray might hit
a reﬂective surface, such as a mirror, or a translucent surface, such as a piece of glass.
In addition, if the shadow ray hits a surface that itself is illuminated, some of this light
should contribute to the color at q. Generally, we ignore these possible contributions
because they will slow the calculation signiﬁcantly. Practical ray tracing requires that
we make some compromises and is never quite physically correct.
Next, we have two recursive steps that compute the contributions from the re-
ﬂected and transmitted rays starting at q using trace. It is these recursions that make
this code a ray tracer rather than a simple ray-casting rendering in which we ﬁnd the
566   Chapter 11   Advanced Rendering
ﬁrst intersection and apply a lighting model at that point. Finally, we add the three
colors to obtain the color at p.

#### 11.3.2 Calculating Intersections

Most of the time spent in a typical ray tracer is in the calculation of intersections
in the function intersect. Hence, we must be very careful in limiting the objects
to those for which we can ﬁnd intersections easily. The general intersection problem
can be expressed cleanly if we use an implicit representation of our objects. Thus, if
an object is deﬁned by the surface(s)
f (x, y, z) = f (p) = 0,
and a ray from a point p0 in the direction d is represented by the parametric form
p(t) = p0 + td,
then the intersections are given for the values of t such that
f (p0 + td) = 0,
which is a scalar equation in t. If f is an algebraic surface, then f is a sum of poly-
nomial terms of the form x i y j z k and f (p0 + td) is a polynomial in t. Finding the

```cpp
intersections reduces to ﬁnding all the roots of a polynomial. Unfortunately, there
```

are only a few cases that do not require numerical methods.
One is quadrics. In Chapter 10, we saw that all quadrics could be written as the
pT Ap + bT p + c = 0.
Substituting in the equation for a ray leaves us with a scalar quadratic equation to
solve for the values of t that yield zero, one, or two intersections. Because the solution
of the quadratic equation requires only the taking of a single square root, ray tracers
can handle quadrics without difﬁculty. In addition, we can eliminate those rays that
miss a quadric object and those that are tangent to it before taking the square root,
further simplifying the calculation.
Consider, for example, a sphere centered at pc with radius r, which can be
(p − pc ) . (p − pc ) − r 2 = 0.
p(t) = p0 + td,
d . dt 2 + 2(p0 − pc ) . dt + (p0 − pc ) . (p0 − pc ) − r 2 = 0.
                                                                       11.3 Building a Simple Ray Tracer           567
Planes are also simple. We can take the equation for the ray and substitute it into
p . n + c = 0,
which yields a scalar equation that requires only a single division to solve. Thus, for
p = p0 + td,
we ﬁnd
p .n+c
t =− 0     .
n.d
However, planes by themselves have limited applicability in modeling scenes. We
are usually interested either in the intersection of multiple planes that form convex
objects (polyhedra) or a piece of a plane that deﬁnes a ﬂat polygon. For polygons, we
must decide whether the point of intersection lies inside or outside the polygon. The
difﬁculty of such a test depends on whether the polygon is convex and, if not convex,
whether it is simple. These issues are similar to the rendering issues that we discussed
for polygons in Chapter 6. For convex polygons, there are very simple tests that are
similar to the tests for ray intersections with polyhedra that we consider next.
Although we can deﬁne polyhedra by their faces, we can also deﬁne them as the
convex objects that are formed by the intersection of planes. Thus, a parallelepiped
is deﬁned by six planes and a tetrahedron by four. For ray tracing, the advantage of
this deﬁnition is that we can use the simple ray–plane intersection equation to derive
a ray–polyhedron intersection test.
We develop the test as follows. Let’s assume that all the planes deﬁning our
polyhedron have normals that are outward facing. Consider the ray in Figure 11.8            FIGURE 11.8 Ray intersecting
that intersects the polyhedron. It can enter and leave the polygon only once. It must       a polyhedron with outward-
enter through a plane that is facing the ray and leave through a plane that faces in the    facing normals shown.
direction of the ray. However, this ray must also intersect all the planes that form the
polyhedron (except those parallel to the ray).
Consider the intersections of the ray with all the front-facing planes—that is,
those whose normals point toward the starting point of the ray. The entry point
must be the intersection farthest along the ray. Likewise, the exit point is the nearest

```cpp
intersection point of all the planes facing away from the origin of the ray, and the
```

entry point must be closer to the initial point than the exit point. If we consider a ray
that misses the same polyhedron, as shown in Figure 11.9, we see that the farthest

```cpp
intersection with a front-facing plane is farther from the initial point than the closest   FIGURE 11.9 Ray missing
intersection with a back-facing plane. Hence, our test is to ﬁnd these possible entry       a polyhedron with outward-
```

and exit points by computing the ray–plane intersection points, in any order, and           facing normals shown.
updating the possible entry and exit points as we ﬁnd the intersections. The test can
be halted if we ever ﬁnd a possible exit point closer than the present entry point or a
possible entry point farther than the present exit point.
568   Chapter 11   Advanced Rendering

*FIGURE 11.10 Ray intersecting a convex polygon.*


*FIGURE 11.11 Ray missing a convex polygon.*

Consider the two-dimensional example illustrated in Figure 11.10 that tests for
a ray–convex polygon intersection in a plane. Here lines replace planes, but the logic
is the same. Suppose that we do the intersections with the lines in the order 1, 2, 3, 4.
Starting with line 1, we ﬁnd that this line faces the initial point by looking at the sign
of the dot product of the normal with the direction of the ray. The intersection with
line 1 then yields a possible entry point. Line 2 faces away from the initial point and
yields a possible exit point that is farther away than our present estimate of the entry
point. Line 3 yields an even closer exit point but still one that is farther than the entry
point. Line 4 yields a farther exit point that can be discarded. At this point, we have
tested all the lines and conclude the ray passes through the polygon.
Figure 11.11 has the same lines and the same convex polygon but shows a ray that
misses the polygon. The intersection with line 1 still yields a possible entry point. The

```cpp
intersections with lines 2 and 3 still yield possible exit points that are farther than the
```

entry point. But the intersection with line 4 yields an exit point closer than the entry
point, which indicates that the ray must miss the polygon.

#### 11.3.3 Ray-Tracing Variations

Most ray tracers employ multiple methods for determining when to stop the recursive
process. One method that is fairly simple to implement is to neglect all rays that go
past some distance, assuming that such rays go off to inﬁnity. We can implement
this test by assuming that all objects lie inside a large sphere centered at the origin.
                                                                               11.4 The Rendering Equation   569
Thus, if we treat this sphere as an object colored with a speciﬁed background color,
whenever the intersection calculation determines that this sphere is the closest object,
we terminate the recursion for the ray and return the background color.
Another simple termination strategy is to look at the fraction of energy remain-
ing in a ray. When a ray passes through a translucent material or reﬂects from a shiny
surface, we can estimate the fraction of the incoming energy that is in these outgoing
rays and how much has been absorbed at the surface. If we add an energy parameter
to the ray tracer,
trace(point p, vector d, int steps, float energy);
then we need only to add a line of code to check if there is sufﬁcient energy remaining
to continue tracing a ray.
There are many improvements we can make to speed up a ray tracer or make it
more accurate. For example, it is fairly simple to replace the recursion in the ray tracer
with iteration. Much of the work in ﬁnding intersections often can be avoided by
the use of bounding boxes or bounding spheres, because the intersection with these
objects can be done very quickly. Often, bounding volumes can be used to group
objects effectively, as can the BSP trees that we introduced in Chapter 8.
Because ray tracing is a sampling method, it is subject to aliasing errors. As
we saw in Chapter 7, aliasing errors occur when we do not have enough samples.
However, in our basic ray tracer, the amount of work is proportional to the number
of rays. Many ray tracers use a stochastic sampling method in which the decision as to
where to cast the next ray is based upon the results of rays cast thus far. Thus, if rays do
not intersect any objects in a particular region, few additional rays will be cast toward
it, while the opposite holds for rays that are cast in a direction where they intersect
many objects. This strategy is also used in RenderMan (Section 11.6). Although we
could argue that stochastic sampling only works in a probabilistic sense—as there
may well be small objects in areas that are not well sampled—it has the advantage
that images produced by stochastic sampling tend not to show the moiré patterns

```cpp
characteristic of images produced using uniform sampling.
```

Ray tracing is an inherently parallel process, as every ray can be cast indepen-
dently of every other ray. However, the difﬁculty is that every ray can potentially

```cpp
intersect any object. Hence, every tracing of a ray needs access to all objects. In ad-
```

dition, when we follow reﬂected and transmitted rays, we tend to lose any locality
that might have helped us avoid a lot of data movement. Consequently, parallel ray
tracers are best suited for shared-memory parallel architectures. With the availability
of multi-core processors with 64-bit addressing, commodity computers can support
sufﬁcient memory to make ray tracing a viable alternative in many applications.

### 11.4     THE RENDERING EQUATION

Most of the laws of physics can be expressed as conservation laws, such as the con-
servation of momentum and the conservation of energy. Because light is a form of
570         Chapter 11       Advanced Rendering
energy, an energy-based approach can provide an alternative to ray tracing. Con-
sider the closed environment shown in Figure 11.12. We see some surfaces that deﬁne
the closed environment, some objects, and a light source inside. Physically, all these
surfaces, including the surface of the light source, can be modeled in the same way.
Although each one may have different parameters, each obeys the same physical laws.
Any surface can absorb some light and reﬂect some light. Any surface can be an emit-
ter of light. From the ray-tracing perspective, we can say that the shades that we see
are the result of an inﬁnite number of rays bouncing around the environment, start-
ing with sources and not ending until all the energy has been absorbed. However,

*FIGURE 11.12 Closed envi-         when we look at the scene, we see the steady state; that is, we see each surface having*

ronment with four objects and     its own shades. We do not see how the rays have bounced around; we see only the
a light source.
end result. The energy approach allows us to solve for this steady state directly, thus
avoiding tracing many rays through many reﬂections.
Let’s consider just one surface, as shown in Figure 11.13. We see rays of light en-
tering from many directions and other rays emerging, also possibly in all directions.
The light leaving the surface can have two components. If the surface is a light source,
then some fraction of the light leaving the surface is from emission. The rest of the
light is the reﬂection of incoming light from other surfaces. Hence, the incoming light
also consists of emissions and reﬂections from other surfaces.
We can simplify the analysis by considering two arbitrary points p and p, as

*FIGURE 11.13 A simple             shown in Figure 11.14. If we look at the light arriving at and leaving p, the energy*

surface.                          must balance. Thus, the emission of energy, if there is a source at p, and the reﬂected
light energy must equal the incoming light energy from all possible points p. Let
p′        i(p, p) be the intensity of light leaving the point p and arriving at the point p.1 The

i(p, p ) = ν(p, p )((p, p ) + ρ(p, p , p)i(p , p)dp),
                   
p                     expresses this balance. The intensity leaving p consists of two parts. If p is an emitter
of light (a source), there is a term (p, p) in the direction of p. The second term is
the contribution from reﬂections from every possible point (p) that are reﬂected at

*FIGURE 11.14 Light from p        p in the direction of p. The reﬂection function ρ(p, p , p) characterizes the material*

arriving at p.                    properties at p. The term ν(p, p) has two possible values. If there is an opaque
surface between p and p, then the surface occludes p from p and no light from p
reaches p. In this case, ν(p, p) = 0. Otherwise, we must account for the effect of the
distance between p and p and
ν(p, p) =       ,
r2
where r is the distance between the two points.
1. We are being careful to avoid introducing the units and terminology of radiometry. We see the

```cpp
intensity of light. Energy is the integral of intensity over time, but, if the light sources are unchanging,
```

we are in the steady state, and this distinction does not matter. Most references work with the energy
or intensity per unit area (the energy ﬂux) rather than energy or intensity.
                                                                                              11.5 Radiosity   571
Although the form of the rendering equation is wonderfully simple, solving it is
not an easy task. The main difﬁculty is the dimensionality. Because p and p are points
in three-dimensional space, i(p, p) has six variables and ρ has nine. In addition, we
have not included an additional variable for the wavelength of light, which would be
necessary to work with color.
There have been some efforts to solve a general form of the rendering equation
by numerical methods. Most of these have been Monte Carlo methods that are some-
what akin to stochastic sampling. Recently, photon mapping has become a viable
approach. Photon mapping follows individual photons, the carriers of light energy,
from when they are produced at the light sources to when they are ﬁnally absorbed
by surfaces in the scene. Photons typically go through multiple reﬂections and trans-
missions from creation to ﬁnal absorption. The potential advantage of this approach
is that it can handle complex lighting of the sort that characterizes real-world scenes.
Although we argued when we discussed ray tracing that, because such a small
percentage of light emitted from sources reaches the viewer, tracing rays from a source
is inefﬁcient, photon mapping uses many clever strategies to make the process com-
putationally feasible. In particular, photon mapping uses a conservation of energy
approach combined with Monte Carlo methods. For example, consider what happens
when light strikes a diffuse surface. As we have seen, the reﬂected light is diffused in
all directions. In photon mapping, when a photon strikes a diffuse surface, a pho-
ton can be reﬂected or absorbed. Whether the photon is absorbed or not—and the
speciﬁed angle of reﬂection, if it is reﬂected—is determined stochastically in a man-
ner that yields the correct results on the average. Thus, what happens to two photons
that strike a surface at the same place and with the same angle of incidence can be
very different. The more photons that are generated from sources, the greater the
accuracy—but at the cost of tracing more photons.
There are special circumstances that simplify the rendering equation. For exam-
ple, for perfectly specular surfaces, the reﬂection function is nonzero only when the
angle of incidence equals the angle of reﬂection and the vectors lie in the same plane.
Under these circumstances, ray tracing can be looked at as a method for solving the
rendering equation.
The other special case that leads to a viable rendering method occurs when all
surfaces are perfectly diffuse. In this case, the amount of light reﬂected is the same in
all directions. Thus, the intensity function depends only on p. We examine this case
in the next section.

### 11.5    RADIOSITY

One way to simplify the rendering equation is to consider an environment in which
all the surfaces are perfectly diffuse reﬂectors. Because a perfectly diffuse surface looks
the same to all viewers, the corresponding BRDF is far simpler than the general cases
considered by the rendering equation.
Note, however, that although a perfectly diffuse surface reﬂects light equally in
all directions, even if the surface is ﬂat, it can show variations in shading that are
not shown when we render using the modiﬁed Phong lighting model. Suppose that
572   Chapter 11   Advanced Rendering

*FIGURE 11.15 Simple scene with diffuse surfaces.*

we have a simple scene, such as that shown in Figure 11.15, in which all the surfaces
are perfectly diffuse. If we render this scene with a distant light source, each polygon
surface is rendered as a single color. If this were a real scene, however, some of the
diffuse reﬂections from the red wall would fall on the white wall, causing red light to
be added to the white light reﬂected from those parts of the white wall that are near
the red wall. Diffuse light reﬂected from the white wall would have a similar effect
on the red wall. Our simple shading model has not considered these diffuse–diffuse

```cpp
interactions.
```

An ideal global renderer would capture these interactions. The radiosity method
can approximate them very well using an energy approach that was originally used
for solving problems in heat transfer.
The basic radiosity method breaks up the scene into small ﬂat polygons, or
patches, each of which can be assumed to be perfectly diffuse and renders in a

```cpp
constant shade. First we must ﬁnd these shades. Once we have found them, we
```

have effectively assigned a color to each patch that is independent of the viewer.
Effectively, we have assigned colors to a set of polygon patches in a three-dimensional
environment. We can now place the viewer wherever we wish and render the scene in
a conventional manner, using a pipeline renderer.

#### 11.5.1 The Radiosity Equation

Let’s assume our scene consists of n patches numbered from 1 to n. The radiosity of
patch i, bi , is the light intensity (energy/unit time) per unit area leaving the patch.
Typically, the radiosity would be measured in units such as watts/meter 2. As we are
measuring intensity at a ﬁxed wavelength, we can think of a radiosity function bi (λ)
that determines the color of patch i. Suppose that patch i has area ai . Because we have
assumed that each patch is a perfectly diffuse surface, the total intensity leaving patch
i is bi ai . Following reasoning similar to that which we used to derive the rendering
equation, the emitted intensity consists of an emissive component, also assumed to
be constant across the patch, and a reﬂective component due to the intensities of all
other patches whose light strikes patch i, and we obtain the equation

bi ai = ei ai + ρi         fji bj aj .
j=0
                                                                                          11.5 Radiosity   573
The term fij is called the form factor between patch i and patch j. It represents the
fraction of the energy leaving patch i that reaches patch j. The form factor depends
on how the two patches are oriented relative to each other, how far they are from
each other, and whether any other patches occlude the light from patch j and prevent
it from reaching patch i. We will discuss the calculation of these factors in the next
subsection. The reﬂectivity of patch i is ρi .
There is a simple relationship between the factors fij and fji known as the reci-
procity equation,
fij ai = fji aj .
Substituting into the equation for the equation for the patch intensities, we ﬁnd

bi ai = ei ai + ρi              fij bj ai .
j=0
We can now divide by ai , obtaining an equation for the patch radiosity:

bi = e i + ρ i            fij bj .
j=0
This result is called the radiosity equation.
Assuming that we have computed the form factors, we have a set of n linear
equations in the n unknown radiosities. Comparing this equation to the rendering
equation, we can see that if the patches were made smaller and smaller, in the limit we
would have an inﬁnite number of patches. The sum would become an integral, and
the resulting radiosity equation would be a special case of the rendering equation in
which all surfaces are perfectly diffuse reﬂectors.
We can put these equations in matrix form by deﬁning the column matrix of
b = [bi ],
e = [ei ],
a diagonal matrix from the reﬂection coefﬁcients

ρi   if i = j,
R = [ rij ] ,             aij =
0    otherwise,
F = [fij ].
574   Chapter 11   Advanced Rendering
Now the set of equations for the radiosities becomes
b = e + RFb.
We can write the formal solution as
b = [I − RF]−1e.

#### 11.5.2 Solving the Radiosity Equation

Although it can be shown that the radiosity equation must have a solution, the
real difﬁculties are practical. A typical scene will have thousands of patches, so that
solving the equations by a direct method, such as Gaussian elimination, usually is
not possible. Most methods rely on the fact that the matrix F is sparse. Most of its
elements are effectively zero because most patches are sufﬁciently far from each other
so that almost none of the light that a patch emits or reﬂects reaches most other
patches.
Solution of sets of equations involving sparse matrices are based on iterative
methods that require the multiplication of these sparse matrices, an efﬁcient opera-
tion. Suppose that we use the equation for the patches to create the iterative equation
bk+1 = e + RFbk .
Each iteration of this equation requires the matrix multiplication RFbk , which, as-
suming F is sparse, requires O(n) operations, rather than the O(n2) operations for
the general case. In terms of the individual radiosities, we have

bik+1 = ei +         ρi fij bjk .
j=1
This method, which is known as Jacobi’s method, will converge for this problem
regardless of the initial starting point b0.
In general, patches are not self-reﬂecting so that fij = 0. If we apply updates as
soon as they are available, we obtain the Gauss-Seidel method:

i−1                    
bik+1 = ei +         ρi fij bjk+1 +           ρi fij bjk .
j=1                    j=i+1
There is another possible iteration that provides additional physical insight. Con-
∞
=     xi ,
1− x   i=0
                                                                                                  11.5 Radiosity                      575
which holds if |x| < 1. The matrix form of this equation for RF is2
∞

−1
[I − RF]        =   (RF)i .
i=0
Thus, we can write b as
∞

b=         (RF)i e
i=0
= e + (RF)e + (RF)2e + (RF)3e + . . . .
We can use this expression, terminating it at some point, to approximate b. Each
term has an interesting physical interpretation. The ﬁrst term, e, is the direct light
emitted from each patch so that an approximate solution with just this term will
show only the sources. The second term, (RF)e, adds in the light that is the result
of exactly one reﬂection of the sources onto other patches. The next term adds on the
contribution from double reﬂections, and so on for the other terms.                                        ni                         Pj

#### 11.5.3 Computing Form Factors                                                                                              nj

At this point, we have ways of solving for the patch radiosities that are reasonably
efﬁcient, but we have yet to discuss the calculation of the form factors. We start with
a general derivation of an equation for the form factor between two patches, and then                                           Pi
we will look at how we might ﬁnd approximate form factors using the tools that we
have available.
Consider two perfectly diffuse ﬂat patches, Pi and Pj , as shown in Figure 11.16,           FIGURE 11.16 Two patches.
which we have drawn without any other patches that might cause occlusion. Each
patch has a normal that gives its orientation. Although each patch emits light equally
in all directions, the amount of light leaving two different points on Pj that reaches                               nj         daj
any point on Pi is different because of the different distances between two points on                                     φj
Pj and a point on Pi . Thus, to collect all the light from Pj that falls on Pi , we must                    ni

```cpp
integrate over all points on Pj . The same reasoning applies to two points on Pi that
```

receive light from a point on Pj ; they will receive a different amount of light, and thus                      φi
to determine the light falling on Pi , we must also integrate over all points on Pi .
We can derive the required integral based on our understanding of diffuse shad-
ing from Chapter 5. Consider a small area on each patch, dai and daj . Each can be                                    dai
considered to be an ideal diffuse reﬂector (Figure 11.17). They are a distance r apart,

*FIGURE 11.17 Foreshorten-*

where r is the distance between two points, pi and pj , at the center of the two small           ing between two small patches.
areas. The light leaving daj goes in the direction d = pi − pj . However, the intensity
of this light is reduced or foreshortened by cos φj , where φj is the angle between the
normal for patch j and the vector d. Likewise, the light arriving at dai is foreshortened
2. The formula converges if the magnitudes of all the eigenvalues of BF are less than unity, a
condition that must be true for the radiosity equation.
576   Chapter 11   Advanced Rendering
by cos φi , where φi is the angle between the normal for patch i and the vector d. To
obtain the desired form factor, fij , we must also account for the distance between the
patches and the possibility of occlusion. We can do both by ﬁrst deﬁning the term

1 if pj is visible from pi ,
oij =
0 otherwise.
Then, by averaging over the total area of the patch, we obtain the form-factor equa-
         cos φi cos φj
fij =          oij               dai daj .
ai ai aj         π r2
Although the form of this integral is simple, there are only a few special cases
for which it can be solved analytically. For real scenes, we need numerical methods.
Because there are n2 form factors, most methods are a compromise between accuracy
and time.
We will sketch two approaches. The ﬁrst method starts with the notion of two-
step mappings that we used in our discussion of texture mapping in Chapter 7.
Consider two patches again, this time with a hemisphere separating them, as shown
in Figure 11.18. Suppose that we want to compute the light from Pi that reaches Pj
at a point pi . We center the hemisphere at this point and orient the patch so that the
normal is pointing up. We can now project Pj onto the hemisphere and observe that
we can use the projected patch for our computation of the form factor rather than the
original patch. If we convert to polar coordinates, the form factor equation becomes
simpler for the projected patch. However, for each small area of Pi , we have to move
the hemisphere and add the contribution from each area.
A simpler approach for most graphics applications is to use a hemicube rather
than a hemisphere, as shown in Figure 11.19. The hemicube is centered like the
hemisphere but its surface is divided into small squares called pixels. The light that
strikes patch i is independent of the type of intermediate surface that we use. The
advantage of the hemicube is that its surfaces are either parallel to or orthogonal

*FIGURE 11.18 Projecting patch on a hemisphere.*

                                                                                              11.5 Radiosity   577

*FIGURE 11.19 Projecting patch on a hemicube and onto another patch.*

to Pi . Consequently, it is straightforward to project Pj onto the hemicube and to
compute the contribution of each pixel on the hemicube to the light that reaches
point pi . Thus, if the surface of the hemicube is divided into m pixels, numbered
1 to m, then we can compute the contribution of those pixels that are projections of
Pj and are visible from pi and add them to form the delta form factor fij , which is
the contribution of Pj to the small area dai at the center of the hemicube. We then get
the desired form factor by adding the contributions of all the delta form factors. The
contributions from each pixel can be computed analytically once we know whether
Pj projects on it, a calculation that is similar to the ray-tracing calculation required to
determine whether an object is visible from a light source. The details are included in
the Suggested Readings at the end of the chapter.
Another approach to computing form factors exploits our ability to use a graph-
ics system to compute simple renderings very rapidly. Suppose that we want to com-
pute fij . If we illuminate Pi with a known amount of light only from Pj , we will have
an experimental measurement of the desired form factor. We can approximate this
measurement by placing point light sources on Pj and then rendering the scene with
whatever renderer we have available. Because of the possibility that another patch
obscures Pi from Pj , we must use a renderer that can handle shadows.

#### 11.5.4 Carrying Out Radiosity

In practice, radiosity rendering has three major steps. First, we divide the scene into
a mesh of patches, as shown in Figure 11.20. This step requires some skill, because
more patches require the calculation of more form factors. However, it is the division
of surfaces into patches that allows radiosity to yield images with subtle diffuse–
diffuse interactions. The creation of the initial mesh often can be done interactively,
allowing the placement of more patches in regions such as corners between surfaces
578   Chapter 11   Advanced Rendering

*FIGURE 11.20 Division of surfaces into patches. (Courtesy of A. Van*

Pernis, K. Rasche, and R. Geist, Clemson University)
where we expect to see diffuse–diffuse interactions. Another approach is based on the
observation that the radiosity of a large surface is equal to the area-weighted sum of
the radiosities of any subdivision of it. Hence, we can start with a fairly rough mesh
and reﬁne it later (progressive radiosity). Once we have a mesh, we can compute the
form factors, the most computationally intense part of the process.
Once we have the mesh and the form factors, we can solve the radiosity equation.
We form the emission array e using the values for the light sources in the scene and
assign colors to the surfaces, forming R. Now we can solve the radiosity equation to
determine b.
The components of b act as the new colors of the patches. We can now place the
viewer in the scene and render with a conventional renderer.
The image in Color Plate 16 was rendered using radiosity. It started with the
initial mesh in Figure 11.20, which was then altered with a particle system to achieve
a better set of patches. It shows the strength of radiosity for rendering interiors that
are composed of diffuse reﬂectors.

### 11.6    RENDERMAN

There are other approaches to rendering that have arisen from the needs of the ani-
mation industry. Although interaction is required in the design of an animation, real-
                                                                                    11.7 Parallel Rendering   579
time rendering is not required when the ﬁnal images are produced. Of greater impor-
tance is producing images free of rendering artifacts such as the jaggedness and moiré
patterns that arise from aliasing. However, rendering the large number of frames re-
quired for a feature-length ﬁlm cannot be done with ray tracers or radiosity renderers,
even though animations are produced using large numbers of computers—render
farms—whose sole task is to render scenes at the required resolution. In addition,
neither ray tracers nor radiosity renderers alone produce images that have the desired
artistic qualities.
The RenderMan interface is based on the use of the modeling–rendering para-
digm that we introduced in Chapter 1. The design of a scene is done interactively,
using simple renderers of the type that we discussed in Chapter 6. When the design is
complete, the objects, lights, material properties, cameras, motion descriptions, and
textures can be described in a ﬁle that can be sent to a high-quality renderer or to a
render farm.
In principle, this off-line renderer could be any type of renderer. However,
given the special needs of the animation industry, Pixar developed both the interface
(RenderMan) and a renderer called Reyes that was designed to produce the types of
images needed for commercial motion pictures. Like a ray tracer, Reyes was designed
to work a pixel at a time. Unlike a ray tracer, it was not designed to incorporate global
illumination effects. By working a pixel at a time, Reyes collects all the light from all
objects at a resolution that avoids aliasing problems. Reyes divides (dices) objects—
both polygonal and curved—into micropolygons, which are small quadrilaterals that
project to a size of about half of a pixel. Because each micropolygon projects to such a
small area, it can be ﬂat-shaded, thereby simplifying its rendering. The smooth shad-
ing of surfaces is accomplished by coloring the micropolygons carefully during the
dicing process.
Reyes incorporates many other interesting techniques. It uses random or stochas-
tic sampling rather than point sampling to reduce visible aliasing effects. Generally,
it works on small regions of the frame at one time to allow efﬁcient use of textures.
Note that even with its careful design, a single scene with many objects and complex
lighting effects can take hours to render.
There are many renderers of this type available, some either public domain or
shareware. Some renderers are capable of incorporating different rendering styles
within one product. Thus, we might use ray tracing on a subset of the objects that
have shiny surfaces. Likewise, we might want to use radiosity on some other subset
of the surfaces. In general, these renderers support a large variety of effects and allow
the user to balance rendering time against sophistication and image quality.

### 11.7    PARALLEL RENDERING

In many applications, particularly in the scientiﬁc visualization of large geometric
data sets, we create images from data sets that might contain more than 500 million
data points and generate more than 100 million polygons. This situation presents
two immediate challenges. First, if we are to display this many polygons, how can we
580   Chapter 11   Advanced Rendering

*FIGURE 11.21 Power wall using six projectors.*

do so when even the best commodity displays contain only about 2 million pixels?
Second, if we have multiple frames to display, either from new data or because of
transformations of the original data set, we need to be able to render this large
amount of geometry faster than can be achieved even with high-end systems.
A popular solution to the display-resolution problem is to build a power wall, a
large projection surface that is illuminated by an array of projectors (Figure 11.21),
each with the resolution of 1024 × 1280, or 1920 × 1080 for HD projectors. Gener-
ally, the light output from the projectors is tapered at the edges, and the displays are
overlapped slightly to created a seamless image. We can also create high-resolution
displays from arrays of standard-size LCD panels, although at the cost of seeing small
separations between panels, which can give the appearance of a window with multiple
panes.
One approach to both these problems is to use clusters of standard comput-
ers connected with a high-speed network. Each computer might have a commodity
graphics card. Note that such conﬁgurations are one aspect of a major revolution in
high-performance computing. Formerly, supercomputers were composed of expen-
sive, fast processors that usually incorporated a high degree of parallelism in their
designs. These processors were custom designed and required special interfaces, pe-
ripheral systems, and environments that made them extremely expensive and thus
affordable only by a few government laboratories and large corporations. Over the
last few years, commodity processors have become extremely fast and inexpensive. In
                                                                                    11.7 Parallel Rendering   581
addition, the trend is toward incorporating multiple processors in the same CPU or
GPU, thus creating the potential for various types of parallelism ranging from using a
single CPU with multiple graphics cards, using many multicore CPUs, to GPUs with
hundreds of programmable processors.
Consequently, there are multiple ways we can distribute the work that must be
done to render a scene among the various processors. The simplest approach is to
execute the same application program on each processor but have each use a different
window that corresponds to where the processor’s display is located in the output
array. Given the speed of modern CPUs and GPUs and the large amount of memory
available to them, this is often a viable approach.
We will examine three other possibilities. In this taxonomy, the key difference is
where in the rendering process we assign, or sort, primitives to the correct areas of
the display. Where we place this step leads to the designations sort ﬁrst, sort last, and
sort middle.
Suppose that we start with a large number of processors of two types: geome-
try processors and raster processors. This distinction corresponds to the two phases
of the rendering pipeline that we discussed in Chapter 6. The geometry processors
can handle front-end ﬂoating-point calculations, including transformations, clip-
ping, and shading. The raster processors manipulate bits and handle operations such
as scan conversion. Note that the present general-purpose processors and graphics
processors can each do either of these tasks. Consequently, we can apply the following
strategies to either the CPUs or the GPUs. We can achieve parallelism among distinct
nodes, within a processor chip through multiple cores, or within the GPU. The use
of the sorting paradigm will help us organize the architectural possibilities.

#### 11.7.1 Sort-Middle Rendering

Consider a group of geometry processors (each labeled with a G) and raster proces-
sors (each labeled with an R) connected as shown in Figure 11.22. Suppose that we
have an application that generates a large number of geometric primitives. It can use
multiple geometry processors in two obvious ways. It can run on a single processor
and send different parts of the geometry generated by the application to different ge-
ometry processors. Alternatively, we can run the application on multiple processors,
each of which generates only part of the geometry. At this point, we need not worry
about how the geometry gets to the geometry processors—as the best way is often
application dependent—but on how to best employ the geometry processors that are
available.
Assume that we can send any primitive to any of the geometry processors, each
of which acts independently. When we use multiple processors in parallel, a major
concern is load balancing, that is, having each of the processors do about the same
amount of work, so that none is sitting idle for a signiﬁcant amount of time, thus
wasting resources. One obvious approach would be to divide the object-coordinate
space equally among the processors. Unfortunately, this approach often leads to
poor load balancing because in many applications the geometry is not uniformly
distributed in object space. An alternative approach is to distribute the geometry
582   Chapter 11   Advanced Rendering

*FIGURE 11.22 Sort-middle*

rendering.
uniformly among the processors as objects are generated, independently of where
the geometric objects are located. Thus, with n processors, we might send the ﬁrst
geometric entity to the ﬁrst processor, the second to the second processor, the nth
to the nth processor, the (n + 1)-st to the ﬁrst processor, and so on. Now consider
the raster processors. We can assign each of these to a different region of the frame
buffer or, equivalently, assign each to a different region of the display. Thus, each
raster processor renders a ﬁxed part of screen space.
Now the problem is how to assign the outputs of the geometry processors to the
raster processors. Note that each geometry processor can process objects that could go
anywhere on the display. Thus, we must sort their outputs and assign primitives that
emerge from the geometry processors to the correct raster processors. Consequently,
some sorting must be done before the raster stage. We refer to this architecture as sort
middle.
This conﬁguration was popular with high-end graphics workstations a few years
ago, when special hardware was available for each task and there were fast internal
buses to convey information through the sorting step. Recent GPUs contain multiple
geometry processors and multiple fragment processors and so can be looked at as
sort-middle processors.3 For now, let’s consider a graphics card with a single GPU as
a combination of one geometry processor and one raster processor, thus aggregating
the parallelism inside the GPU. Now the problem is how to use a group of commodity
3. Some GPUs now provide a large number of processors that can be used as either vertex or
fragment processors.
                                                                                    11.7 Parallel Rendering   583

*FIGURE 11.23 Sort-last rendering.*

cards or GPUs. If we can use a GPU or a CPU as either a geometry processor or a
raster processor and connect them with a standard network, the sorting step in sort
middle can be a bottleneck, and two other approaches have proved simpler.

#### 11.7.2 Sort-Last Rendering

With sort-middle rendering, the number of geometry processors and the number of
raster processors could be different. Now suppose that each geometry processor is
connected to its own raster processor, as shown in Figure 11.23. This conﬁguration
would be what we would have with a collection of standard PCs, each with its own
graphics card, or on some of the most recent graphics cards that have multiple inte-
grated vertex and fragment processors. Once again, let’s not worry about how each
processor gets the application data and instead focus on how this conﬁguration can
process the geometry generated by the application.
Just as with sort middle, we can load-balance the geometry processors by sending
primitives to them in an order that ignores where on the display they might lie once
they are rasterized. However, precisely because of this way of assigning geometry and
lacking a sort in the middle, each raster processor must have a frame buffer that is
the full size of the display. Because each geometry/raster pair contains a full pipeline,
each pair produces a correct hidden-surface–removed image for part of the geometry.
Figure 11.24 shows three images that are each correct, while the fourth shows how
they must be combined to form a correct image containing all the geometry.
We can combine the partial images with a compositing step, as displayed in Fig-
ure 11.24. For the compositing calculations, we need not only the images in the color
buffers of the geometry processors but also the depth information, because we must
know for each pixel which of the raster processors contains the pixel corresponding
584   Chapter 11   Advanced Rendering
(a)                                               (b)
(c)                                               (d)

*FIGURE 11.24 Example of sort-last rendering. (a)–(c) Partial renderings. (d) Composited image.*

(Courtesy of Ge Li, University of New Mexico)
                                                                                    11.7 Parallel Rendering   585
P0          P1          P2         P3
P1                     P3
P3

*FIGURE 11.25 Binary-tree compositing.*

to the closest point to the viewer.4 Fortunately, if we are using our standard OpenGL
pipeline, the necessary information is in the z buffer. For each pixel, we need only
compare the depths in each of the z buffers and write the color in the frame buffer
of the processor with the closest depth. The difﬁculty is determining how to do this
comparison efﬁciently when the information is stored on many processors.
Conceptually, the simplest approach, sometimes called binary-tree composit-
ing, is to have pairs of processors combine their information. Consider the example
shown in Figure 11.25, where we have four geometry/raster pipelines, numbered 0–3.
Processors 0 and 1 can combine their information to form a correct image for the ge-
ometry they have seen, while processors 2 and 3 do the same thing concurrently with
their information. Let’s assume that we form these new images on processors 1 and
3. Thus, processors 0 and 2 have to send both their color buffers and their z buffers
to their neighbors (processors 1 and 3, respectively). We then repeat the process be-
tween processors 1 and 3, with the ﬁnal image being formed in the frame buffer of
processor 3. Note that the code is very simple. The geometry/raster pairs each do an
ordinary rendering. The compositing step requires only reading pixels and some sim-
ple comparisons. However, in each successive step of the compositing process, only
half the processors that were used in the previous step are still needed. In the end, the
ﬁnal image is prepared on a single processor.
There is another approach to the compositing step known as binary-swap com-
positing that avoids the idle-processor problem. In this technique, each processor is
responsible for one part of the ﬁnal image. Hence, for compositing to be correct, each
4. For simplicity, we are assuming that all the geometric objects are opaque.
586   Chapter 11   Advanced Rendering
P0       P1        P2                          Pn

*FIGURE 11.26 Binary-swap compositing.*

processor must see all the data. If we have n processors involved in the compositing,
we can arrange them in a round-robin fashion, as shown in Figure 11.26. The com-
positing takes n steps (rather than the log n steps required by tree compositing). On
the ﬁrst step, processor 0 sends portion 0 of its frame buffer to processor 1 and re-
ceives portion n from processor n. The other processors do a similar send and receive
of the portion of the color and depth buffers of their neighbors. At this point, each
processor can update one area of the display that will be correct for the data from a
pair of processors. For processor 0, this will be region n. On the second round, pro-
cessor 0 will receive from processor n the data from region n − 1, which is correct for
the data from processors n and n − 1. Processor 0 will also send the data from region
n, as will the other processors for part of their frame buffers. All the processors will
now have a region that is correct for the data from three processors. Inductively, it
should be clear that after n − 1 steps, each processor has 1/n of the ﬁnal image. Al-
though we have taken more steps, far less data has been transferred than with tree
compositing, and we have used all processors in each step.

#### 11.7.3 Sort-First Rendering

One of the most appealing features of sort-last rendering is that we can pair geometric
and raster processors and use standard computers with standard graphics cards.
Suppose that we could decide ﬁrst where each primitive lies on the ﬁnal display.
Then we could assign a separate portion of the display to each geometry/raster pair
and avoid the necessity of a compositing network. The conﬁguration might look as
illustrated in Figure 11.27. Here we have included a processor at the front end to make
the assignment as to which primitives go to which processors.
This front-end sort is the key to making this scheme work. In one sense, it might
seem impossible, since we are implying that we know the solution—where primitives
appear in the display—before we have solved the problem for which we need the
geometric pipeline. But things are not hopeless. Many problems are structured so
that we may know this information in advance. We also can get the information back
from the pipeline using glGetFloatv to ﬁnd the mapping from object coordinates
                                                                                  11.7 Parallel Rendering   587

*FIGURE 11.27 Sort-first rendering.*

to screen coordinates. In addition, we need not always be correct. A primitive can
be sent to multiple geometry processors if it straddles more than one region of the
display. Even if we send a primitive to the wrong processor, that processor may be
able to send it on to the correct processor. Because each geometry processor performs
a clipping step, we are assured that the resulting image will be correct.
Sort-ﬁrst rendering does not address the load-balancing issue, because if there
are regions of the screen with very few primitives, the corresponding processors
may not be very heavily loaded. However, sort-ﬁrst rendering has one important
advantage over sort-last rendering: It is ideally suited for generating high-resolution
displays. Suppose that we want to display our output at a resolution much greater
than we get with typical CRT or LCD displays that have a resolution in the range of 1–
3 million pixels. Such displays are needed when we wish to examine high-resolution
data that might contain more than 100 million geometric primitives.
One approach to this problem is to build a tiled display or power wall consisting
of an array of standard displays (or tiles). The tiles can be CRTs, LCD panels, or
the output of projectors. From the rendering perspective, we want to render an
image whose resolution is the array of the entire display, which can exceed 4000 ×
4000 pixels. Generally, these displays are driven by a cluster of PCs with commodity
graphics cards. Hence, the candidate rendering strategies are sort ﬁrst and sort last.
However, sort-last rendering cannot work in this setting, because each geome-
try/rasterizer processor must have a frame buffer the size of the ﬁnal image, and for
the compositing step, extremely large amounts of data must be exchanged between
processors. Sort-ﬁrst renderers do not have this problem. Each geometry/processor
588   Chapter 11   Advanced Rendering
pair need only be responsible for a small part of the ﬁnal image, typically an image
the size of a standard frame buffer.

### 11.8     VOLUME RENDERING

Our development of computer graphics has focused on the display of surfaces. Hence,
even though we can render an object so we see its three-dimensionality, we do so by
modeling it as a set of two-dimensional surfaces within a three-dimensional space
and then rendering these surfaces. This approach does not work well if we have a set
of data in which each value represents a value at a point within a three-dimensional
region.
Consider a function f that is deﬁned over some region of three-dimensional
space. Thus, at every point within this region, we have a scalar value f (x, y, z) and
we say that f deﬁnes a scalar ﬁeld. For example, at each point f might be the density
inside an object, or the absorption of X-rays in the human body as measured by a
computed-tomography (CT) scan, or the translucency of a slab of glass. Visualization
of scalar ﬁelds is more difﬁcult than the problems that we have considered thus far for
two reasons. First, three-dimensional problems have more data with which we must
work. Consequently, operations that are routine for two-dimensional data, such as
reading ﬁles and performing transformations, present practical difﬁculties. Second,
when we had a problem with two independent variables, we were able to use the third
dimension to visualize scalars. When we have three independent variables, we lack the
extra dimension to use for display. Nevertheless, if we are careful, we can extend our
previously developed methods to visualize three-dimensional scalar ﬁelds.
The ﬁeld of volume rendering deals with these problems. Most of the methods
for visualizing such volumetric data sets are extensions of the methods we have
developed, and we will survey a few approaches in the next few sections. Further
detail can be found in the Suggested Readings at the end of the chapter.

#### 11.8.1 Volumetric Data Sets

We start with a discrete set of data that might have been obtained from a set of
measurements of some physical process as with a medical data set from a CT scan.
Alternately, we might obtain data by evaluating (or sampling) a function f (x, y, z) at
a set of points {xi , yi , zi }, creating a volumetric data set.
Assume that our samples are taken at equally spaced points in x, y, and z, as
shown in Figure 11.28—a simplifying but not necessary assumption. Thus,
xi = x0 + i x,
yi = y0 + j y,
zi = z 0 + k z ,
and we can deﬁne
                                                                                    11.8 Volume Rendering   589
Δy
z                                    Δz
(x 0 , y0 , z 0 )    Δx

*FIGURE 11.28 A volumetric data set.*

fijk = f (xi , yj , zk ).
Each fijk can be thought of as the average value of the scalar ﬁeld within a right
parallelepiped of sides x, y, z centered at (xi , yj , zk ). We call this parallelepiped
a volume element, or voxel.
The three-dimensional array of voxel values that corresponds to equally spaced
samples is called a structured data set, because we do not need to store the informa-
tion about where each sample is located in space. The terms structured data set and
set of voxels often are used synonymously.
Scattered data require us to store this information in addition to the scalar values,
and such data sets are called unstructured. Visualization of unstructured data sets is
more complex but can be done with the same techniques that we use for structured
data sets; consequently, we shall not pursue this topic.
Even more so than for two-dimensional data, there are multiple ways to display
these data sets. However, there are two basic approaches: direct volume rendering
and isosurfaces. Direct volume rendering makes use of every voxel in producing an
image; isosurface methods use only a subset of the voxels. For a function f (x, y, z),
an isosurface is the surface deﬁned by the implicit equation
f (x, y, z) = c.
The value of the constant c is the isosurface value. For the discrete problem where
we start with a set of voxels, isosurface methods seek to ﬁnd approximate isosurfaces.

#### 11.8.2 Visualization of Implicit Functions

Isosurface visualization is the natural extension of contours to three dimensions and
thus has a connection to visualization of implicit functions. Consider the implicit
g(x, y, z) = 0,
590   Chapter 11   Advanced Rendering
f (x, y, z) = 0

*FIGURE 11.29 Ray casting of an implicit function.*

where g is known analytically. If any points satisfy this equation, then this function
describes one or more surfaces. Simple examples include spheres, planes, more gen-
eral quadrics, and the torus of radius r and cross section a:
(x 2 + y 2 + z 2 − r 2 − a2)2 − 4a2(r 2 − z 2) = 0.
As we discussed in Chapter 10, g is a membership function that allows us to test
whether a particular point lies on the surface, but there is no general method for ﬁnd-
ing points on the surface. Thus, given a particular g, we need visualization methods
to “see” the surface.
One way to attack this problem involves using a simple form of ray tracing
sometimes referred to as ray casting. Figure 11.29 shows a function, a viewer, and a
projection plane. Any projector can be written in the form of a parametric function:
p(t) = p0 + td.
It also can be written in terms of the individual components:
x(t) = x0 + tdx ,
y(t) = y0 + tdy ,
z(t) = z0 + tdz .
Substituting into the implicit equation, we obtain the scalar equation in t,
f (x0 + tdx , y0 + tdy , z0 + tdz ) = u(t) = 0.
                                                                   11.9 Isosurfaces and Marching Cubes   591
The solutions of this equation correspond to the points where the projector (ray)
enters or leaves the isosurface. If f is a simple function, such as a quadric or a torus,
then u(t) may be solvable directly, as we saw in our discussion of ray tracing in
Section 11.3.
Once we have the intersections, we can apply a simple shading model to the
surface. The required normal at the surface is given by the partial derivatives at the
point of intersection:
⎡ ∂f (x , y , z) ⎤
∂x
⎢ ∂f (x , y , z) ⎥
n=⎢
⎣ ∂y
⎥.
⎦
∂f (x , y , z)
∂z
Usually, we do not bother with global illumination considerations and thus do not
compute either shadow rays (to determine whether the point of intersection is illu-
minated) or any reﬂected and traced rays. For scenes composed of simple objects,
such as quadrics, ray casting not only is a display technique but also performs visible
surface determination and often is used with CSG models. For functions more com-
plex than quadrics, the amount of work required by the intersection calculations is
prohibitive and we must consider alternate methods. First, we generalize the problem
from one of viewing surfaces to one of viewing volumes.
Suppose that instead of the surface described by g(x, y, z) = 0, we consider
a scalar ﬁeld f (x, y, z), which is speciﬁed at every point in some region of three-
dimensional space. If we are interested in a single value c of f , then the visualization
problem is that of displaying the isosurface:
g(x, y, z) = f (x, y, z) − c = 0.
Sometimes, displaying a single isosurface for a particular value of c is sufﬁcient. For
example, if we are working with CT data, we might pick c to correspond to the X-ray
density of the tissues that we want to visualize. In other situations, we might display
multiple isosurfaces.
Finding isosurfaces usually involves working with a discretized version of the
problem, replacing the continuous function g by a set of samples taken over some
grid. Our prime isosurface visualization method is called marching cubes, the three-
dimensional version of marching squares.

### 11.9      ISOSURFACES AND MARCHING CUBES

Assume that we have data set {fijk }, where each voxel value is a sample of the scalar
ﬁeld f (x, y, z), and, when the samples are taken on a regular grid, that the discrete
data form a set of voxels. We seek an approximate isosurface using the sampled data to
deﬁne a polygonal mesh. For any value of c, there may be no surface, one surface, or
many surfaces that satisfy the equation for a given value of c. Given how well we can
display three-dimensional triangles, we describe a method, called marching cubes,
592   Chapter 11   Advanced Rendering
fi, j + 1, k + 1                               fi + 1, j + 1, k + 1
fi, j + 1, k
fi + 1, j + 1, k
fi + 1, j, k + 1
fi, j, k + 1
fi, j, k                            fi + 1, j, k

*FIGURE 11.30 Voxel cell.*

0                      1                    2                      3    4    5    6
7                      8                    9                      10   11   12   13

*FIGURE 11.31 Vertex colorings.*

that approximates a surface by generating a set of three-dimensional triangles, each
of which is an approximation to a piece of the isosurface.
We have assumed that our voxel values {fijk } are on a regular three-dimensional
grid that passes through the centers of the voxels. If they are not, we can use an

```cpp
interpolation scheme to obtain values on such a grid. Eight adjacent grid points
```

specify a three-dimensional cell, as shown in Figure 11.30. Vertex (i, j, k) of the cell is
assigned the data value fijk . We can now look for parts of isosurfaces that pass through
each of these cells, based on only the values at the vertices.
For a given isosurface value c, we can color the vertices of each cell black or white,
depending on whether the value at the vertex is greater than or less than c. There are
256 = 28 possible vertex colorings, but, once we account for symmetries, there are
only the 14 unique cases shown in Figure 11.31.5 Using the simplest interpretation of
the data, we can generate the points of intersection between the surface and the edges
of the cubes by linear interpolation between the values at the vertices. Finally, we can
use the triangular polygons to tessellate these intersections, forming pieces of a tri-
5. The original paper by Lorensen and Cline [Lor87] and many of the subsequent papers refer to 15
cases, but 2 of those cases are symmetric.
                                                                  11.9 Isosurfaces and Marching Cubes   593
0            1           2            3           4            5           6
7            8           9           10           11          12           13

*FIGURE 11.32 Tessellations for marching cubes.*

angular mesh passing through the cell. These tessellations are shown in Figure 11.32.
Note that not all these tessellations are unique.
Like the cells from our contour plots, each three-dimensional cell can be pro-
cessed individually. In terms of the sampled data, each interior voxel value contributes
to eight cells. We can go through the data, row by row, then plane by plane. As we do
so, the location of the cell that we generate marches through the data set, giving the
algorithm its name.
As each cell is processed, any triangles that it generates are sent off to be dis-
played through our graphics pipeline, where they can be lit, shaded, rotated, texture
mapped, and rasterized. Because the algorithm is so easy to parallelize and, like the
contour plot, can be table-driven, marching cubes is a popular way of displaying
three-dimensional data.
Marching cubes is both a data-reduction algorithm and a modeling algorithm.
Both simulations and imaging systems can generate data sets containing from 107 to
109 voxels. With data sets this large, simple operations (such as reading in the data,
rescaling the values, or rotating the data set) are time-consuming, memory-intensive
tasks. In many of these applications, however, after executing the algorithm, we might
have only 103 to 104 three-dimensional triangles—a number of geometric objects
handled easily by a graphics system. We can rotate, color, and shade the surfaces
in real time to interpret the data. In general, few voxels contribute to a particular
isosurface; consequently, the information in the unused voxels is not in the image.
There is an ambiguity problem in marching cubes. The problem can arise when-
ever we have different colors assigned to the diagonally opposite vertices of a side of
a cell. Consider the cell coloring in Figure 11.33(a). Figures 11.33(b) and 11.33(c)
show two ways to assign triangles to these data. If we compare two isosurfaces gen-
erated with the two different interpretations, areas where these cases arise will have
completely different shapes and topologies. The wrong selection of an interpretation
for a particular cell can leave a hole in an otherwise smooth surface. Researchers have
attempted to deal with this problem; no approach works all the time. As we saw with
594   Chapter 11   Advanced Rendering
(a)             (b)             (c)

*FIGURE 11.33 Ambiguity problem for marching cubes. (a) Cell. (b) One*


```cpp
interpretation of cell. (c) Second interpretation.
```

contour plots, an always-correct solution requires more information than is present
in the data.

### 11.10     MESH SIMPLIFICATION

We have looked at marching cubes as a method of generating small triangular pieces
of an isosurface. Equivalently, we can view the output of the algorithm as one or more
triangular meshes. These meshes are highly irregular even though they are composed
of only triangles.
One of the disadvantages of marching cubes is that the algorithm can generate
many more triangles than are really needed to display the isosurface. One reason for
this phenomenon is that the number of triangles primarily depends on the resolu-
tion of the data set, rather than on the smoothness of the isosurface. Thus, we can
often create a new mesh with far fewer triangles such that the rendered surfaces are
visually indistinguishable. There are multiple approaches to this mesh simpliﬁcation
problem.
One popular approach, called triangle decimation, seeks to simplify the mesh
by removing some edges and vertices. Consider the mesh in Figure 11.34. If we
move vertex A to coincide with vertex B, we eliminate two triangles and obtain the
simpliﬁed mesh in Figure 11.35. Decisions as to which triangles are to be removed
can be made using criteria such as the local smoothness or the shape of the triangles.
The latter criterion is important because long, thin triangles do not render well.
Other approaches are based on resampling the surface generated by the original
mesh, thus creating a new set of points that lie on the surface. These points are

*FIGURE 11.34 Original mesh.*

                                                                        11.11 Direct Volume Rendering   595

*FIGURE 11.35 Mesh after simplification.*

unstructured, lacking the connectivity in the original mesh. Thus, we are free to
connect them in some optimal way. Probably the most popular technique is the
Delaunay triangulation procedure from Chapter 10.
Another approach to resampling is to place points on the original mesh or select
a subset of the vertices and then use a particle system to control the ﬁnal placement
of the points (particles). Repulsive forces among the particles cause the particles to
move to positions that lead to a good mesh.

### 11.11     DIRECT VOLUME RENDERING

The weakness of isosurface rendering is that not all voxels contribute to the ﬁnal im-
age. Consequently, we could miss the most important part of the data by selecting the
wrong isovalue. Direct volume rendering constructs images in which all voxels can
make a contribution to the image. Usually these techniques are either extensions of
the compositing methods we introduced in Chapter 7 or applications of ray tracing.
Because the voxels typically are located on a rectangular grid, once the location of
the viewer is known, there is an ordering by which we can do either front-to-back or
back-to-front rendering.
Early methods for direct volume rendering treated each voxel as a small cube that
was either transparent or completely opaque. If the image was rendered in a front-to-
back manner, rays were traced until the ﬁrst opaque voxel was encountered on each
ray; then, the corresponding pixel in the image was colored black. If no opaque voxel
was found along the ray, the corresponding pixel in the image was colored white. If
the data set was rendered back to front, a painter’s algorithm was used to paint only
the opaque voxels. Both techniques produced images with serious aliasing artifacts
due to treating each voxel as a cube that was projected to the screen. They also failed
to display the information in all the voxels. With the use of color and opacity, we can
avoid or mitigate these problems.
596   Chapter 11   Advanced Rendering
X-ray density

*FIGURE 11.36 Histogram of CT data.*


#### 11.11.1 Assignment of Color and Opacity

We start by assigning a color and transparency to each voxel. For example, if the data
are from a CT scan of a person’s head, we might assign colors based on the X-ray
density. Soft tissues (low densities) might be red, fatty tissues (medium densities)
might be blue, hard tissues (high densities) might be white, and empty space might
be black. Often, these color assignments can be based on looking at the distribution
of voxel values—the histogram of the data. Figure 11.36 shows a histogram with four
peaks. We can assign a color to each peak, and, if we use indexed color, we can assign
red, green, and blue to the color indices through tables determined from curves such
as those shown in Figure 11.37. If these data came from a CT scan, the skull might
account for the low peak on the left and be assigned white, whereas empty space
might correspond to the rightmost peak in the histogram and be colored black.
Opacities are assigned on the basis of which voxels we wish to emphasize in the
image. If we want to show the brain but not the skull, we can assign zero opacity to the
values corresponding to the skull. The assignment of colors and opacities is a pattern-
recognition problem that we will not pursue. Often, a user interface allows the user
to control these values interactively. Here, we are interested in how to construct a
two-dimensional image after these assignments have been made.

#### 11.11.2 Splatting

Once colors and opacities are assigned, we can assign a geometric shape to each
voxel and apply the compositing techniques from Chapter 7. One method is to apply
back-to-front painting. Consider the group of voxels in Figure 11.38. Here, the term
front is deﬁned relative to the viewer. For three-dimensional data sets, once we have
positioned the viewer relative to the data set, front deﬁnes the order in which we
process the array of voxels. As we saw in Chapter 8, octrees can provide an efﬁcient
mechanism for storing voxel data sets. Positioning the viewer determines an order for
traversing the octree.
One particularly simple way to generate an image is known as splatting. Each
voxel is assigned a simple shape, and this shape is projected onto the image plane.
Figure 11.39 shows a spherical voxel and the associated splat, or footprint. Note that
Red                                                       11.11 Direct Volume Rendering   597
X-ray density
X-ray density
X-ray density

*FIGURE 11.37 Color curves for computed-tomography data.*


*FIGURE 11.38 Volume of voxels.*

598   Chapter 11   Advanced Rendering

*FIGURE 11.39 Splat, or footprint, of a voxel.*

if we are using a parallel projection and each voxel is assigned the same shape, the
splats differ in only color and opacity. Thus, we do not need to carry out a projection
for each voxel, but rather can save the footprint as a bitmap that can be bitblt into the
frame buffer.
The shape to assign each voxel is a sampling issue of the type we considered in

## Chapter 7. If the process that generated the data were ideal, each splat would be the

projection of a three-dimensional sinc function. The use of hexagonal or elliptical
splats is based on approximating a voxel with a parallelepiped or ellipsoid rather
than using the reconstruction part of the sampling theorem. A better approximation
is to use a Gaussian splat, which is the projection of a three-dimensional Gaussian
approximation to the sinc function.
The key issue in creating a splatted image is how each splat is composited into
the image. The data, being on a grid, are already sorted with respect to their distance
from the viewer or the projection plane. We can go through the data back to front,
adding the contributions of each voxel through its splat. We start with a background
image and blend in successive splats.

#### 11.11.3 Volume Ray Tracing

An alternative direct volume-rendering technique is front-to-back rendering by ray
tracing (Figure 11.40). Using the same compositing formulas that we used for splat-
ting along a ray, we determine when an opaque voxel is reached, and we stop trac-
ing this ray immediately. The difﬁculty with this approach is that a given ray passes
through many slices of the data, and thus we need to keep all the data available.
                                                                         11.11 Direct Volume Rendering   599
(a)                                        (b)

*FIGURE 11.40 Volume ray casting. (a) Three-dimensional view. (b) Top*

view.
It should be clear that the issues that govern the choice between a back-to-front
and a front-to-back renderer are similar to the issues that arise when we choose
between an image-oriented renderer and an object-oriented renderer. We have merely
added opacity to the process. Consequently, a volume ray tracer can produce images
that have a three-dimensional appearance and can make use of all the data. However,
the ray-traced image must be recomputed from scratch each time that the viewing
conditions change or that we make a transformation on the data.
The approach used most often in volume ray tracing is often called ray casting
because we generally display only the shading at the intersection of the ray with
the voxels and do not bother with shadow rays. Recently, researchers have explored
various strategies to use GPUs for much of the calculations.

#### 11.11.4 Texture Mapping of Volumes

The hardware and software support for texture mapping is the basis for another
approach to direct volume rendering using three-dimensional textures. Suppose that
we have sufﬁcient texture memory to hold our entire data set. We can now deﬁne a set
of planes parallel to the viewer. We can map texture coordinates to world coordinates
such that these planes cut through the texture memory, forming a set of parallel
polygons, as shown in Figure 11.41. We now texture map the voxels to these polygons
for display. Because we need only a few hundred polygons to be compatible with the
number of data points that we have in most problems, we place little burden on our
rendering hardware. Unlike all the other volume-rendering methods, this one is fast
600        Chapter 11   Advanced Rendering

*FIGURE 11.41 Slicing of three-dimensional texture memory with poly-*

gons.
enough that we can move the viewer in real time and do interactive visualization.
There is, however, an aliasing problem with this technique that depends on the angle
the polygons make with the texture array.

### 11.12     IMAGE-BASED RENDERING

Recently there has been a great deal of interest in starting with a set of two-
dimensional images and either extracting three-dimensional information or form-
ing new images from them. This problem has appeared in many forms over the years.
Some of the most important examples in the past have included the following:
Using aerial photographs to obtain terrain information
Using a sequence of two-dimensional X-rays to obtain a three-dimensional
image in computerized axial tomography (CT)
Obtaining geometric models from cameras in robotics
q                       Warping one image into another (morphing)
Newer applications have focused on creating new images from a sequence of
stored images that have been carefully collected. For example, suppose that we take
a sequence of photographs of an object—a person, a building, or a CAD model—
and want to see the object from a different viewpoint. If we had a three-dimensional
model, we would simply move the viewer or the object and construct the new image.
But what can we do if we have only two-dimensional information? These problems
all ﬁt under the broad heading image-based rendering. Techniques involve elements
of computer graphics, image processing, and computer vision.
p1         p2          11.12.1 A Simple Example

*FIGURE 11.42 Two cameras     We can get some idea of the issues involved by considering the problem shown in*

imaging the same point.      Figure 11.42. On the left is a perspective camera located at a point p1, and on the
                                                                       11.12 Image-Based Rendering   601
(x, z)
z=d
x1 xp               xp x2

*FIGURE 11.43 Top view of the two cameras.*

right is a second camera located at p2. Consider a point q that is imaged by both
cameras. Assuming that we know everything about these cameras—their locations,
orientations, ﬁelds of view—can we determine q from the two images produced by
the cameras? Figure 11.43 has a top view of a simpliﬁed version of the problem with
the two cameras both located on the x-axis and with their image planes parallel at
z = d. Using our standard equations for projections, we have the two relationships
x1 − xp1       x1 − x
=          ,
x2 − xp2       x2 − x
=          .
These are two linear equations in the unknowns x and z that we can solve, yielding
z=              ,
x − xp
x p 1 x − x 1 xp
x=                        ,
x−     xp
where x = x2 − x1 and xp = xp2 − xp1.
Thus, we have determined q from the two images. This result does not depend on
where the cameras are located; moving them only makes the equations a little more
complex. Once we have q, we can obtain an image from any viewpoint.
On closer examination, we can see some practical problems. First, there are nu-
merical problems. Any small errors in the measurement of the camera position can
cause large errors in the estimate of q. Such numerical issues have plagued many
of the traditional applications, such as terrain measurement. One way around such
602        Chapter 11   Advanced Rendering
q              problems is to use more than two measurements and then to determine a best esti-
mate of the desired position.
There are other potentially serious problems. For example, how do we obtain
the points p1 and p2? Given the images from the two cameras, we need a method of
identifying corresponding points. This problem is one of the fundamental problems
in computer vision and one for which there are no perfect solutions. Note that if there
is occlusion, the same point may not even be present in the two images, as shown in
Figure 11.44.
Many early techniques were purely image based, using statistical methods to
ﬁnd corresponding points. Other techniques were interactive, requiring the user to
identify corresponding points. Recently, within the computer graphics community,
p1          p2         there have been some novel approaches to the problem. We will mention a few of the
more noteworthy ones. The details of each are referenced in the Suggested Readings

*FIGURE 11.44 Imaging with*

occlusion.
at the end of the chapter.
One way around the difﬁculties in pure image-based approaches has been to
use geometric models rather than points for the registration. For example, in a real
environment, we might know that there are many objects that are composed of right
parallelepipeds. This extra information can be used to derive very accurate position
information.
One use of image-based techniques has been to generate new images for a single
viewer from a sequence of images. Variations of this general problem have been used
in the movie industry, in providing new images in virtual reality applications, such as
Apple’s QuickTime VR, and for viewing objects remotely.
Others have looked at the mathematical relationship between two-dimensional
images and the light distribution in a three-dimensional environment. Each two-
dimensional image is a sample of a four-dimensional light ﬁeld. In a manner akin
to how three-dimensional images are constructed from two-dimensional projections
in computerized axial tomography, two-dimensional projections from multiple cam-
eras can be used to reconstruct the three-dimensional world. Two of these techniques
are known as the lumigraph and light-ﬁeld rendering. Because all the information
about a scene is contained in the light ﬁeld, there is growing interest in measuring the
light ﬁeld, something that, given the large amount of data involved, was not possible
until recently. One of the interesting applications of measuring the light ﬁeld is in re-
lighting a scene. In such applications, the lighting that was in the scene is removed
and it is relit using the light ﬁeld generated by sources at other locations.
This chapter has illustrated that there are many approaches to rendering. The physical
basis for rendering with global illumination is contained in the rendering equation.
Unfortunately, it contains too many variables to be solvable, even by numerical meth-
ods, for the general case. Radiosity and ray tracing can handle some global effects,
although they make opposite assumptions on the types of surfaces that are in the
scene. As GPUs become more and more powerful, they can handle much of the com-
                                                                                          Suggested Readings   603
putation required by alternate rendering methods. Consequently, we may see less of a
distinction than presently exists between pipeline approaches and all other rendering
methods for real-time applications.
Although the speed and cost of computers make it possible to ray-trace scenes
and do radiosity calculations that were not possible a few years ago, these techniques
alone are not the solution to all problems in computer graphics. If we look at what has
been happening in the ﬁlm, television, and game industries, it appears that we can
create photorealistic imagery using a wealth of modeling methods and a variety of
commercial and shareware renderers. However, there is a growing acceptance of the
view that photorealism is not the ultimate goal. Hence, we see increasing interest in
such areas as combining realistic rendering and computer modeling with traditional
hand animation. The wide range of image-based methods ﬁts in well with many of
these applications.
Most of what we see on the consumer end of graphics is driven by computer
games. It appears that no matter how fast and inexpensive processors are, the de-
mands of consumers for more sophisticated computer games will continue to force
developers to come up with faster processors with new capabilities. As high-deﬁnition
television (HDTV) becomes more of a standard, we are seeing a greater variety of
high-resolution displays available at reasonable prices.
On the scientiﬁc side, the replacement of traditional supercomputers by clusters
of commodity computers will continue to have a large effect on scientiﬁc visualiza-
tion. The enormous data sets generated by applications run on these clusters will
drive application development on the graphics side. Not only will these applications
need imagery generated for high-resolution displays, but the difﬁculties of storing
these data sets will drive efforts to visualize these data as fast as they can be generated.
What is less clear is the future of computer architectures and how it will affect
computer graphics. Commodity computers, such as the Apple MacPro, have mul-
tiple buses that support multiple graphics cards and multiple processors, each with
multiple cores. Game boxes are starting to use alternate components, such the IBM
cell processor that drives the Sony PlayStation 3. How we can best use these compo-
nents is an open issue. What can be said with a great degree of certainty is that there
is still much to be done in computer graphics.
Ray tracing was introduced by Appel [App68] and popularized by Whitted [Whi80].
Many of the early papers on ray tracing are included in a volume by Joy and colleagues
[Joy88]. The book by Glassner [Gla89] is particularly helpful if you plan to write your
own ray tracer. Many of the tests for intersections are described in Haines’s chapter
in [Gla89] and in the Graphics Gems series [Gra90, Gra91, Gra92, Gra94, Gra95].
See also [Suf07] and [Shi03]. There are many excellent ray tracers available (see, for
example, [War94]).
The Rendering Equation is due to Kajiya [Kaj86]. Radiosity is based on a method
ﬁrst used in heat transfer [Sie81]. It was ﬁrst applied in computer graphics by Goral
604   Chapter 11   Advanced Rendering
and colleagues [Gor84]. Since its introduction, researchers have done a great deal of
work on increasing its efﬁciency [Coh85, Coh88, Coh93] and incorporating specular
terms [Sil89]. The method of using point light sources to ﬁnd form factors appeared
in [Kel97]. Photon mapping has been popularized by Jensen [Jen01].
The RenderMan Interface is described in [Ups89]. The Reyes rendering architec-
ture was ﬁrst presented in [Coo87]. Maya [Wat02] allows multiple types of renderers.
The sorting classiﬁcation of parallel rendering was suggested by Molnar and
colleagues [Mol94]. The advantages of sort-middle architectures were used in SGI’s
high-end workstations such as the Inﬁnite Reality Graphics [Mon97]. The sort-last
architecture was developed as part of the Pixel Flow architecture [Mol92]. Binary-
swap compositing was suggested by [Ma94]. Software for sort-last renderings using
clusters of commodity computers is discussed in [Hum01]. Power walls are described
in [Her00, Che00].
The marching-squares method is a special case of the marching-cubes method
popularized by Lorenson and Kline [Lor87]. The method has been rediscovered many
times. The ambiguity problem is discussed in [Van94]. Early attempts to visualize vol-
umes were reported by Herman [Her79] and by Fuchs [Fuc77]. Ray-tracing volumes
was introduced by Levoy [Lev88]. Splatting is due to Westover [Wes90]. Particles can
also be used for visualization [Wit94a, Cro97]. Many other visualization strategies
are discussed in [Gal95, Nie97]. One approach to building visualization applications
is to use an object-oriented toolkit [Schr06].
Image-based rendering by warping frames was part of Microsoft’s Talisman
hardware [Tor96]. Apple’s Quicktime VR [Che95] was based on creating new views
from a single viewpoint from a 360-degree panorama. Debevec and colleagues
[Deb96] showed that by using a model-based approach, new images from multi-
ple viewpoints could be constructed from a small number of images. Other warping
methods were proposed in [Sei96]. Work on the lumigraph [Gor96] and light ﬁelds
[Lev96] established the mathematical foundations for image-based techniques. Ap-
plications to image-based lighting are in [Rei05].

### 11.1   Devise a test for whether a point is inside a convex polygon based on the idea

that the polygon can be described by a set of intersecting lines in a single
plane.

### 11.2   Extend your algorithm from Exercise 11.1 to polyhedra that are formed by

the intersection of planes.

### 11.3   Derive an implicit equation for a torus whose center is at the origin. You can

derive the equation by noting that a plane that cuts through the torus reveals
two circles of the same radius.

### 11.4   Using the result from Exercise 11.3, show that you can ray-trace a torus using

the quadratic equation to ﬁnd the required intersections.
                                                                                           Exercises   605

### 11.5  Consider a ray passing through a sphere. Find the point on this ray closest to

the center of the sphere. Hint: Consider a line from the center of the sphere
that is normal to the ray. How can you use this result for intersection testing?

### 11.6 We can get increased accuracy from a ray tracer by using more rays. Suppose

for each pixel, we cast a ray through the center of the pixel and through its
four corners. How much more work does this approach require as compared
to the one-ray-per-pixel ray tracer?

### 11.7 In the sort-middle approach to parallel rendering, what type of information

must be conveyed between the geometry processors and raster processors?

### 11.8 What changes would you have to make to our parallel rendering strategies if

we were to allow translucent objects?

### 11.9 One way to classify parallel computers is by whether their memory is shared

among the processors or distributed so that each processer has its own mem-
ory that is not accessible to other processors. How does this distinction affect
the various rendering strategies that we have discussed?

### 11.10 Generalize the simple example of imaging the same point from two viewers to

the general case in which the two viewers can be located at arbitrary locations
in three dimensions.

### 11.11 Build a simple ray tracer that can handle only planes and spheres. There are

many interesting data sets available on the Internet with which to test your
code.

### 11.12 Suppose that you have an algebraic function in which the highest term is

x i y j z k . What is the degree of the polynomial that we need to solve for the

```cpp
intersection of a ray with the surface deﬁned by this function?
```


### 11.13 Consider again an algebraic function in which the highest term is x i y j z k . If

i = j = k, how many terms are in the polynomial that is created when we

```cpp
intersect the surface with a parametric ray?
```


### 11.14 For one or more OpenGL implementations, ﬁnd how many triangles per

second can be rendered. Determine what part of the rendering time is spent in
hidden-surface removal, shading, texture mapping, and rasterization. If you
are using a commodity graphics card, how does the performance that you
measure compare with the speciﬁcations for the card?

### 11.15 Determine the pixel performance of your graphics card. Determine how

many pixels per second can be read or written. Do the reading and writing of
pixels occur at different rates? Is there a difference in writing texture maps?

### 11.16 Build a sort-last renderer using OpenGL for the rendering on each processor.

You can do performance tests using applications that generate triangles or
triangular meshes.

### 11.17 Explain why, as we add more processors, the performance of sort-ﬁrst ren-

dering will eventually get worse.

### 11.18 Consider a two-dimensional implicit function of the form f (x, y) = c. For

each value of c, the resulting curves (if any) are contours of the function.
606   Chapter 11   Advanced Rendering
We can display such contours using marching cubes, a three-dimensional
version of marching squares. Consider a two-dimensional rectangular cell
and a contour value. We color each corner of the cell black or white depending
whether the value at the corner is greater or less than the contour value.
How may cell colorings are there? Is there an ambiguity in how contours pass
through the cells?

### 11.19 Write an OpenGL program to carry out marching squares.

                                                          AP P E NDI X                 A
This appendix contains the source code for many of the example programs that
we developed in the text. These programs and others that are referred to in the
text are also available at www.cs.unm.edu/~angel. Present there are the include ﬁles
Angel.h, matrix.h, and vector.h, as well as a Makefile that should take care
of the differences between architectures. Also there are additional sample programs
and some implementation notes. Other examples, including those in the OpenGL
Programmer’s Guide and GLUT, can be found starting from the OpenGL Web site
(www.opengl.org).
The drivers for particular graphics cards implement OpenGL using a combina-
tion of hardware and software. Thus, as long as the drivers are properly installed,
using OpenGL is identical on all systems. Graphics cards differ in their performance
and what extras they provide in the form of extensions. They also differ in how re-
cent a version of OpenGL they support. You can ﬁnd this information either at the
manufacturer’s Web site or under display properties on your computer.
OpenGL is standard on almost all workstations. For systems that use Microsoft
Windows, the dynamic library for GL is in the system folder. The corresponding .lib
ﬁles and include ﬁles are provided with language compilers such as Visual C++. The
corresponding GLUT ﬁles (glut32.dll, glut32.lib, and glut.h) are available over the
Web (see www.opengl.org). A more up-to-date choice is freeglut, which is also
available on the Web. One of the advantages of freeglut is that you can check
whether your code is is compatible with a particular version of OpenGL. We have
included these functions in our examples but have commented them in the ﬁrst
couple of examples.
Most users will want to use the GLEW library so they won’t have to deal with
versions and extensions. At the time of this writing, OpenGL 3.0 and above are not
part of Mac OS X. We expect to see such support soon as OpenGL is core to all
graphics on Macs. You can run these examples on the Mac with only minor changes.
Examples are on our Web site. Mac users should not need to use GLEW.
Most Linux distributions provide the Mesa distribution of OpenGL
(www.mesa3D.org) and include GLUT. Although Mesa is a pure software implemen-
tation of OpenGL (and includes the source for those interested in implementation
608   Appendix A   Sample Programs
issues), increasingly manufacturers of commodity graphics cards are providing Linux
drivers that allow Linux users to take advantage of the cards’ capabilities.
The programs that follow use the GLUT library for interfacing with the window
system. The naming of the functions follows the OpenGL Programming Guide and
the GLUT Users Guide. These programs share much of the same code. You should
ﬁnd functions, such as the reshape callback, the initialization function, and the main
function, almost identical across the programs. Consequently, only the ﬁrst instance
of each function contains extensive comments.
In all these programs, illustration of graphical principles, other than efﬁciency,
was the most important design criterion. You should ﬁnd numerous ways to extend
these programs and to make them run more efﬁciently. In some instances, the same
visual results can be generated in a completely different manner, using OpenGL
capabilities other than the ones we used in the sample program.
The following programs include:
1. InitShader function
2. A program that generates 5000 points on the Sierpinski gasket (Chapter 2)
3. A version of the gasket program using recursion (Chapter 2)
4. Rotating-cube program sending rotation angles to GPU (Chapter 3)
5. Cube viewing with perspective (Chapter 4)
6. Rotating shaded cube (Chapter 5)
7. Shaded recursively generated sphere with per-fragment lighting (Chapter 5)
8. Rotating cube with texture (Chapter 7)
9. Tree-based ﬁgure program (Chapter 8)
10. Teapot renderer (Chapter 10)
A.1     SHADER INITIALIZATION FUNCTION
A.1.1 Application Code

```cpp
#include "Angel.h" // Book header file
```

namespace Angel {

```cpp
// Create a NULL-terminated string by reading the provided file
static char*
```

readShaderSource(const char* shaderFile)

```cpp
{
```

FILE* fp = fopen(shaderFile, "r");
if ( fp == NULL ) { return NULL; }
fseek(fp, 0L, SEEK_END);
long size = ftell(fp);
                                                          A.1 Shader Initialization Function   609
fseek(fp, 0L, SEEK_SET);

```cpp
char* buf = new char[size + 1];
```

fread(buf, 1, size, fp);
buf[size] = ’ ’;
fclose(fp);
return buf;

```cpp
}
// Create a GLSL program object from vertex and fragment shader files
```

InitShader(const char* vShaderFile, const char* fShaderFile)

```cpp
{
struct Shader {
const char* filename;
```

GLenum       type;
GLchar*      source;

```cpp
} shaders[2] = {
{ vShaderFile, GL_VERTEX_SHADER, NULL },
{ fShaderFile, GL_FRAGMENT_SHADER, NULL }
};
```

GLuint program = glCreateProgram( void );
for ( int i = 0; i < 2; ++i ) {
Shader& s = shaders[i];
s.source = readShaderSource( s.filename );
if ( shaders[i].source == NULL ) {
std::cerr << "Failed to read " << s.filename << std::endl;
exit( EXIT_FAILURE );

```cpp
}
```

GLuint shader = glCreateShader( s.type );
glShaderSource( shader, 1, (const GLchar**) &s.source, NULL );
glCompileShader( shader );
GLint compiled;
glGetShaderiv( shader, GL_COMPILE_STATUS, &compiled );
if ( !compiled ) {
std::cerr << s.filename << " failed to compile:" << std::endl;
GLint logSize;
glGetShaderiv( shader, GL_INFO_LOG_LENGTH, &logSize );

```cpp
char* logMsg = new char[logSize];
```

glGetShaderInfoLog( shader, logSize, NULL, logMsg );
std::cerr << logMsg << std::endl;
delete [] logMsg;
610   Appendix A   Sample Programs
exit( EXIT_FAILURE );

```cpp
}
```

delete [] s.source;
glAttachShader( program, shader );

```cpp
}
// link and error check
```

glLinkProgram(program);
GLint linked;
glGetProgramiv( program, GL_LINK_STATUS, &linked );
if ( !linked ) {
std::cerr << "Shader program failed to link" << std::endl;
GLint logSize;
glGetProgramiv( program, GL_INFO_LOG_LENGTH, &logSize);

```cpp
char* logMsg = new char[logSize];
```

glGetProgramInfoLog( program, logSize, NULL, logMsg );
std::cerr << logMsg << std::endl;
delete [] logMsg;
exit( EXIT_FAILURE );

```cpp
}
// use program object
```

glUseProgram(program);
return program;

```cpp
}
}   // Close namespace Angel block
```

A.2       SIERPINSKI GASKET PROGRAM
A.2.1 Application Code

```cpp
// Two-Dimensional Sierpinski Gasket
// Generated using randomly selected vertices and bisection
#include "Angel.h"
const int NumPoints = 5000;
void
```

init( void )

```cpp
{
```

vec2 points[NumPoints];
                                                              A.2 Sierpinski Gasket Program   611

```cpp
// Specify the vertices for a triangle
```

vec2 vertices[3] = {
vec2( -1.0, -1.0 ), vec2( 0.0, 1.0 ), vec2( 1.0, -1.0 )

```cpp
};
// Select an arbitrary initial point inside of the triangle
```

points[0] = vec2( 0.25, 0.50 );

```cpp
// compute and store N-1 new points
```

for ( int i = 1; i < NumPoints; ++i ) {

```cpp
int j = rand( void ) % 3;   // pick a vertex at random
// Compute the point halfway between the selected vertex
//   and the previous point
```

points[i] = ( points[i - 1] + vertices[j] ) / 2.0;

```cpp
}
// Load shaders and use the resulting shader program
```

GLuint program = InitShader( "vshader21.glsl", "fshader21.glsl" );
glUseProgram( program );

```cpp
// Create a vertex array object
```

GLuint vao;
glGenVertexArrays( 1, &vao );
glBindVertexArray( vao );

```cpp
// Create and initialize a buffer object
```

GLuint buffer;
glGenBuffers( 1, &buffer );
glBindBuffer( GL_ARRAY_BUFFER, buffer );
glBufferData( GL_ARRAY_BUFFER, sizeof(points), points, GL_STATIC_DRAW );

```cpp
// Initialize the vertex position attribute from the vertex shader
```

GLuint loc = glGetAttribLocation( program, "vPosition" );
glEnableVertexAttribArray( loc );
glVertexAttribPointer( loc, 2, GL_FLOAT, GL_FALSE, 0,
BUFFER_OFFSET(0) );
glClearColor( 1.0, 1.0, 1.0, 1.0 ); // white background

```cpp
}
//----------------------------------------------------------------------
void
```

display( void )

```cpp
{
```

glClear( GL_COLOR_BUFFER_BIT );     // clear the window
glDrawArrays( GL_POINTS, 0, NumPoints );    // draw the points
612   Appendix A   Sample Programs
glFlush( void );

```cpp
}
//----------------------------------------------------------------------
int
```

main( int argc, char **argv )

```cpp
{
```

glutInit( &argc, argv );
glutInitDisplayMode( GLUT_RGBA );
glutInitWindowSize( 512, 512 );

```cpp
// If you are using freeglut, the next two lines will check if
// the code is truly 3.2. Otherwise, comment them out
```

glutInitContextVersion( 3, 2 );
glutInitContextProfile( GLUT_CORE_PROFILE );
glutCreateWindow( "Sierpinski Gasket" );
glewInit( void );
init( void );
glutDisplayFunc( display );
glutMainLoop( void );
return 0;

```cpp
}
```

A.2.2 Vertex Shader

```cpp
#version 150   //GLSL Version 1.5
```

in vec4 vPosition;

```cpp
void main()
{
```

gl_Position = vPosition;

```cpp
}
```

A.2.3 Fragment Shader

```cpp
#version 150
```

out vec4   fColor;

```cpp
void main()
{
```

fColor = vec4( 1.0, 0.0, 0.0, 1.0 );

```cpp
}
```

                                             A.3 Recursive Generation of Sierpinski Gasket   613
A.3   RECURSIVE GENERATION OF SIERPINSKI GASKET
A.3.1 Application Code

```cpp
// Recursive subdivision of triangle to form Sierpinski gasket
//   Number of recursive steps given on command line
#include "Angel.h"
```

using namespace Angel;

```cpp
const int NumTimesToSubdivide = 5;
const int NumTriangles = 729; // 3^5 triangles generated
const int NumVertices = 3 * NumTriangles;
```

vec2 points[NumVertices];

```cpp
int Index = 0;
//----------------------------------------------------------------------
void
```

triangle( const vec2& a, const vec2& b, const vec2& c )

```cpp
{
```

points[Index++] = a;
points[Index++] = b;
points[Index++] = c;

```cpp
}
//----------------------------------------------------------------------
void
```

divide_triangle( const vec2& a, const vec2& b, const vec2& c, int count )

```cpp
{
```

if ( count > 0 ) {

```cpp
//compute midpoints of sides
```

vec2 v0 = ( a + b ) / 2.0;
vec2 v1 = ( a + c ) / 2.0;
vec2 v2 = ( b + c ) / 2.0;

```cpp
//subdivide all but middle triangle
```

divide_triangle( a, v0, v1, count - 1 );
divide_triangle( c, v1, v2, count - 1 );
divide_triangle( b, v2, v0, count - 1 );

```cpp
}
```

else {
triangle( a, b, c );     // draw triangle at end of recursion

```cpp
}
}
```

614   Appendix A   Sample Programs

```cpp
//----------------------------------------------------------------------
void
```

init( void )

```cpp
{
```

vec2 vertices[3] = {
vec2( -1.0, -1.0 ), vec2( 0.0, 1.0 ), vec2( 1.0, -1.0 )

```cpp
};
// Subdivide the original triangle
```

divide_triangle( vertices[0], vertices[1], vertices[2],
NumTimesToSubdivide );

```cpp
// Load shaders and use the resulting shader program
```

GLuint program = InitShader( "vshader22.glsl", "fshader22.glsl" );
glUseProgram( program );

```cpp
// Create a vertex array object
```

GLuint vao;
glGenVertexArrays( 1, &vao );
glBindVertexArray( vao );

```cpp
// Create and initialize a buffer object
```

GLuint buffer;
glGenBuffers( 1, &buffer );
glBindBuffer( GL_ARRAY_BUFFER, buffer );
glBufferData( GL_ARRAY_BUFFER, sizeof(points), points,
GL_STATIC_DRAW );

```cpp
// Initialize the vertex position attribute from the vertex shader
```

GLuint loc = glGetAttribLocation( program, "vPosition" );
glEnableVertexAttribArray( loc );
glVertexAttribPointer( loc, 2, GL_FLOAT, GL_FALSE, 0,
BUFFER_OFFSET(0) );
glClearColor( 1.0, 1.0, 1.0, 1.0 ); // white background

```cpp
}
//----------------------------------------------------------------------
void
```

display( void )

```cpp
{
```

glClear( GL_COLOR_BUFFER_BIT );
glDrawArrays( GL_TRIANGLES, 0, NumTriangles );
glFlush( void );

```cpp
}
//----------------------------------------------------------------------
```

                                                   A.4 Rotating Cube with Rotation in Shader   615

```cpp
int
```

main( int argc, char **argv )

```cpp
{
```

glutInit( &argc, argv );
glutInitDisplayMode( GLUT_RGBA );
glutInitWindowSize( 512, 512 );
glutInitContextVersion( 3, 2 );
glutInitContextProfile( GLUT_CORE_PROFILE );
glutCreateWindow( "Sierpinski Gasket" );
glewInit( void );
init( void );
glutDisplayFunc( display );
glutMainLoop( void );
return 0;

```cpp
}
```

A.3.2 Vertex Shader

```cpp
#version 150
```

in vec4 vPosition;

```cpp
void main()
{
```

gl_Position = vPosition;

```cpp
}
```

A.3.3 Fragment Shader

```cpp
#version 150
```

out vec4 fColor;

```cpp
void main()
{
```

fColor = vec4( 1.0, 0.0, 0.0, 1.0 );

```cpp
}
```

A.4     ROTATING CUBE WITH ROTATION IN SHADER
A.4.1 Application Code

```cpp
//
// Display a rotating color cube
```

616   Appendix A   Sample Programs

```cpp
// In this version, idle function increments angles
// which are sent to vertex shader where rotation takes place
#include "Angel.h"
```

typedef Angel::vec4    color4;
typedef Angel::vec4    point4;

```cpp
const int NumVertices = 36; //(6 faces)(2 triangles/face)
```

(3 vertices/triangle)
point4 points[NumVertices];
color4 colors[NumVertices];

```cpp
// Vertices of a unit cube centered at origin, sides aligned with axes
```

point4 vertices[8] = {
point4( -0.5, -0.5, 0.5, 1.0 ),
point4( -0.5, 0.5, 0.5, 1.0 ),
point4( 0.5, 0.5, 0.5, 1.0 ),
point4( 0.5, -0.5, 0.5, 1.0 ),
point4( -0.5, -0.5, -0.5, 1.0 ),
point4( -0.5, 0.5, -0.5, 1.0 ),
point4( 0.5, 0.5, -0.5, 1.0 ),
point4( 0.5, -0.5, -0.5, 1.0 )

```cpp
};
// RGBA colors
```

color4 vertex_colors[8] = {
color4( 0.0, 0.0, 0.0, 1.0 ),    // black
color4( 1.0, 0.0, 0.0, 1.0 ),    // red
color4( 1.0, 1.0, 0.0, 1.0 ),    // yellow
color4( 0.0, 1.0, 0.0, 1.0 ),    // green
color4( 0.0, 0.0, 1.0, 1.0 ),    // blue
color4( 1.0, 0.0, 1.0, 1.0 ),    // magenta
color4( 1.0, 1.0, 1.0, 1.0 ),    // white
color4( 0.0, 1.0, 1.0, 1.0 )     // cyan

```cpp
};
// Array of rotation angles (in degrees) for each coordinate axis
```

enum { Xaxis = 0, Yaxis = 1, Zaxis = 2, NumAxes = 3 };

```cpp
int      Axis = Xaxis;
```

GLfloat Theta[NumAxes] = { 0.0, 0.0, 0.0 };
GLuint   theta;   // The location of the "theta" shader uniform variable

```cpp
//----------------------------------------------------------------------
// quad generates two triangles for each face and assigns colors
//    to the vertices
```

                                                 A.4 Rotating Cube with Rotation in Shader   617

```cpp
int Index = 0;
void
```

quad( int a, int b, int c, int d )

```cpp
{
```

colors[Index] = vertex_colors[a]; points[Index] = vertices[a]; Index++;
colors[Index] = vertex_colors[b]; points[Index] = vertices[b]; Index++;
colors[Index] = vertex_colors[c]; points[Index] = vertices[c]; Index++;
colors[Index] = vertex_colors[a]; points[Index] = vertices[a]; Index++;
colors[Index] = vertex_colors[c]; points[Index] = vertices[c]; Index++;
colors[Index] = vertex_colors[d]; points[Index] = vertices[d]; Index++;

```cpp
}
//----------------------------------------------------------------------
// generate 12 triangles: 36 vertices and 36 colors
void
```

colorcube( void )

```cpp
{
```

quad( 1, 0, 3, 2 );
quad( 2, 3, 7, 6 );
quad( 3, 0, 4, 7 );
quad( 6, 5, 1, 2 );
quad( 4, 5, 6, 7 );
quad( 5, 4, 0, 1 );

```cpp
}
//----------------------------------------------------------------------
// OpenGL initialization
void
```

init( void )

```cpp
{
```

colorcube( void );

```cpp
// Load shaders and use the resulting shader program
```

GLuint program = InitShader( "vshader36.glsl", "fshader36.glsl" );
glUseProgram( program );

```cpp
// Create a vertex array object
```

GLuint vao;
glGenVertexArrays( 1, &vao );
glBindVertexArray( vao );

```cpp
// Create and initialize a buffer object
```

GLuint buffer;
glGenBuffers( 1, &buffer );
glBindBuffer( GL_ARRAY_BUFFER, buffer );
glBufferData( GL_ARRAY_BUFFER, sizeof(points) + sizeof(colors),
NULL, GL_STATIC_DRAW );
618   Appendix A   Sample Programs
glBufferSubData( GL_ARRAY_BUFFER, 0, sizeof(points), points );
glBufferSubData( GL_ARRAY_BUFFER, sizeof(points),
sizeof(colors), colors );

```cpp
// set up vertex arrays
```

GLuint vPosition = glGetAttribLocation( program, "vPosition" );
glEnableVertexAttribArray( vPosition );
glVertexAttribPointer( vPosition, 4, GL_FLOAT, GL_FALSE, 0,
BUFFER_OFFSET(0) );
GLuint vColor = glGetAttribLocation( program, "vColor" );
glEnableVertexAttribArray( vColor );
glVertexAttribPointer( vColor, 4, GL_FLOAT, GL_FALSE, 0,
BUFFER_OFFSET(sizeof(points)) );
theta = glGetUniformLocation( program, "theta" );
glEnable( GL_DEPTH_TEST );
glClearColor( 1.0, 1.0, 1.0, 1.0 );

```cpp
}
//----------------------------------------------------------------------
void
```

display( void )

```cpp
{
```

glClear( GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT );
glUniform3fv( theta, 1, Theta );
glDrawArrays( GL_TRIANGLES, 0, NumVertices );
glutSwapBuffers( void );

```cpp
}
//----------------------------------------------------------------------
void
```

keyboard( unsigned char key, int x, int y )

```cpp
{
```

switch( key ) {
case 033: // Escape Key
case ’q’: case ’Q’:
exit( EXIT_SUCCESS );
break;

```cpp
}
}
//----------------------------------------------------------------------
```

                                                 A.4 Rotating Cube with Rotation in Shader   619

```cpp
void
```

mouse( int button, int state, int x, int y )

```cpp
{
```

if ( state == GLUT_DOWN ) {
switch( button ) {
case GLUT_LEFT_BUTTON:   Axis = Xaxis;   break;
case GLUT_MIDDLE_BUTTON: Axis = Yaxis;   break;
case GLUT_RIGHT_BUTTON:  Axis = Zaxis;   break;

```cpp
}
}
}
//----------------------------------------------------------------------
void
```

idle( void )

```cpp
{
```

Theta[Axis] += 0.01;
if ( Theta[Axis] > 360.0 ) {
Theta[Axis] -= 360.0;

```cpp
}
```

glutPostRedisplay( void );

```cpp
}
//----------------------------------------------------------------------
int
```

main( int argc, char **argv )

```cpp
{
```

glutInit( &argc, argv );
glutInitDisplayMode( GLUT_RGBA | GLUT_DOUBLE | GLUT_DEPTH );
glutInitWindowSize( 512, 512 );
glutCreateWindow( "Color Cube" );
glewInit( void );
init( void );
glutDisplayFunc( display );
glutKeyboardFunc( keyboard );
glutMouseFunc( mouse );
glutIdleFunc( idle );
glutMainLoop( void );
return 0;

```cpp
}
```

620   Appendix A   Sample Programs
A.4.2 Vertex Shader

```cpp
#version 150
```

in vec4 vPosition;
in vec4 vColor;
out vec4 color;
uniform vec3 theta;

```cpp
void main()
{
// Compute the sines and cosines of theta for each of
//   the three axes in one computation.
```

vec3 angles = radians( theta );
vec3 c = cos( angles );
vec3 s = sin( angles );

```cpp
// Remember: these matrices are column-major
```

mat4 rx = mat4( 1.0, 0.0, 0.0, 0.0,
0.0, c.x, -s.x, 0.0,
0.0, s.x, c.x, 0.0,
0.0, 0.0, 0.0, 1.0 );
mat4 ry = mat4(    c.y, 0.0, s.y, 0.0,
0.0, 1.0, 0.0, 0.0,
-s.y, 0.0, c.y, 0.0,
0.0, 0.0, 0.0, 1.0 );
mat4 rz = mat4( c.z, -s.z, 0.0, 0.0,
s.z, c.z, 0.0, 0.0,
0.0, 0.0, 1.0, 0.0,
0.0, 0.0, 0.0, 1.0 );
color = vColor;
gl_Position = rx * ry * rz * vPosition;

```cpp
}
```

A.4.3 Fragment Shader

```cpp
#version 150
```

in vec4 color;
out vec4 fColor;

```cpp
void main()
{
```

fColor = color;

```cpp
}
```

                                                                   A.5 Perspective Projection   621
A.5      PERSPECTIVE PROJECTION
A.5.1 Application Code

```cpp
// Perspective view of a color cube using LookAt( void ) and Frustum( void )
#include "Angel.h"
```

typedef Angel::vec4     color4;
typedef Angel::vec4     point4;

```cpp
const int NumVertices = 36; //(6 faces)(2 triangles/face)(3 vertices/triangle)
```

point4 points[NumVertices];
color4 colors[NumVertices];

```cpp
// Vertices of a unit cube centered at origin, sides aligned with axes
```

point4 vertices[8] ={
point4( -0.5, -0.5, 0.5, 1.0 ),
point4( -0.5, 0.5, 0.5, 1.0 ),
point4( 0.5, 0.5, 0.5, 1.0 ),
point4( 0.5, -0.5, 0.5, 1.0 ),
point4( -0.5, -0.5, -0.5, 1.0 ),
point4( -0.5, 0.5, -0.5, 1.0 ),
point4( 0.5, 0.5, -0.5, 1.0 ),
point4( 0.5, -0.5, -0.5, 1.0 )

```cpp
};
// RGBA colors
```

color4 vertex_colors[8] ={
color4( 0.0, 0.0, 0.0, 1.0 ),     // black
color4( 1.0, 0.0, 0.0, 1.0 ),     // red
color4( 1.0, 1.0, 0.0, 1.0 ),     // yellow
color4( 0.0, 1.0, 0.0, 1.0 ),     // green
color4( 0.0, 0.0, 1.0, 1.0 ),     // blue
color4( 1.0, 0.0, 1.0, 1.0 ),     // magenta
color4( 1.0, 1.0, 1.0, 1.0 ),     // white
color4( 0.0, 1.0, 1.0, 1.0 )      // cyan

```cpp
};
// Viewing transformation parameters
```

GLfloat radius = 1.0;
GLfloat theta = 0.0;
GLfloat phi = 0.0;

```cpp
const GLfloat   dr = 5.0 * DegreesToRadians;
```

GLuint    model_view;   // model-view matrix uniform shader variable location
622   Appendix A   Sample Programs

```cpp
// Projection transformation parameters
```

GLfloat    left = -1.0, right = 1.0;
GLfloat    bottom = -1.0, top = 1.0;
GLfloat    zNear = 0.5, zFar = 3.0;
GLuint    projection; // projection matrix uniform shader variable location

```cpp
//----------------------------------------------------------------------
// quad generates two triangles for each face and assigns colors
//    to the vertices
int Index = 0;
void
```

quad( int a, int b, int c, int d )

```cpp
{
```

colors[Index] = vertex_colors[a]; points[Index] = vertices[a];
Index++;
colors[Index] = vertex_colors[b]; points[Index] = vertices[b];
Index++;
colors[Index] = vertex_colors[c]; points[Index] = vertices[c];
Index++;
colors[Index] = vertex_colors[a]; points[Index] = vertices[a];
Index++;
colors[Index] = vertex_colors[c]; points[Index] = vertices[c];
Index++;
colors[Index] = vertex_colors[d]; points[Index] = vertices[d];
Index++;

```cpp
}
//----------------------------------------------------------------------
// generate 12 triangles: 36 vertices and 36 colors
void
```

colorcube( void )

```cpp
{
```

quad( 1, 0, 3, 2 );
quad( 2, 3, 7, 6 );
quad( 3, 0, 4, 7 );
quad( 6, 5, 1, 2 );
quad( 4, 5, 6, 7 );
quad( 5, 4, 0, 1 );

```cpp
}
//----------------------------------------------------------------------
```

                                                                  A.5 Perspective Projection   623

```cpp
// OpenGL initialization
void
```

init( void )

```cpp
{
```

colorcube( void );

```cpp
// Load shaders and use the resulting shader program
```

GLuint program = InitShader( "vshader42.glsl", "fshader42.glsl" );
glUseProgram( program );

```cpp
// Create a vertex array object
```

GLuint vao;
glGenVertexArrays( 1, &vao );
glBindVertexArray( vao );

```cpp
// Create and initialize a buffer object
```

GLuint buffer;
glGenBuffers( 1, &buffer );
glBindBuffer( GL_ARRAY_BUFFER, buffer );
glBufferData( GL_ARRAY_BUFFER, sizeof(points) + sizeof(colors),
NULL, GL_STATIC_DRAW );
glBufferSubData( GL_ARRAY_BUFFER, 0, sizeof(points), points );
glBufferSubData( GL_ARRAY_BUFFER, sizeof(points), sizeof(colors), colors );

```cpp
// set up vertex arrays
```

GLuint vPosition = glGetAttribLocation( program, "vPosition" );
glEnableVertexAttribArray( vPosition );
glVertexAttribPointer( vPosition, 4, GL_FLOAT, GL_FALSE, 0,
BUFFER_OFFSET(0) );
GLuint vColor = glGetAttribLocation( program, "vColor" );
glEnableVertexAttribArray( vColor );
glVertexAttribPointer( vColor, 4, GL_FLOAT, GL_FALSE, 0,
BUFFER_OFFSET(sizeof(points)) );
model_view = glGetUniformLocation( program, "model_view" );
projection = glGetUniformLocation( program, "projection" );
glEnable( GL_DEPTH_TEST );
glClearColor( 1.0, 1.0, 1.0, 1.0 );

```cpp
}
//----------------------------------------------------------------------
void
```

display( void )

```cpp
{
```

glClear( GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT );
624   Appendix A   Sample Programs
point4 eye( radius*sin(theta)*cos(phi),
radius*sin(theta)*sin(phi),
radius*cos(theta),
1.0 );
point4 at( 0.0, 0.0, 0.0, 1.0 );
vec4    up( 0.0, 1.0, 0.0, 0.0 );
mat4 mv = LookAt( eye, at, up );
glUniformMatrix4fv( model_view, 1, GL_TRUE, mv );
mat4 p = Frustum( left, right, bottom, top, zNear, zFar );
glUniformMatrix4fv( projection, 1, GL_TRUE, p );
glDrawArrays( GL_TRIANGLES, 0, NumVertices );
glutSwapBuffers( void );

```cpp
}
//----------------------------------------------------------------------
void
```

keyboard( unsigned char key, int x, int y )

```cpp
{
```

switch( key ) {
case 033: // Escape Key
case ’q’: case ’Q’:
exit( EXIT_SUCCESS );
break;
case ’x’: left *= 1.1; right *= 1.1; break;
case ’X’: left *= 0.9; right *= 0.9; break;
case ’y’: bottom *= 1.1; top *= 1.1; break;
case ’Y’: bottom *= 0.9; top *= 0.9; break;
case ’z’: zNear *= 1.1; zFar *= 1.1; break;
case ’Z’: zNear *= 0.9; zFar *= 0.9; break;
case ’r’: radius *= 2.0; break;
case ’R’: radius *= 0.5; break;
case ’o’: theta += dr; break;
case ’O’: theta -= dr; break;
case ’p’: phi += dr; break;
case ’P’: phi -= dr; break;
case ’ ’: // reset values to their defaults
left = -1.0;
right = 1.0;
bottom = -1.0;
top = 1.0;
zNear = 0.5;
zFar = 3.0;
                                                                   A.5 Perspective Projection   625
radius = 1.0;
theta = 0.0;
phi    = 0.0;
break;

```cpp
}
```

glutPostRedisplay( void );

```cpp
}
//----------------------------------------------------------------------
void
```

reshape( int width, int height )

```cpp
{
```

glViewport( 0, 0, width, height );

```cpp
}
//----------------------------------------------------------------------
int
```

main( int argc, char **argv )

```cpp
{
```

glutInit( &argc, argv );
glutInitDisplayMode( GLUT_RGBA | GLUT_DOUBLE | GLUT_DEPTH );
glutInitWindowSize( 512, 512 );
glutCreateWindow( "Color Cube" );
glewInit( void );
init( void );
glutDisplayFunc( display );
glutKeyboardFunc( keyboard );
glutReshapeFunc( reshape );
glutMainLoop( void );
return 0;

```cpp
}
```

A.5.2 Vertex Shader
in vec4 vPosition;
in vec4 vColor;
out vec4 color;
uniform mat4 model_view;
uniform mat4 projection;
626   Appendix A   Sample Programs

```cpp
void main()
{
```

gl_Position = projection*model_view*vPosition/vPosition.w;
color = vColor;

```cpp
}
```

A.5.3 Fragment Shader

```cpp
#version 150
```

in vec4 color;
out vec4 fColor;

```cpp
void main()
{
```

fColor = color;

```cpp
}
```

A.6     ROTATING SHADED CUBE
A.6.1 Application Code

```cpp
//    Display a rotating cube with lighting
//
//    Light and material properties are sent to the shader as uniform
//      variables. Vertex positions and normals are sent after each
//      rotation.
#include "Angel.h"
```

typedef Angel::vec4    color4;
typedef Angel::vec4    point4;

```cpp
const int NumVertices = 36; //(6 faces)(2 triangles/face)
```

(3 vertices/triangle)
point4 points[NumVertices];
vec3   normals[NumVertices];

```cpp
// Vertices of a unit cube centered at origin, sides aligned with axes
```

point4 vertices[8] = {
point4( -0.5, -0.5, 0.5, 1.0 ),
point4( -0.5, 0.5, 0.5, 1.0 ),
point4( 0.5, 0.5, 0.5, 1.0 ),
point4( 0.5, -0.5, 0.5, 1.0 ),
point4( -0.5, -0.5, -0.5, 1.0 ),
point4( -0.5, 0.5, -0.5, 1.0 ),
point4( 0.5, 0.5, -0.5, 1.0 ),
point4( 0.5, -0.5, -0.5, 1.0 )

```cpp
};
```

                                                                A.6 Rotating Shaded Cube   627

```cpp
// Array of rotation angles (in degrees) for each coordinate axis
```

enum { Xaxis = 0, Yaxis = 1, Zaxis = 2, NumAxes = 3 };

```cpp
int      Axis = Xaxis;
```

GLfloat Theta[NumAxes] = { 0.0, 0.0, 0.0 };

```cpp
// Model-view and projection matrices uniform location
```

GLuint ModelView, Projection;

```cpp
//----------------------------------------------------------------------
// quad generates two triangles for each face and assigns colors
//    to the vertices
int Index = 0;
void
```

quad( int a, int b, int c, int d )

```cpp
{
// Initialize temporary vectors along the quad’s edge to
//   compute its face normal
```

vec4 u = vertices[b] - vertices[a];
vec4 v = vertices[c] - vertices[b];
vec3 normal = normalize( cross(u, v) );
normals[Index] = normal; points[Index] = vertices[a]; Index++;
normals[Index] = normal; points[Index] = vertices[b]; Index++;
normals[Index] = normal; points[Index] = vertices[c]; Index++;
normals[Index] = normal; points[Index] = vertices[a]; Index++;
normals[Index] = normal; points[Index] = vertices[c]; Index++;
normals[Index] = normal; points[Index] = vertices[d]; Index++;

```cpp
}
//----------------------------------------------------------------------
// generate 12 triangles: 36 vertices and 36 colors
void
```

colorcube( void )

```cpp
{
```

quad( 1, 0, 3, 2 );
quad( 2, 3, 7, 6 );
quad( 3, 0, 4, 7 );
quad( 6, 5, 1, 2 );
quad( 4, 5, 6, 7 );
quad( 5, 4, 0, 1 );

```cpp
}
//----------------------------------------------------------------------
// OpenGL initialization
```

628   Appendix A   Sample Programs

```cpp
void
```

init( void )

```cpp
{
```

colorcube( void );

```cpp
// Create a vertex array object
```

GLuint vao;
glGenVertexArrays( 1, &vao );
glBindVertexArray( vao );

```cpp
// Create and initialize a buffer object
```

GLuint buffer;
glGenBuffers( 1, &buffer );
glBindBuffer( GL_ARRAY_BUFFER, buffer );
glBufferData( GL_ARRAY_BUFFER, sizeof(points) + sizeof(normals),
NULL, GL_STATIC_DRAW );
glBufferSubData( GL_ARRAY_BUFFER, 0, sizeof(points), points );
glBufferSubData( GL_ARRAY_BUFFER, sizeof(points),
sizeof(normals), normals );

```cpp
// Load shaders and use the resulting shader program
```

GLuint program = InitShader( "vshader53.glsl", "fshader53.glsl" );
glUseProgram( program );

```cpp
// set up vertex arrays
```

GLuint vPosition = glGetAttribLocation( program, "vPosition" );
glEnableVertexAttribArray( vPosition );
glVertexAttribPointer( vPosition, 4, GL_FLOAT, GL_FALSE, 0,
BUFFER_OFFSET(0) );
GLuint vNormal = glGetAttribLocation( program, "vNormal" );
glEnableVertexAttribArray( vNormal );
glVertexAttribPointer( vNormal, 3, GL_FLOAT, GL_FALSE, 0,
BUFFER_OFFSET(sizeof(points)) );

```cpp
// Initialize shader lighting parameters
```

point4 light_position( 0.0, 0.0, -1.0, 0.0 );
color4 light_ambient( 0.2, 0.2, 0.2, 1.0 );
color4 light_diffuse( 1.0, 1.0, 1.0, 1.0 );
color4 light_specular( 1.0, 1.0, 1.0, 1.0 );
color4 material_ambient( 1.0, 0.0, 1.0, 1.0 );
color4 material_diffuse( 1.0, 0.8, 0.0, 1.0 );
color4 material_specular( 1.0, 0.8, 0.0, 1.0 );

```cpp
float material_shininess = 100.0;
```

color4 ambient_product = light_ambient * material_ambient;
color4 diffuse_product = light_diffuse * material_diffuse;
color4 specular_product = light_specular * material_specular;
                                                                  A.6 Rotating Shaded Cube   629
glUniform4fv( glGetUniformLocation(program, "AmbientProduct"),
1, ambient_product );
glUniform4fv( glGetUniformLocation(program, "DiffuseProduct"),
1, diffuse_product );
glUniform4fv( glGetUniformLocation(program, "SpecularProduct"),
1, specular_product );
glUniform4fv( glGetUniformLocation(program, "LightPosition"),
1, light_position );
glUniform1f( glGetUniformLocation(program, "Shininess"),
material_shininess );

```cpp
// Retrieve transformation uniform variable locations
```

ModelView = glGetUniformLocation( program, "ModelView" );
Projection = glGetUniformLocation( program, "Projection" );
glEnable( GL_DEPTH_TEST );
glShadeModel(GL_FLAT);
glClearColor( 1.0, 1.0, 1.0, 1.0 );

```cpp
}
//----------------------------------------------------------------------
void
```

display( void )

```cpp
{
```

glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);

```cpp
//   Generate the model-view matrix
const vec3 viewer_pos( 0.0, 0.0, 2.0 );
```

mat4 model_view = ( Translate( -viewer_pos ) *
RotateX( Theta[Xaxis] ) *
RotateY( Theta[Yaxis] ) *
RotateZ( Theta[Zaxis] ) );
glUniformMatrix4fv( ModelView, 1, GL_TRUE, model_view );
glDrawArrays( GL_TRIANGLES, 0, NumVertices );
glutSwapBuffers( void );

```cpp
}
//----------------------------------------------------------------------
void
```

mouse( int button, int state, int x, int y )
630   Appendix A   Sample Programs

```cpp
{
```

if ( state == GLUT_DOWN ) {
switch( button ) {
case GLUT_LEFT_BUTTON:        Axis = Xaxis;   break;
case GLUT_MIDDLE_BUTTON:      Axis = Yaxis;   break;
case GLUT_RIGHT_BUTTON:       Axis = Zaxis;   break;

```cpp
}
}
}
//----------------------------------------------------------------------
void
```

idle( void )

```cpp
{
```

Theta[Axis] += 0.01;
if ( Theta[Axis] > 360.0 ) {
Theta[Axis] -= 360.0;

```cpp
}
```

glutPostRedisplay( void );

```cpp
}
//----------------------------------------------------------------------
void
```

keyboard( unsigned char key, int x, int y )

```cpp
{
```

switch( key ) {
case 033: // Escape Key
case ’q’: case ’Q’:
exit( EXIT_SUCCESS );
break;

```cpp
}
}
//----------------------------------------------------------------------
void
```

reshape( int width, int height )

```cpp
{
```

glViewport( 0, 0, width, height );
GLfloat aspect = GLfloat(width)/height;
mat4 projection = Perspective( 45.0, aspect, 0.5, 3.0 );
glUniformMatrix4fv( Projection, 1, GL_TRUE, projection );

```cpp
}
```

                                                                A.6 Rotating Shaded Cube   631

```cpp
//----------------------------------------------------------------------
int
```

main( int argc, char **argv )

```cpp
{
```

glutInit( &argc, argv );
glutInitDisplayMode( GLUT_RGBA | GLUT_DOUBLE | GLUT_DEPTH );
glutInitWindowSize( 512, 512 );
glutCreateWindow( "Color Cube" );
glewInit( void );
init( void );
glutDisplayFunc( display );
glutKeyboardFunc( keyboard );
glutReshapeFunc( reshape );
glutMouseFunc( mouse );
glutIdleFunc( idle );
glutMainLoop( void );
return 0;

```cpp
}
```

A.6.2 Vertex Shader

```cpp
#version 150
```

in vec4 vPosition;
in vec3 vNormal;
out vec4 color;
uniform vec4 AmbientProduct, DiffuseProduct, SpecularProduct;
uniform mat4 ModelView;
uniform mat4 Projection;
uniform vec4 LightPosition;
uniform float Shininess;

```cpp
void main()
{
// Transform vertex position into eye coordinates
```

vec3 pos = (ModelView * vPosition).xyz;
vec3 L = normalize( LightPosition.xyz - pos );
vec3 E = normalize( -pos );
vec3 H = normalize( L + E );

```cpp
// Transform vertex normal into eye coordinates
```

vec3 N = normalize( ModelView*vec4(vNormal, 0.0) ).xyz;
632   Appendix A   Sample Programs

```cpp
// Compute terms in the illumination equation
```

vec4 ambient = AmbientProduct;

```cpp
float Kd = max( dot(L, N), 0.0 );
```

vec4 diffuse = Kd*DiffuseProduct;

```cpp
float Ks = pow( max(dot(N, H), 0.0), Shininess );
```

vec4 specular = Ks * SpecularProduct;
if( dot(L, N) < 0.0 ) specular = vec4(0.0, 0.0, 0.0, 1.0);
gl_Position = Projection * ModelView * vPosition;
color = ambient + diffuse + specular;
color.a = 1.0;

```cpp
}
```

A.6.3 Fragment Shader

```cpp
#version 150
```

in vec4 color;
out vec4 fColor;

```cpp
void main()
{
```

fColor = color;

```cpp
}
```

A.7     PER-FRAGMENT LIGHTING OF SPHERE MODEL
A.7.1 Application Code

```cpp
// fragment shading of sphere model
#include "Angel.h"
const int NumTimesToSubdivide = 5;
const int NumTriangles        = 4096;
// (4 faces)^(NumTimesToSubdivide + 1)
const int NumVertices         = 3 * NumTriangles;
```

typedef Angel::vec4 point4;
typedef Angel::vec4 color4;
point4 points[NumVertices];
vec3   normals[NumVertices];

```cpp
// Model-view and projection matrices uniform location
```

                                                   A.7 Per-Fragment Lighting of Sphere Model   633
GLuint   ModelView, Projection;

```cpp
//----------------------------------------------------------------------
int Index = 0;
void
```

triangle( const point4& a, const point4& b, const point4& c )

```cpp
{
```

vec3 normal = normalize( cross(b - a, c - b) );
normals[Index] = normal;      points[Index] = a;   Index++;
normals[Index] = normal;      points[Index] = b;   Index++;
normals[Index] = normal;      points[Index] = c;   Index++;

```cpp
}
//----------------------------------------------------------------------
```

point4
unit( const point4& p )

```cpp
{
float len = p.x*p.x + p.y*p.y + p.z*p.z;
```

point4 t;
if ( len > DivideByZeroTolerance ) {
t = p / sqrt(len);
t.w = 1.0;

```cpp
}
```

return t;

```cpp
}
void
```

divide_triangle( const point4& a, const point4& b,

```cpp
const point4& c, int count )
{
```

if ( count > 0 ) {
point4 v1 = unit( a + b );
point4 v2 = unit( a + c );
point4 v3 = unit( b + c );
divide_triangle( a, v1, v2, count - 1 );
divide_triangle( c, v2, v3, count - 1 );
divide_triangle( b, v3, v1, count - 1 );
divide_triangle( v1, v3, v2, count - 1 );

```cpp
}
```

else {
triangle( a, b, c );

```cpp
}
}
```

634   Appendix A   Sample Programs

```cpp
void
```

tetrahedron( int count )

```cpp
{
```

point4 v[4] = {
vec4( 0.0, 0.0, 1.0, 1.0 ),
vec4( 0.0, 0.942809, -0.333333, 1.0 ),
vec4( -0.816497, -0.471405, -0.333333, 1.0 ),
vec4( 0.816497, -0.471405, -0.333333, 1.0 )

```cpp
};
```

divide_triangle( v[0], v[1], v[2], count );
divide_triangle( v[3], v[2], v[1], count );
divide_triangle( v[0], v[3], v[1], count );
divide_triangle( v[0], v[2], v[3], count );

```cpp
}
//----------------------------------------------------------------------
// OpenGL initialization
void
```

init( void )

```cpp
{
// Subdivide a tetrahedron into a sphere
```

tetrahedron( NumTimesToSubdivide );

```cpp
// Create a vertex array object
```

GLuint vao;
glGenVertexArrays( 1, &vao );
glBindVertexArray( vao );

```cpp
// Create and initialize a buffer object
```

GLuint buffer;
glGenBuffers( 1, &buffer );
glBindBuffer( GL_ARRAY_BUFFER, buffer );
glBufferData( GL_ARRAY_BUFFER, sizeof(points) + sizeof(normals),
NULL, GL_STATIC_DRAW );
glBufferSubData( GL_ARRAY_BUFFER, 0, sizeof(points), points );
glBufferSubData( GL_ARRAY_BUFFER, sizeof(points),
sizeof(normals), normals );

```cpp
// Load shaders and use the resulting shader program
```

GLuint program = InitShader( "vshader56.glsl", "fshader56.glsl" );
glUseProgram( program );

```cpp
// set up vertex arrays
```

GLuint vPosition = glGetAttribLocation( program, "vPosition" );
glEnableVertexAttribArray( vPosition );
glVertexAttribPointer( vPosition, 4, GL_FLOAT, GL_FALSE, 0,
BUFFER_OFFSET(0) );
                                                A.7 Per-Fragment Lighting of Sphere Model   635
GLuint vNormal = glGetAttribLocation( program, "vNormal" );
glEnableVertexAttribArray( vNormal );
glVertexAttribPointer( vNormal, 3, GL_FLOAT, GL_FALSE, 0,
BUFFER_OFFSET(sizeof(points)) );

```cpp
// Initialize shader lighting parameters
```

point4 light_position( 0.0, 0.0, 2.0, 0.0 );
color4 light_ambient( 0.2, 0.2, 0.2, 1.0 );
color4 light_diffuse( 1.0, 1.0, 1.0, 1.0 );
color4 light_specular( 1.0, 1.0, 1.0, 1.0 );
color4 material_ambient( 1.0, 0.0, 1.0, 1.0 );
color4 material_diffuse( 1.0, 0.8, 0.0, 1.0 );
color4 material_specular( 1.0, 0.0, 1.0, 1.0 );

```cpp
float material_shininess = 5.0;
```

color4 ambient_product = light_ambient * material_ambient;
color4 diffuse_product = light_diffuse * material_diffuse;
color4 specular_product = light_specular * material_specular;
glUniform4fv( glGetUniformLocation(program, "AmbientProduct"),
1, ambient_product );
glUniform4fv( glGetUniformLocation(program, "DiffuseProduct"),
1, diffuse_product );
glUniform4fv( glGetUniformLocation(program, "SpecularProduct"),
1, specular_product );
glUniform4fv( glGetUniformLocation(program, "LightPosition"),
1, light_position );
glUniform1f( glGetUniformLocation(program, "Shininess"),
material_shininess );

```cpp
// Retrieve transformation uniform variable locations
```

ModelView = glGetUniformLocation( program, "ModelView" );
Projection = glGetUniformLocation( program, "Projection" );
glEnable( GL_DEPTH_TEST );
glClearColor( 1.0, 1.0, 1.0, 1.0 ); // white background

```cpp
}
//----------------------------------------------------------------------
void
```

display( void )

```cpp
{
```

glClear( GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT );
point4 at( 0.0, 0.0, 0.0, 1.0 );
636   Appendix A   Sample Programs
point4 eye( 0.0, 0.0, 2.0, 1.0 );
vec4   up( 0.0, 1.0, 0.0, 0.0 );
mat4 model_view = LookAt( eye, at, up );
glUniformMatrix4fv( ModelView, 16, GL_TRUE, model_view );
glDrawArrays( GL_TRIANGLES, 0, NumVertices );
glutSwapBuffers( void );

```cpp
}
//----------------------------------------------------------------------
void
```

keyboard( unsigned char key, int x, int y )

```cpp
{
```

switch( key ) {
case 033: // Escape Key
case ’q’: case ’Q’:
exit( EXIT_SUCCESS );
break;

```cpp
}
}
//----------------------------------------------------------------------
void
```

reshape( int width, int height )

```cpp
{
```

glViewport( 0, 0, width, height );
GLfloat left = -2.0, right = 2.0;
GLfloat top = 2.0, bottom = -2.0;
GLfloat zNear = -20.0, zFar = 20.0;
GLfloat aspect = GLfloat(width)/height;
if ( aspect > 1.0 ) {
left *= aspect;
right *= aspect;

```cpp
}
```

else {
top /= aspect;
bottom /= aspect;

```cpp
}
```

mat4 projection = Ortho( left, right, bottom, top, zNear, zFar );
glUniformMatrix4fv( Projection, 1, GL_TRUE, projection );

```cpp
}
//----------------------------------------------------------------------
```

                                                   A.7 Per-Fragment Lighting of Sphere Model   637

```cpp
int
```

main( int argc, char **argv )

```cpp
{
```

glutInit( &argc, argv );
glutInitDisplayMode( GLUT_RGBA | GLUT_DEPTH );
glutInitWindowSize( 512, 512 );
glutCreateWindow( "Sphere" );
glewInit( void );
init( void );
glutDisplayFunc( display );
glutReshapeFunc( reshape );
glutKeyboardFunc( keyboard );
glutMainLoop( void );
return 0;

```cpp
}
```

A.7.2 Vertex Shader

```cpp
#version 150
```

in   vec4 vPosition;
in   vec3 vNormal;

```cpp
// output values that will be interpolated per-fragment
```

out vec3 fN;
out vec3 fE;
out vec3 fL;
uniform mat4 ModelView;
uniform vec4 LightPosition;
uniform mat4 Projection;

```cpp
void main()
{
```

fN = vNormal;
fE = vPosition.xyz;
fL = LightPosition.xyz;
if( LightPosition.w != 0.0 ) {
fL = LightPosition.xyz - vPosition.xyz;

```cpp
}
```

gl_Position = Projection*ModelView*vPosition;

```cpp
}
```

638   Appendix A   Sample Programs
A.7.3 Fragment Shader

```cpp
#version 150
// per-fragment interpolated values from the vertex shader
```

in vec3 fN;
in vec3 fL;
in vec3 fE;
out vec4 fColor;
uniform vec4 AmbientProduct, DiffuseProduct, SpecularProduct;
uniform mat4 ModelView;
uniform vec4 LightPosition;
uniform float Shininess;

```cpp
void main()
{
// Normalize the input lighting vectors
```

vec3 N = normalize(fN);
vec3 E = normalize(fE);
vec3 L = normalize(fL);
vec3 H = normalize( L + E );
vec4 ambient = AmbientProduct;

```cpp
float Kd = max(dot(L, N), 0.0);
```

vec4 diffuse = Kd*DiffuseProduct;

```cpp
float Ks = pow(max(dot(N, H), 0.0), Shininess);
```

vec4 specular = Ks*SpecularProduct;

```cpp
// discard the specular highlight if the light’s behind the vertex
```

if( dot(L, N) < 0.0 ) {
specular = vec4(0.0, 0.0, 0.0, 1.0);

```cpp
}
```

fColor = ambient + diffuse + specular;
fColor.a = 1.0;

```cpp
}
```

A.8     ROTATING CUBE WITH TEXTURE
A.8.1 Application Code

```cpp
// rotating cube with two texture objects
// change textures with 1 and 2 keys
```

                                                            A.8 Rotating Cube with Texture   639

```cpp
#include "Angel.h"
const int   NumTriangles = 12; // (6 faces)(2 triangles/face)
const int   NumVertices = 3 * NumTriangles;
const int   TextureSize = 64;
```

typedef Angel::vec4 point4;
typedef Angel::vec4 color4;

```cpp
// Texture objects and storage for texture image
```

GLuint textures[2];
GLubyte image[TextureSize][TextureSize][3];
GLubyte image2[TextureSize][TextureSize][3];

```cpp
// Vertex data arrays
```

point4 points[NumVertices];
color4 quad_colors[NumVertices];
vec2    tex_coords[NumVertices];

```cpp
// Array of rotation angles (in degrees) for each coordinate axis
```

enum { Xaxis = 0, Yaxis = 1, Zaxis = 2, NumAxes = 3 };

```cpp
int      Axis = Xaxis;
```

GLfloat Theta[NumAxes] = { 0.0, 0.0, 0.0 };
GLuint   theta;

```cpp
//----------------------------------------------------------------------
int Index = 0;
void
```

quad( int a, int b, int c, int d )

```cpp
{
```

point4 vertices[8] = {
point4( -0.5, -0.5, 0.5, 1.0 ),
point4( -0.5, 0.5, 0.5, 1.0 ),
point4( 0.5, 0.5, 0.5, 1.0 ),
point4( 0.5, -0.5, 0.5, 1.0 ),
point4( -0.5, -0.5, -0.5, 1.0 ),
point4( -0.5, 0.5, -0.5, 1.0 ),
point4( 0.5, 0.5, -0.5, 1.0 ),
point4( 0.5, -0.5, -0.5, 1.0 )

```cpp
};
```

color4 colors[8] = {
color4( 0.0, 0.0, 0.0, 1.0 ),   // black
color4( 1.0, 0.0, 0.0, 1.0 ),   // red
color4( 1.0, 1.0, 0.0, 1.0 ),   // yellow
color4( 0.0, 1.0, 0.0, 1.0 ),   // green
640   Appendix A   Sample Programs
color4( 0.0, 0.0, 1.0, 1.0 ),   // blue
color4( 1.0, 0.0, 1.0, 1.0 ),   // magenta
color4( 0.0, 1.0, 1.0, 1.0 ),   // white
color4( 1.0, 1.0, 1.0, 1.0 )    // cyan

```cpp
};
```

quad_colors[Index] = colors[a];
points[Index] = vertices[a];
tex_coords[Index] = vec2( 0.0, 0.0 );
Index++;
quad_colors[Index] = colors[a];
points[Index] = vertices[b];
tex_coords[Index] = vec2( 0.0, 1.0 );
Index++;
quad_colors[Index] = colors[a];
points[Index] = vertices[c];
tex_coords[Index] = vec2( 1.0, 1.0 );
Index++;
quad_colors[Index] = colors[a];
points[Index] = vertices[a];
tex_coords[Index] = vec2( 0.0, 0.0 );
Index++;
quad_colors[Index] = colors[a];
points[Index] = vertices[c];
tex_coords[Index] = vec2( 1.0, 1.0 );
Index++;
quad_colors[Index] = colors[a];
points[Index] = vertices[d];
tex_coords[Index] = vec2( 1.0, 0.0 );
Index++;

```cpp
}
//----------------------------------------------------------------------
void
```

colorcube( void )

```cpp
{
```

quad( 1, 0, 3, 2 );
quad( 2, 3, 7, 6 );
quad( 3, 0, 4, 7 );
quad( 6, 5, 1, 2 );
quad( 4, 5, 6, 7 );
quad( 5, 4, 0, 1 );

```cpp
}
```

                                                            A.8 Rotating Cube with Texture   641

```cpp
//----------------------------------------------------------------------
void
```

init( void )

```cpp
{
```

colorcube( void );

```cpp
// Create a checkerboard pattern
```

for ( int i = 0; i < 64; i++ ) {
for ( int j = 0; j < 64; j++ ) {
GLubyte c = (((i & 0x8) == 0) ^ ((j & 0x8)   == 0)) * 255;
image[i][j][0] = c;
image[i][j][1] = c;
image[i][j][2] = c;
image2[i][j][0] = c;
image2[i][j][1] = 0;
image2[i][j][2] = c;

```cpp
}
}
// Initialize texture objects
```

glGenTextures( 2, textures );
glBindTexture( GL_TEXTURE_2D, textures[0] );
glTexImage2D( GL_TEXTURE_2D, 0, GL_RGB, TextureSize, TextureSize, 0,
GL_RGB, GL_UNSIGNED_BYTE, image );
glTexParameterf( GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT );
glTexParameterf( GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_REPEAT );
glTexParameterf( GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST );
glTexParameterf( GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST );
glBindTexture( GL_TEXTURE_2D, textures[1] );
glTexImage2D( GL_TEXTURE_2D, 0, GL_RGB, TextureSize, TextureSize, 0,
GL_RGB, GL_UNSIGNED_BYTE, image2 );
glTexParameterf( GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT );
glTexParameterf( GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_REPEAT );
glTexParameterf( GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST );
glTexParameterf( GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST );
glActiveTexture( GL_TEXTURE0 );
glBindTexture( GL_TEXTURE_2D, textures[0] );

```cpp
// Create a vertex array object
```

GLuint vao;
glGenVertexArrays( 1, &vao );
glBindVertexArray( vao );

```cpp
// Create and initialize a buffer object
```

GLuint buffer;
642   Appendix A   Sample Programs
glGenBuffers( 1, &buffer );
glBindBuffer( GL_ARRAY_BUFFER, buffer );
glBufferData( GL_ARRAY_BUFFER,
sizeof(points) + sizeof(quad_colors) +
sizeof(tex_coords), NULL, GL_STATIC_DRAW );

```cpp
// Specify an offset to keep track of where we’re placing data in
//   our vertex array buffer. We’ll use the same technique when we
//   associate the offsets with vertex attribute pointers.
```

GLintptr offset = 0;
glBufferSubData( GL_ARRAY_BUFFER, offset, sizeof(points), points );
offset += sizeof(points);
glBufferSubData( GL_ARRAY_BUFFER, offset,
sizeof(quad_colors), quad_colors );
offset += sizeof(quad_colors);
glBufferSubData( GL_ARRAY_BUFFER, offset, sizeof(tex_coords),
tex_coords );

```cpp
// Load shaders and use the resulting shader program
```

GLuint program = InitShader( "vshader71.glsl", "fshader71.glsl" );
glUseProgram( program );

```cpp
// set up vertex arrays
```

offset = 0;
GLuint vPosition = glGetAttribLocation( program, "vPosition" );
glEnableVertexAttribArray( vPosition );
glVertexAttribPointer( vPosition, 4, GL_FLOAT, GL_FALSE, 0,
BUFFER_OFFSET(offset) );
offset += sizeof(points);
GLuint vColor = glGetAttribLocation( program, "vColor" );
glEnableVertexAttribArray( vColor );
glVertexAttribPointer( vColor, 4, GL_FLOAT, GL_FALSE, 0,
BUFFER_OFFSET(offset) );
offset += sizeof(quad_colors);
GLuint vTexCoord = glGetAttribLocation( program, "vTexCoord" );
glEnableVertexAttribArray( vTexCoord );
glVertexAttribPointer( vTexCoord, 2, GL_FLOAT, GL_FALSE, 0,
BUFFER_OFFSET(offset) );

```cpp
// Set the value of the fragment shader texture sampler variable
//   ("texture") to the appropriate texture unit. In this case,
//   zero, for GL_TEXTURE0 which was previously set by calling
//   glActiveTexture( void ).
```

glUniform1i( glGetUniformLocation(program, "texture"), 0 );
                                                                 A.8 Rotating Cube with Texture   643
theta = glGetUniformLocation( program, "theta" );
glEnable( GL_DEPTH_TEST );
glClearColor( 1.0, 1.0, 1.0, 1.0 );

```cpp
}
void
```

display( void )

```cpp
{
```

glClear( GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT );
glUniform3fv( theta, 1, Theta );
glDrawArrays( GL_TRIANGLES, 0, NumVertices );
glutSwapBuffers( void );

```cpp
}
//----------------------------------------------------------------------
void
```

mouse( int button, int state, int x, int y )

```cpp
{
```

if ( state == GLUT_DOWN ) {
switch( button ) {
case GLUT_LEFT_BUTTON:   Axis = Xaxis;     break;
case GLUT_MIDDLE_BUTTON: Axis = Yaxis;     break;
case GLUT_RIGHT_BUTTON:  Axis = Zaxis;     break;

```cpp
}
}
}
//----------------------------------------------------------------------
void
```

idle( void )

```cpp
{
```

Theta[Axis] += 0.01;
if ( Theta[Axis] > 360.0 ) {
Theta[Axis] -= 360.0;

```cpp
}
```

glutPostRedisplay( void );

```cpp
}
//----------------------------------------------------------------------
void
```

keyboard( unsigned char key, int mousex, int mousey )

```cpp
{
```

644   Appendix A   Sample Programs
switch( key ) {
case 033: // Escape Key
case ’q’: case ’Q’:
exit( EXIT_SUCCESS );
break;
case ’1’:
glBindTexture( GL_TEXTURE_2D, textures[0] );
break;
case ’2’:
glBindTexture( GL_TEXTURE_2D, textures[1] );
break;

```cpp
}
```

glutPostRedisplay( void );

```cpp
}
//----------------------------------------------------------------------
int
```

main( int argc, char **argv )

```cpp
{
```

glutInit( &argc, argv );
glutInitDisplayMode( GLUT_RGBA | GLUT_DOUBLE | GLUT_DEPTH );
glutInitWindowSize( 512, 512 );
glutInitContextVersion( 3, 2 );
glutInitContextProfile( GLUT_CORE_PROFILE );
glutCreateWindow( "Color Cube" );
glewInit( void );
init( void );
glutDisplayFunc( display );
glutKeyboardFunc( keyboard );
glutMouseFunc( mouse );
glutIdleFunc( idle );
glutMainLoop( void );
return 0;

```cpp
}
```

A.8.2 Vertex Shader

```cpp
#version 150
```

in   vec4 vPosition;
in   vec4 vColor;
in   vec2 vTexCoord;
                                                            A.8 Rotating Cube with Texture   645
out vec4 color;
out vec2 texCoord;
uniform vec3 theta;

```cpp
void main()
{
const float    DegreesToRadians = 3.14159265 / 180.0;
```

vec3 c = cos( DegreesToRadians * theta );
vec3 s = sin( DegreesToRadians * theta );
mat4 rx = mat4( 1.0, 0.0, 0.0, 0.0,
0.0, c.x, -s.x, 0.0,
0.0, s.x, c.x, 0.0,
0.0, 0.0, 0.0, 1.0);
mat4 ry = mat4(   c.y, 0.0, s.y, 0.0,
0.0, 1.0, 0.0, 0.0,
-s.y, 0.0, c.y, 0.0,
0.0, 0.0, 0.0, 1.0 );
mat4 rz = mat4( c.z, -s.z, 0.0, 0.0,
s.z, c.z, 0.0, 0.0,
0.0, 0.0, 1.0, 0.0,
0.0, 0.0, 0.0, 1.0 );
color       = vColor;
texCoord    = vTexCoord;
gl_Position = rz * ry * rx * vPosition;

```cpp
}
```

A.8.3 Fragment Shader

```cpp
#version 150
```

in   vec4 color;
in   vec2 texCoord;
out vec4 fColor;
uniform sampler2D texture;

```cpp
void main()
{
```

fColor = color * texture2D( texture, texCoord );

```cpp
}
```

646   Appendix A   Sample Programs
A.9   FIGURE WITH TREE TRAVERSAL
A.9.1 Application Code

```cpp
#include "Angel.h"
#include <assert.h>
```

typedef Angel::vec4 point4;
typedef Angel::vec4 color4;

```cpp
const int NumVertices = 36; //(6 faces)(2 triangles/face)
```

(3 vertices/triangle)
point4 points[NumVertices];
color4 colors[NumVertices];
point4 vertices[8] = {
point4( -0.5, -0.5, 0.5, 1.0 ),
point4( -0.5, 0.5, 0.5, 1.0 ),
point4( 0.5, 0.5, 0.5, 1.0 ),
point4( 0.5, -0.5, 0.5, 1.0 ),
point4( -0.5, -0.5, -0.5, 1.0 ),
point4( -0.5, 0.5, -0.5, 1.0 ),
point4( 0.5, 0.5, -0.5, 1.0 ),
point4( 0.5, -0.5, -0.5, 1.0 )

```cpp
};
// RGBA colors
```

color4 vertex_colors[8] = {
color4( 0.0, 0.0, 0.0, 1.0 ),   // black
color4( 1.0, 0.0, 0.0, 1.0 ),   // red
color4( 1.0, 1.0, 0.0, 1.0 ),   // yellow
color4( 0.0, 1.0, 0.0, 1.0 ),   // green
color4( 0.0, 0.0, 1.0, 1.0 ),   // blue
color4( 1.0, 0.0, 1.0, 1.0 ),   // magenta
color4( 1.0, 1.0, 1.0, 1.0 ),   // white
color4( 0.0, 1.0, 1.0, 1.0 )    // cyan

```cpp
};
//----------------------------------------------------------------------
class MatrixStack {
int    _index;
int    _size;
```

mat4* _matrices;

```cpp
public:
```

MatrixStack( int numMatrices = 32 ):_index(0), _size(numMatrices)
                                                             A.9 Figure with Tree Traversal   647

```cpp
{ _matrices = new mat4[numMatrices]; }
```

~MatrixStack( void )

```cpp
{ delete[]_matrices; }
```

mat4& push( const mat4& m ) {
assert( _index + 1 < _size );
_matrices[_index++] = m;

```cpp
}
```

mat4& pop( void ) {
assert( _index - 1 >= 0 );
_index--;
return _matrices[_index + 1];

```cpp
}
};
```

MatrixStack   mvstack;
mat4          model_view;
GLuint        ModelView, Projection;

```cpp
//----------------------------------------------------------------------
#define TORSO_HEIGHT 5.0
#define TORSO_WIDTH 1.0
#define UPPER_ARM_HEIGHT 3.0
#define LOWER_ARM_HEIGHT 2.0
#define UPPER_LEG_WIDTH 0.5
#define LOWER_LEG_WIDTH 0.5
#define LOWER_LEG_HEIGHT 2.0
#define UPPER_LEG_HEIGHT 3.0
#define UPPER_LEG_WIDTH 0.5
#define UPPER_ARM_WIDTH 0.5
#define LOWER_ARM_WIDTH 0.5
#define HEAD_HEIGHT 1.5
#define HEAD_WIDTH 1.0
// Set up menu item indices, which we can also use with the joint angles
```

enum {
Torso = 0,
Head = 1,
Head1 = 1,
Head2 = 2,
LeftUpperArm = 3,
LeftLowerArm = 4,
RightUpperArm = 5,
RightLowerArm = 6,
LeftUpperLeg = 7,
648   Appendix A   Sample Programs
LeftLowerLeg = 8,
RightUpperLeg = 9,
RightLowerLeg = 10,
NumNodes,

```cpp
};
// Joint angles with initial values
```

theta[NumNodes] = {
0.0,    // Torso
0.0,    // Head1
0.0,    // Head2
0.0,    // LeftUpperArm
0.0,    // LeftLowerArm
0.0,    // RightUpperArm
0.0,    // RightLowerArm
180.0, // LeftUpperLeg
0.0,     // LeftLowerLeg
180.0, // RightUpperLeg
0.0    // RightLowerLeg

```cpp
};
```

GLint angle = Head2;

```cpp
//----------------------------------------------------------------------
struct Node {
```

mat4 transform;

```cpp
void (*render)( void );
```

Node* sibling;
Node* child;
Node( void ) :
render(NULL), sibling(NULL), child(NULL) {}
Node( mat4& m, void (*render)( void ), Node* sibling, Node* child ) :
transform(m), render(render), sibling(sibling), child(child) {}

```cpp
};
```

Node    nodes[NumNodes];

```cpp
//----------------------------------------------------------------------
int Index = 0;
void
```

quad( int a, int b, int c, int d )
                                                             A.9 Figure with Tree Traversal   649

```cpp
{
```

colors[Index] = vertex_colors[a]; points[Index] = vertices[a]; Index++;
colors[Index] = vertex_colors[a]; points[Index] = vertices[b]; Index++;
colors[Index] = vertex_colors[a]; points[Index] = vertices[c]; Index++;
colors[Index] = vertex_colors[a]; points[Index] = vertices[a]; Index++;
colors[Index] = vertex_colors[a]; points[Index] = vertices[c]; Index++;
colors[Index] = vertex_colors[a]; points[Index] = vertices[d]; Index++;

```cpp
}
void
```

colorcube( void )

```cpp
{
```

quad( 1, 0, 3, 2 );
quad( 2, 3, 7, 6 );
quad( 3, 0, 4, 7 );
quad( 6, 5, 1, 2 );
quad( 4, 5, 6, 7 );
quad( 5, 4, 0, 1 );

```cpp
}
//----------------------------------------------------------------------
void
```

traverse( Node* node )

```cpp
{
```

if ( node == NULL ) { return; }
mvstack.push( model_view );
model_view *= node->transform;
node->render( void );
if ( node->child ) { traverse( node->child ); }
model_view = mvstack.pop( void );
if ( node->sibling ) { traverse( node->sibling ); }

```cpp
}
//----------------------------------------------------------------------
void
```

torso( void )

```cpp
{
```

mvstack.push( model_view );
mat4 instance = ( Translate( 0.0, 0.5 * TORSO_HEIGHT, 0.0 ) *
Scale( TORSO_WIDTH, TORSO_HEIGHT, TORSO_WIDTH ) );
650   Appendix A   Sample Programs
glUniformMatrix4fv( ModelView, 1, GL_TRUE, model_view * instance );
glDrawArrays( GL_TRIANGLES, 0, NumVertices );
model_view = mvstack.pop( void );

```cpp
}
void
```

head( void )

```cpp
{
```

mvstack.push( model_view );
mat4 instance = (Translate( 0.0, 0.5 * HEAD_HEIGHT, 0.0 ) *
Scale( HEAD_WIDTH, HEAD_HEIGHT, HEAD_WIDTH ) );
glUniformMatrix4fv( ModelView, 1, GL_TRUE, model_view * instance );
glDrawArrays( GL_TRIANGLES, 0, NumVertices );
model_view = mvstack.pop( void );

```cpp
}
void
```

left_upper_arm( void )

```cpp
{
```

mvstack.push( model_view );
mat4 instance = (Translate( 0.0, 0.5 * UPPER_ARM_HEIGHT, 0.0 ) *
Scale( UPPER_ARM_WIDTH,
UPPER_ARM_HEIGHT,
UPPER_ARM_WIDTH ) );
glUniformMatrix4fv( ModelView, 1, GL_TRUE, model_view * instance );
glDrawArrays( GL_TRIANGLES, 0, NumVertices );
model_view = mvstack.pop( void );

```cpp
}
void
```

left_lower_arm( void )

```cpp
{
```

mvstack.push( model_view );
mat4 instance = ( Translate( 0.0, 0.5 * LOWER_ARM_HEIGHT, 0.0 ) *
Scale( LOWER_ARM_WIDTH,
LOWER_ARM_HEIGHT,
LOWER_ARM_WIDTH ) );
glUniformMatrix4fv( ModelView, 1, GL_TRUE, model_view * instance );
glDrawArrays( GL_TRIANGLES, 0, NumVertices );
                                                             A.9 Figure with Tree Traversal   651
model_view = mvstack.pop( void );

```cpp
}
void
```

right_upper_arm( void )

```cpp
{
```

mvstack.push( model_view );
mat4 instance = (Translate( 0.0, 0.5 * UPPER_ARM_HEIGHT, 0.0 ) *
Scale( UPPER_ARM_WIDTH,
UPPER_ARM_HEIGHT,
UPPER_ARM_WIDTH ) );
glUniformMatrix4fv( ModelView, 1, GL_TRUE, model_view * instance );
glDrawArrays( GL_TRIANGLES, 0, NumVertices );
model_view = mvstack.pop( void );

```cpp
}
void
```

right_lower_arm( void )

```cpp
{
```

mvstack.push( model_view );
mat4 instance = (Translate( 0.0, 0.5 * LOWER_ARM_HEIGHT, 0.0 ) *
Scale( LOWER_ARM_WIDTH,
LOWER_ARM_HEIGHT,
LOWER_ARM_WIDTH ) );
glUniformMatrix4fv( ModelView, 1, GL_TRUE, model_view * instance );
glDrawArrays( GL_TRIANGLES, 0, NumVertices );
model_view = mvstack.pop( void );

```cpp
}
void
```

left_upper_leg( void )

```cpp
{
```

mvstack.push( model_view );
mat4 instance = ( Translate( 0.0, 0.5 * UPPER_LEG_HEIGHT, 0.0 ) *
Scale( UPPER_LEG_WIDTH,
UPPER_LEG_HEIGHT,
UPPER_LEG_WIDTH ) );
glUniformMatrix4fv( ModelView, 1, GL_TRUE, model_view * instance );
glDrawArrays( GL_TRIANGLES, 0, NumVertices );
652   Appendix A   Sample Programs
model_view = mvstack.pop( void );

```cpp
}
void
```

left_lower_leg( void )

```cpp
{
```

mvstack.push( model_view );
mat4 instance = (Translate( 0.0, 0.5 * LOWER_LEG_HEIGHT, 0.0 ) *
Scale( LOWER_LEG_WIDTH,
LOWER_LEG_HEIGHT,
LOWER_LEG_WIDTH ) );
glUniformMatrix4fv( ModelView, 1, GL_TRUE, model_view * instance );
glDrawArrays( GL_TRIANGLES, 0, NumVertices );
model_view = mvstack.pop( void );

```cpp
}
void
```

right_upper_leg( void )

```cpp
{
```

mvstack.push( model_view );
mat4 instance = (Translate( 0.0, 0.5 * UPPER_LEG_HEIGHT, 0.0 ) *
Scale( UPPER_LEG_WIDTH,
UPPER_LEG_HEIGHT,
UPPER_LEG_WIDTH ) );
glUniformMatrix4fv( ModelView, 1, GL_TRUE, model_view * instance );
glDrawArrays( GL_TRIANGLES, 0, NumVertices );
model_view = mvstack.pop( void );

```cpp
}
void
```

right_lower_leg( void )

```cpp
{
```

mvstack.push( model_view );
mat4 instance = ( Translate( 0.0, 0.5 * LOWER_LEG_HEIGHT, 0.0 ) *
Scale( LOWER_LEG_WIDTH,
LOWER_LEG_HEIGHT,
LOWER_LEG_WIDTH ) );
glUniformMatrix4fv( ModelView, 1, GL_TRUE, model_view * instance );
glDrawArrays( GL_TRIANGLES, 0, NumVertices );
model_view = mvstack.pop( void );

```cpp
}
```

                                                              A.9 Figure with Tree Traversal   653

```cpp
//----------------------------------------------------------------------
void
```

display( void )

```cpp
{
```

glClear( GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT );
traverse( &nodes[Torso] );
glutSwapBuffers( void );

```cpp
}
//----------------------------------------------------------------------
void
```

mouse( int button, int state, int x, int y )

```cpp
{
```

if ( button == GLUT_LEFT_BUTTON && state == GLUT_DOWN ) {
theta[angle] += 5.0;
if ( theta[angle] > 360.0 ) { theta[angle] -= 360.0; }

```cpp
}
```

if ( button == GLUT_RIGHT_BUTTON && state == GLUT_DOWN ) {
theta[angle] -= 5.0;
if ( theta[angle] < 0.0 ) { theta[angle] += 360.0; }

```cpp
}
```

mvstack.push( model_view );
switch( angle ) {
case Torso:
nodes[Torso].transform =
RotateY( theta[Torso] );
break;
case Head1: case Head2:
nodes[Head].transform =
Translate(0.0, TORSO_HEIGHT+0.5*HEAD_HEIGHT, 0.0) *
RotateX(theta[Head1]) *
RotateY(theta[Head2]) *
Translate(0.0, -0.5*HEAD_HEIGHT, 0.0);
break;
case LeftUpperArm:
nodes[LeftUpperArm].transform =
Translate(-(TORSO_WIDTH+UPPER_ARM_WIDTH),
0.9*TORSO_HEIGHT, 0.0) *
RotateX(theta[LeftUpperArm]);
break;
case RightUpperArm:
nodes[RightUpperArm].transform =
654   Appendix A   Sample Programs
Translate(TORSO_WIDTH+UPPER_ARM_WIDTH,
0.9*TORSO_HEIGHT, 0.0) *
RotateX(theta[RightUpperArm]);
break;
case RightUpperLeg:
nodes[RightUpperLeg].transform =
Translate(TORSO_WIDTH+UPPER_LEG_WIDTH,
0.1*UPPER_LEG_HEIGHT, 0.0) *
RotateX(theta[RightUpperLeg]);
break;
case LeftUpperLeg:
nodes[LeftUpperLeg].transform =
Translate(-(TORSO_WIDTH+UPPER_LEG_WIDTH),
0.1*UPPER_LEG_HEIGHT, 0.0) *
RotateX(theta[LeftUpperLeg]);
break;
case LeftLowerArm:
nodes[LeftLowerArm].transform =
Translate(0.0, UPPER_ARM_HEIGHT, 0.0) *
RotateX(theta[LeftLowerArm]);
break;
case LeftLowerLeg:
nodes[LeftLowerLeg].transform =
Translate(0.0, UPPER_LEG_HEIGHT, 0.0) *
RotateX(theta[LeftLowerLeg]);
break;
case RightLowerLeg:
nodes[RightLowerLeg].transform =
Translate(0.0, UPPER_LEG_HEIGHT, 0.0) *
RotateX(theta[RightLowerLeg]);
break;
case RightLowerArm:
nodes[RightLowerArm].transform =
Translate(0.0, UPPER_ARM_HEIGHT, 0.0) *
RotateX(theta[RightLowerArm]);
break;

```cpp
}
```

model_view = mvstack.pop( void );
glutPostRedisplay( void );

```cpp
}
//----------------------------------------------------------------------
```

                                                              A.9 Figure with Tree Traversal   655

```cpp
void
```

menu( int option )

```cpp
{
```

if ( option == Quit ) {
exit( EXIT_SUCCESS );

```cpp
}
```

angle = option;

```cpp
}
//----------------------------------------------------------------------
void
```

reshape( int width, int height )

```cpp
{
```

glViewport( 0, 0, width, height );
GLfloat left = -10.0, right = 10.0;
GLfloat bottom = -10.0, top = 10.0;
GLfloat zNear = -10.0, zFar = 10.0;
GLfloat aspect = GLfloat( width ) / height;
if ( aspect > 1.0 ) {
left *= aspect;
right *= aspect;

```cpp
}
```

else {
bottom /= aspect;
top /= aspect;

```cpp
}
```

mat4 projection = Ortho( left, right, bottom, top, zNear, zFar );
glUniformMatrix4fv( Projection, 1, GL_TRUE, projection );
model_view = mat4( 1.0 );    // An Identity matrix

```cpp
}
//----------------------------------------------------------------------
void
```

initNodes( void )

```cpp
{
```

mat4 m;
m = RotateY( theta[Torso] );
nodes[Torso] = Node( m, torso, NULL, &nodes[Head1] );
m = Translate(0.0, TORSO_HEIGHT+0.5*HEAD_HEIGHT, 0.0) *
656   Appendix A   Sample Programs
RotateX(theta[Head1]) *
RotateY(theta[Head2]);
nodes[Head1] = Node( m, head, &nodes[LeftUpperArm], NULL );
m = Translate(-(TORSO_WIDTH+UPPER_ARM_WIDTH), 0.9*TORSO_HEIGHT,
0.0) * RotateX(theta[LeftUpperArm]);
nodes[LeftUpperArm] =
Node( m, left_upper_arm, &nodes[RightUpperArm],
&nodes[LeftLowerArm] );
m = Translate(TORSO_WIDTH+UPPER_ARM_WIDTH, 0.9*TORSO_HEIGHT, 0.0) *
RotateX(theta[RightUpperArm]);
nodes[RightUpperArm] =
Node( m, right_upper_arm,
&nodes[LeftUpperLeg], &nodes[RightLowerArm] );
m = Translate(-(TORSO_WIDTH+UPPER_LEG_WIDTH),
0.1*UPPER_LEG_HEIGHT, 0.0) *
RotateX(theta[LeftUpperLeg]);
nodes[LeftUpperLeg] =
Node( m, left_upper_leg, &nodes[RightUpperLeg],
&nodes[LeftLowerLeg] );
m = Translate(TORSO_WIDTH+UPPER_LEG_WIDTH,
0.1*UPPER_LEG_HEIGHT, 0.0) * RotateX(theta[RightUpperLeg]);
nodes[RightUpperLeg] =
Node( m, right_upper_leg, NULL, &nodes[RightLowerLeg] );
m = Translate(0.0, UPPER_ARM_HEIGHT, 0.0) *
RotateX(theta[LeftLowerArm]);
nodes[LeftLowerArm] = Node( m, left_lower_arm, NULL, NULL );
m = Translate(0.0, UPPER_ARM_HEIGHT, 0.0) *
RotateX(theta[RightLowerArm]);
nodes[RightLowerArm] = Node( m, right_lower_arm, NULL, NULL );
m = Translate(0.0, UPPER_LEG_HEIGHT, 0.0) *
RotateX(theta[LeftLowerLeg]);
nodes[LeftLowerLeg] = Node( m, left_lower_leg, NULL, NULL );
m = Translate(0.0, UPPER_LEG_HEIGHT, 0.0) *
RotateX(theta[RightLowerLeg]);
nodes[RightLowerLeg] = Node( m, right_lower_leg, NULL, NULL );

```cpp
}
//----------------------------------------------------------------------
void
```

init( void )
                                                                A.9 Figure with Tree Traversal   657

```cpp
{
```

colorcube( void );

```cpp
// Initialize tree
```

initNodes( void );

```cpp
// Create a vertex array object
```

GLuint vao;
glGenVertexArrays( 1, &vao );
glBindVertexArray( vao );

```cpp
// Create and initialize a buffer object
```

GLuint buffer;
glGenBuffers( 1, &buffer );
glBindBuffer( GL_ARRAY_BUFFER, buffer );
glBufferData( GL_ARRAY_BUFFER, sizeof(points) + sizeof(colors),
NULL, GL_DYNAMIC_DRAW );
glBufferSubData( GL_ARRAY_BUFFER, 0, sizeof(points), points );
glBufferSubData( GL_ARRAY_BUFFER, sizeof(points), sizeof(colors),
colors );

```cpp
// Load shaders and use the resulting shader program
```

GLuint program = InitShader( "vshader83.glsl", "fshader83.glsl" );
glUseProgram( program );
GLuint vPosition = glGetAttribLocation( program, "vPosition" );
glEnableVertexAttribArray( vPosition );
glVertexAttribPointer( vPosition, 4, GL_FLOAT, GL_FALSE, 0,
BUFFER_OFFSET(0) );
GLuint vColor = glGetAttribLocation( program, "vColor" );
glEnableVertexAttribArray( vColor );
glVertexAttribPointer( vColor, 4, GL_FLOAT, GL_FALSE, 0,
BUFFER_OFFSET(points) );
ModelView = glGetUniformLocation( program, "ModelView" );
Projection = glGetUniformLocation( program, "Projection" );
glEnable( GL_DEPTH_TEST );
glPolygonMode( GL_FRONT_AND_BACK, GL_LINE );
glClearColor( 1.0, 1.0, 1.0, 1.0 );

```cpp
}
//----------------------------------------------------------------------
void
```

keyboard( unsigned char key, int x, int y )
658   Appendix A   Sample Programs

```cpp
{
```

switch( key ) {
case 033: // Escape Key
case ’q’: case ’Q’:
exit( EXIT_SUCCESS );
break;

```cpp
}
}
//----------------------------------------------------------------------
int
```

main( int argc, char **argv )

```cpp
{
```

glutInit( &argc, argv );
glutInitDisplayMode( GLUT_DOUBLE | GLUT_RGB | GLUT_DEPTH );
glutInitWindowSize( 512, 512 );
glutInitContextVersion( 3, 2 );
glutInitContextProfile( GLUT_CORE_PROFILE );
glutCreateWindow( "robot" );
glewInit( void );
init( void );
glutDisplayFunc( display );
glutReshapeFunc( reshape );
glutKeyboardFunc( keyboard );
glutMouseFunc( mouse );
glutCreateMenu( menu );
glutAddMenuEntry( "torso", Torso );
glutAddMenuEntry( "head1", Head1 );
glutAddMenuEntry( "head2", Head2 );
glutAddMenuEntry( "right_upper_arm", RightUpperArm );
glutAddMenuEntry( "right_lower_arm", RightLowerArm );
glutAddMenuEntry( "left_upper_arm", LeftUpperArm );
glutAddMenuEntry( "left_lower_arm", LeftLowerArm );
glutAddMenuEntry( "right_upper_leg", RightUpperLeg );
glutAddMenuEntry( "right_lower_leg", RightLowerLeg );
glutAddMenuEntry( "left_upper_leg", LeftUpperLeg );
glutAddMenuEntry( "left_lower_leg", LeftLowerLeg );
glutAddMenuEntry( "quit", Quit );
glutAttachMenu( GLUT_MIDDLE_BUTTON );
glutMainLoop( void );
return 0;

```cpp
}
```

                                                                    A.10 Teapot Renderer   659
A.9.2 Vertex Shader

```cpp
#version 150
```

in vec4 vPosition;
in vec4 vColor;
out vec4 color;
uniform mat4 ModelView;
uniform mat4 Projection;

```cpp
void main()
{
```

color = vColor;
gl_Position = Projection*ModelView*vPosition;

```cpp
}
```

A.9.3 Fragment Shader

```cpp
#version 150
```

in vec4 color;
out vec4 fColor;

```cpp
void main()
{
```

fColor = color;

```cpp
}
```

A.10   TEAPOT RENDERER
A.10.1 Application Code

```cpp
#include "Angel.h"
```

typedef Angel::vec4 point4;

```cpp
// Define a convenient type for referencing patch control points, which
//   is used in the declaration of the vertices’ array (used in "vertices.h")
```

typedef GLfloat      point3[3];

```cpp
#include "vertices.h"
#include "patches.h"
const int NumTimesToSubdivide = 3;
const int PatchesPerSubdivision = 4;
const int NumQuadsPerPatch =
```

(int) pow( PatchesPerSubdivision, NumTimesToSubdivide );

```cpp
const int NumTriangles =
```

660   Appendix A   Sample Programs
( NumTeapotPatches * NumQuadsPerPatch * 2 // triangles / quad );

```cpp
const int NumVertices =
```

( NumTriangles * 3 // vertices / triangle );

```cpp
int      Index = 0;
```

point4   points[NumVertices];
GLuint   Projection;
enum { X = 0, Y = 1, Z = 2 };

```cpp
//----------------------------------------------------------------------
void
```

divide_curve( point4 c[4], point4 r[4], point4 l[4] )

```cpp
{
// Subdivide a Bezier curve into two equivalent Bezier curves:
//   left (l) and right (r) sharing the midpoint of the middle
//   control point
```

point4 t, mid = ( c[1] + c[2] ) / 2;
l[0] = c[0];
l[1] = ( c[0] + c[1] ) / 2;
l[2] = ( l[1] + mid ) / 2;
r[3] = c[3];
r[2] = ( c[2] + c[3] ) / 2;
r[1] = ( mid + r[2] ) / 2;
l[3] = r[0] = ( l[2] + r[1] ) / 2;
for ( int i = 0; i < 4; ++i ) {
l[i].w = 1.0;
r[i].w = 1.0;

```cpp
}
}
//----------------------------------------------------------------------
void
```

draw_patch( point4 p[4][4] )

```cpp
{
// Draw the quad (as two triangles) bounded by the corners of the
//   Bezier patch.
```

points[Index++] = p[0][0];
points[Index++] = p[3][0];
points[Index++] = p[3][3];
points[Index++] = p[0][0];
points[Index++] = p[3][3];
                                                                    A.10 Teapot Renderer   661
points[Index++] = p[0][3];

```cpp
}
//----------------------------------------------------------------------
```

transpose( point4 a[4][4] )

```cpp
{
```

for ( int i = 0; i < 4; i++ ) {
for ( int j = i; j < 4; j++ ) {
point4 t = a[i][j];
a[i][j] = a[j][i];
a[j][i] = t;

```cpp
}
}
}
void
```

divide_patch( point4 p[4][4], int count )

```cpp
{
```

if ( count > 0 ) {
point4 q[4][4], r[4][4], s[4][4], t[4][4];
point4 a[4][4], b[4][4];

```cpp
// subdivide curves in u direction, transpose results, divide
// in u direction again (equivalent to subdivision in v)
```

for ( int k = 0; k < 4; ++k ) {
divide_curve( p[k], a[k], b[k] );

```cpp
}
```

transpose( a );
transpose( b );
for ( int k = 0; k < 4; ++k ) {
divide_curve( a[k], q[k], r[k] );
divide_curve( b[k], s[k], t[k] );

```cpp
}
// recursive division of 4 resulting patches
```

divide_patch( q, count - 1 );
divide_patch( r, count - 1 );
divide_patch( s, count - 1 );
divide_patch( t, count - 1 );

```cpp
}
```

else {
draw_patch( p );

```cpp
}
}
```

662   Appendix A   Sample Programs

```cpp
//----------------------------------------------------------------------
void
```

init( void )

```cpp
{
```

for ( int n = 0; n < NumTeapotPatches; n++ ) {
point4 patch[4][4];

```cpp
// Initialize each patch’s control point data
```

for ( int i = 0; i < 4; ++i ) {
for ( int j = 0; j < 4; ++j ) {
point3& v = vertices[indices[n][i][j]];
patch[i][j] = point4( v[X], v[Y], v[Z], 1.0 );

```cpp
}
}
// Subdivide the patch
```

divide_patch( patch, NumTimesToSubdivide );

```cpp
}
// Create a vertex array object
```

GLuint vao;
glGenVertexArrays( 1, &vao );
glBindVertexArray( vao );

```cpp
// Create and initialize a buffer object
```

GLuint buffer;
glGenBuffers( 1, &buffer );
glBindBuffer( GL_ARRAY_BUFFER, buffer );
glBufferData( GL_ARRAY_BUFFER, sizeof(points), points,
GL_STATIC_DRAW );

```cpp
// Load shaders and use the resulting shader program
```

GLuint program = InitShader( "vshader101.glsl", "fshader101.glsl" );
glUseProgram( program );

```cpp
// set up vertex arrays
```

GLuint vPosition = glGetAttribLocation( program, "vPosition" );
glEnableVertexAttribArray( vPosition );
glVertexAttribPointer( vPosition, 4, GL_FLOAT, GL_FALSE, 0,
BUFFER_OFFSET(0) );
Projection = glGetUniformLocation( program, "Projection" );
glPolygonMode( GL_FRONT_AND_BACK, GL_LINE );
glClearColor( 1.0, 1.0, 1.0, 1.0 );

```cpp
}
```

                                                                    A.10 Teapot Renderer   663

```cpp
//----------------------------------------------------------------------
void
```

display( void )

```cpp
{
```

glClear( GL_COLOR_BUFFER_BIT );
glDrawArrays( GL_TRIANGLES, 0, NumVertices );
glutSwapBuffers( void );

```cpp
}
//----------------------------------------------------------------------
void
```

reshape( int width, int height )

```cpp
{
```

glViewport( 0, 0, width, height );
GLfloat   left = -4.0, right = 4.0;
GLfloat   bottom = -3.0, top = 5.0;
GLfloat   zNear = -10.0, zFar = 10.0;
GLfloat   aspect = GLfloat(width)/height;
if ( aspect > 0 ) {
left *= aspect;
right *= aspect;

```cpp
}
```

else {
bottom /= aspect;
top /= aspect;

```cpp
}
```

mat4 projection = Ortho( left, right, bottom, top, zNear, zFar );
glUniformMatrix4fv( Projection, 1, GL_TRUE, projection );

```cpp
}
//----------------------------------------------------------------------
void
```

keyboard( unsigned char key, int x, int y )

```cpp
{
```

switch ( key ) {
case ’q’: case ’Q’: case 033 // Escape key:
exit( EXIT_SUCCESS );
break;

```cpp
}
}
//----------------------------------------------------------------------
```

664   Appendix A   Sample Programs

```cpp
int
```

main( int argc, char *argv[] )

```cpp
{
```

glutInit( &argc, argv );
glutInitDisplayMode( GLUT_RGBA | GLUT_DOUBLE );
glutInitWindowSize( 512, 512 );
glutInitContextVersion( 3, 2 );
glutInitContextProfile( GLUT_CORE_PROFILE );
glutCreateWindow( "teapot" );
glewInit( void );
init( void );
glutDisplayFunc( display );
glutReshapeFunc( reshape );
glutKeyboardFunc( keyboard );
glutMainLoop( void );
return 0;

```cpp
}
```

A.10.2 Vertex Shader

```cpp
#version 150
```

in   vec4 vPosition;
uniform mat4 Projection;

```cpp
void main()
{
```

gl_Position = Projection * vPosition;

```cpp
}
```

A.10.3 Fragment Shader

```cpp
#version 150
```

out vec4 fColor;

```cpp
void main()
{
```

fColor = vec4( 0.0, 0.0, 0.0, 1.0 );

```cpp
}
```

                                                                        AP P E NDI X         B
Computer graphics is concerned with the representation and manipulation of sets of
geometric elements, such as points and line segments. The necessary mathematics is
found in the study of various types of abstract spaces. In this appendix, we review the
rules governing three such spaces: the (linear) vector space, the afﬁne space, and the
Euclidean space. The (linear) vector space contains only two types of objects: scalars,
such as real numbers, and vectors. The afﬁne space adds a third element: the point.
Euclidean spaces add the concept of distance.
The vectors of interest in computer graphics are directed line segments and the n-
tuples of numbers that are used to represent them. In Appendix C, we discuss matrix
algebra as a tool for manipulating n-tuples. In this appendix, we are concerned with
the underlying concepts and rules. It is probably helpful to think of these entities
(scalars, vectors, points) as abstract data types, and the axioms as deﬁning the valid
operations on them.
B.1     SCALARS
Ordinary real numbers and the operations on them are one example of a scalar ﬁeld.
Let S denote a set of elements called scalars, α, β , . . . . Scalars have two fundamental
operations deﬁned between pairs. These operations are often called addition and
multiplication, and are symbolized by the operators + and .,1 respectively. Hence, for
∀α, β ∈ S, α + β ∈ S, and α . β ∈ S. These operations are associative, commutative,
and distributive, ∀α, β , γ ∈ S:
α + β = β + α,
α . β = β . α,
α + (β + γ ) = (α + β) + γ ,
1. Often, if there is no ambiguity, we can write αβ instead of α . β.
666        Appendix B      Spaces
α . (β . γ ) = (α . β) . γ ,
α . (β + γ ) = (α . β) + (α . γ ).
There are two special scalars—the additive identity (0) and the multiplicative identity
(1)—such that ∀α ∈ S:
α + 0 = 0 + α = α,
α . 1 = 1 . α = α.
Each element α has an additive inverse, denoted −α, and a multiplicative inverse,
denoted α −1 ∈ S, such that
α + (−α) = 0,
α . α −1 = 1.
The real numbers using ordinary addition and multiplication form a scalar ﬁeld, as
do the complex numbers (under complex addition and multiplication) and rational
functions (ratios of two polynomials).
B.2    VECTOR SPACES
A vector space, in addition to scalars, contains a second type of entity: vectors. Vec-
tors have two operations deﬁned: vector–vector addition and scalar–vector multi-
plication. Let u, v, w denote vectors in a vector space V . Vector addition is deﬁned
to be closed (u + v ∈ V , ∀u, v ∈ V ), commutative (u + v = v + u), and associative
(u + (v + w) = (u + v) + w). There is a special vector (the zero vector) 0 deﬁned
such that ∀u ∈ V :
u + 0 = u.
Every vector u has an additive inverse denoted by −u such that
u + (−u) = 0.
Scalar–vector multiplication is deﬁned such that, for any scalar α and any vector u,
αu is a vector in V . The scalar–vector operation is distributive. Hence,
α(u + v) = αu + αv,
(α + β)u = αu + βu.
The two examples of vector spaces that we use are geometric vectors (directed
FIGURE B.1 Directed line       line segments) and the n-tuples of real numbers. Consider a set of directed line
segments.                      segments that we can picture as shown in Figure B.1. If our scalars are real numbers,
                                                                                                 B.2 Vector Spaces                667
then scalar–vector multiplication changes the length of a vector, but not that vector’s
direction (Figure B.2).                                                                                   u
Vector–vector addition can be deﬁned by the head-to-tail axiom, which we can                                   u
visualize easily for the example of directed line segments. We form the vector u + v
by connecting the head of u to the tail of v, as shown in Figure B.3. You should be
able to verify that all the rules of a vector ﬁeld are satisﬁed.                                                         u
The second example of a vector space is n-tuples of scalars—usually, real or
complex numbers. Hence, a vector can be written as
FIGURE B.2 Scalar–vector
multiplication.
v = (v1, v2 , . . . , vn).
Scalar–vector multiplication and vector–vector addition are given by
u + v = (u1, u2 , . . . , un) + (v1, v2 , . . . , vn)
= (u1 + v1, u2 + v2 , . . . , un + vn),                                                               u+v             v
αv = (αv1, αv2 , . . . , αvn).
This space is denoted R n and is the vector space in which we can manipulate vectors                            u
using matrix algebra (Appendix C).
FIGURE B.3 Head-to-tail
In a vector space, the concepts of linear independence and basis are crucial. A               axiom for vectors.
linear combination of n vectors u1, u2 , . . . , un is a vector of the form
u = α1u1 + α2u2 + . . . + αnun .
α1u1 + α2u2 . . . + αnun = 0
α1 = α2 = . . . = αn = 0,
then the vectors are said to be linearly independent. The greatest number of linearly
independent vectors that we can ﬁnd in a space gives the dimension of the space. If a
vector space has dimension n, any set of n linearly independent vectors form a basis.
If v1, v2 , . . . , vn is a basis for V , any vector v can be expressed uniquely in terms of
v = β1v1 + β2v2 + . . . + βnvn .
The scalars {βi } give the representation of v with respect to the basis v1, v2 , . . . , vn.
If v1 , v2 , . . . , vn is some other basis (the number of vectors in a basis is constant),
there is a representation of v with respect to this basis; that is,
v = β1 v1 + β2 v2 + . . . + βn vn .
There exists an n × n matrix M such that
668         Appendix B     Spaces
⎡     ⎤    ⎡     ⎤
β1        β1
⎢ β ⎥     ⎢β ⎥
⎢ 2⎥       ⎢     ⎥
⎢ .. ⎥ = M ⎢ ..2 ⎥ .
⎣ . ⎦      ⎣ . ⎦
βN        βN
We derive M in Appendix C. This matrix gives a way of changing representations
through a simple linear transformation involving only scalar operations for carrying
out matrix multiplication. More generally, once we have a basis for a vector space,
we can work only with representations. If the scalars are real numbers, then we can
work with n-tuples of reals and use matrix algebra, instead of doing operations in the
original abstract vector space.
B.3    AFFINE SPACES
A vector space lacks any geometric concepts, such as location and distance. If we use
the example of directed line segments as the natural vector space for our geometric
problems, we get into difﬁculties because these vectors, just like the physicist’s vectors,
have magnitude and direction, but have no position. The vectors shown in Figure B.4
are identical.
If we think of this problem in terms of coordinate systems, we can express a
vector in terms of a set of basis vectors that deﬁne a coordinate system. Figure B.5(a)
FIGURE B.4 Identical vectors.   shows three basis vectors emerging from a particular reference point, the origin. The
location of the vectors in Figure B.5(b) is equally valid, however, because vectors have
no position. In addition, we have no way to express this special point, because our
vector space has only vectors and scalars as its members.
We can resolve this difﬁculty by introducing an afﬁne space that adds a third type
of entity—points—to a vector space. The points (P, Q, R, . . .) form a set. There is a
e2
e3
e1
e1
e2
e3
(a)                               (b)
FIGURE B.5 Coordinate system. (a) Basis vectors located at the origin.
(b) Arbitrary placement of basis vectors.
                                                                                        B.4 Euclidean Spaces            669
single new operation, point–point subtraction, that yields a vector. Hence, if P and
Q are any two points, the subtraction
v=P−Q
always yields a vector in V . Conversely, for every v and every P, we can ﬁnd a Q such
that the preceding relation holds. We can thus write
Q = v + P,
deﬁning a vector–point addition. A consequence of the head-to-tail axiom is that for
any three points P, Q, R,                                                                            PR         PQ
(P − Q) + (Q − R) = (P − R).
If we visualize the vector P − Q as the line segment from the point Q to the point P,                  QR
using an arrow to denote direction, the head-to-tail axiom can be drawn as shown in
Figure B.6.                                                                                   FIGURE B.6 Head-to-tail ax-
Various properties follow from afﬁne geometry. Perhaps the most important is             iom for points.
that if we use a frame, rather than a coordinate system, we can specify both points
and vectors in an afﬁne space. A frame consists of a point P0, and a set of vectors
v1, v2 , . . . , vn that deﬁnes a basis for the vector space. Given a frame, an arbitrary
v = α1v1 + α2v2 + . . . + αnvn ,
and an arbitrary point can be written uniquely as
P = P0 + β1v1 + β2v2 + . . . + βnvn .
The two sets of scalars, {α1, . . . , αn} and {β1, . . . , βn}, give the representations of
the vector and point, respectively, with each representation consisting of n scalars.
We can regard the point P0 as the origin of the frame; all points are deﬁned from this
reference point.
If the origin never changes, we can worry about only those changes of frames
corresponding to changes in coordinate systems. In computer graphics, however, we
usually have to deal with making changes in frames and with representing objects
in different frames. For example, we usually deﬁne our objects within a physical
frame. The viewer, or camera, can be expressed in terms of this frame, but, as part
of the image-creation process, it is to our advantage to express object positions with
respect to the camera frame—a frame whose origin usually is located at the center of
projection.
B.4    EUCLIDEAN SPACES
Although afﬁne spaces contain the necessary elements for building geometric models,
there is no concept of how far apart two points are, or of what the length of a vector
670   Appendix B   Spaces
is. Euclidean spaces have such a concept. Strictly speaking, a Euclidean space contains
only vectors and scalars.
Suppose that E is a Euclidean space. It is a vector space containing scalars
(α, β , γ , . . .) and vectors (u, v, w, . . .). We assume that the scalars are the ordinary
real numbers. We add a new operation—the inner (dot) product—that combines
two vectors to form a real. The inner product must satisfy the properties that, for any
three vectors u, v, w and scalars α, β,
u . v = v . u,
(αu + βv) . w = αu . w + βv . w,
v . v > 0 if v = 0,
0 . 0 = 0.
u . v = 0,
then u and v are orthogonal. The magnitude (length) of a vector is usually measured
√
|v| = v . v.
Once we add afﬁne concepts, such as points, to the Euclidean space, we naturally get
a measure of distance between points, because, for any two points P and Q, P − Q is
a vector, and hence

|P − Q| = (P − Q) . (P − Q).
We can use the inner product to deﬁne a measure of the angle between two vectors:
u . v = |u||v| cos θ .
It is easy to show that cos θ as deﬁned by this formula is 0 when the vectors are
orthogonal, lies between –1 and +1, and has magnitude 1 if the vectors are parallel
(u = αv).
B.5    PROJECTIONS
We can derive several of the important geometric concepts from the use of orthog-
onality. The concept of projection arises from the problem of ﬁnding the shortest
distance from a point to a line or plane. It is equivalent to the following problem.
Given two vectors, we can take one of them and divide it into two parts, one parallel
                                                                       B.6 Gram-Schmidt Orthogonalization                 671
and one orthogonal to the other vector, as shown in Figure B.7 for directed line seg-
ments. Suppose that v is the ﬁrst vector and w is the second. Then, w can be written
as                                                                                                   w        u
w = αv + u.
v              v
The parallel part is αv, but, for u to be orthogonal to v, we must have
FIGURE B.7 Projection of one
vector onto another.
u . v = 0.
Because u and v are deﬁned to be orthogonal,
w . v = αv . v + u . v = αv . v,
allowing us to ﬁnd
w .v
α=         .
v .v
The vector αv is the projection of w onto v, and
w .v
u=w−              v.
v .v
We can extend this result to construct a set of orthogonal vectors from an arbitrary
set of linearly independent vectors.
B.6     GRAM-SCHMIDT ORTHOGONALIZATION
Given a set of basis vectors, a1, a2 , . . . , an, in a space of dimension n, it is relatively
straightforward to create another basis b1, b2 , . . . , bn that is orthonormal, that is, a
basis in which each vector has unit length and is orthogonal to each other vector in
the basis, or mathematically:

1   if i = j,
bi bj =
.
0 otherwise.
Hence, there is no real loss of generality in using orthogonal (Cartesian) coordinate
systems.
We proceed iteratively. We look for a vector of the form
b2 = a2 + αb1,
which we can make orthogonal to b1 by choosing α properly. Taking the dot product,
b2 . b1 = 0 = a2 . b1 + αb1 . b1.
672   Appendix B   Spaces
Solving, we have
a .b
α=− 2 1
b1 . b1
a2 . b1
b 2 = a2 −           b1.
b1 . b1
We have constructed the orthogonal vector by removing the part parallel to b1—that
is, the projection of a2 onto b1.
The general iterative step is to ﬁnd a

k−1
bk = a k +         αi b i
i=1
that is orthogonal to b1, . . . , bk−1. There are k − 1 orthogonality conditions that
allow us to ﬁnd
a .b
αi = − k i .
bi . bi
We can normalize each vector, either at the end of the process, by replacing bi by
bi /|bi |, or, more efﬁciently, by normalizing each bi as soon as possible.
There are many excellent books on linear algebra and vector spaces. For practitioners
of computer graphics, the preferred approach is to start with vector-space ideas and
to see linear algebra as a tool for working with general vector spaces. Unfortunately,
most of the linear-algebra textbooks are concerned with only the Euclidean spaces of
n-tuples, R n. See Bowyer and Woodwark [Bow83] and Banchoff and Werner [Ban83].
Afﬁne spaces can be approached in a number of ways. See Foley [Fol90] for a
more geometric development.
B.1    Prove that the complex numbers form a scalar ﬁeld. What are the additive
and multiplicative identity elements?
B.2    Prove that the rational functions form a scalar ﬁeld.
B.3    Prove that the rational functions with real coefﬁcients form a vector space.
B.4    Prove that the number of elements in a basis is unique.
                                                                                          Exercises   673
B.5   Consider a set of n real functions {fi (x)}, i = 1, . . . , n. Show how to form a
vector space of functions with these elements. Deﬁne basis and dimension for
this space.
B.6   Show that the set of polynomials of degree up to n form an n-dimensional
vector space.
B.7   The most important Euclidean space is the space of n-tuples, a1, . . . , an: R n.
Deﬁne the operations of vector addition and scalar-vector multiplication in
this space. What is the dot product in R n?
B.8   Suppose that you are given three vectors in R 3. How can you ﬁnd whether
they form a basis?
B.9   Consider the three vectors in R 3: (1, 0, 0), (1, 1, 0), and (1, 1, 1). Show that
they are linearly independent. Derive an orthonormal basis from these vec-
tors, starting with (1, 0, 0).
This page intentionally left blank
                                                               AP P E NDI X                   C
In computer graphics, the major use of matrices is in the representation of changes in
coordinate systems and frames. In the studies of vector analysis and linear algebra, the
use of the term vector is somewhat different. Unfortunately, computer graphics relies
on both these ﬁelds, and the interpretation of vector has caused confusion. To remedy
this situation, we use the terms row matrix and column matrix, rather than the linear
algebra terms of row vector and column vector. We reserve the vector to denote directed
line segments, and occasionally, as in Appendix B, to denote the abstract-data-type
vector that is an element of a vector space.
This appendix reviews the major results that you will need to manipulate ma-
trices in computer graphics. We almost always use matrices that are 4 × 4. Hence,
the parts of linear algebra that deal with manipulations of general matrices, such as
the inversion of an arbitrary square matrix, are of limited interest. Most implemen-
tations, instead, implement inversion of 4 × 4 matrices directly in the hardware or
software.
C.1     DEFINITIONS
A matrix is an n × m array of scalars, arranged conceptually as n rows and m
columns. Often, n and m are referred to as the row and column dimensions of the
matrix, and, if m = n, we say that the matrix is a square matrix of dimension n.
We use real numbers for scalars, almost exclusively, although most results hold for
complex numbers as well. The elements of a matrix A are the members of the set of
scalars, {aij }, i = 1, . . . , n, j = 1, . . . , m. We write A in terms of its elements as
A = [ aij ] .
The transpose of an n × m matrix A is the m × n matrix that we obtain by inter-
changing the rows and columns of A. We denote this matrix as AT , and it is given
AT = [ aji ] .
676   Appendix C   Matrices
The special cases of matrices with one column (n × 1 matrix) and one row (1 ×
m matrix) are called column matrices and row matrices. We denote column matrices
with lowercase letters:
b = [ bi ] .
The transpose of a row matrix is a column matrix; we write it as bT .
C.2      MATRIX OPERATIONS
There are three basic matrix operations: scalar–matrix multiplication, matrix–matrix
addition, and matrix–matrix multiplication. You can assume that the scalars are
real numbers, although all these operations are deﬁned in the same way when the
elements of the matrices and the scalar multipliers are of the same type.
Scalar–matrix multiplication is deﬁned for any size matrix A; it is simply the
element-by-element multiplication of the elements of the matrix by a scalar α. The
αA = [ αaij ] .
We deﬁne matrix–matrix addition, the sum of two matrices, by adding the corre-
sponding elements of the two matrices. The sum makes sense only if the two matrices
have the same dimensions. The sum of two matrices of the same dimensions is given
C = A + B = [ aij + bij ] .
For matrix–matrix multiplication, the product of an n × l matrix A by an l × m
matrix B is the n × m matrix
C = AB = [ cij ] ,

cij =         aik bkj .
k=1
The matrix–matrix product is thus deﬁned only if the number of columns of A is
the same as the number of rows of B. We say that A premultiplies B, or that B
postmultiplies A.
Scalar–matrix multiplication obeys a number of simple rules that hold for any
matrix A, and for scalars α and β, such as
α(βA) = (αβ)A,
αβA = βαA,
                                                                               C.3 Row and Column Matrices   677
all of which follow from the fact that our matrix operations reduce to scalar multipli-
cations on the scalar elements of a matrix. For matrix–matrix addition, we have the
commutative property. For any n × m matrices A and B:
A + B = B + A.
We also have the associative property, which states that for any three n × m matrices
A, B, and C:
A + (B + C) = (A + B) + C.
Matrix–matrix multiplication, although associative,
A(BC) = (AB)C,
is almost never commutative. So not only is it almost always the case that AB = BA,
but also one product may not even be deﬁned when the other is. In graphics appli-
cations, where matrices represent transformations such as translation and rotation,
these results express that the order in which you carry out a sequence of transforma-
tions is important. A rotation followed by a translation is not the same as a translation
followed by a rotation. However, if we do a rotation followed by a translation followed
by a scaling, we get the same result if we ﬁrst combine the scaling and translation,
preserving the order, and then apply the rotation to the combined transformation.
The identity matrix I is a square matrix with 1s on the diagonal and 0s elsewhere:

1 if i = j;
I = [ aij ] ,    aij =
0 otherwise.
Assuming that the dimensions make sense,
AI = A,
IB = B.
C.3    ROW AND COLUMN MATRICES
The 1 × n and n × 1 row and column matrices are of particular interest to us. We can
represent either a vector or a point in three-dimensional space,1 with respect to some
frame, as the column matrix
⎡ ⎤
p=⎣y⎦.
1. The homogeneous-coordinate representation introduced in Chapter 3 distinguishes between the
representation of a point and the representation of a vector.
678   Appendix C   Matrices
We use lowercase letters for column matrices. The transpose of p is the row matrix
pT = [ x   y   z ].
Because the product of an n × l and an l × m matrix is an n × m matrix, the
product of a square matrix of dimension n and a column matrix of dimension n is a
new column matrix of dimension n. Our standard mode of representing transforma-
tions of points is to use a column matrix of two, three, or four dimensions to represent
a point (or vector), and a square matrix to represent a transformation of the point (or
vector). Thus, the expression
p = Ap
yields the representation of a transformed point (or vector), and expressions such as
p = ABCp
describe sequences, or concatenations, of transformations. Note that because the
matrix–matrix product is associative, we do not need parentheses in this expression.
Many graphics books prefer to use row matrices to represent points. If we do so,
using the fact that the transpose of a product can be written as
(AB)T = BT AT ,
then the concatenation of the three transformations can be written in row form as
pT = pT CT BT AT .
The professed advantage of this form is that, in English, we read the transformations
in the order in which they are performed; ﬁrst C, then B, then A. Almost all the
scientiﬁc, mathematics, and engineering literature, however, uses column matrices
rather than row matrices. Consequently, we prefer the column form. Although the
choice is conceptually simple, in practice you have to be careful regarding which one
your API is using, as not only is the order of transformations reversed, but also the
transformation matrices themselves must be transposed.
C.4    RANK
In computer graphics, the primary use of matrices is as representations of points and
of transformations. If a square matrix represents the transformation of a point or
vector, we are often interested in whether or not the transformation is reversible or
invertible. Thus, if
q = Ap,
we want to know whether we can ﬁnd a square matrix B such that
                                                                                  C.5 Change of Representation   679
p = Bq.
Substituting for q,
p = Bq = BAp = Ip = p
BA = I.
If such a B exists, it is the inverse of A, and A is said to be nonsingular. A noninvert-
ible matrix is singular. The inverse of A is written as A −1.
The fundamental result about inverses is as follows: The inverse of a square matrix
exists if and only if the determinant of the matrix is nonzero. Although the determinant
of A is a scalar, denoted by |A|, its computation, for anything but low-dimensional
matrices, requires almost as much work as does computation of the inverse. These
calculations are O(n3) for an n-dimensional matrix. For the two-, three-, and four-
dimensional matrices of interest in computer graphics, we can compute determinants
by Cramer’s rule and inverses using determinants, or we can use geometric reasoning.
For example, the inverse of a translation is a translation back, and thus the inverse of
a translation matrix must be a translation matrix. We pursue this course in Chapter 3.
For general nonsquare matrices, the concept of rank is important. We can regard
a square matrix as a row matrix whose elements are column matrices or, equivalently,
as a column matrix whose elements are row matrices. In terms of the vector-space
concepts of Appendix B, the rows of an n × m matrix are elements of the Euclidean
space R m, whereas the columns are elements of R n. We can determine how many rows
(or columns) are linearly independent. The row (column) rank is the maximum
number of linearly independent rows (columns), and thus for an n × n matrix, the
row rank and the column rank are the same and the matrix is nonsingular if and only
if the rank is n. Thus, a matrix is invertible if and only if its rows (and columns) are
linearly independent.
C.5     CHANGE OF REPRESENTATION
We can use matrices to represent changes in bases for any set of vectors satisfying
the rules of Appendix B. Suppose that we have a vector space of dimension n. Let

```cpp
{u1, u2 , . . . , un} and {v1, v2 , . . . , vn} be two bases for the vector space. Hence, a given
```

v = α1u1 + α2u2 + . . . + αnun
v = β1v1 + β2v2 + . . . + βnvn .
680   Appendix C   Matrices
Thus, (α1, α2 , . . . , αn) and (β1, β2 , . . . , βn) are two different representations of v,
and each can be expressed, equivalently, as a vector in the Euclidean space R n or as
a column matrix of dimension n. When we are working with representations, rather
than with the vectors, we have to be careful to make sure that our notation reﬂects
the difference. We write the representations of v as either
v = [ α 1 α2     ...    αn ]T
v  = [ β 1 β2    ...   βn ]T ,
depending on which basis we use.
We can now address the problem of how we convert from the representation v
to the representation v . The basis vectors {v1, v2 , . . . , vn} can be expressed as vectors
in the basis {u1, u2 , . . . , un}. Thus, there exists a set of scalars γij such that
ui = γi1v1 + γi2v2 + . . . + γinvn ,   i = 1, . . . , n.
We can write the expression in matrix form for all ui as
⎡ ⎤         ⎡ ⎤
u1           v1
⎢u ⎥        ⎢v ⎥
⎢ 2⎥        ⎢ 2⎥
⎢ . ⎥=A⎢ . ⎥,
⎣ .. ⎦      ⎣ .. ⎦
where A is the n × n matrix:
A = [ γij ] .
We can use column matrices to express both v and v  in terms of the vectors’ repre-
⎡ ⎤
u1
⎢u ⎥
⎢ ⎥
v = a T ⎢ .2 ⎥ ,
⎣ .. ⎦
a = [ αi ] .
We can deﬁne b as
b = [ βi ] ,
and we can write v  as
                                                                                                  C.6 The Cross Product   681
⎡   ⎤
v1
⎢v ⎥
⎢ ⎥
v  = bT ⎢ .2 ⎥ .
⎣ .. ⎦
The matrix A relates the two bases, so we ﬁnd by direct substitution that
bT = a T A.
The matrix A is the matrix representation of the change between the two bases.
It allows us to convert directly between the two representations. Equivalently, we
can work with matrices of scalars rather than with abstract vectors. For geometric
problems, although our vectors may be directed line segments, we can represent them
by sets of scalars, and we can represent changes of bases or transformations by direct
manipulation of these scalars.
C.6     THE CROSS PRODUCT
Given two nonparallel vectors, u and v, in a three-dimensional space, the cross prod-
uct gives a third vector, w, that is orthogonal to both. Regardless of the representation,
w . u = w . v = 0.
We can assign one component of w arbitrarily, because it is the direction of w,
rather than the length, that is of importance, leaving us with three conditions for the
three components of w. Within a particular coordinate system, if u has components
α1, α2 , α3, and v has components β1, β2 , β3, then, in this system, the cross product
is deﬁned as
⎡               ⎤
α2β3 − α3β2
w = u × v = ⎣ α3β1 − α1β3 ⎦ .
α1β2 − α2β1
Note that vector w is deﬁned by u and v; we use their representation only when
we wish to compute w in a particular coordinate system. The cross product gives a
consistent orientation for u × v. For example, consider the x-, y-, and z-axes as three
vectors that determine the three coordinate directions of a right-handed coordinate
system.2 If we use the usual x- and y-axes, the cross product x × y points in the
direction of the positive z-axis.
2. A right-handed coordinate system has positive directions determined by the thumb, index ﬁnger,
and middle ﬁnger of the right hand used for the x-, y-, and z-axes, respectively. Equivalently, on a
piece of paper, if positive x points left to right and positive y points bottom to top, then positive z
points out of the page.
682   Appendix C   Matrices
C.7     EIGENVALUES AND EIGENVECTORS
Square matrices are operators that transform column matrices into other column
matrices of the same dimension. Because column matrices can represent points and
vectors, we are interested in questions such as, When does a transformation leave a
point or vector unchanged? For example, every rotation matrix leaves a particular
point—the ﬁxed point—unchanged. Let’s consider a slightly more general problem.
Mu = λu
have a nontrivial solution for some scalar λ, that is, a solution with u not being a
matrix of zeros? If such a solution exists, then M transforms certain vectors u—its
eigenvectors—into scalar multiples of themselves. The values of λ for which this rela-
tionship holds are called the eigenvalues of the matrix. Eigenvalues and eigenvectors
are also called characteristic values and characteristic vectors, respectively. These
values characterize many properties of the matrix that are invariant under such op-
erations as changes in representation.
We can ﬁnd the eigenvalues by solving the equivalent matrix equation
Mu − λu = Mu − λIu = (M − λI)u = 0.
This equation can have a nontrivial solution if and only if the determinant3
|M − λI| = 0.
If M is n × n, then the determinant yields a polynomial of degree n in λ. Thus, there
are n roots, some of which may be repeated or complex. For each distinct eigenvalue,
we can then ﬁnd a corresponding eigenvector. Note that every multiple of an eigen-
vector is itself an eigenvector, so that we can choose an eigenvector with unit mag-
nitude. Eigenvectors corresponding to distinct eigenvalues are linearly independent.
Thus, if all the eigenvalues are distinct, then any set of eigenvectors corresponding to
the distinct eigenvalues form a basis for an n-dimensional vector space.
If there are repeated eigenvalues, the situation can be more complex. However,
we need not worry about these cases for the matrices we will use in graphics. Thus, if
R is a 3 × 3 rotation matrix and p = [ x y z ]T is the ﬁxed point, then
Rp = p.
Thus, every rotation matrix must have an eigenvalue of 1. This result is the same
whether we work in three dimensions or use the four-dimensional homogenous
coordinate representation in Chapter 3.
3. The general statement, known as the Fredholm alternative, states that the n linear equations in
n unknowns Ax = b have a unique solution if and only if |A| = 0. If |A| = 0, there are multiple
nontrivial solutions.
                                                                          C.8 Vector and Matrix Classes   683
Suppose that T is a nonsingular matrix. Consider the matrix
Q = T−1MT.
Its eigenvalues and eigenvectors are solutions of the equation
Qv = T−1MTv = λv.
But if we multiply by T, this equation becomes
MTv = λTv.
Thus, the eigenvalues of Q are the same as those of M, and the eigenvectors are
the transformations of the eigenvectors of M. The matrices M and Q are said to be
similar. Many of the transformations that arise in computer graphics involve simi-
lar matrices. One interpretation of this result is that changes of coordinate systems
leave fundamental properties, such as the eigenvalues, unchanged. If we can ﬁnd a
similarity transformation that converts M to a diagonal matrix Q, then the diagonal
elements of Q are the eigenvalues of both matrices.
Eigenvalues and eigenvectors have a geometric interpretation. Consider an ellip-
soid, centered at the origin, with its axes aligned with the coordinate axes. It can be
λ1x 2 + λ2y 2 + λ3z 2 = 1
for positive values of λ1, λ2 , and λ3 , or in matrix form,
⎡                 ⎤⎡ ⎤
λ1 0         0      x
[x    y      ⎣
z ] 0 λ2          0  ⎦ ⎣ y ⎦ = 1.
0 0          λ3     z
Thus, λ1, λ2 , and λ3 are both the eigenvalues of the diagonal matrix and the inverses
of the lengths of the major and minor axes of the ellipsoid. If we apply a change of
coordinate system through a rotation matrix, we create a new ellipsoid that is no
longer aligned with the coordinate axes. However, we have not changed the length
of axes of the ellipse, a property that is invariant under coordinate system changes.
C.8       VECTOR AND MATRIX CLASSES
Although we have avoided using the term vector for matrices of one row or one col-
umn, much of the literature prefers to use vector for such matrices. More problematic
for this book has been that GLSL uses vector this way. Consequently, we created sepa-
rate vector and matrix C++ classes to use with our examples. These classes are deﬁned
in the ﬁles vector.h and matrix.h that are included by the ﬁle Angel.h used in
all the examples.
684   Appendix C   Matrices
The vector class deﬁnes separate vec2, vec3, and vec4 types for one-, two-,
and three-dimensional vectors. It includes overloaded arithmetic operators for these
types and the usual constructors to create them and work with multiple types in a
single application. The classes are for general vectors of these dimensions and are not
specialized for homogeneous coordinates. We also include the standard functions for
normalization, cross products, dot products, and length.
The matrix class supports two-, three-, and four-dimensional square matrices
(mat2, mat3, and mat4) and overloads the standard arithmetic operators to support
their manipulation and operations between vectors and matrices. We also included
many of the functions that were in earlier versions of OpenGL and the deprecated.
These include most of the transformation and viewing functions. In most cases, we
used the same names as did OpenGL, for example, Rotate, Scale, Translate,
Ortho, Frustum, LookAt.
Some of the standard references on linear algebra and matrices include Strang [Str93]
and Banchoff and Werner [Ban83]. See also Rogers and Adams [Rog90] and the
Graphics Gems series [Gra90, Gra91, Gra92, Gra94, Gra95].
The issue of row versus column matrices is an old one. Early graphics books
[New73] used row matrices. The trend now is to use column matrices [Fol90], al-
though a few books still use row representations [Wat00]. Within the API, it may not
be clear which is being used, because the elements of a square matrix can be repre-
sented as a simple array of n2 elements. Certain APIs, such as OpenGL, allow only
postmultiplication of an internal matrix by a user-deﬁned matrix; others, such as
PHIGS, support both pre- and postmultiplication.
C.1     In R 3, consider the two bases {(1, 0, 0), (1, 1, 0), (1, 1, 1)} and {(1, 0, 0),
(0, 1, 0), (0, 0, 1)}. Find the two matrices that convert representations be-
tween the two bases. Show that they are inverses of each other.
C.2     Consider the vector space of polynomials of degree up to 2. Show that the
sets of polynomials {1, x, x 2} and {1, 1 + x, 1 + x + x 2} are bases. Give the
representation of the polynomial 1 + 2x + 3x 2 in each basis. Find the matrix
that converts between representations in the two bases.
C.3     Suppose that i, j, and k represent the unit vectors in the x-, y-, and z-
directions, respectively, in R 3. Show that the cross product u × v is given
             
 i    j k 
             
u × v =  u1 u2 u3  .
v v v 
1    2   3
                                                                                     Exercises   685
C.4   Show that, in R 3 ,
|u × v| = |u||v|| sin θ |,
where θ is the angle between u and v.
C.5   Find the eigenvalues and eigenvectors of the two-dimensional rotation matrix

cos θ − sin θ
R=                      .
sin θ   cos θ
C.6   Find the eigenvalues and eigenvectors of the three-dimensional rotation ma-
⎡                    ⎤
cos θ − sin θ 0
R = ⎣ sin θ    cos θ    0⎦.
0        0     1
This page intentionally left blank
                                                           AP P E NDI X               D
D.1    INITIALIZATION AND WINDOW FUNCTIONS

```cpp
void glutInit(int *argc, char **argv)
```

initializes GLUT. The arguments from main are passed in and can be used by the
application.

```cpp
void glewInit(void)
```

initializes GLEW if used by application.

```cpp
int glutCreateWindow(char *title)
```

creates a window on the display. The string title can be used to label the window.
The return value provides a reference to the window that can be used when there are
multiple windows.

```cpp
void glutInitDisplayMode(unsigned int mode)
```

requests a display with the properties in mode. The value of mode is determined by
the logical or of options including the color model (GLUT_RGB, GLUT_INDEX) and
buffering (GLUT_SINGLE, GLUT_DOUBLE).

```cpp
void glutInitWindowSize(int width, int height)
```

speciﬁes the initial height and width of the window in pixels.
688   Appendix D   Synopsis of OpenGL Functions

```cpp
void glutInitWindowPosition(int x, int y)
```

speciﬁes the initial position of the top-left corner of the window in pixels.

```cpp
void glViewport(int x, int y, GLsizei width, GLsizei height)
```

speciﬁes a width × height viewport in pixels whose lower-left corner is at (x, y)
measured from the origin of the window.

```cpp
void glutMainLoop()
```

causes the program to enter an event-processing loop. It should be the last statement
in main.

```cpp
void glutDisplayFunc(void (*func)(void))
```

registers the display function func that is executed when the window needs to be
redrawn.

```cpp
void glutPostRedisplay()
```

requests that the display callback be executed after the current callback returns.

```cpp
void glutSwapBuffers()
```

swaps the front and back buffers.

```cpp
void glFlush()
```

forces any buffered OpenGL commands to execute.

```cpp
void glutSetWindow(int id)
```

sets the current window to the window with identiﬁer id.
                                                                             D.2 Vertex Buffer Objects   689

```cpp
void glutContextVersion(int major_version, int minor_version)
```

sets the desired context, e.g., glutContextVersion(3, 1) for OpenGL 3.1. Only
available in freeglut.

```cpp
void glutContextProfile(init profile)
```

sets the desired context to either GLUT_CORE_PROFILE or GLUT_COMPATIBILITY_
PROFILE. Compatibility proﬁle allows backward compatibility. Only available with
freeglut.
D.2    VERTEX BUFFER OBJECTS

```cpp
void glGenVertexArrays(GLsizei n, GLuint *array)
```

generates n unused identiﬁers for vertex array objects in array.

```cpp
void glBindVertexArray(GLuint id)
```

creates a new vertex array object with identiﬁer id. Subsequent calls with the same
identiﬁer make that the active array.

```cpp
void glGenBuffers(GLsizei n, GLuint *buffer)
```

generates n unused identiﬁers for buffer objects in buffer.

```cpp
void glBindBuffer(GLenum target, GLint id)
```

creates a new buffer object with identiﬁer id. Subsequent calls with the same identi-
ﬁer make that the active buffer object. The type of buffer object is given by target.
Types include GL_ARRAY_BUFFER for vertex attribute data.

```cpp
void glBufferData(GLenum target, GLsizeiptr size, const GLvoid *data,
```

GLenum usage)
allocates size units of server memory for vertex array objects of type target
pointed to by data. Types include GL_ARRAY_BUFFER. The usage parameter spec-
iﬁes how the data will be read and includes GL_STATIC_DRAW and GL_DYNAMIC_
DRAW.
690   Appendix D   Synopsis of OpenGL Functions

```cpp
void glBufferSubData(GLenum target, GLintiptr offset, GLsizeiptr size,
const GLvoid *data)
```

updates size bytes starting at offset in the current buffer object with data of type
target starting at data.

```cpp
void glVertexAttrib[1234][sfd](GLunit index, TYPE data);
void glVertexAttrib[1234][sfd]v(GLunit index, TYPE *data);
```

speciﬁes values for vertex attributes with the given index.

```cpp
void glVertexAttribPointer(GLuint index, GLint size, GLenum type,
```

GLboolean normalized, GLsizei stride, const GLvoid* data)
points to data where vertex data of size components corresponding to index
are stored. Data are one of the standard types such as GL_INT and GL_FLOAT. If
normalized is set to GL_TRUE, the data will be normalized when stored. If stride
is set to 0, the data are assumed to be contiguous.

```cpp
void glEnableVertexAttribArray(GLuint index)
```

enables the vertex array with identiﬁer index.

```cpp
void glDrawArrays(GLenum mode, GLint first,          GLsizei count)
```

creates count elements of the standard OpenGL types mode, such as GL_TRIANGLES
or GL_LINES starting at first.
D.3    INTERACTION

```cpp
void glutMouseFunc(void *f(int button, int state, int x, int y))
```

registers the mouse callback function f. The callback function returns the button
(GLUT_LEFT_BUTTON, GLUT_MIDDLE_BUTTON, GLUT_RIGHT_BUTTON), the state
of the button after the event (GLUT_UP, GLUT_DOWN), and the position of the mouse
relative to the top-left corner of the window.
                                                                                         D.3 Interaction   691

```cpp
void glutReshapeFunc(void *f(int width, int height))
```

registers the reshape callback function f. The callback returns the height and width
of the new window. The reshape callback invokes a display callback.

```cpp
void glutKeyboardFunc(void *f(char key, int width, int height))
```

registers the keyboard callback function f. The callback function returns the ASCII
code of the key pressed and the position of the mouse.

```cpp
void glutIdleFunc(void (*f)(void))
```

registers the display callback function f that is executed whenever there are no other
events to be handled.

```cpp
int glutCreateMenu(void (*f)(int value))
```

returns an identiﬁer for a top-level menu and registers the callback function f that
returns an integer value corresponding to the menu entry selected.

```cpp
void glutSetMenu(int id)
```

sets the current menu to the menu with identiﬁer id.

```cpp
void glutAddMenuEntry(char *name, int value)
```

adds an entry with the string name displayed to the current menu. value is returned
to the menu callback when the entry is selected.

```cpp
void glutAttachMenu(int button)
```

attaches the current menu to the speciﬁed mouse button.

```cpp
void glutAddSubMenu(char *name, int menu)
```

adds a submenu entry name to the current menu. The value of menu is the identiﬁer
returned when the submenu was created.
692   Appendix D   Synopsis of OpenGL Functions

```cpp
void glutTimerFunc(int delay, void (*f)(int v), int value)
```

registers the timer callback function f and delays the event loop by delay millisec-
onds. After the timer counts down, f is executed with the parameter v. value is
available to f.

```cpp
void glutMotionFunc(void (*f)(int x, int y))
```

registers the motion callback function f. The position of the mouse is returned by the
callback when the mouse is moved at with least one of the mouse buttons pressed.

```cpp
void glutPassiveMotionFunc(void (*f)(int x, int y))
```

registers the motion callback function f. The position of the mouse is returned by the
callback when the mouse is moved.
D.4    SETTING ATTRIBUTES AND ENABLING FEATURES

```cpp
void glEnable(GLenum feature)
```

enables an OpenGL feature. Features that can be enabled include GL_DEPTH_
TEST, GL_TEXTURE_1D, GL_TEXTURE_2D, GL_TEXTURE_3D, GL_LINE_SMOOTH,
GL_POLYGON_SMOOTH, GL_POINT_SMOOTH, GL_BLEND.

```cpp
void glDisable(GLenum feature)
```

disables an OpenGL feature.

```cpp
void glPolygonMode(glEnum faces, glEnum mode)
```

sets the desired mode for polygon rendering the faces (GL_FRONT_AND_BACK). mode
can be GL_POINTS, GL_LINES, or GL_FILL.

```cpp
void glClearColor(GLclampf r, GLclampf g, GLclampf b, GLclampf a)
```

sets the present RGBA clear color used when clearing the color buffer. Variables of
type GLclampf are ﬂoating-point numbers between 0.0 and 1.0.
                                                                      D.5 Texture and Image Functions   693

```cpp
void glPointSize(GLfloat size)
```

sets the point size attribute in pixels.

```cpp
void glPolygonOffset(GLfloat factor, GLfloat units)
```

offsets polygon depths by a linear combination of factor and units. The multi-
plicative constants in the computation depend on the slope of the polygon and the
precision of the depth values.
glDepthMask(GLboolean flag)
sets flag to make the depth buffer read-only (GL_FALSE) or writable (GL_TRUE).

```cpp
void glBlendFunc(GLenum source, GLenum destination)
```

sets the source and destination blending factors. Options include GL_ONE, GL_
ZERO, GL_SRC_COLOR, GL_SRC_ALPHA, GL_ONE_MINUS_SRC_COLOR, GL_ONE_
MINUS_SRC_ALPHA, GL_DST_COLOR, GL_ONE_MINUS_DST_COLOR, GL_DST_
ALPHA, GL_ONE_MINUS_DST_ALPHA.
D.5    TEXTURE AND IMAGE FUNCTIONS
glTexImage2D[ui us f]v(GLenum target, GLint level, GLint iformat,
GLsizei width, GLsizei height, GLint border, GLenum format,
GLenum type, GLvoid *texels)
sets up a two-dimensional texture of height × width texels of type and format.
The array texels is of format iformat. A border of 0 or 1 texels can be speciﬁed.
glTexParameter[if](GLenum target, GLenum param, TYPE value)
glTexParameter[if]v(GLenum target, GLenum param, TYPE *value)
sets the texture parameter param to value for texture of type target
(GL_TEXTURE_1D, GL_TEXTURE_2D, or GL_TEXTURE_3D).
glGenTextures(GLsizei n, GLuint name)
returns in name the ﬁrst integer of n unused integer for texture-object identiﬁers.
694   Appendix D   Synopsis of OpenGL Functions
glBindTexture(GLenum target, GLuint name)
binds name to texture of type target (GL_TEXTURE_1D, GL_TEXTURE_2D, GL_
TEXTURE_3D, GL_TEXTURE_CUBE_MAP).
glDeleteTextures(GLsizei n, GLuint *namearray)
deletes n texture objects from the array namearray that holds texture-object names.
D.6    STATE AND BUFFER MANIPULATION

```cpp
void glDrawBuffer(GLenum buffer)
```

selects the color buffer buffer for rendering.

```cpp
void glLogicOp(GLenum op)
```

selects one of the 16 logical writing modes if the feature GL_COLOR_LOGIC_OP is
enabled. Modes include replacement (GL_COPY), the default, and exclusive or (GL_
XOR).
glReadPixels(GLint x, GLint y, GLsizei width, GLsizei height,
GLenum format, GLenum type, GLvoid *image)
reads a width × height rectangle of pixels from the present read buffer starting at
x, y into the array image. The pixels are in the speciﬁed format in the read buffer
and written as the speciﬁed data type.
glPixelStore[if](GLenum param, TYPE value)
sets the pixel store parameter param to value. Parameters include GL_UNPACK_
SWAP_BYTES, GL_PACK_SWAP_BYTES, GL_PACK_ALIGNMENT, GL_UNPACK_
ALIGNMENT.
D.7    QUERY FUNCTIONS

```cpp
void glGetBooleanv(GLenum name, GLboolean *param)
void glGetIntegerv(GLenum name, GLinteger *param)
void glGetFloatv(GLenum name, GLfloat *param)
```

                                                                               D.8 GLSL Functions   695

```cpp
void glGetDoublev(GLenum name, GLdouble *param)
void glGetPointerv(GLenum name, GLvoid **param)
```

writes the present value of the parameter name into param.

```cpp
int glutGet(GLenum state)
```

returns the current value of a GLUT state variable such as GLUT_WINDOW_WIDTH,
GLUT_WINDOW_HEIGHT, GLUT_ELAPSED_TIME.
D.8    GLSL FUNCTIONS
GLuint glCreateProgram()
creates an empty program object and returns an identiﬁer for it.
GLuint glCreateShader(GLenum type)
creates an empty shader object of type GL_VERTEX_SHADER or GL_FRAGMENT_
SHADER and returns an identiﬁer for it.

```cpp
void glShaderSource(GLuint shader, GLsizei nstrings, const GLchar
```

**strings, const GLint *lengths)
identiﬁes the source code for shader as coming from an array of nstrings
strings of lengths characters. If the shader is a single null-terminated string,
then nstrings is 1 and lengths is NULL.

```cpp
void glCompileShader(GLuint shader)
```

compiles shader object shader.

```cpp
void glAttachShader(GLunit program, GLuint shader)
```

attaches shader object shader to program object program.

```cpp
void glLinkProgram(GLuint program)
```

links together the application and shaders in program object program.
696   Appendix D   Synopsis of OpenGL Functions

```cpp
void glUseProgram(GLuint program)
```

makes program the active program object.
GLint glGetAttribLocation(GLuint program, const GLchar *name)
returns the index of the attribute name from the linked program object name.

```cpp
void glVertexAttrib[1234][sfd](GLuint index, TYPE value1,
```

TYPE value2,...)

```cpp
void glVertexAttrib[123][sfd]v(GLuint index, TYPE *value)
```

speciﬁes the value of the vertex attribute with the speciﬁed index.
GLint glGetUniformLocation(GLuint program, const GLchar *name)
returns the index of uniform variable name from the linked program object pro-
gram.

```cpp
void glUniform1234[if](GLint index, TYPE value)
void glUniform1234[if]v(GLint index, GLsizei num, TYPE value)
void glUniformMatrix[234]f(GLint index, GLsizei num,
```

GLboolean transpose, const GLfloat *value)
sets the value of a uniform variable, array, or matrix with the speciﬁed index. For
the array and matrix, num is the number of elements to be changed.

```cpp
void glGetProgram(GLuint program, GLenum pname, GLinit *param)
```

returns in param the value of parameter pname for program object program. Param-
eters include link status GL_LINK_STATUS, which returns GL_TRUE or GL_FALSE,
and GL_INFO_LOG_LENGTH, which returns the number of characters in the infor-
mation log.

```cpp
void glGetShaderiv(GLuint shader, GLenum pname, GLint *param)
```

returns in param the value of parameter pname for shader object shader. Param-
eters include compile status GL_COMPILE_STATUS, which returns GL_TRUE or GL_
FALSE, and GL_INFO_LOG_LENGTH, which returns the number of characters in the
information log.
                                                                               D.8 GLSL Functions   697

```cpp
void getProgramInfoLog(GLuint program, GLsizei maxL, GLsizei *len,
```

GLchar *infoLog)
returns the info log string for program object program into the array infoLog of
lenth maxL and the length of the string in len.

```cpp
void getShaderInfoLog(GLuint program, GLsizei maxL, GLsizei *len,
```

GLchar *infoLog)
returns the info log string for shader object program into the array infoLog of
length maxL and the length of the string in len.
This page intentionally left blank
                                                              RE FE RE NCES
Ado85    Adobe Systems Incorporated, PostScript Language Reference Manual,
Addison-Wesley, Reading, MA, 1985.
Ake88    Akeley, K., and T. Jermoluk, “High Performance Polygon Rendering,”
Computer Graphics, 22(4), 239–246, 1988.
Ake93    Akeley, K., “Reality Engine Graphics,” Computer Graphics, 109–116,
1993.
Ang90    Angel, E., Computer Graphics, Addison-Wesley, Reading, MA, 1990.
Ang08    Angel, E., OpenGL, A Primer, Third Edition, Addison-Wesley, Reading,
MA, 2008.
ANSI85   American National Standards Institute (ANSI), American National Stan-
dard for Information Processing Systems—Computer Graphics—Graphical
Kernel System (GKS) Functional Description, ANSI, X3.124-1985, ANSI,
New York, 1985.
ANSI88   American National Standards Institute (ANSI), American National Stan-
dard for Information Processing Systems—Programmer’s Hierarchical In-
teractive Graphics System (PHIGS), ANSI, X3.144-1988, ANSI, New York,
1988.
App68    Appel, A., “Some Techniques for Shading Machine Renderings of Solids,”
Spring Joint Computer Conference, 37–45, 1968.
Arn96    Arnold, K., and J. Gosling, The Java Programming Language, Addison-
Wesley, Reading, MA, 1996.
Ban83    Banchoff, T., and J. Werner, Linear Algebra Through Geometry, Springer-
Verlag, New York, 1983.
Bar93    Barnsley, M., Fractals Everywhere, Second Edition, Academic Press, San
Diego, CA, 1993.
Bar83    Barsky, B.A., and C. Beatty, “Local Control of Bias and Tension in Beta-
Splines,” ACM Transactions on Graphics, 2(2), 109–134, 1983.
Bar87    Bartels, R.H., C. Beatty, and B.A. Barsky, An Introduction to Splines for
Use in Computer Graphics and Geometric Modeling, Morgan Kaufmann,
Los Altos, CA, 1987.
Bli76    Blinn, J.F., and M.E. Newell, “Texture and Reﬂection in Computer
Generated Images,” CACM, 19(10), 542–547, 1976.
700   References
Bli77    Blinn, J.F., “Models of Light Reﬂection for Computer-Synthesized Pic-
tures,” Computer Graphics, 11(2), 192–198, 1977.
Bow83    Bowyer, A., and J. Woodwark, A Programmer’s Geometry, Butterworth,
London, 1983.
Bre65    Bresenham, J.E., “Algorithm for Computer Control of a Digital Plotter,”
IBM Systems Journal, January, 25–30, 1965.
Bre87    Bresenham, J.E., “Ambiguities in Incremental Line Rastering,” IEEE
Computer Graphics and Applications, May, 31–43, 1987.
Car78    Carlbom, I., and J. Paciorek, “Planar Geometric Projection and Viewing
Transformations,” Computing Surveys, 10(4), 465–502, 1978.
Cas96    Castleman, K.C., Digital Image Processing, Prentice-Hall, Englewood
Cliffs, NJ, 1996.
Cat78a   Catmull, E., “A Hidden-Surface Algorithm with Antialiasing,” Computer
Graphics, 12(3), 6–11, 1978.
Cat78b   Catmull, E., and J. Clark, “Recursively Generated B-Spline Surfaces on
Arbitrary Topological Meshes,” Proceedings of Computer-Aided Design,
10, 350-355, 1978.
Cha98    Chan, P., and R. Lee, The Java Class Libraries: Java.Applet, Java.Awt,
Java.Beans (Vol. 2), Addison-Wesley, Reading, MA, 1998.
Che95    Chen, S.E., “Quicktime VR: An Image-Based Approach to Virtual Envi-
ronment Navigation,” Computer Graphics, 29–38, 1995.
Che00    Chen, K.L. et al., “Building and Using a Scalable Display Wall System,”
IEEE Computer Graphics and Applications, 20(4), 29–37, 2000.
Cla82    Clark, J.E., “The Geometry Engine: A VLSI Geometry System for Graph-
ics,” Computer Graphics, 16, 127–133, 1982.
Coh85    Cohen, M.F., and D.P. Greenberg, “The Hemi-Cube: A Radiosity So-
lution for Complex Environments,” Computer Graphics, 19(3), 31–40,
1985.
Coh88    Cohen, M.F., S.E. Chen, J.R. Wallace, and D.P. Greenberg, “A Progressive
Reﬁnement Approach to Fast Radiosity Image Generation,” Computer
Graphics, 22(4), 75–84, 1988.
Coh93    Cohen, M.F., and J.R. Wallace, Radiosity and Realistic Image Synthesis,
Academic Press Professional, Boston, MA, 1993.
Coo82    Cook, R.L., and K.E. Torrance, “A Reﬂectance Model for Computer
Graphics,” ACM Transactions on Graphics, 1(1), 7–24, 1982.
Coo87    Cook, R.L., L. Carpenter, and E. Catmull, “The Reyes Image Rendering
Architecture,” Computer Graphics, 21(4), 95–102, July 1987.
Cro81    Crow, F.C., “A Comparison of Antialiasing Techniques,” IEEE Computer
Graphics and Applications, 1(1), 40–48, 1981.
Cro97    Crossno, P.J., and E. Angel, “Isosurface Extraction Using Particle Sys-
tems,” IEEE Visualization, 1997.
                                                                                      References   701
Deb96    Debevec, P.E., C.J. Taylor, and J. Malik, “Modeling and Rendering
Architecture from Photographs: A Hybrid Geometry- and Image-Based
Approach,” Computer Graphics, 11–20, 1996.
deB08    deBerg, M., C. Otfried, M. van Kreveld, and M. Overmars, Computa-
tional Geometry, Third Edition, Springer-Verlag, Berlin Heidelberg, 2008.
DeR88    DeRose, T.D., “A Coordinate-Free Approach to Geometric Program-
ming,” SIGGRAPH Course Notes, SIGGRAPH, 1988.
DeR89    DeRose, T.D., “A Coordinate-Free Approach to Geometric Program-
ming,” in Theory and Practice of Geometric Modeling, W. Strasser and
H.P. Seidel (Eds.), Springer-Verlag, Berlin, 1989.
Dre88    Drebin, R.A., L. Carpenter, and P. Hanrahan, “Volume Rendering,”
Computer Graphics, 22(4), 65–74, 1988.
Ebe01    Eberly, D.H., 3D Game Engine Design, Morgan Kaufmann, San Francisco,
2001.
Ebe02    Ebert, D., F.K. Musgrave, D. Peachey, K. Perlin, and S. Worley, Texturing
and Modeling, A Procedural Approach, Third Edition, Morgan Kaufman,
San Francisco, 2002.
End84    Enderle, G., K. Kansy, and G. Pfaff, Computer Graphics Programming:
GKS—The Graphics Standard, Springer-Verlag, Berlin, 1984.
Far88    Farin, G., Curves and Surfaces for Computer Aided Geometric Design,
Academic Press, New York, 1988.
Fau80    Faux, I.D., and M.J. Pratt, Computational Geometry for Design and
Manufacturing, Halsted, Chichester, England, 1980.
Fern03   Fernando, R., and M.J. Kilgard, The Cg Tutorial: The Deﬁnitive Guide to
Programmable Real-Time Graphics, Addison-Wesley, Reading, MA, 2003.
Fern04   Fernando, R., GPU Gems: Programming Techniques, Tips, and Tricks for
Real-Time Graphics, Addison-Wesley, Reading, MA, 2004.
Fol90    Foley, J.D., A. van Dam, S.K. Feiner, and J.F. Hughes, Computer Graphics,
Second Edition, Addison-Wesley, Reading, MA, 1990 (C Version 1996).
Fol94    Foley, J.D., A. van Dam, S.K. Feiner, J.F. Hughes, and R. Phillips, Intro-
duction to Computer Graphics, Addison-Wesley, Reading, MA, 1994.
Fos97    Fosner, R., OpenGL Programming for Windows 95 and Windows NT,
Addison-Wesley, Reading, MA, 1997.
Fou82    Fournier, A., D. Fussell, and L. Carpenter, “Computer Rendering of
Stochastic Models,” CACM, 25(6), 371–384, 1982.
Fuc77    Fuchs, H., J. Duran, and B. Johnson, “A System for Automatic Acquisi-
tion of Three-Dimensional Data,” Proceedings of the 1977 NCC, AFIPS
Press, 49–53, Montvale, NJ, 1977.
Fuc80    Fuchs, H., Z.M. Kedem, and B.F. Naylor, “On Visible Surface Generation
by A Priori Tree Structures,” SIGGRAPH 80, 124–133, 1980.
Gal95    Gallagar, R.S., Computer Visualization: Graphics Techniques for Scientiﬁc
and Engineering Analysis, CRC Press, Boca Raton, FL, 1995.
702   References
Gla89   Glassner, A.S. (Ed.), An Introduction to Ray Tracing, Academic Press, New
York, 1989.
Gla95   Glassner, A.S., Principles of Digital Image Synthesis, Morgan Kaufmann,
San Francisco, 1995.
Gon08   Gonzalez, R., and R.E. Woods, Digital Image Processing, Third Edition,
Addison-Wesley, Reading, MA, 2008.
Gor84   Goral, C.M., K.E. Torrance, D.P. Greenberg, and B. Battaile, “Modeling
the Interaction of Light Between Diffuse Surfaces,” Computer Graphics
(SIGGRAPH 84), 18(3), 213–222, 1984.
Gor96   Gortler, S.J., R. Grzeszczuk, R. Szeliski, and M.F. Cohen, “The Lumi-
graph,” Computer Graphics, 43–54, 1996.
Gou71   Gouraud, H., “Computer Display of Curved Surfaces,” IEEE Trans.
Computers, C-20, 623–628, 1971.
Gra90   Graphics Gems I, Glassner, A.S. (Ed.), Academic Press, San Diego, CA,
1990.
Gra91   Graphics Gems II, Arvo, J. (Ed.), Academic Press, San Diego, CA, 1991.
Gra92   Graphics Gems III, Kirk, D. (Ed.), Academic Press, San Diego, CA, 1992.
Gra94   Graphics Gems IV , Heckbert, P. (Ed.), Academic Press, San Diego, CA,
1994.
Gra95   Graphics Gems V , Paeth, A. (Ed.), Academic Press, San Diego, CA, 1995.
Gra03   Gray, K., The Microsoft DirectX 9 Programmable Graphics Pipeline, Mi-
crosoft Press, 2003.
Gre88   Greengard, L.F., The Rapid Evolution of Potential Fields in Particle Systems,
MIT Press, Cambridge, MA, 1988.
Hal89   Hall, R., Illumination and Color in Computer Generated Imagery,
Springer-Verlag, New York, 1989.
Har96   Hartman, J., and J. Wernecke, The VRML 2.0 Handbook, Addison-
Wesley, Reading, MA, 1996.
Hea11   Hearn, D., M.P. Baker, and W.R. Carithers, Computer Graphics, Fourth
Edition, Prentice-Hall, Englewood Cliffs, NJ, 2011.
Hec84   Heckbert, P.S., and P. Hanrahan, “Beam Tracing Polygonal Objects,”
Computer Graphics, 18(3), 119–127, 1984.
Hec86   Heckbert, P.S., “Survey of Texture Mapping,” IEEE Computer Graphics
and Applications, 6(11), 56–67, 1986.
Her79   Herman, G.T., and H.K. Liu, “Three-Dimensional Display of Human
Organs from Computed Tomograms,” Computer Graphics and Image
Processing, 9, 1–21, 1979.
Her00   Hereld, M., I.R. Judson, and R.L. Stevens, “Tutorial: Introduction
to Building Projection-Based Tiled Display Systems,” IEEE Computer
Graphics and Applications, 20(4), 22–26, 2000.
                                                                                      References   703
Hil07    Hill, F.S., Jr., and S.M. Kelley,, Computer Graphics, Third Edition, Pren-
tice Hall, Upper Saddle River, NJ, 2007.
Hop83    Hopgood, F.R.A., D.A. Duce, J.A. Gallop, and D.C. Sutcliffe, Introduction
to the Graphical Kernel System: GKS, Academic Press, London, 1983.
Hop91    Hopgood, F.R.A., and D.A. Duce, A Primer for PHIGS, John Wiley &
Sons, Chichester, England, 1991.
Hum01    Humphreys, G., M. Elridge, I. Buck, G. Stoll, M. Everett, and P. Han-
rahan, “WireGL: A Scalable Graphics System for Clusters,” SIGGRAPH
2001.
ISO88    International Standards Organization, International Standard Informa-
tion Processing Systems—Computer Graphics—Graphical Kernel System
for Three Dimensions (GKS-3D), ISO Document Number 8805:1988(E),
American National Standards Institute, New York, 1988.
Jar76    Jarvis, J.F., C.N. Judice, and W.H. Ninke, “A Survey of Techniques for
the Image Display of Continuous Tone Pictures on Bilevel Displays,”
Computer Graphics and Image Processing, 5(1), 13–40, 1976.
Jen01    Jensen, H.W., “Realistic Image Synthesis Using Photon Mapping,” A K
Peters, 2001.
Joy88    Joy, K.I., C.W. Grant, N.L. Max, and L. Hatﬁeld, Computer Graphics:
Image Synthesis, Computer Society Press, Washington, DC, 1988.
Kaj86    Kajiya, J.T., “The Rendering Equation,” Computer Graphics, 20(4), 143–
150, 1986.
Kel97    Keller, H., “Instant Randiosity,” Computer Graphics, 49–56, 1997.
Kil94a   Kilgard, M.J., “OpenGL and X, Part 3: Integrated OpenGL with Motif,”
The X Journal, SIGS Publications, July/August 1994.
Kil94b   Kilgard, M.J., “An OpenGL Toolkit,” The X Journal, SIGS Publications,
November/December 1994.
Kil96    Kilgard, M.J., OpenGL Programming for the X Windows System, Addison-
Wesley, Reading, MA, 1996.
Knu87    Knuth, D.E., “Digital Halftones by Dot Diffusion,” ACM Transactions on
Graphics, 6(40), 245–273, 1987.
Kov97    Kovatch, P.J., The Awesome Power of Direct3D/DirectX, Manning Publi-
cations Company, Greenwich, CT, 1997.
Kue08    Kuehhne, R.P., and J.D. Sullivan, OpenGL Programming on Mac OS X,
Addison-Wesley, Boston, MA, 2008.
Kui99    Kuipers, J.B., Quaternions and Rotation Sequences, Princeton University
Press, Princeton, NJ, 1999.
Las87    Lasseter, J., “Principles of Traditional Animation Applied to 3D Com-
puter Animation,” Computer Graphics, 21(4), 33–44, 1987.
Lev88    Levoy, M., “Display of Surface from Volume Data,” IEEE Computer
Graphics and Applications, 8(3), 29–37, 1988.
704   References
Lev96   Levoy, M., and P. Hanrahan, “Light Field Rendering,” Computer Graph-
ics, 31–42, 1996.
Lia84   Liang, Y., and B. Barsky, “A New Concept and Method for Line Clipping,”
ACM Transactions on Graphics, 3(1), 1–22, 1984.
Lin68   Lindenmayer, A., “Mathematical Models for Cellular Interactions in
Biology,” Journal of Theoretical Biology, 18, 280–315, 1968.
Lin01   Linholm, E., M.J. Kilgard, and H. Morelton, “A User-Programmable
Vertex Engine,” SIGGRAPH, 2001.
Lor87   Lorensen, W.E., and H.E. Cline, “Marching Cubes: A High Resolution
3D Surface Construction Algorithm,” Computer Graphics, 21(4), 163–
169, 1987.
Ma94    Ma, K.L., J. Painter, C. Hansen, and M. Krogh, “Parallel Volume Ren-
dering Using Binary-Swap Compositing,” IEEE Computer Graphics and
Applications, 14(4), 59–68, 1994.
Mag85   Magnenat-Thalmann, N., and D. Thalmann, Computer Animation: The-
ory and Practice, Springer-Verlag, Tokyo, 1985.
Man82   Mandelbrot, B., The Fractal Geometry of Nature, Freeman Press, New
York, 1982.
Mat95   The MathWorks, Student Edition of MatLab Version 4 Users Guide,
Prentice-Hall, Englewood Cliffs, NJ, 1995.
Mau06   Maurina, E., The Game Programmer’s Guide to Torque, A K Peters, 2006.
Max51   Maxwell, E.A., General Homogeneous Coordinates in Space of Three
Dimensions, Cambridge University Press, Cambridge, England, 1951.
Mia99   Miamo, J., Compressed Image File Formats, ACM Press, New York, 1999.
Mol92   Molnar, S., J. Eyles, and J. Poulton, “PixelFlow: High-Speed Rendering
Using Image Composition,” Computer Graphics, 26(2), 231–240, 1992.
Mol94   Molnar, S., M. Cox, D. Ellsworth, and H. Fuchs, “A Sorting Classiﬁcation
of Parallel Rendering,” IEEE Computer Graphics and Applications, 26(2),
231–240, 1994.
Mol02   Möller, T., and E. Haines, Real-Time Rendering, Second, Edition, A K
Peters, 2002.
Mon97   Montrym, J., D. Baum, D. Dignam, and C. Migdal, “InﬁniteReality: A
Real-Time Graphics System,” Computer Graphics, 293–392, 1997.
Mur94   Murray, J.D., and W. Van Ryper, Encyclopedia of Graphics File Formats,
O’Reilly and Associates, Sebastopol, CA, 1994.
New73   Newman, W.M., and R.F. Sproull, Principles of Interactive Computer
Graphics, McGraw-Hill, New York, 1973.
Ngu07   Nguyen, H. (Ed), GPU Gems 3, Addison-Wesley Professional, Boston,
MA, 2007.
                                                                                    References   705
Nie97   Nielson, G.M., H. Hagen, and H. Muller, Scientiﬁc Visualization:
Overviews, Methodologies, and Techniques, IEEE Computer Society, Pis-
cataway, NJ, 1997.
Ope05   OpenGL Architecture Review Board, OpenGL Reference Manual, Fourth
Edition, Addison-Wesley, Reading, MA, 2005.
OSF89   Open Software Foundation, OSF/Motif Style Guide, Prentice-Hall, Engle-
wood Cliffs, NJ, 1989.
Ost94   Osterhaut, J., Tcl and the Tk Toolkit, Addison-Wesley, Reading, MA, 1994.
Pap81   Papert, S., LOGO: A Language for Learning, Creative Computer Press,
Middletown, NJ, 1981.
Pav95   Pavlidis, T., Interactive Computer Graphics in X, PWS Publishing, Boston,
MA, 1995.
Per85   Perlin, K. “An Image Synthesizer,” Computer Graphics, 19(3), 287–297,
1985.
Per89   Perlin, K., and E. Hoffert, “Hypertexture,” Computer Graphics, 23(3),
253–262, 1989.
Per02   Perlon, K., “Improved Noise,” Computer Graphics, 35(3), 2002.
Pei88   Peitgen, H.O., and S. Saupe (Eds.), The Science of Fractal Images,
Springer-Verlag, New York, 1988.
Pha05   Pharr, M., and R. Fernando (Eds.), GPU Gems 2: Programming Tech-
niques for High-Performance Graphics and General-Purpose Computation,
Addison-Wesley Professional, Boston, MA, 2005.
PHI89   PHIGS+ Committee, “PHIGS+ Functional Description, Revision 3.0,”
Computer Graphics, 22(3), 125–218, July 1989.
Pho75   Phong, B.T., “Illumination for Computer Generated Scenes,” Communi-
cations of the ACM, 18(6), 311–317, 1975.
Por84   Porter, T., and T. Duff, “Compositing Digital Images,” Computer Graph-
ics, 18(3), 253–259, 1984.
Pra78   Pratt, W.K., Digital Image Processing, Wiley, New York, 1978.
Pru90   Prusinkiewicz, P., and A. Lindenmayer, The Algorithmic Beauty of Plants,
Springer-Verlag, Berlin, 1990.
Ree83   Reeves, W.T., “Particle Systems—A Technique for Modeling a Class of
Fuzzy Objects,” Computer Graphics, 17(3), 359–376, 1983.
Rei05   Reinhard, Erik, G. Ward, S. Pattanaik, and P. Debevec, High Dynamic
Range Imaging: Acquisition, Display, and Image-Based Lighting, Morgan
Kaufmann, San Francisco, 2005.
Rey87   Reynolds, C.W., “Flocks, Herds, and Schools: A Distributed Behavioral
Model,” Computer Graphics, 21(4), 25–34, 1987.
Rie81   Riesenfeld, R.F., “Homogeneous Coordinates and Projective Planes in
Computer Graphics,” IEEE Computer Graphics and Applications, 1(1),
50–56, 1981.
706   References
Rob63    Roberts, L.G., “Homogenous Matrix Representation and Manipulation
of N-Dimensional Constructs,” MS-1505. MIT Lincoln Laboratory,
Lexington, MA, 1963.
Rog98    Rogers, D.F., Procedural Elements for Computer Graphics, Second Edition,
McGraw-Hill, New York, 1998.
Rog90    Rogers, D.F., and J.A. Adams, Mathematical Elements for Computer
Graphics, McGraw-Hill, New York, 1990.
Rog00    Rogers, D.F., An Introduction to NURBS: With Historical Perspective,
Morgan Kaufmann, San Francisco, CA, 2000.
Ros09    Rost, R.J., B. Licea-Kane, D. Ginsberg, and J.M. Kessenich, OpenGL
Shading Language, Third Edition, Addison-Wesley, Reading, MA, 2009.
Sch88    Schieﬂer, R.W., J. Gettys, and R. Newman, X Window System, Digital
Press, Woburn, MA, 1988.
Schr06   Schroeder, W., K. Martin, and B. Lorensen, The Visualization Toolkit:
An Object-Oriented Approach to 3D Graphics, Fourth Edition, Kitware,
Clifton Park, NY, 2006.
Sei96    Seitz, S.M., and C.R. Dyer, “View Morphing,” Computer Graphics, 21–30,
1996.
Seg92    Segal, M., and K. Akeley, The OpenGL Graphics System: A Speciﬁcation,
Version 1.0, Silicon Graphics, Mountain View, CA, 1992.
Shi03    Shirley, P., R.K. Morley, and K. Morley, Realistic Ray Tracing, Second
Edition, A K Peters, 2003.
Shi05    Shirley, P., M. Ashikhmin, M. Gleicher, S. Marschner, E. Reinhard, K.
Sung, W. Thompson, and P. Willemsen, Fundamentals of Computer
Graphics, Second Edition, A K Peters, 2005.
Sho85    Shoemake, K., “Animating Rotation with Quaternion Curves,” Computer
Graphics, 19(3), 245–254, 1985.
Shr10    Shreiner, D., OpenGL Programming Guide, Seventh Edition, Addison-
Wesley, Reading, MA, 2010.
Sie81    Siegel, R., and J. Howell, Thermal Radiation Heat Transfer, Hemisphere,
Washington, DC, 1981.
Sil89    Sillion, F.X., and C. Puech, “A General Two-Pass Method Integrating
Specular and Diffuse Reﬂection,” Computer Graphics, 22(3), 335–344,
1989.
Smi84    Smith, A.R., “Plants, Fractals and Formal Languages,” Computer Graph-
ics, 18(3), 1–10, 1984.
Sta03    Stam, J., and C. Loop, “Quad/Triangle Subdivision,” Computer Graphics
Forum, 22, 1–7, 2003.
Str93    Strang, G., Introduction to Linear Alegbra, Wellesley-Cambridge Press,
Wellesley, MA, 1993.
Suf07    Suffern, K., Ray Tracing from the Ground Up, A K Peters, 2007.
                                                                                     References   707
Sut63    Sutherland, I.E., Sketchpad, A Man–Machine Graphical Communication
System, SJCC, 329, Spartan Books, Baltimore, MD, 1963.
Sut74a   Sutherland, I.E., and G.W. Hodgeman, “Reentrant Polygon Clipping,”
Communications of the ACM, 17, 32–42, 1974.
Sut74b   Sutherland, I.E., R.F. Sproull, and R.A. Schumacker, “A Characterization
of Ten Hidden-Surface Algorithms,” Computer Surveys, 6(1), 1–55, 1974.
Swo00    Swoizral, H., K. Rushforth, and M. Deering, The Java 3D API Speciﬁca-
tion, Second Edition, Addison-Wesley, Reading, MA, 2000.
Tor67    Torrance, K.E., and E.M. Sparrow, “Theory for Off–Specular Reﬂection
from Roughened Surfaces,” Journal of the Optical Society of America,
57(9), 1105–1114, 1967.
Tor96    Torborg, J., and J.T. Kajiya, “Talisman: Commodity Realtime 3D Graph-
ics for the PC,” Computer Graphics, 353–363, 1996.
Tuf83    Tufte, E.R., The Visual Display of Quantitative Information, Graphics
Press, Cheshire, CT, 1983.
Tuf90    Tufte, E.R., Envisioning Information, Graphics Press, Cheshire, CT, 1990.
Tuf97    Tufte, E.R., Visual Explanations, Graphics Press, Cheshire, CT, 1997.
Ups89    Upstill, S., The RenderMan Companion: A Programmer’s Guide to Realistic
Computer Graphics, Addison-Wesley, Reading, MA, 1989.
Van94    Van Gelder, A., and J. Wilhelms, “Topological Considerations in Isosur-
face Generation,” ACM Transactions on Graphics, 13(4), 337–375, 1994.
War94    Ward, G., “The RADIANCE Lighting Simulation and Rendering System,”
Computer Graphics, July 1994.
War03    Warren, J., and H. Weimer, Subdivision Methods for Geometric Design,
Morgan Kaufmann, San Francisco, 2003.
War04    Warren, J., and S. Schaefer, “A Factored Approach to Subdivision Sur-
faces,” IEEE Computer Graphics and Applications, 24(3), 74-81, 2004.
Wat92    Watt, A., and M. Watt, Advanced Animation and Rendering Techniques,
Addison-Wesley, Wokingham, England, 1992.
Wat98    Watt, A., and F. Policarpo, The Computer Image, Addison-Wesley, Wok-
ingham, England, 1998.
Wat00    Watt, A., 3D Computer Graphics, Third Edition, Addison-Wesley, Wok-
ingham, England, 2000.
Wat02    Watkins, A., The Maya 4 Handbook, Charles River Media, Hingham, MA,
2002.
Wer94    Wernecke, J., The Inventor Mentor, Addison-Wesley, Reading, MA, 1994.
Wes90    Westover, L., “Footprint Evaluation for Volume Rendering,” Computer
Graphics, 24(4), 367–376, 1990.
Whi80    Whitted, T., “An Improved Illumination Model for Shaded Display,”
Communications of ACM, 23(6), 343–348, 1980.
708   References
Wit94a   Witkin, A.P., and P.S. Heckbert, “Using Particles to Sample and Control
Implicit Surfaces,” Computer Graphics, 28(3), 269–277, 1994.
Wit94b   Witkin, A. (Ed.), An Introduction to Physically Based Modeling, Course
Notes, SIGGRAPH 94, 1994.
Wol91    Wolfram, S., Mathematica, Addison-Wesley, Reading, MA, 1991.
Wri11    Wright, R.S., Jr., H. Haemel, G. Sellers, and B. Lipchak, OpenGL Super-
bible, Fifth Edition, Addison-Wesley, Boston, MA, 2011.
Wys82    Wyszecki, G., and W.S. Stiles, Color Science, Wiley, New York, 1982.
       OPEN GL FU NCTI ON I NDEX
This index includes the appearances of those OpenGL functions that are used in
the text. A complete description of OpenGL functions is contained in the OpenGL
Programming Guide and the GLUT documentation. The functions are grouped
according to their inclusion in the GL or GLUT libraries. Functions with multiple
forms, such as glUniform, are listed once. Appendix D contains a synopsis of these
functions.
GL Functions                           glDrawBuffer, 693                       glLineWidth, 66
glAttachShader, 695                    glEnable, 97, 241, 366, 406, 692        glLinkProgram, 695
glBindBuffer, 84, 689                  glEnableVertexAttribArray, 87, 690      glLogicOp, 366, 694
glBindTexture, 376, 693                glewInit, 687                           glNormal, 274
glBindVertexArray, 83–84, 689          glFlush, 85, 688                        glPixelStore, 694
glBlendFunc, 406, 693                  glGenBuffers, 84, 689                   glPointSize, 73, 692
glBufferData, 84, 689                  glGenerateMipmap, 383                   glPolygonMode, 59, 692
glBufferSubData, 96, 689               glGenTextures, 375, 693                 glPolygonOffset, 246, 692
glClear, 71, 97, 241                   glGenVertexArrays, 83–84, 689           glReadPixels, 694
glClearColor, 71, 73, 692              glGetAttribLocation, 695                glShaderSource, 695
glColor, 66                            glGetAttributeLocation, 87              glStipple, 66
glCompileShader, 695                   glGetBooleanv, 694                      glTexImage2D, 376, 390, 392, 693
glCreateProgram, 694                   glGetDoublev, 694                       glTexParameter, 693
glCreateShader, 695                    glGetFloatv, 586, 694                   glTexParameteri, 381–383
glDeleteTextures, 376, 693             glGetIntegerv, 350, 694                 glUniform1234, 696
glDepthFunc, 337                       glGetPointerv, 694                      glUniformif, 179
glDepthMask, 407, 692                  glGetProgram, 696                       glUniformMatrix, 696
glDisable, 410, 692                    glGetShaderiv, 696                      glUseProgram, 695
glDrawArrays, 61, 84–85, 427, 432,     glGetUniformLocation, 179, 696          glVertexAttribPointer, 87
447, 690                         glGetUniformMatrix4fv, 179
710         OpenGL Function Index
GLUT Functions                      glutGetModiﬁers, 103                glutMotionFunc, 691
glutAddMenuEntry, 107, 691          glutIdleFunc, 103, 691              glutMouseFunc, 99, 690
glutAddSubMenu, 691                 glutInit, 79, 687                   glutPassiveMotionFunc, 692
glutAttachMenu, 107, 691            glutInitDisplayMode, 79, 97, 105,   glutPostDisplay, 106
glutContextProﬁle, 689                    410, 687                      glutPostRedisplay, 101, 104, 688
glutContextVersion, 688             glutInitWindowPosition, 79, 688     glutReshapeFunc, 690
glutCreateMenu, 106, 691            glutInitWindowSize, 79–80, 687      glutSetMenu, 691
glutCreateWindow, 79, 106, 687      glutKeyboardFunc, 102, 690          glutSetWindow, 106, 688
glutDisplayFunc, 82, 106, 688       glutKeyboardUpFunc, 102             glutSwapBuffers, 105, 688
glutFlush, 105                      glutMainLoop, 81, 98, 688           glutTimerFunc, 691
glutGet, 694
                                                  SU BJ E CT I NDEX
A                                        overview, 342–344                  applications, 2
absolute input device positioning,       in rendering, 413–421                design, 3
11–12                             in texture generation, 371           display of information, 2–3
abstract data types (ADTs)             alpha blending, 404                    lighting models in, 286–289
geometric, 119–120                   alpha channels                         simulation and animation, 3–4
overview, 119                          antialiasing, 407–409                user interfaces, 4–5
accepted primitives in clipping, 310     compositing, 404                   approximating spheres
addition                                 RGBA system, 70                      fans and strips, 60–62
afﬁne, 121–122                       ambient light, 262–263, 283            recursive subdivision, 280–283
matrix-matrix, 676–677               ambient reﬂection, 267               arcade games, 3
point-vector, 117–118, 120           American Standard Code for           architectures
scalars, 118                                Information Interchange         graphics, 33–37
vector-vector, 118, 120, 666–668            (ASCII), 10                     pipeline. See pipeline architectures
additive color models, 68              angles                               area averaging
addressing schemes on Internet, 453      Euler, 184–185                       antialiasing by, 343
ADTs (abstract data types)               incidence, 274                       in sampling, 417
geometric, 119–120                     joint, 430–431, 442                areas, screen, 180–181
overview, 119                          reﬂection, 274–275                 arithmetic pipelines, 34–35
afﬁne addition, 121–122                  rotation, 170–171                  arms, robot, 429–432
afﬁne spaces, 665                        twist, 215                         arrays
mathematical view, 118–119             view, 21–22, 30, 229                 knot, 530
origins, 128                         animation                              texture, 376
overview, 668–669                      applications, 3–4                    vertex, 146
afﬁne transformations                    hierarchical models, 441–442       articulation, 441
concatenation, 164–172               antialiasing. See aliasing and       ASCII (American Standard Code for
overview, 152–155                           antialiasing                         Information Interchange), 10
agent-based modeling, 500              apertures, sampling, 418             aspect ratio
algebraic surfaces, 506, 545–546       applets, 454                           synthetic-camera model, 30
algorithmic models, 465–467            application coordinate systems, 55     and viewports, 79–80
aliasing and antialiasing              application programming interfaces   associative property, 677
alpha channel, 407–410                      (APIs), 26                    at points, 30, 212–213
area averaging, 343                    OpenGL. See OpenGL API             attenuation of light, 284
fragment processing, 309               three-dimensional, 28–31           attractive forces for Newtonian
Nyquist criteria, 415–416              viewing, 209–212                          particles, 472–473
712           Subject Index
attractors for Mandelbrot set, 494      Bernstein polynomials, 522–523              color, 6, 69–70
attributes                              Bezier curves and surfaces, 520–521         depth, 6, 240, 358
color, 72–73                            geometry matrices, 521–522                double, 105
functions, 52                           patches, 523–524, 542–544                 for gaskets, 84
graphical objects, 443–444              subdivision, 541–542                      OpenGL functions, 693–694
OpenGL functions, 692–693               Utah teapot, 542–544                      source, 362
primitives, 65–67                     Bezier polynomials, 537–539                 stencil, 359
axis-aligned bounding boxes, 318        bicubic surface patches, 515, 517           working with, 357–359
axonometric views, 198–200              billboarding, 255                           writing into, 362–366
azimuth, 214–215                        binary spatial-partition trees (BSP         XOR operations, 365–366
trees), 457–459                      z, 240
B                                       binary-swap compositing, 585–586          bump maps, 32, 38, 396–397
B-splines                               binary-tree compositing, 585                example, 400–404
Catmull-Rom, 534–535                  binormal vectors, 398                       ﬁnding, 397–400
control points, 528                   bit-block-transfer (bitblt) operations,     texture mapping, 367
cubic curves, 524–529, 540–541               64, 362                            byte code, 454
general, 529–530                      bit blocks, 64, 362
geometry matrices, 526                bit operations, 6                         C
nonuniform, 532                       bitmaps for characters, 64                CAD (computer-aided design), 3
NURBS, 532–533                        bitplanes in buffers, 358                 callback functions
patches, 529                          black box package, 51                       display, 82, 101
recursively-deﬁned, 530–531           blending                                    idle, 103–105
subdivision curves, 547                 alpha, 404                                input devices, 14
surfaces, 528–529                       for Bezier curves, 522                    keyboard, 102–103
uniform, 532                            in compositing techniques, 404–405        menu, 106
back buffers, 105                         interpolation, 513–515                    mouse, 99–100, 106
back clipping planes, 231                 OpenGL, 406–407                           reshape, 102
back-end processing, 39                   patches, 517                            calligraphic CRTs, 8
back-face removal, 334–335              Blinn-Phong models, 271, 563              cameras and camera models, 17,
back-to-front painting, 594             bound attributes, 66                             72–73
back-to-front rendering                 bounding boxes                              frames, 129, 141–144, 196, 204–209
in compositing, 409–410                 axis-aligned, 318                         orthographic views, 74–77
painter’s algorithm, 340                for clipping, 318                         pinhole, 20–22
backing stores, 365                     Bresenham’s algorithm, 325–327              positioning, 204–215
backward in-order traversal, 459        brightness in human visual systems,         speciﬁcations, 29–30
balancing, load, 581                           22–23                                synthetic-camera model, 23–25, 30
band-limited functions, 415–416         Brownian motion, 490–491                  canonical view volumes, 218
barycentric coordinates, 124            browsers, 454                             cartographers, 2
basic tenet of three-color theory, 68   BSP trees (binary spatial-partition       cast rays, 561. See also ray tracing
basis splines, 528, 530–531                    trees), 457–459                    cathode-ray tubes (CRTs)
basis vectors, 126, 129–133, 668        buffers, 5–6                                additive color systems for, 68
beat patterns, 350, 416                   back and front, 105                       overview, 7–9
bellows camera, 23                        clipping in, 319                          reconstruction for, 419
                                                                                       Subject Index          713
Catmull-Clark subdivision, 548–549       collisions                              inward- and outward-pointing
Catmull-Rom splines, 535                   and clipping, 318                         faces, 146–147
center of projection (COP)                 particle systems, 476–479, 482      colored noise, 497
light, 259–260                         color                                 column matrices, 676–678
synthetic-camera model, 24, 196          attributes, 72–73                   commutative property of matrices,
central processing units (CPUs), 6–7,      blending, 408–409                         677
289                                 buffers, 6, 69–70                   complementary colors, 68
centroids, 548                             cone, 348                           complex numbers, 116
CERN (European Particle Physics            current, 66                           Mandelbrot set, 493–494
Center), 453–454                    direct volume rendering, 596          and quaternions, 186–187

```cpp
characteristic values and vectors, 682     dithering and halftoning, 349–350   components of vectors, 126–127
```

choice input devices, 13                   frame buffers for, 6, 69–70         compositing techniques, 404
chromaticity coordinates, 346–347          gamma correction, 349                 antialiasing, 407–410
CIE Lab color system, 346                  gamut, 69, 345                        back-to-front and front-to-back
clamping, 381                              human visual system response, 23          rendering, 409–410

```cpp
classes, matrix and vector, 144–145,       indexed, 69, 71–72                    images, 406, 411–412
```

683–684                             light sources, 262                    multipath methods, 412

```cpp
classical viewing, 197–199                 lighting, 283, 286–287                opacity and blending, 404–405
```

clear color, 72–73                         matrices, 348–349                     OpenGL, 406–407
clip coordinates, 47, 141                  overview, 67–69                       sort-last rendering, 585–586
clipped objects, 77, 229–230               palette, 71                         compressed images, 360
clipping, 310                              particles, 481                      compression ratios, 360
Cohen-Sutherland, 310–315                pen-plotter models, 28              computed tomography (CT), 2
in frame buffers, 319                    ray tracing, 561                    computer-aided design (CAD), 3
Liang-Barsky, 313–315                    RGB, 69–71                          concatenating transformations,
line-segment, 310–314                    systems, 345–348                          164–172, 678
polygon, 314–317                       color-lookup tables, 71               concave polygons
primitives, 36–37, 317–319             color solids, 68–69                     clipping, 315
projections, 229–230                   colorcube function, 149                 rasterization, 329
three dimensions, 319–322                perspective projections, 622        cones
clipping planes, 231                       rotating cube with rotation in        color, 348
clipping rectangles                             shader, 617                      eye, 22–23
camera models, 24–25                     rotating cube with texture, 640     conservation laws, 571
two-dimensional viewing, 77              rotating shaded cube, 627           constant shading, 276–277
clipping volumes, 36–37, 318               spinning cubes, 177                 constraints in particle systems,
clouds, 500, 556                           with tree traversal, 649                  476–479
clusters, 459                            colored cubes, 146                    constructive solid geometry (CSG)
CMY color system, 68                       code, 148–150                         primitives, 126
CMYK color system, 346                     data structures, 147–148              trees, 455–456
coefﬁcient of restitution, 477–478         displaying, 151–152                 contact forces in collisions, 479
Cohen-Sutherland clipping                  face modeling, 146                  continuity
three dimensions, 320                    interactive viewers, 224–226          curves, 509
two dimensions, 310–315                  interpolation, 150–151                parametric and geometric, 519–520
714          Subject Index
continuous frequency light sources,     Cramer’s rule, 679                       general B-splines, 529–535
17                               cross products, 122–123, 681             Hermite, 517–520
control functions, 52, 78               crossing tests, 328                      interpolating, 510–517
aspect ratio and viewports, 79–80     CRTs (cathode-ray tubes) displays        Koch, 485–489
main, display, and init, 80–83          additive color systems for, 68         parametric cubic polynomial,
program structure, 83                   overview, 7–9                              510–511
window system interaction, 78–79        reconstruction for, 419                rendering, 510, 535–542
control points                          CSG (constructive solid geometry)        representation, 503–509
Bezier polynomials, 537–538             primitives, 126                        segments, 508
cubic B-spline curves, 525–527          trees, 455–456                         space-ﬁlling, 111
curves, 510                           CT (computed tomography), 2              subdivision, 546–550
geometry matrices of, 540             CTMs (current transformation             Utah teapot, 542–544
convex hulls                                  matrices), 173–175               cylinders, 372–373
Bezier polynomials, 537–538           cube class, 446
deﬁned, 122                           cube maps, 393–396                     D
convex objects, 59, 122                 cubes                                  DAGs (directed acyclic graphs), 429
convex polygons, 568                      colored. See colored cubes           damping term in Hooke’s law, 472
convolution matrices, 411–412             creating, 445–447                    dark ﬁeld intensity, 349
coordinates and coordinate systems        implementing, 448                    data abstraction, 119
afﬁne spaces, 668                       marching, 591–594                    data gloves, 12
changing, 129–133                       rotating. See rotating cube          data sets, volumetric, 588–589
chromaticity, 346–347                       programs                         data structures
clip, 47                                spinning, 176–180                      objects, 147–148, 445–446
coordinate-free geometry, 117–118     cubic B-spline curves, 524–529           polygonal shading, 278
frames, 140–141                       cubic Bezier patches, 542–544            trees, 437–441, 455–456
geometric objects, 126–139            cubic Bezier surfaces, 541–542         data tablets, 12
homogeneous, 133–136, 159–164         cubic interpolating patches, 515–517   DDA algorithm, 323–324
object, 55, 140                       culling, 241                           decaling technique, 383
right-handed, 123                       back-facing polygons, 334–335        deCasteljau recursion, 530
screen, 309                             faces, 147                           decimation, triangle, 594
synthetic-camera model, 30              occlusion, 451–452                   decision variables, 325
texture, 368, 376–382, 384–386        current color, 66                      degrees of freedom
transformations in, 159–164           current textures, 375                    input devices, 12
two-dimensional applications,         current transformation matrices          transformations, 153–154
55–56                                  (CTMs), 173–175                  Delaunay triangulation, 63, 551–555
viewing, 210                          curvatures, 373, 504, 544              delta form factors, 577
window, 55, 78–79, 141, 308           curves, 503                            dependent variables, 503
COP (center of projection)                Bezier. See Bezier curves and        depth buffers, 6, 240, 358
light, 259–260                              surfaces                         depth of ﬁeld, 21, 412–413
synthetic-camera model, 24, 196         clipping, 319                        depth sorts, 340–342
Cox-deBoor recursion, 530–531             cubic B-splines, 524–529             derivatives of curves, 509
CPUs (central processing units), 6–7,     design criteria, 509–510             design of curves, 509–510
289                                fractal, 490–491                     destination bits in buffers, 363
                                                                                            Subject Index            715
destination blending, 405                   digital images, 359–362                distant light sources, 264–265
destination buffers, 362                    environment maps, 388–393              distortion in texture mapping, 373
detection of collisions, 477–478            mapping methods, 366–368               distributive operations, 665–666
determinants, matrix, 679                   texture generation, 387–388            dithering, 349–350
device-independent graphics, 55             texture mapping. See texture           divide_curve function, 543, 660
devices                                         mapping                            divide_patch function, 544, 661
coordinates, 55                           writing into buffers, 362–366          divide_tetra function, 94
input, 9–13                             displacement functions, 397              divide_triangle function, 89–90
output, 7–9                             display, 344                               per-fragment lighting of sphere,
pointing, 9–10, 98–101                    callback functions, 82                        633
dicing, 579                                 color matrix, 348–349                    Sierpinski gaskets, 613
differences                                 color systems, 345–348                   sphere approximation, 282
forward, 536–537                          colored cubes, 151–152                 division
sets, 456                                 dithering and halftoning, 349–350        midpoint, 490–491
differential equations, 474–475             gamma correction, 349                    perspective, 141, 229
diffuse-diffuse interactions, 572           historical overview, 2–3                 subdivision. See subdivision
diffuse reﬂection, 267–269                  output devices, 7–9                    DLP (digital light projection) systems,
diffuse surfaces, 261                       particles, 480–481                            9
digital halftones, 350                      processors, 34                         DOP (direction of projection), 74,
digital images, 359–362                   display function                                196
digital light projection (DLP) systems,     callback, 82                           dot products, 122–123, 670
9                                    ﬁgure with tree traversal, 653         double buffering, 105
dimensions                                  Mandelbrot sets, 496                   drag term in Hooke’s law, 472
fractal, 489–490                          parallel projections, 224              drivers, 26
matrices, 675                             particle systems, 481                  dual-ported memory, 323
vector space, 123, 667                    per-fragment lighting of sphere,       duplicating pixels, 64–65
dimetric views, 199                             635–636                            dynamics, inverse, 442
diminution of size, 201                     perspective projections, 623–624
direct volume rendering, 589,               robot arm, 431                         E
595–600                              rotating cube with rotation in         edges
directed acyclic graphs (DAGs), 429             shader, 618                          graphs, 428
directed graphs, 428–429                    rotating cube with texture, 643          polygons, 59–60
directed line segments. See vectors         rotating shaded cube, 628                silhouette, 294
direction angles, 170–171                   shadows, 252                           eigenvalues, 184, 682–683
direction cosines, 170–171                  Sierpinski gaskets, 88, 611–612, 614   eigenvectors, 184, 682–683
direction in ﬂat shading, 276               spinning cubes, 180                    elastic collisions, 477–478
direction of projection (DOP), 74,          teapot renderer, 663                   electromagnetic radiation, 17
196                                  three-dimensional gaskets, 91, 97      elevation in views, 214
DirectX, 51, 452                            tree structures, 440                   enabling functions, 692–693
discrete techniques, 357                  display lists, 34                        Encapsulated PostScript (EPS)
buffers overview, 357–359               distance                                        images, 360–361
bump maps, 396–404                        Euclidean spaces, 670                  encoding information, 453
compositing, 404–413                      ﬂat shading, 276                       energy conservation, 571
716          Subject Index
energy ﬂux, 570                         ﬁeld of view, 21–22, 30, 231               rulers and length, 488
energy function, 479                    ﬁgure function, 434–437                  fragment shaders
environment maps, 32, 367, 388–396      ﬁll areas, 58                              gaskets, 85–88, 92, 96, 612
EPS (Encapsulated PostScript)           ﬁlled primitives, 66                       texture mapping, 375–380
images, 360–361                  ﬁlls in polygon rasterization, 329–330   fragments
Euclidean space, 665                    ﬁlm plane, 30                              overlapping, 407–409
description, 118                      ﬁlters, linear, 382–384, 411–412           processing, 37, 309
overview, 669–670                     ﬁxed points                              frame buffers, 5–6, 358
R3, 129                                 rotating about, 165–166, 175             clipping in, 319
Euler angles, 184–185                     transformations, 156                     RGB color, 69–70
Euler’s identity, 186                   ﬂat-panel displays, 8                    frames
Euler’s method, 474–476                 ﬂat shading, 276–277                       afﬁne spaces, 669
European Particle Physics Center        ﬂight simulators, 3–4                      Frenet, 507
(CERN), 453–454                  ﬂipping operation, 554–555                 geometric objects, 126–139
event-driven input                      ﬂocking in particle systems, 483–484       OpenGL, 139–144
idle callbacks, 103–105               ﬂood-ﬁll algorithm, 330                  freeglut, 607
keyboard, 102–103                     ﬂuids, particle movement in, 490–491     Frenet frames, 507
pointing devices, 98–101              focal length, 30                         frequency, sampling, 414–416
window, 101–102, 106                  fonts, 64–65                             frequency-domain representation,
event modes, 14                         footprints, 596–597                             414
event processing, 81                    forces                                   frequency spectrum, 414–416
event queues, 14, 81                      collisions, 479                        front buffers, 105
events, 14, 81                            particle systems, 468–473, 483         front clipping planes, 231
exclusive OR operations, 365–366          spring, 471–472                        front-end processing, 308
exit function, 98                       foreshortening                           front-to-back rendering
explicit representation of curves and     line segments, 171, 199–200              compositing, 409–410
surfaces, 503–505                  nonuniform, 227                          painter’s algorithm, 340
eye frames, 129, 141                    form factors for radiosity, 573,         frustums, 230–231
eye points, 212–213                            575–577                           full-color systems, 6
eyes, 22–23                             forward differences, 536–537             functions
four-color systems                         callback. See callback functions
F                                         CMYK, 346                                control, 78–83
faces                                     RGBA, 70                                 OpenGL. See OpenGL API
cube, 146                             Fourier analysis, 414, 416               fuzzy objects, 500
inward- and outward-pointing,         Fourier synthesis, 497
146–147                          fractal mountains, 110                   G
polyhedrons, 567                      fractals and fractal geometry, 467,      games and game consoles, 3–4, 9
principal, 197                               487–488                           gamma correction, 349
facets, 146                               dimensions, 489–490                    gamut, color, 69, 345
fans, 60–62                               Mandelbrot set, 493–496                gaskets
far distances in synthetic-camera         midpoint division and Brownian           Sierpinski. See Sierpinski gaskets
model, 30                               motion, 490–491                     three-dimensional, 43, 91–98
feeler rays, 561                          mountains, 492                         Gauss-Seidel method, 574
                                                                                        Subject Index           717
Gaussian approximations, 419–420       GLEW (OpenGL Extension                    interaction, 98–106
Gaussian elimination method, 574              Wrangler) library, 53              menus, 106–107
Gaussian random numbers, 491           global illumination, 297–298              OpenGL, 50–56
general rotation, 167–168              GLSL (OpenGL Shading Language),           polygon recursion, 88–90
geometric continuity, 519–520                 85                                 primitives, 56–65
geometric objects, 115                   functions, 694–696                      Sierpinski gaskets, 43–46
ADTs, 119–120                          noise in, 498                           three-dimensional gaskets, 91–98
afﬁne sums, 121–122                  GLUT (OpenGL Utility Toolkit),            two-dimensional applications,
colored cube modeling, 146–152              53–54, 78                              46–50
computer science view, 119           Gouraud shading, 277–279                  views, 73–77
convexity, 122                       GPUs (graphics processing units), 7,    graphics systems and models, 1
coordinate-free geometry, 117–118           38, 289                            applications, 2–5
coordinate systems and frames,       gradient noise, 498–499                   architectures, 33–37
126–139                          gradient vectors, 273                     CPUs and GPUs, 6–7
data structures, 147–148             graftals, 488                             images, 15–20
dot and cross products, 122–123      Gram-Schmidt orthogonalization,           imaging systems, 20–23
frames, 139–144                             671–672                            input devices, 9–13
incremental rotations, 185–186       grammars                                  output devices, 7–9
lines, 120–121                         shape, 487                              performance characteristics, 38–39
mathematical view, 118–119             tree, 484                               pixels, 5–6
matrix and vector classes, 144–145   graphical objects, 443                    programmer interfaces, 25–33
overview, 448–449                      creating, 445–447                       programmer pipelines, 37–38
planes, 123–124                        geometric, 448–449                      synthetic-camera model, 23–25, 30
primitives, 56                         hierarchy, 447–448                    graphs
quaternions, 186–189                   implementing, 448                       directed, 428–429
screen areas, 180–181                  methods, attributes, and messages,      and Internet, 453–455
smooth rotations, 184–185                   443–444                            scene. See scene graphs
three-dimensional interfaces,        graphical user interfaces (GUIs), 4–5   gravitational forces, 470–471, 473
180–186                          graphics architectures, 33–34           group nodes, 449–450
three-dimensional primitives,          clipping and primitive assembly,      GUIs (graphical user interfaces), 4–5
125–126                                 36–37
transformations. See transforma-       display processors, 34                H
tions                              fragment processing, 37               HAL (hardware abstraction layer),
virtual trackballs, 181–184            pipeline, 34–35, 53                         451
geometric optics, 18                     rasterization, 37                     halftoning, 349–350
geometric pipelines, 35–37               vertex processing, 36                 halfway angles, 271
geometry                               graphics processing units (GPUs), 7,    halfway vectors, 270–271
fractal. See fractals and fractal           38, 289                          hand-held devices, programmable, 38
geometry                         graphics programming, 43                hard constraints in particle systems,
graphics pipeline, 35–37               attributes, 65–67                           476
processing, 307–308                    color, 67–73                          hardware abstraction layer (HAL),
geometry engine, 38                      control functions, 78–83                    451
GIF format, 360                          gaskets, 83–88                        “has-a” relationships, 447–448
718          Subject Index
head function, 434, 436, 650            HTML (Hypertext Markup                   display considerations, 344–350
head-to-tail rule, 117, 120, 667, 669         Language), 453–454                 fragment processing, 309
height ﬁelds, 241–242, 551              hue in HLS systems, 348                  geometry processing, 307–308
Hermite curves and surfaces, 517–520    hue-saturation-lightness (HLS)           hidden-surface removal. See
Hermite geometry matrices, 518                systems, 348                            hidden-surface removal
hidden-surface removal, 31, 203, 331    human visual system, 22–23               modeling process, 306–307
back-face removal, 334–335            hypermedia, 453–454                      polygon rasterization, 327–331
depth sort and painter’s algorithm,   Hypertext Markup Language                rasterization, 308–309, 323–325
340–342                                 (HTML), 453–454                  implicit equation for spheres, 272
geometry processing, 307                                                     implicit functions, 589–591
object-space and image-space          I                                      implicit representation of curves and
approaches, 331–332               identity (I), 677                             surfaces, 505–506
overview, 238–241                     idle function                          improved Euler method, 476
scan conversion with z-buffers,          event-driven input, 103–105         in-betweening, 442
338–339                              particle systems, 481               incremental rotations, 185–186
scanline algorithms, 333–334             rotating cube with rotation in      incremental z-buffer algorithm,
sorting in, 332                              shader, 619                            337–338
three-dimensional gaskets, 96–98         rotating cube with texture, 643     independence in synthetic-camera
z-buffer algorithm, 335–338              rotating shaded cube, 629                  model, 30
hidden-surface-removal (HSR)            illumination function, 261             independent particles, 470–471
algorithm, 31, 239                image-based rendering, 600–602         independent variables, 503
hierarchical menus, 107                 image-oriented approach, 304–305       indexed color model, 69, 71–72
hierarchical models, 425, 427–429       image-space algorithms, 239, 331–332   inelastic collisions, 477
animation, 441–442                    images and imaging, 15, 20             initialization functions, 687–689
graphical objects, 443–449               compositing, 406                    initShader function
robot arm, 429–432                       digital, 359–362                      gaskets, 83, 86–87
scene graphs. See scene graphs           human visual system, 22–23            shader initialization, 609–610
tree structures, 437–441, 455–461        light for, 16–18                    inner products, 122–123, 670
tree traversal, 432–437                  models, 18–20                       input
high dynamic range applications, 6         objects and viewers, 15–16            event-driven. See event-driven
Hilbert curves, 485–486                    OpenGL functions, 693                      input
histograms, 596                            pinhole cameras, 20–22                functions, 52
HLS (hue-saturation-lightness)             primitives, 56                      input devices, 9–10
systems, 348                         processing, 411–412                   logical, 12–13
homogeneous coordinates                    sample, 31–32                         modes, 13–14
transformations, 159–164              imaginary numbers in Mandelbrot          physical, 10–13
vectors, 133–136                             sets, 493                       inside-outside testing, 327–328
homomorphic, 129                        immediate mode graphics, 45            instability in particle systems, 475
Hooke’s law, 471–472                    implementation strategies, 304–305     instances
Horner’s method, 536                       antialiasing, 342–344                 graphical objects, 445
HSR (hidden-surface-removal)               Bresenham’s algorithm, 325–327        tables and transformations,
algorithm, 31, 239                   clipping. See clipping                     168–169, 426–427
                                                                                        Subject Index          719

```cpp
intensity function for light sources,   isometric views, 199, 207–208           lateral inhibition, 277
```

262                             isosurface values, 589                  LCDs (liquid-crystal displays), 8

```cpp
interaction, 98                         isosurfaces, 589, 591–594               leaf nodes, 429
double buffering, 105                                                        LED (light-emitting diode) displays,
```

idle callbacks, 103–105              J                                              8
keyboard, 102–103                    Jacobi’s method, 574                    left-child structure, 437–438
menus, 106–107                       jaggedness. See aliasing and            left_lower_arm function, 650–651
OpenGL functions, 690–692                    antialiasing                    left_lower_leg function, 652
pointing devices, 98–101             Java applets, 454                       left_upper_arm function, 439, 650
windows, 78–79, 101–102, 106         jitter, 412                             left_upper_leg function, 651–652

```cpp
interactive graphics, 454–455           join points, 509                        Lempel-Ziv algorithm, 360
interactive viewers, 224–226            joint angles, 430–431, 442              length in fractal geometry, 488
interfaces                              joysticks, 12                           Lennard-Jones particle system, 502
```

OpenGL, 53–55                        JPEG image format, 359–362              lens, eye, 22
programmer, 25–33                                                            level-of-detail rendering, 452
three-dimensional applications,      K                                       Liang-Barsky clipping
180–186                         key framing, 442                           overview, 313–315

```cpp
interlaced displays, 8                  keyboard function                          in three dimensions, 320
```

Internet, 453–455                         ﬁgure with tree traversal, 657–658    libraries for X Window System, 53–54

```cpp
interpolating geometry matrices, 513      per-fragment lighting of sphere,      light and lighting, 257
interpolating polynomials, 511                636                                  in applications, 286–289
interpolation                             perspective projections, 624–625         Blinn-Phong model, 271, 563
```

blending functions, 513–515            rotating cube with rotation in           efﬁciency, 289–290
color, 150–151                             shader, 618                          global illumination, 297–298
curves and surfaces, 510–517           rotating cube with texture, 643–644      human visual system, 22–23
Phong shading, 279                     rotating shaded cube, 629                images, 16–20

```cpp
intersect function, 564–565               teapot renderer, 663                     materials, 284–286
intersections                           keyboards, 9–10                            and matter, 258–261
```

ray tracing, 565–568                   devices, 10                              modiﬁed Phong model, 270–271
sets, 456                              events, 102–103                          per-fragment, 295–297, 632–638
Inventor program, 451                   Khronos Group, 455                         Phong model, 265–271
inverse                                 kinematics, 441–442                        pinhole cameras, 20–22
dynamics, 442                        knot arrays, 530                           in rendering equation, 569–571
kinematics, 442                      knots, 530                                 shading. See shading
matrices, 679                        Koch curves, 485–489                       sources, 259, 261–265, 283–284
operations, 666                      Koch snowﬂakes, 111, 485                   sphere, 632–638
vectors, 117                                                                    synthetic-camera model, 23–25
invertible transformations, 678–679     L                                          vector computation, 271–275
inward-pointing faces, 146–147          Lambertian surfaces, 267                   in vertex shaders, 290–294
iris, eye, 22                           Lambert’s law, 267–268                  light-emitting diode (LED) displays,
irrational numbers, 466                 language-based models, 467, 484–487            8
“is-a” relationship, 448                latency, 35                             light-ﬁeld rendering, 602
720          Subject Index
lighting. See light and lighting        magniﬁcation in texture sampling,    model-view. See model-view
lightness in HLS systems, 348                382                                 matrices
line-rasterization algorithm, 325–327   magnitude of vectors, 120            normal, 402
line segments                           Mandelbrot, Benoit, 487              operations, 676–677
attributes, 66                       Mandelbrot set, 493–496              orthogonal, 163
clipping, 310–314                    maps and mapping, 2                  perspective-normalization, 235
foreshortened, 171, 199–200           bump. See bump maps                 perspective-projection, 232–238
three-dimensional, 29                 cube, 393–396                       projection, 217–218
type speciﬁcations, 58                environment, 32, 367, 388–396       rank, 678–679
between vertices, 57                  methods, 366–368                    representation, 132–133, 137–139,
linear combination of vectors, 667       normal, 399                             679–681
linear ﬁlters, 382–384, 411–412          photon, 20, 571                     row and column, 676–678
linear functions, 153                    spheres, 390–392                    similar, 683
linear vector space, 118                 surface, 366–368                    sparse, 574
linearly independent matrices, 679       texture. See texture mapping        square, 675
linearly independent vectors, 123,      marching cubes, 591–594              texture, 384–385
667                             mat.h class, 683–684                 transformation. See transformation
lines                                   material class, 448                      matrices
attributes, 66                       mathematical view of vectors and     translation, 160–161
equation, 504                             afﬁne spaces, 118–119           transpose, 675–676
overview, 120–121                    matrices                             trees with, 433
links                                    B-spline geometry, 526              view-orientation, 210
hypermedia, 453–454                   Bezier geometry, 521–522           matrix-matrix addition, 676–677
language-based models, 486            Catmull-Rom geometry, 535          matrix-matrix multiplication, 160,
liquid-crystal displays (LCDs), 8        classes for, 144–145, 683–684           676–677
load balancing, 581                      color, 348–349                     MatrixStack class, 646–647
local lighting model, 297–298            control point geometry, 540        Maxwell triangles, 113, 424
locator devices, 13                      control points, 525                maze, 111
logical devices, 10, 12–13               convolution, 411–412               measures of input devices, 13
LookAt function, 26, 212–214,            coordinate system changes,         medical imaging, 2
247–249                              129–133                        members, 446
lookup tables, 71                        cross products, 681                membership function, 505
loop subdivision, 549                    deﬁnitions, 675–676                memory for buffers, 357–359
lossless coding, 360                     determinant, 679                   menu function, 655
low-pass ﬁlters, 418–419                 eigenvalues and eigenvectors,      menus, 106–107
lumigraph rendering, 602                     682–683                        Mercator projections, 373
luminance function, 262                  frame changes, 136–137             mesh display
luminance images, 359                    Hermite geometry, 518               overview, 241–244
homogeneous coordinates, 133–       polygon offset, 246–247
M                                            136                             as surfaces, 244–246
Mach bands, 277                          identity, 677                       walking through scenes, 247–249
magnetic resonance imaging (MRI),        interpolating geometry, 513        mesh generation, 550
2                                   inverse, 679                        Delaunay triangulation, 551–555
                                                                                    Subject Index           721
height ﬁelds, 551                     shadows, 251–253                     multiview orthographic projections,
point clouds, 556                     spinning cube, 176                        197–199
meshes, 112                            stack-based traversal, 434–436
polygonal shading, 278                symbols for, 426–427                 N
simpliﬁcation, 594–595                tree data structures, 438–439        n-tuples, 128–129
subdivided, 547–550                   vertex shaders, 290, 293–294         National Center for SuperComputer
messages for objects, 443–444         model-view transformations,                 Applications (NCSA), 454
methods for objects, 443–444                141–144                         National Television Systems
micropolygons, 579                    modes                                       Committee (NTSC) system,
midpoint division, 490–491             input, 13–14                               345
miniﬁcation, 382–383                   writing, 363–364                     natural languages, 484
mipmapping, 382–383                   modiﬁed Phong lighting model,         near distance in synthetic-camera
mirrors in ray tracing, 562                 270–271                               model, 30
model frames, 129, 140, 426           modiﬁer keys, 103                     Netscape Navigator browser, 454
modeling-rendering paradigm, 32–33    modulus operator, 49                  Newell, Mike, 543
models                                Moiré patterns, 350, 416             Newtonian particles, 468–471
colored cubes, 146–152               monitors, 7–9                           attractive and repulsive forces,
coordinates, 140                     monochromatic images, 359                   472–473
graphics. See graphics systems and   Monte Carlo methods, 571                independent, 470–471
models                           Mosaic browser, 454                     spring forces, 471–472
hierarchy. See hierarchical models   motion-blur effect, 412               Newton’s second law, 468–469
images, 18–20                        mountains, fractal, 492               Nintendo Wiimote devices, 12
Phong lighting, 265–271              mouse, 9–10                           nodes
process, 306–307                      callback functions, 99–100, 106        graphs, 428–429
symbols and instances, 426–427        overview, 10                           group, 449–450
model-view matrices                   mouse function, 104–105               noise, 496–500
bump maps, 402                        ﬁgure with tree traversal, 653–654   nonconvex polygons, 315
camera orientation, 210–212           rotating cube with rotation in       noninterlaced displays, 8
camera positioning, 204–206,               shader, 619                     nonphotorealistic shading, 297
208–209, 214                      rotating cube with texture, 643      nonsingular matrices, 679
camera views, 204                     rotating shaded cube, 628–629        nonuniform B-splines, 532
coordinate system changes, 129        spinning cubes, 177                  nonuniform foreshortening, 227
CTM, 174, 176                        move events, 99                       nonuniform rational B-spline
display callback, 177                MRI (magnetic resonance imaging),           (NURBS) curves and surfaces,
frames, 141–144                            2                                     532–533
with orthogonal-projection           multiplication                        nonuniform scaling, 158–159
matrices, 219                     matrix-matrix, 160, 676–677          normal maps, 399
mesh display, 243, 247–249            scalar-matrix, 676–677               normal matrix, 402
perspective projections, 229          scalars, 118                         normal vectors, 272–274
reﬂection maps, 395                   vectors, 117, 120, 666–668           normalization
robot arm, 431–432                   multirendering, 393                     matrices, 217–228
rotating cube, 224                   multisampling, 410–411                  projection, 217–228
scene graphs, 449–450                multitexturing, 386–387                 transformations, 209
722          Subject Index
normalize function, 282                  RGBA system, 70                          vector, 666–668
normalized device coordinates, 141,    Open Inventor program, 451               optic nerves, 23
310                               Open Scene Graph (OSG) program,          optical detectors, 11
normalizing vectors, 169                      450–453                           order of transformations, 176
normals to planes, 124                 open splines, 532                        ordinary differential equations,
NTSC (National Television Systems      OpenGL API, 1, 50–51                           474–475
Committee) system, 345              attribute and enabling functions,      orientation in camera speciﬁcation,
numerical instability in particle             692–693                                 30
systems, 475                        camera positioning, 204–209            origins
numerical ordinary differential          compositing and blending, 406–407        afﬁne spaces, 128, 668
equations, 474–475                  concave polygon rasterization, 329       window systems, 78–79
NURBS (nonuniform rational B-            frames, 139–144                        Ortho function, 216, 219
spline) curves and surfaces,        GLSL functions, 694–696                orthogonal matrices, 163
532–533                             graphics functions, 51–53              orthogonal-projection matrices,
Nyquist frequency, 415–416               graphics pipeline and state                  219–220
Nyquist sampling theorem, 415, 418            machines, 53                      orthogonal projections, 215–216
initialization and window              orthogonal vectors, 122–123, 670–671
O                                             functions, 687–689                orthogonalization, Gram-Schmidt,
object coordinates, 47                   interaction functions, 690–692               671–672
object-oriented approach, 304–305        interface, 53–55                       orthographic projections, 197–199
object-space algorithms, 239,            parallel projections, 216–217          orthographic views
331–332                            perspective projections, 229–232         clipping, 321–322
objects, 15–16                           perspective transformations,             creating, 74–77
clipped, 77, 229–230                        236–238                           orthonormal vectors, 671
coordinate systems, 55, 140            primitives, 56–65                      OSG (Open Scene Graph), 450–453
curved, 65                             query functions, 694                   outcodes, 311–312
frames, 129, 140, 142, 426             state and buffers functions, 693–694   outer products, 122–123
geometric. See geometric objects       texture and image functions, 693       output devices, 7–9
graphical, 443–449                     texture mapping, 374–387               outward-pointing faces, 146–147
texture, 375–376                       three-dimensional, 29                  overdetermined problems, 3
three-dimensional, 29                  transformations, 172–176               overlapping fragments, 407–409
oblique projections, 220–224             vertex buffer object functions,
oblique views                                 689–690                           P
clipping in, 321–322                   views, 73–77                           painter’s algorithm, 340–342
obtaining, 200–201                   OpenGL Extension Wrangler                palettes, color, 71
occlusion culling, 451–452                    (GLEW) library, 53                parallel projections, 196–197, 254
octrees, 459–461                       OpenGL Utility Toolkit (GLUT),             interactive viewers, 224–226
odd-even tests, 328                           53–54, 78                           normalization, 217–228
one-dimensional reconstruction, 419    operations                                 oblique, 220–224
one-point perspective views, 41, 202     afﬁne spaces, 669                        with OpenGL, 216–217
opacity                                  Euclidean spaces, 670                    orthogonal, 215–216
in compositing techniques, 404–405     matrix, 676–677                          orthogonal-projection matrices,
direct volume rendering, 596           scalar, 665–666                              219–220
                                                                                Subject Index           723
parallel ray tracers, 569          performance characteristics, 38–39     rasterization, 323–325
parallel rendering, 579–581        periodic B-splines, 532              planar geometric projections, 197
sort-ﬁrst, 586–588               periodic functions, 414              planes
sort-last, 583–586               Perlin noise, 498                      clipping, 231
sort-middle, 581–582             perspective division, 141, 229         overview, 123–124
parallel views, 196–197            Perspective function, 26               projection, 24–25
parametric continuity, 519–520     perspective-normalization              tangent, 273
parametric form                           transformations, 232–236      plasma panels, 8–9
curves and surfaces, 368–369,    perspective-projection matrices,     plotting techniques, 2
506–511                            232–238                       point clouds, 556
lines, 121                       perspective projections, 226         point light sources, 18, 263–264
planes, 124                        with OpenGL, 229–232               point-point subtraction, 117–119,
spheres, 273                       sample program, 621–626                   669
texture mapping, 368–369           simple, 226–229                    point-vector addition, 117–118, 120
parents in tree structures, 429,   perspective transformations, 228,    pointing devices, 9–10, 98–101
447–448                            236–238                       points
particle structure, 480            perspective views, 41, 196–197,        attributes, 66
particle systems, 467–468                 201–202                         Euclidean spaces, 670
collisions, 476–479, 482         PET (positron-emission                 geometric objects, 116

```cpp
constraints, 476–479                    tomography), 2                  join, 509
```

ﬂocking, 483–484                 Phong lighting model, 265–266          OpenGL, 47
forces, 483                        ambient reﬂection, 267               rendering, 84–85
Newtonian particles, 468–473       diffuse reﬂection, 267–269           representing, 47
particle display, 480–481          modiﬁed, 270–271                     sampling, 382–384, 417
position updates, 481–482          specular reﬂection, 269–270          shrink-wrapping, 122
solving, 473–476                 Phong shading, 279–280                 three-dimensional APIs, 29
particles in ﬂuids, 490–491        phosphors, 8                           three-dimensional gaskets, 91–92
pass-through vertex shaders, 85    photon mapping, 20, 571                type speciﬁcations, 57–58
passive move events, 99            physical-device coordinates, 55      polygon scan conversions, 327
patches                            physical images, 15                  polygonal shading, 275–280
Bezier, 523–524, 542–544         physical input devices, 10–13        polygons
cubic interpolating, 515–517     physically based models, 467–468       area, 335
radiosity, 572                   pick devices, 13                       attributes, 67
surface, 509, 523–524            pinhole cameras, 20–22, 229            back-face removal, 334–335
pen-plotter models, 27–28          pipeline architectures                 basics, 58–59
penalty function, 479                clipping, 316–317                    clipping, 314–317
penumbra, 264                        graphics, 34–35, 53                  offsets in mesh display, 246–247
per-fragment lighting, 295–297,      for performance, 39                  rasterization, 327–331
632–638                       programmer, 37–38                    recursion, 88–90
per-fragment shading, 280          pitch, 214                             shadow, 250–251
perception, color, 67–68           pixels                                 texture mapping, 381
perfectly diffuse surfaces, 261      buffers, 5–6, 358                    three-dimensional, 29, 92–96, 126
perfectly specular surfaces, 261     raster characters, 64–65             triangulation, 62–63
724          Subject Index
polygons (cont.)                         three-dimensional, 29, 125–126        protected members, 446
types, 59–60                           triangles, 59–60                      pseudorandom numbers, 496–498
polyhedron faces, 567                  principal faces, 197                    public members, 446
polylines, 58                          private members, 446                    push function, 435
polynomials                            procedural methods, 465
Bernstein, 522–523                     algorithmic models, 465–467           Q
curves and surfaces evaluation,        language-based models, 484–487        quad function
536–537                           noise, 496–500                          color cube, 148–150

```cpp
interpolating, 511–517                 particle systems. See particle          cube reﬂection map, 394–395
```

parametric curves, 507–508,                systems                             ﬁgure with tree traversal, 648–649
510–511                           recursion and fractals, 487–496         perspective projections, 622
parametric surfaces, 508–509         processors                                rotating cube with rotation in
subdivision of, 537–541                CPUs, 6–7                                   shader, 617
pop function, 435                        display, 34                             rotating cube with texture, 639–
pop-up menus, 106                        GPUs, 7, 38                                 640
position                               product function in lighting, 287         rotating shaded cube, 627
cameras, 29, 204–215                 productions, 484–485                      texture mapping, 377–378
input devices, 11–12                 products, dot and cross, 122–123, 681   quadratic form, 545
light, 284                           program objects, 86                     quadrics for surfaces, 506, 545
particles, 481–482                   programmer interfaces, 25–26            quadrilaterals, 62–63, 383–385
trackballs, 181–183                    modeling-rendering paradigm,          quadtrees, 459–461
positron-emission tomography                 32–33                             quantization, 413, 420–421
(PET), 2                          pen-plotter models, 27–28             quantizers, 420
postmultiplication by matrices, 173,     sample images, 31–32                  quaternions, 186
176                               three-dimensional APIs, 28–31           and complex numbers, 186–187
postorder traversal, 456               programmer pipelines, 37–38               and rotation, 187–189
PostScript fonts, 64                   progressive radiosity, 578              query functions, 53, 694
power walls, 580                       projection planes, 24–25                queues, event, 14, 81
pre-order traversal, 434               projections, 670–671
precision, buffers, 6, 358               axonometric, 198–200                  R
preimages, pixel, 371                    display systems, 9                    R3 space, 129
premultiplication by matrices, 173       Mercator, 373                         radiosity, 298, 560, 571–572
primary colors, 23                       normalization, 217–228                  carrying out, 577–578
primitives, 56–58                        orthogonal-projection matrices,         equation, 572–575
assembly, 36–37                            219–220                             form factors, 575–577
attributes, 65–67                      orthographic, 197–199                 radiosity method, 20
clipping, 36–37, 317–319               parallel. See parallel projections    rand function
curved, 65                             perspective, 226–232, 621–626           procedural noise, 496
in geometry processing, 307            planar, 197                             Sierpinski gaskets, 49
OpenGL functions, 51–53                points, 21                            random numbers
polygons, 58–60                        and shadows, 249–253                    Brownian motion, 491
spheres, 60–62                       projectors, 24                            procedural noise, 496–498
text, 64–65                          properties. See attributes                Sierpinski gaskets, 49
                                                                                      Subject Index          725
random particle movement in ﬂuids,     reﬂect function, 275                     volumes, 588–591
491                             reﬂection maps, 367, 388–396           rendering farms, 579
random-scan CRTs, 8                    reﬂections                             RenderMan renderer, 578–579
rank, matrices, 678–679                  collisions, 477–478                    interface, 33
raster operations (raster-ops), 362      Phong lighting model, 267–270          noise function, 498
raster primitives, 56                    ray tracing, 562–564                   ray tracing, 569
raster replication, 64–65                in scaling, 159                      replicating pixels, 64–65
raster text, 64–65                       surface, 258–261                     representation
rasterization, 7, 37                     in vector computations, 274–275        curves and surfaces, 503–509
polygons, 327–331                    refraction, 261                          spheres, 273–274
primitives, 323–325                  refresh rates                            vectors and matrices, 126–128,
process, 308–309                       buffers, 105                                132–133, 137–139, 667,
rasters, 5                               CRTs, 8                                     679–681
ray casting, 258–259, 546, 590, 599    refresh CRT displays, 8                repulsive forces, 472–473
ray tracing, 20, 298, 560–564          register functions, 82                 request modes, 13–14

```cpp
intersections, 565–568               rejected primitives in clipping, 310   reshape events, 101–102
```

recursive, 564–566                   relative-positioning devices, 11–12    reshape function, 102
variations, 568–569                  render function, 446–447               resolution
volumes, 598–599                     render_cube function, 445                display, 9, 79
ray trees, 563                         rendering, 559–560                       frame buffers for, 6
rays                                     back-to-front and front-to-back,       human visual systems, 22
light, 18–19                                409–410                         retained mode graphics, 45
lines, 121–122                         curves and surfaces, 510, 535–542    retinas, 22
raytrace function, 564–566               direct volume, 595–600               reversible transformations, 678–679
real numbers                             equations, 258, 560, 569–571         Reyes renderer, 579
scalars, 665–666                       image-based, 600–602                 RGB color systems, 6, 68–71, 346–347
for vectors, 116                       isosurfaces and marching cubes,      RGBA color system, 70, 404
reciprocity equation, 573                     591–594                         right-hand rule, 147
reconstruction step, 414, 418–420        mesh simpliﬁcation, 594–595          right-handed coordinate system, 123
rectangles, viewing, 77                  modeling-rendering paradigm,         rigid-body transformations, 157
rectangular approximation, 419–420            32–33                           robot ﬁgure, 429–433
recursion                                multirendering, 393                  rods, eye, 22–23
B-splines, 530–531                     parallel, 579–588                    roll, 214
fractals. See fractals and fractal     points, 84–85                        root nodes, 429
geometry                          polygons, 58                         rotating cube programs
polygons, 88–90                        radiosity, 298, 571–578                interactive viewer, 224–226
ray tracing, 564–566                   ray casting, 546                       lighting, 289
Sierpinski gaskets, 613–615            ray tracing. See ray tracing           reﬂection map, 393–396
recursive subdivision                    RenderMan, 578–579                     rotation in shader, 615–620
Bezier polynomials, 537–539            sort-ﬁrst, 586–588                     shaded, 626–632
sphere approximation, 280–283          sort-last, 583–586                     texture, 638–645
Utah teapot, 543–544                   sort-middle, 581–582                 rotation
reﬁnement, curve, 547                    teapot, 659–664                        about arbitrary axes, 169–172
726          Subject Index
rotation (cont.)                       scalars, 665–666                          shading
about ﬁxed points, 165–166, 175        addition and multiplication, 118,          ﬂat, 276–277
general, 167–168                            666–668, 676–677                      GLSL, 85, 498, 694–696
in homogeneous coordinates,            in geometric objects, 116                  nonphotorealistic, 297
162–163                          scaling                                      Phong, 279–280
incremental, 185–186                   homogeneous coordinates, 161–              polygonal, 275–280
objects, 155–158                            162                                   smooth and Gouraud, 277–279
and quaternions, 187–189               objects, 158–159                           sphere models, 294–295
smooth, 184–185                        transformation matrices, 174–175        shadow masks, 8
transformation matrices, 174–175     scan conversion, 7                        shadow polygons, 250–251
virtual trackballs, 181–184            polygon, 327                            shadow rays, 561
row matrices, 676–678                    process, 308–309                        shadows
rulers in fractal geometry, 488          with z-buffers, 338–339                    point light sources, 263–264
Runge-Kutta method of order 2, 476     scanlines, 305, 333–334                      and projections, 249–253
scattered light, 261                      shape grammars, 487
S                                      scene graphs, 33, 449–451                 shear transformations, 163–164
sample-mode inputs, 14                   and Internet, 453–455                   shininess coefﬁcient, 269
sample programs, 607–608                 OSG, 450–453                            shrink-wrapping points, 122
ﬁgure with tree traversal, 646–659   scientiﬁc visualization applications, 3   Sierpinski gaskets, 83–84
per-fragment lighting of sphere,     scissoring technique, 319                    fractals, 490
632–638                         screen                                       fragment shader, 86
perspective projections, 621–626       areas, 180–181                             polygons, 88–90
rotating cube with rotation in         coordinates, 55, 78–79, 141, 309           procedural methods, 487
shader, 615–620                   resolution, 79                             program, 47–50, 610–612
rotating cube with texture, 638–     searching for resources, 453                 recursive generation programs,
645                             seed points, 330                                 613–615
rotating shaded cube, 626–632        segments                                     rendering points, 84–85
shader initialization function,        aliased, 342                               vertex shader, 85–86
608–610                           curve, 508                              silhouette edges, 294
Sierpinski gaskets, 610–615            line. See line segments                 simple polygons, 58–59
teapot renderer, 659–664             self-emission, 258                        simpliﬁcation of meshes, 594–595
samplers, texture, 376–382             self-similarity, 488                      simulation applications, 3–4
sampling                               separable surfaces, 517                   sinc function, 418–419
aliasing in, 413, 415–418            sets                                      singular matrices, 679
multisampling, 410–411                 CSG modeling, 456                       singularities, 330–331
reconstruction step, 414, 418–420      Mandelbrot, 493–496                     slope of lines, 504
rendering, 413–421                     mathematical, 118                       smooth rotations, 184–185
saturated color values, 70           shaders                                   smooth shading, 277–279
texture, 382–384                       fragment, 86                            smoothness of curves, 509
sampling apertures, 418                  initialization function program,        snowﬂakes, Koch, 485
sampling theory, 413–418                      608–610                            soft constraints in particle systems,
saturation in HLS systems, 348           vertex, 85–86                                  476, 479
scalar ﬁelds, 118, 588, 665            shades of color, 257                      sort-ﬁrst rendering, 586–588
                                                                                       Subject Index            727
sort-last rendering, 583–586           stack-based traversals, 434–437           spline, 528–529
sort-middle rendering, 581–582         state machines, 53                        subdivision, 546–550
sorts                                  states                                    tensor-product, 517
hidden-surface removal, 332             OpenGL functions, 693–694            Sutherland, Ivan, 3
polygon rasterization, 329–330          in transformations, 173              Sutherland-Hodgeman clipping, 315,
source bits, 363                       stencil buffers, 359                          320
source blending, 405                   stipple patterns, 339                   symbols
source buffers, 362                    stochastic sampling method, 569           grammars, 484–485
space-ﬁlling curves, 111, 486          string devices, 13                        objects, 426–427
spaceballs, 12                         strips, 60–62                           synthetic-camera model, 23–25, 30,
spaces, 665                            stroke text, 64, 67                           196
afﬁne, 668–669                       structured data sets, 589               synthetic images, 15
Euclidean, 118, 669–670              subdivision
Gram-Schmidt orthogonalization,         Bezier surfaces, 541–542             T
671–672                            curves and surfaces, 546–550         tables
projections, 670–671                    loop, 549                              forward differences, 536–537
scalars, 665–666                        meshes, 547–550                        lookup, 71
vector, 123, 666–668                    polynomials, 537–541                 tablets, 12
spans in hidden-surface removal, 333      sphere approximation, 280–283        tangent planes, 273
sparse matrices, 574                      tetrahedrons, 93–94                  tangent space, 398–400
spatial-domain aliasing, 343              Utah teapot, 543–544                 tangent vectors, 398
spectrum, frequency, 414–416           subtraction, point-point, 117–119,      taxonomies, 448
specular reﬂection, 269–270                    669                             Taylor’s theorem, 474–476
specular surfaces, 261                 subtractive color models, 68            teapot
spheres                                subwindows, 106                           drawing, 542–544
fan and strip approximation, 60–62   sums. See addition                        renderer program, 659–664
mapping, 390–392                     surfaces                                  texture making, 384–386
per-fragment lighting, 632–638          algebraic, 506, 545–546              tensor-product surfaces, 517
recursive subdivision                   Bezier. See Bezier curves and        terminal nodes, 429
approximation, 280–283                  surfaces                        tessellation
representation, 273–274                 bump maps, 396–404                     curved objects, 65
shading, 294–295                        clipping, 319                          isosurfaces, 592–593
spinning cubes, 176–180                   design criteria, 509–510               meshes, 548
splatting, 596–597                        Hermite, 517–520                       polygons, 63, 315, 329
spline surfaces, 528–529                  hidden. See hidden-surface removal     triangles, 126
spotlights, 264                           interpolation, 510–517               tetrahedron function
spring forces, 471–472                    mapping methods, 366–368               per-fragment lighting of sphere,
square matrices                           mesh display as, 244–246                    634
deﬁned, 675                             mesh generation, 550–556               sphere approximation, 281–282
inverse of, 679                         patches, 509, 523–524                tetrahedrons
stability                                 reﬂections, 258–261                    fractals, 492
curves, 510                             rendering, 366–368, 535–542            sphere approximation, 280–282
particle systems, 475                   representation, 503–509                subdividing, 93–94
728         Subject Index
texels                                trackballs                              trees and tree structures, 455–456
linear ﬁltering, 382                  variants, 10                            BSP, 457–459
texture mapping, 368                  virtual, 181–184                        CSG, 455–456
text                                  transformation matrices, 434              graphs, 429
attributes, 67                        cube spinning, 176–180                  language-based models, 484–485
clipping, 319                         OpenGL, 172–176                         quadtrees and octrees, 459–461
graphical, 64–65                      order, 176                              ray, 563
texture mapping, 32, 367–368            rotation, translation,                  scene, 449–450
coordinates and samplers, 376–382          and scaling, 174–                  traversal, 432–437, 459–460,
multitexturing, 386–387                    175                                     646–659
OpenGL, 374–387                     transformations                           working with, 437–441
texture arrays, 376                   afﬁne, 152–155                        triads, 8
texture coordinates, 384–386          concatenating, 164–172, 678           triangle function, 88–89
texture objects, 375–376              homogeneous coordinates, 159–164        per-fragment lighting of sphere,
texture sampling, 382–384             instance, 168–169, 426–427                   633
two-dimensional, 368–375              invertible, 678–679                     Sierpinski gaskets, 93–95, 613
volumes, 599–600                      model-view, 141–144                     sphere approximation, 280–281
texture matrices, 384–385               normalization, 209                      sphere model shading, 294–295
textures                                OpenGL, 52, 236–238                   triangles, 59–60
coordinates, 368, 384–386             perspective, 228                        decimation, 594
cube with, 638–645                    perspective-normalization, 232–         Maxwell, 113, 424
generating, 387–388                        236                                sphere approximation, 280–281
mapping. See texture mapping          rotating, 155–158, 162–163              texture mapping, 381
objects, 375–376                      scaling, 158–159, 161–162             triangular polygons, 126
OpenGL functions, 693                 shear, 163–164                        triangulation, 62–63, 551–555
sampling, 382–384                     translations, 155, 160–161, 174–175   triggers, 13–14
three-dimensional, 388              translate function, 436                 trimetric views, 199
three-color theory, 68                translation matrices, 160–161           tristimulus values, 68–69, 262,
three-dimensional objects             translations, 155, 160–161, 174–175            345–346
clipping, 319–322                   translucent objects, 309                true-color systems, 6
gaskets, 91–98                      translucent surfaces, 261               turtle graphics, 110–111, 485

```cpp
interfaces, 28–31, 180–186          transmission, ray tracing, 563–564      twist angle, 215
```

meshes, 551–552                     transparency                            twists, 191, 524
primitives, 29, 125–126               opacity, 404                          two-dimensional applications, 46–50
textures, 388                         RGBA system, 70                       two-dimensional views, 77
three-point perspectives, 202         transpose of matrices, 675–676          2-1/2–dimensional surfaces, 241
throughput, 35                        transpose4 function, 544                two-part texture mapping, 372
TIFF images, 360–361                  traversal                               two-point perspectives, 30, 41, 202
time-domain aliasing, 343–344           CSG modeling, 456
top-level windows, 106                  tree, 432–437, 459–460, 646–659       U
topology, cube, 147                   traverse function, 439, 441, 649        u-v-n systems, 210
touch-sensitive screens, 12           tree grammars, 484                      ultrasound, 2
trace function, 564, 569              treenode structure, 437                 umbra, 263
                                                                                     Subject Index           729
underdetermined problems, 3             knot, 530                            viewing rectangles, 77
uniform qualiﬁed variables, 180–181     light computation, 271–275           viewing volumes, 77
Uniform Resource Locators (URLs),       linearly independent, 123, 667       viewports, 79–80
453–454                           mathematical view, 118–119           views, 72–73, 195–196
uniform scaling, 158–159                normalizing, 169                       axonometric, 198–200
uniform splines, 532                    operations, 666–668                    cameras. See cameras and camera
union of sets, 456                      Phong lighting model, 265, 270–271          models
unit function, 633                      representation, 126–128, 132–133,      classical, 197–199
unstructured data sets, 589                 137–139, 667                       computer, 202–204
URLs (Uniform Resource Locators),       tangent, 398                           functions, 52
453–454                           view-up, 209                           isometric, 199, 207–208
user interfaces, 4–5                  velocity of Newtonian particles,         mesh display, 241–249
Utah teapot                                 468–469                            oblique, 200–201
drawing, 542–544                    vertex arrays, 146                       orthographic, 74–77
texture making, 384–386             vertex attributes, 66                    orthographic projections, 197–199
vertex buffer object functions,          parallel, 196–197
V                                           689–690                            perspective, 41, 196–197, 201–202
valence of vertices, 548              vertex lists, 148                        perspective projections, 226–232
valuator input devices, 13            vertex shaders                           projections. See projections
value noise, 497                        gaskets, 85–86                         two-dimensional, 77
vanishing points, 41, 202               lighting in, 290–294                 virtual reality (VR), 4
variables                             vertices                               virtual trackballs, 181–184
curves and surfaces, 503–505          coordinates, 55                      visibility testing, 457
uniform, 180–181                      isosurfaces, 592                     visible color, 67
variation-diminishing property, 537     meshes, 242–243, 551–555             visible spectrum, 17
vec.h class, 683–684                    normals, 278                         visible-surface algorithms, 97, 239,
vector CRTs, 8                          objects, 15, 28–29                          307
vector-point addition, 117–118, 120     OpenGL, 47                           visual acuity, 22
vector-scalar multiplication, 120       primitives, 57                       visual system, human, 22–23
vector space, 118, 665–668              processing, 36                       visualization of implicit functions,
vector-vector addition, 118, 120,       triangulation, 62–63                        589–591
666–668                           valence, 548                         VLSI (very-large-scale integrated)
vectors                               very-large-scale integrated (VLSI)            circuits, 3, 38
binormal, 398                             circuits, 3, 38                  volumes

```cpp
characteristic, 682                 view-orientation, 210                    clipping, 36–37, 318
classes for, 144–145, 683–684       view-plane normals (VPNs), 209           direct volume rendering, 595–600
```

components, 126–127                 view-reference points (VRPs), 209        ray tracing, 598–599
computer science view, 119          view-up vectors (VUPs), 209              rendering, 588–591
coordinate systems and frames,      view volumes, 229–230, 307               texture mapping, 599–600
126–139                         viewers                                  view, 77, 229–230
dot and cross products, 122–123       image-formation process, 15–16       volumetric data sets, 588–589
geometric objects, 116–117            interactive viewer, 224–226          voxels
homogeneous coordinates, 133–136    viewing-coordinate systems, 209          direct volume rendering, 596–597
730          Subject Index
voxels (cont.)                      winding numbers, 328                Y
isosurfaces, 591–592              winding test, 328                   y-intercepts, 504
octrees for, 461                  windows and window systems          y–x algorithm, 334
volume rendering, 589              coordinates, 55, 78–79, 141, 308   yaw, 214
VPNs (view-plane normals), 209       events, 101–102, 106               YUV color system, 346
VR (virtual reality), 4              iconifying, 101
VRPs (view-reference points), 209    interaction, 78–79                 Z
VUPs (view-up vectors), 209          managing, 106                      z-buffer algorithm, 239–241
OpenGL functions, 687–689          z-buffers
W                                   world coordinate system, 55, 140      hidden-surface removal, 97,
walking through scenes, 247–249     world frames, 129, 140, 426               239–241, 335–338
wavelengths, light, 17              World Wide Web, 453–455               scan conversion with, 338–339
web browsers, 454                   writing into buffers, 362–366       zero (0) vectors, 117, 666
WebGL, 38, 455                      writing modes, 363–364              zip ﬁles, 361
white noise, 496
widgets, 13                         X
XOR operations, 365–366
XYZ color system, 346–347
Color Plate 1 Image of sun object created
bump mapping.
(Courtesy of Fulldome Project, University of New
Mexico.)
Color Plate 2 Wire-frame representation of
sun object surfaces.
(Courtesy of Fulldome Project, University of New
Mexico.)
Color Plate 3 Flat-shaded polygonal
rendering of sun object.
(Courtesy of Fulldome Project, University of New
Mexico.)
Color Plate 4 Smooth-shaded polygonal
rendering of sun object.
(Courtesy of Fulldome Project, University of New
Mexico.)
Color Plate 5 Wire-frame of NURBS
representation of sun object showing the high
number of polygons used in rendering the
NURBS surfaces.
(Courtesy of Fulldome Project, University of New
Mexico.)
Color Plate 6 Rendering of sun object
showing bump map.
(Courtesy of Fulldome Project, University of New
Mexico.)
                                                                                     Color Plate 7 Rendering of sun object with
an environment map.
(Courtesy of Fulldome Project, University of New
Mexico.)
Color Plate 8 Rendering of a small part of the sun object with an environment map. (Courtesy of Fulldome Project, University of New Mexico.)
(a) Without antialiasing                                                  (b) With antialiasing
Color Plate 9 Axonometric view
from outside of temple.
(Courtesy of Richard Nordhaus,
Architect, Albuquerque, NM.)
Color Plate 10 Perspective view
of interior of temple.
(Courtesy of Richard Nordhaus,
Architect, Albuquerque, NM.)
                                                                Color Plate 11 Cartoon-shaded teapot.
(Courtesy of Ed Angel, University of New Mexico.)
Color Plate 12 Reflection map from a color
cube on teapot.
(Courtesy of Ed Angel, University of New Mexico.)
Color Plate 13 Interface for animation using Maya.
(Courtesy of Hue Walker, ARTS Lab, University of New Mexico.)
Color Plate 14 (a) Wire-frame model of a wave.
(Courtesy of Sony Pictures Entertainment.)
Color Plate 14 (b) White water and spray
created by particle system.
(Courtesy of Sony Pictures Entertainment.)
Color Plate 14 (c) Final composited image from
“Surf’s Up.”
(Courtesy of Sony Pictures Entertainment.)
             Color Plate 15 Rendering using
ray tracer.
(Courtesy of Patrick McCormick.)
Color Plate 16 Radiosity rendering showing
soft shadows and diffuse–diffuse reflections.
(Courtesy of A. Van Pernis, K. Rasche, R. Geist,
Clemson University.)
Color Plate 17 Array of Utah teapots with different
material properties.
(Courtesy of SGI.)
Color Plate 18 Phong-Blinn shaded teapots. (Courtesy of Ed Angel, University of New Mexico.)
(a) Using per-vertex lighting                                          (b) Using per-fragment lighting
(c) Area near highlight                                                (d) Area near highlight
Color Plate 19 Fluid dynamics of the mantle
of the Earth. Pseudocolor mapping of
temperatures and isotemperature surface.
(Courtesy of James Painter, Los Alamos National
Laboratory.)
Color Plate 20 Volume rendering of CT
data.
(Courtesy of J. Kniss, G. Kindlmann, C. Hansen,
Scientific Computing and Imaging Institute,
University of Utah.)
                             Color Plate 21 RGB color cube.
(Courtesy of University of New Mexico.)
Color Plate 22 Avatar representing a patient
remotely located health professional (inset).
(Courtesy of Tom Caudell, Visualization Laboratory,
Albuquerque High Performance Computing Center,
University of New Mexico.)
Color Plate 23 One frame from
Pixar’s “Geri’s Game” showing
Geri’s glasses.
(Courtesy of Pixar Animation Studios.)
Color Plate 24 Reflection map
the center of the lens on Geri’s
glasses. The reflection map is then
the rendering process.
(Courtesy of Pixar Animation Studios.)
Color Plate 25 Elevation data for
Honolulu, Hawaii, displayed using a
for a Bezier surface.
(Courtesy of Brian Wylie, University
Laboratories.)
Color Plate 26 Wire frame of the
in flat areas.
(Courtesy of Brian Wylie, University
Laboratories.)
Color Plate 27 Rendering of hierarchical robot figure.
(Courtesy of University of New Mexico.)
Color Plate 28 Sphere computed by recursive subdivision
of tetrahedrons; triangle colors assigned randomly.
(Courtesy of University of New Mexico.)
                               Color Plate 29 Shadows from a cube onto
ground. Computed by two passes over the data
with viewpoint shifted between viewer and light
source.
(Courtesy of University of New Mexico.)
Color Plate 30 Visualization of thermohaline flows
in the Carribean Sea using streamtubes colored by
water temperature.
(Courtesy of David Munich, High Performance Computing
Center, University of New Mexico.)
Color Plate 31 Particle system.
(Courtesy of Team One Advertising.)
(a) Mesh of particles
b) Model of Lexus with surface
(c) Wind blowing mesh off Lexus
(d) Mesh blown away from Lexus

